<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="CS-Multimodal Machine Learning, JackHCC">
    <meta name="description" content="Multimodal Machine Learning Notes">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <title>CS-Multimodal Machine Learning | JackHCC</title>
    <link rel="icon" type="image/png" href="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/medias/favicon.png">

    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/libs/awesome/css/all.css">
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/libs/materialize/materialize.min.css">
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/libs/aos/aos.css">
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/libs/animate/animate.min.css">
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/libs/lightGallery/css/lightgallery.min.css">
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/css/matery.css">
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/css/my.css">
    
    <script src="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/libs/jquery/jquery.min.js"></script>
    
<meta name="generator" content="Hexo 4.2.0"><link rel="alternate" href="/atom.xml" title="JackHCC" type="application/atom+xml">
<link rel="stylesheet" href="/css/prism-hopscotch.css" type="text/css">
<link rel="stylesheet" href="/css/prism-line-numbers.css" type="text/css"></head>


<body>
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper head-container">
            <div class="brand-logo">
                <a href="/" class="waves-effect waves-light">
                    
                    <img src="/images/loading.gif" data-original="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/medias/me.jpg" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">JackHCC</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="" class="waves-effect waves-light">

      
      <i class="fas fa-list" style="zoom: 0.6;"></i>
      
      <span>Tools</span>
      <i class="fas fa-chevron-down" aria-hidden="true" style="zoom: 0.6;"></i>
    </a>
    <ul class="sub-nav menus_item_child ">
      
      <li>
        <a href="https://creativecc.cn/" target="_blank" rel="noopener">
          
          <i class="fas fa-book" style="margin-top: -20px; zoom: 0.6;"></i>
          
          <span>Creative工具导航</span>
        </a>
      </li>
      
      <li>
        <a href="https://blog.creativecc.cn/Arxiv-NLP-Reporter/" target="_blank" rel="noopener">
          
          <i class="fas fa-film" style="margin-top: -20px; zoom: 0.6;"></i>
          
          <span>NLP每日论文</span>
        </a>
      </li>
      
      <li>
        <a href="http://chat.creativecc.cn/" target="_blank" rel="noopener">
          
          <i class="fas fa-music" style="margin-top: -20px; zoom: 0.6;"></i>
          
          <span>RocketChat聊天室</span>
        </a>
      </li>
      
      <li>
        <a href="/contact">
          
          <i class="fas fa-comments" style="margin-top: -20px; zoom: 0.6;"></i>
          
          <span>Contact留言板</span>
        </a>
      </li>
      
    </ul>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/friends" class="waves-effect waves-light">
      
      <i class="fas fa-address-book" style="zoom: 0.6;"></i>
      
      <span>友情链接</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/about" class="waves-effect waves-light">
      
      <i class="fas fa-user-circle" style="zoom: 0.6;"></i>
      
      <span>关于</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>

<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/images/loading.gif" data-original="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/medias/me.jpg" class="logo-img circle responsive-img">
        
        <div class="logo-name">JackHCC</div>
        <div class="logo-desc">
            
            Make the world betterrrr!!!
            
        </div>
    </div>

    

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="javascript:;">
			
				<i class="fa-fw fas fa-list"></i>
			
			Tools
			<span class="m-icon"><i class="fas fa-chevron-right"></i></span>
		</a>
            <ul  style="background:  ;" >
              
                <li>   
				
                  <a href="https://creativecc.cn/ " target="_blank" rel="noopener" style="margin-left:75px";>
				  
				   <i class="fa fas fa-book" style="position: absolute;left:50px" ></i>
			      
		          <span>Creative工具导航</span>
                  </a>
                </li>
              
                <li>   
				
                  <a href="https://blog.creativecc.cn/Arxiv-NLP-Reporter/ " target="_blank" rel="noopener" style="margin-left:75px";>
				  
				   <i class="fa fas fa-film" style="position: absolute;left:50px" ></i>
			      
		          <span>NLP每日论文</span>
                  </a>
                </li>
              
                <li>   
				
                  <a href="http://chat.creativecc.cn/ " target="_blank" rel="noopener" style="margin-left:75px";>
				  
				   <i class="fa fas fa-music" style="position: absolute;left:50px" ></i>
			      
		          <span>RocketChat聊天室</span>
                  </a>
                </li>
              
                <li>   
				
                  <a href="/contact " style="margin-left:75px";>
				  
				   <i class="fa fas fa-comments" style="position: absolute;left:50px" ></i>
			      
		          <span>Contact留言板</span>
                  </a>
                </li>
               
            </ul>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/friends" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-address-book"></i>
			
			友情链接
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/about" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-user-circle"></i>
			
			关于
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/JackHCC/JackHCC.github.io" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>

        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/JackHCC/JackHCC.github.io" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    
<script src="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/libs/cryptojs/crypto-js.min.js"></script>
<script>
    (function() {
        let pwd = '';
        if (pwd && pwd.length > 0) {
            if (pwd !== CryptoJS.SHA256(prompt('请输入访问本文章的密码')).toString(CryptoJS.enc.Hex)) {
                alert('密码错误，将返回主页！');
                location.href = '/';
            }
        }
    })();
</script>




<div class="bg-cover pd-header post-cover" style="background-image: url('https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/medias/featureimages/6.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">CS-Multimodal Machine Learning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/libs/tocbot/tocbot.css">
<style>
    #articleContent h1::before,
    #articleContent h2::before,
    #articleContent h3::before,
    #articleContent h4::before,
    #articleContent h5::before,
    #articleContent h6::before {
        display: block;
        content: " ";
        height: 100px;
        margin-top: -100px;
        visibility: hidden;
    }

    #articleContent :focus {
        outline: none;
    }

    .toc-fixed {
        position: fixed;
        top: 64px;
    }

    .toc-widget {
        width: 345px;
        padding-left: 20px;
    }

    .toc-widget .toc-title {
        margin: 35px 0 15px 0;
        padding-left: 17px;
        font-size: 1.5rem;
        font-weight: bold;
        line-height: 1.5rem;
    }

    .toc-widget ol {
        padding: 0;
        list-style: none;
    }

    #toc-content {
        height: calc(100vh - 250px);
        overflow: auto;
    }

    #toc-content ol {
        padding-left: 10px;
    }

    #toc-content ol li {
        padding-left: 10px;
    }

    #toc-content .toc-link:hover {
        color: #42b983;
        font-weight: 700;
        text-decoration: underline;
    }

    #toc-content .toc-link::before {
        background-color: transparent;
        max-height: 25px;
    }

    #toc-content .is-active-link {
        color: #42b983;
    }

    #toc-content .is-active-link::before {
        background-color: #42b983;
    }

    #floating-toc-btn {
        position: fixed;
        right: 30px;
        bottom: 146px;
        padding-top: 15px;
        margin-bottom: 0;
        z-index: 998;
    }

    #floating-toc-btn .btn-floating {
        width: 48px;
        height: 48px;
    }

    #floating-toc-btn .btn-floating i {
        line-height: 48px;
        font-size: 1.4rem;
    }
</style>
<div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/tags/Multimodal-Machine-Learning/">
                                <span class="chip bg-color">Multimodal Machine Learning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/categories/Deep-Learning/" class="post-category">
                                Deep Learning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2021-09-25
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2021-11-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    5.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    30 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>

        </div>
        <hr class="clearfix">
        <div class="card-content article-card-content">
            <div id="articleContent">
                <h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><h2 id="What-is-Multimodal"><a href="#What-is-Multimodal" class="headerlink" title="What is Multimodal?"></a>What is Multimodal?</h2><p><img src="/images/loading.gif" data-original="../images/ML/image-20210925094447381.png" alt=""></p>
<h3 id="Multimodal-Communicative-Behaviors"><a href="#Multimodal-Communicative-Behaviors" class="headerlink" title="Multimodal Communicative Behaviors"></a>Multimodal Communicative Behaviors</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210925094517445.png" alt=""></p>
<h3 id="Modality"><a href="#Modality" class="headerlink" title="Modality"></a><strong>Modality</strong></h3><p>事物发生或经历的方式</p>
<ul>
<li>Modality：表示某种类型的信息和/或存储信息的表示格式</li>
<li>Sensory modality：感觉的主要形式之一，如视觉或触觉；沟通渠道</li>
</ul>
<h3 id="Multiple-Communities-and-Modalities"><a href="#Multiple-Communities-and-Modalities" class="headerlink" title="Multiple Communities and Modalities"></a>Multiple Communities and Modalities</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210925094934821.png" alt=""></p>
<h2 id="A-Historical-View"><a href="#A-Historical-View" class="headerlink" title="A Historical View"></a>A Historical View</h2><h3 id="Prior-Research-on-“Multimodal”"><a href="#Prior-Research-on-“Multimodal”" class="headerlink" title="Prior Research on “Multimodal”"></a>Prior Research on “Multimodal”</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210925095026927.png" alt=""></p>
<h2 id="Core-Technical-Challenges"><a href="#Core-Technical-Challenges" class="headerlink" title="Core Technical Challenges"></a>Core Technical Challenges</h2><p><strong><a href="https://arxiv.org/pdf/1705.09406.pdf" target="_blank" rel="noopener">1705.09406.pdf (arxiv.org)</a></strong></p>
<h3 id="Core-Challenge-1-Representation"><a href="#Core-Challenge-1-Representation" class="headerlink" title="Core Challenge 1: Representation"></a>Core Challenge 1: Representation</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210925095214850.png" alt=""></p>
<h4 id="Early-Examples"><a href="#Early-Examples" class="headerlink" title="Early Examples"></a>Early Examples</h4><p><img src="/images/loading.gif" data-original="../images/ML/image-20210925095257022.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210925095315760.png" alt=""></p>
<p><strong>Kiros et al., Unifying Visual-Semantic Embeddings with Multimodal Neural Language Models, 2014</strong></p>
<h4 id="Representation"><a href="#Representation" class="headerlink" title="Representation"></a>Representation</h4><ul>
<li><strong>Definition:</strong> Learning how to represent and summarize multimodal data in away  that exploits the complementarity and redundancy.（学习如何利用互补性和冗余性在数据库中表示和汇总多模态数据）</li>
</ul>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210925095508458.png" alt=""></p>
<ul>
<li><strong>Joint representations</strong></li>
<li><strong>Coordinated representations</strong></li>
</ul>
<h3 id="Core-Challenge-2-Alignment"><a href="#Core-Challenge-2-Alignment" class="headerlink" title="Core Challenge 2: Alignment"></a>Core Challenge 2: Alignment</h3><ul>
<li><strong>Definition</strong>: Identify the direct relations between (sub)elements from two or  more different modalities.（确定两个或多个不同模式的（子）要素之间的直接关系。）</li>
</ul>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210925095707042.png" alt=""></p>
<h4 id="Explicit-Alignment"><a href="#Explicit-Alignment" class="headerlink" title="Explicit Alignment"></a>Explicit Alignment</h4><p><img src="/images/loading.gif" data-original="../images/ML/image-20210925095742114.png" alt=""></p>
<h4 id="Implicit-Alignment"><a href="#Implicit-Alignment" class="headerlink" title="Implicit Alignment"></a>Implicit Alignment</h4><p><a href="https://arxiv.org/pdf/1406.5679.pdf" target="_blank" rel="noopener"><strong>Karpathy et al., Deep Fragment Embeddings for Bidirectional Image Sentence Mapping</strong></a> </p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210925095804162.png" alt=""></p>
<h3 id="Core-Challenge-3-–-Translation"><a href="#Core-Challenge-3-–-Translation" class="headerlink" title="Core Challenge 3 – Translation"></a>Core Challenge 3 – Translation</h3><p><strong>Marsella et al., Virtual character performance from speech, SIGGRAPH/Eurographics Symposium on Computer Animation, 2013</strong></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210925095848238.png" alt=""></p>
<ul>
<li><strong>Definition</strong>: Process of changing data from one modality to another, where the  translation relationship can often be open-ended or subjective.（将数据从一种模态转换到另一种模态的过程，其中的翻译关系通常是开放的或主观的）</li>
</ul>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210925095953268.png" alt=""></p>
<p><strong>Ahuja, C., &amp; Morency, L. P. (2019). Language2Pose: Natural Language Grounded Pose  Forecasting. Proceedings of 3DV Conference</strong></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210926105621387.png" alt=""></p>
<h3 id="Core-Challenge-4-Fusion"><a href="#Core-Challenge-4-Fusion" class="headerlink" title="Core Challenge 4: Fusion"></a>Core Challenge 4: Fusion</h3><img src="/images/loading.gif" data-original="../images/ML/image-20210925100044906.png" style="zoom: 67%;">

<ul>
<li><strong>Definition</strong>: To join information from two or more modalities to perform a  prediction task.（将来自两个或多个模式的信息连接起来以执行预测任务）</li>
</ul>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210925100209074.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210925100243804.png" alt=""></p>
<h3 id="Core-Challenge-5-Co-Learning"><a href="#Core-Challenge-5-Co-Learning" class="headerlink" title="Core Challenge 5: Co-Learning"></a>Core Challenge 5: Co-Learning</h3><ul>
<li>Definition: Transfer knowledge between modalities, including their  representations and predictive model。（在模式之间转移知识，包括其表示和预测模型。）</li>
</ul>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210925100404060.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210925100425310.png" alt=""></p>
<p><a href="https://arxiv.org/abs/1812.07809" target="_blank" rel="noopener"><strong>Pham et al., Found in Translation: Learning Robust Joint Representations by Cyclic  Translations Between Modalities</strong></a></p>
<h3 id="Taxonomy-of-Multimodal-Research"><a href="#Taxonomy-of-Multimodal-Research" class="headerlink" title="Taxonomy of Multimodal Research"></a>Taxonomy of Multimodal Research</h3><p><strong>Tadas Baltrusaitis, Chaitanya Ahuja, and Louis-Philippe Morency, Multimodal Machine Learning: A Survey and Taxonomy</strong></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210925100520146.png" alt=""></p>
<h2 id="Real-world-tasks-tackled-by-MMML"><a href="#Real-world-tasks-tackled-by-MMML" class="headerlink" title="Real world tasks tackled by MMML"></a>Real world tasks tackled by MMML</h2><p><img src="/images/loading.gif" data-original="../images/ML/image-20210925100606333.png" alt=""></p>
<h1 id="Multimodal-Research-Tasks"><a href="#Multimodal-Research-Tasks" class="headerlink" title="Multimodal Research Tasks"></a>Multimodal Research Tasks</h1><h2 id="Multimodal-Research-Tasks-1"><a href="#Multimodal-Research-Tasks-1" class="headerlink" title="Multimodal Research Tasks"></a>Multimodal Research Tasks</h2><p><img src="/images/loading.gif" data-original="../images/ML/image-20210925101220301.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210925101243858.png" alt=""></p>
<h2 id="Affective-Computing（情感计算）"><a href="#Affective-Computing（情感计算）" class="headerlink" title="Affective Computing（情感计算）"></a>Affective Computing（情感计算）</h2><h3 id="Common-Topics-in-Affective-Computing"><a href="#Common-Topics-in-Affective-Computing" class="headerlink" title="Common Topics in Affective Computing"></a>Common Topics in Affective Computing</h3><ul>
<li><p><strong>Affectivestates</strong>–emotions, moods, and feelings</p>
</li>
<li><p><strong>Cognitivestates</strong>–thinking and information processing</p>
</li>
<li><p><strong>Personality</strong>–patterns of acting, feeling, and thinking</p>
</li>
<li><p><strong>Pathology</strong>–health, functioning, and disorders</p>
</li>
<li><p><strong>Social processes</strong> –groups, cultures, and perception</p>
</li>
<li><p>情感状态–情绪、情绪和感觉</p>
</li>
<li><p>认知状态–思维和信息处理</p>
</li>
<li><p>个性–行为、感觉和思维模式</p>
</li>
<li><p>病理学–健康、功能和紊乱</p>
</li>
<li><p>社会过程–群体、文化和感知</p>
</li>
</ul>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210925101606133.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210925101622627.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210925101645489.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210925101658747.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210925101714374.png" alt=""></p>
<p><strong>AVEC 2011 – The First International Audio/Visual Emotion Challenge, B. Schuller et al., 2011</strong></p>
<p><strong>AVEC 2013 – The Continuous Audio/Visual Emotion and Depression Recognition Challenge, Valstar et  al. 2013</strong></p>
<p><strong>Introducing the RECOLA Multimodal Corpus of Remote Collaborative and Affective Interactions, F.  Ringeval et al., 2013</strong></p>
<h3 id="Multimodal-Sentiment-Analysis（多模态情绪分析）"><a href="#Multimodal-Sentiment-Analysis（多模态情绪分析）" class="headerlink" title="Multimodal Sentiment Analysis（多模态情绪分析）"></a>Multimodal Sentiment Analysis（多模态情绪分析）</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210925102109826.png" alt=""></p>
<h3 id="Multi-Party-Emotion-Recognition（多方情绪识别）"><a href="#Multi-Party-Emotion-Recognition（多方情绪识别）" class="headerlink" title="Multi-Party Emotion Recognition（多方情绪识别）"></a>Multi-Party Emotion Recognition（多方情绪识别）</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210925102210892.png" alt=""></p>
<h3 id="What-are-the-Core-Challenges-Most-Involved-in-Affect-Recognition"><a href="#What-are-the-Core-Challenges-Most-Involved-in-Affect-Recognition" class="headerlink" title="What are the Core Challenges Most Involved in Affect Recognition?"></a>What are the Core Challenges Most Involved in Affect Recognition?</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210925102247978.png" alt=""></p>
<h4 id="Project-Example-Select-Additive-Learning"><a href="#Project-Example-Select-Additive-Learning" class="headerlink" title="Project Example: Select-Additive Learning"></a>Project Example: Select-Additive Learning</h4><p><a href="https://arxiv.org/abs/1609.05244" target="_blank" rel="noopener"><strong>Haohan Wang, Aaksha Meghawat, Louis-Philippe Morency and Eric P . Xing, Select-additive Learning: Improving Generalization In Multimodal Sentiment Analysis, ICME 2017</strong></a></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210925102501363.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210925102515835.png" alt=""></p>
<h4 id="Project-Example-Word-Level-Gated-Fusion"><a href="#Project-Example-Word-Level-Gated-Fusion" class="headerlink" title="Project Example: Word-Level Gated Fusion"></a>Project Example: Word-Level Gated Fusion</h4><p>Minghai Chen, Sen Wang, Paul Pu Liang, Ta d a sBaltrušaitis, Amir Zadeh, Louis-Philippe Morency, Multimodal Sentiment<br>Analysis with Word-Level Fusion and Reinforcement Learning, ICMI 2017, <a href="https://arxiv.org/abs/1802.00924" target="_blank" rel="noopener">https://arxiv.org/abs/1802.00924</a></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210925102611530.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210925102626688.png" alt=""></p>
<h2 id="Media-Description（媒体描述）"><a href="#Media-Description（媒体描述）" class="headerlink" title="Media Description（媒体描述）"></a>Media Description（媒体描述）</h2><p>给定媒体（图像、视频、视听剪辑）提供自由形式的文本描述</p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210925102823601.png" alt=""></p>
<h3 id="Large-Scale-Image-Captioning-Dataset"><a href="#Large-Scale-Image-Captioning-Dataset" class="headerlink" title="Large-Scale Image Captioning Dataset"></a>Large-Scale Image Captioning Dataset</h3><ul>
<li>Microsoft Common Objects in COntext(<a href="http://mscoco.org/dataset/" target="_blank" rel="noopener">MS COCO</a>)</li>
<li>120000 images</li>
<li>Each image is accompanied with five free form sentences describing it (at least 8 words)</li>
<li>Sentences collected using crowdsourcing (Mechanical Turk)</li>
<li>Also contains object detections, boundaries and keypoints</li>
</ul>
<h3 id="Evaluating-Image-Caption-Generations（评估图像字幕生成）"><a href="#Evaluating-Image-Caption-Generations（评估图像字幕生成）" class="headerlink" title="Evaluating Image Caption Generations（评估图像字幕生成）"></a>Evaluating Image Caption Generations（评估图像字幕生成）</h3><ul>
<li>Has an evaluation server</li>
<li>Training and validation -80K images (400K captions)</li>
<li>Testing –40K images (380K captions), a subset contains more captions for better evaluation, these are kept privately (to avoid over-fitting and cheating)</li>
<li>Evaluation is difficult as there is no one “correct” answer for describing an image in a sentence</li>
<li>Given a candidate sentence it is evaluated against a set of “ground truth” sentences</li>
</ul>
<h3 id="Video-captioning（视频字幕）"><a href="#Video-captioning（视频字幕）" class="headerlink" title="Video captioning（视频字幕）"></a>Video captioning（视频字幕）</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210925103223696.png" alt=""></p>
<h3 id="Video-Description-and-Alignment（视频描述和对齐）"><a href="#Video-Description-and-Alignment（视频描述和对齐）" class="headerlink" title="Video Description and Alignment（视频描述和对齐）"></a>Video Description and Alignment（视频描述和对齐）</h3><p><strong>Charade Dataset: <a href="http://allenai.org/plato/charades/" target="_blank" rel="noopener">http://allenai.org/plato/charades/</a></strong></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210925103304991.png" alt=""></p>
<h3 id="How-to-Address-the-Challenge-of-Evaluation？"><a href="#How-to-Address-the-Challenge-of-Evaluation？" class="headerlink" title="How to Address the Challenge of Evaluation？"></a>How to Address the Challenge of Evaluation？</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210925103426921.png" alt=""></p>
<h3 id="Large-Scale-Description-and-Grounding-Dataset"><a href="#Large-Scale-Description-and-Grounding-Dataset" class="headerlink" title="Large-Scale Description and Grounding Dataset"></a>Large-Scale Description and Grounding Dataset</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210925103522505.png" alt=""></p>
<p><a href="https://visualgenome.org/" target="_blank" rel="noopener">VisualGenome</a></p>
<h2 id="Multimodal-QA（多模态问答）"><a href="#Multimodal-QA（多模态问答）" class="headerlink" title="Multimodal QA（多模态问答）"></a>Multimodal QA（多模态问答）</h2><p><img src="/images/loading.gif" data-original="../images/ML/image-20210925103650877.png" alt=""></p>
<h3 id="Multimodal-QA-dataset-1-–VQA-C1"><a href="#Multimodal-QA-dataset-1-–VQA-C1" class="headerlink" title="Multimodal QA dataset 1 –VQA (C1)"></a>Multimodal QA dataset 1 –VQA (C1)</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210925103720950.png" alt=""></p>
<h3 id="VQA-2-0"><a href="#VQA-2-0" class="headerlink" title="VQA 2.0"></a>VQA 2.0</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210925103823050.png" alt=""></p>
<h3 id="Multimodal-QA-–other-VQA-datasets"><a href="#Multimodal-QA-–other-VQA-datasets" class="headerlink" title="Multimodal QA –other VQA datasets"></a>Multimodal QA –other VQA datasets</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210925103851193.png" alt=""></p>
<h3 id="Multimodal-QA-–other-VQA-datasets-C7"><a href="#Multimodal-QA-–other-VQA-datasets-C7" class="headerlink" title="Multimodal QA –other VQA datasets (C7)"></a>Multimodal QA –other VQA datasets (C7)</h3><h4 id="TVQA"><a href="#TVQA" class="headerlink" title="TVQA"></a><a href="http://tvqa.cs.unc.edu/" target="_blank" rel="noopener">TVQA</a></h4><p><img src="/images/loading.gif" data-original="../images/ML/image-20210925103934234.png" alt=""></p>
<h3 id="Multimodal-QA-–Visual-Reasoning-C8"><a href="#Multimodal-QA-–Visual-Reasoning-C8" class="headerlink" title="Multimodal QA –Visual Reasoning (C8)"></a>Multimodal QA –Visual Reasoning (C8)</h3><h4 id="VCR：Visual-Commonsense-Reasoning"><a href="#VCR：Visual-Commonsense-Reasoning" class="headerlink" title="VCR：Visual Commonsense Reasoning"></a><a href="https://visualcommonsense.com/" target="_blank" rel="noopener">VCR：Visual Commonsense Reasoning</a></h4><p><img src="/images/loading.gif" data-original="../images/ML/image-20210925104052157.png" alt=""></p>
<h3 id="Social-IQ-A10"><a href="#Social-IQ-A10" class="headerlink" title="Social-IQ (A10)"></a>Social-IQ (A10)</h3><h4 id="Scocail-IQ"><a href="#Scocail-IQ" class="headerlink" title="Scocail-IQ"></a><a href="https://www.thesocialiq.com/" target="_blank" rel="noopener">Scocail-IQ</a></h4><p><img src="/images/loading.gif" data-original="../images/ML/image-20210925104206920.png" alt=""></p>
<h3 id="Project-Example-Adversarial-Attacks-on-VQA-models"><a href="#Project-Example-Adversarial-Attacks-on-VQA-models" class="headerlink" title="Project Example: Adversarial Attacks on VQA models"></a>Project Example: Adversarial Attacks on VQA models</h3><p><a href="https://nips2018vigil.github.io/static/papers/accepted/33.pdf" target="_blank" rel="noopener"><strong>Vasu Sharma, Ankita Kalra, Vaibhav, SimralChaudhary, LabheshPatel, Louis-Philippe Morency, Attend and Attack: Attention Guided Adversarial Attacks on Visual Question Answering Models. NeurIPS ViGILworkshop 2018.</strong></a> </p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210925104306554.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210925104338389.png" alt=""></p>
<h2 id="Multimodal-Navigation（多模式导航）"><a href="#Multimodal-Navigation（多模式导航）" class="headerlink" title="Multimodal Navigation（多模式导航）"></a>Multimodal Navigation（多模式导航）</h2><ul>
<li>Embedded Assistive Agents</li>
<li>Language, Vision and Actions</li>
<li>Many Technical Challenges</li>
</ul>
<h3 id="Navigating-in-a-Virtual-House（在虚拟房屋中导航）"><a href="#Navigating-in-a-Virtual-House（在虚拟房屋中导航）" class="headerlink" title="Navigating in a Virtual House（在虚拟房屋中导航）"></a><a href="https://bringmeaspoon.org/" target="_blank" rel="noopener">Navigating in a Virtual House（在虚拟房屋中导航）</a></h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210925104527509.png" alt=""></p>
<h3 id="Multiple-Step-Instructions"><a href="#Multiple-Step-Instructions" class="headerlink" title="Multiple Step Instructions"></a><a href="https://github.com/volkancirik/refer360" target="_blank" rel="noopener">Multiple Step Instructions</a></h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210925104550830.png" alt=""></p>
<h3 id="Language-meets-Games"><a href="#Language-meets-Games" class="headerlink" title="Language meets Games"></a>Language meets Games</h3><p><a href="https://arxiv.org/abs/2006.13760" target="_blank" rel="noopener"><strong>Heinrich Kuttler and Nantas Nardelli and Alexander H. Miller and Roberta Raileanu and Marco Selvatici and Edward Grefenstette and Tim Rocktaschel, The Nethack Learning Environment.</strong></a> </p>
<p><a href="https://arxiv.org/abs/2002.02878" target="_blank" rel="noopener"><strong>ShrimaiPrabhumoye, Margaret Li, Jack Urbanek, Emily Dinan, DouweKiela, Jason Weston, Arthur Szlam. I love your chain mail! Making knights smile in a fantasy game world: Open-domain goal-oriented dialogue agents.</strong></a></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210925104649946.png" alt=""></p>
<h3 id="Project-Example-Instruction-Following"><a href="#Project-Example-Instruction-Following" class="headerlink" title="Project Example: Instruction Following"></a>Project Example: Instruction Following</h3><p><a href="https://arxiv.org/abs/1706.07230" target="_blank" rel="noopener"><strong>Devendra Singh Chaplot, KanthashreeMysore Sathyendra, Rama Kumar Pasumarthi, Dheeraj Rajagopal, Ruslan Salakhutdinov, Gated-Attention Architectures for T ask-Oriented Language Grounding. AAAI 2018</strong></a> </p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210925104739048.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210925104807091.png" alt=""></p>
<h3 id="Project-Example-Multiagent-Trajectory-Forecasting"><a href="#Project-Example-Multiagent-Trajectory-Forecasting" class="headerlink" title="Project Example: Multiagent Trajectory Forecasting"></a>Project Example: Multiagent Trajectory Forecasting</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210925104850069.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210925104905628.png" alt=""></p>
<h2 id="Project-Examples-Advice-and-Support"><a href="#Project-Examples-Advice-and-Support" class="headerlink" title="Project Examples, Advice and Support"></a>Project Examples, Advice and Support</h2><h3 id="Latest-List-of-Multimodal-Datasets"><a href="#Latest-List-of-Multimodal-Datasets" class="headerlink" title="Latest List of Multimodal Datasets"></a>Latest List of Multimodal Datasets</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210925105008920.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210925105019955.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210925105031961.png" alt=""></p>
<h3 id="Some-Advice-About-Multimodal-Research"><a href="#Some-Advice-About-Multimodal-Research" class="headerlink" title="Some Advice About Multimodal Research"></a>Some Advice About Multimodal Research</h3><ul>
<li>Think more about the research problems, and less about the datasets themselves<ul>
<li>Aim for generalizable models across several datasets</li>
<li>Aim formodelsinspiredbyexistingresearche.g.psychology</li>
</ul>
</li>
<li>Some areas to consider beyond performance:<ul>
<li>Robustness tomissing/noisy modalities, adversarial attacks</li>
<li>Studying social biases and creating fairer models</li>
<li>Interpretable models</li>
<li>Faster models for training/storage/inference</li>
</ul>
</li>
<li>Theoretical projects are welcome too –make sure there are also experiments to validate theory</li>
</ul>
<h3 id="Some-Advice-About-Multimodal-Datasets"><a href="#Some-Advice-About-Multimodal-Datasets" class="headerlink" title="Some Advice About Multimodal Datasets"></a>Some Advice About Multimodal Datasets</h3><ul>
<li>If you are used to deal with text or speech<ul>
<li>Space will become an issue working with image/video data</li>
<li>Some datasets are in 100s of GB (compressed)</li>
</ul>
</li>
<li>Memory for processingit will become an issue as well<ul>
<li>Won’t be able to store it all in memory</li>
</ul>
</li>
<li>Time to extract features and train algorithms will also become an issue</li>
<li>Plan accordingly!<ul>
<li>Sometimes tricky to experiment on a laptop (might need to do it on a subset of data)</li>
</ul>
</li>
</ul>
<h3 id="Available-Tools"><a href="#Available-Tools" class="headerlink" title="Available Tools"></a>Available Tools</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210925105524373.png" alt=""></p>
<h2 id="List-of-Multimodal-datasets"><a href="#List-of-Multimodal-datasets" class="headerlink" title="List of Multimodal datasets"></a>List of Multimodal datasets</h2><h3 id="Affective-Computing（情感计算）-1"><a href="#Affective-Computing（情感计算）-1" class="headerlink" title="Affective Computing（情感计算）"></a>Affective Computing（情感计算）</h3><p><a href="https://cs.anu.edu.au/few/AFEW.html" target="_blank" rel="noopener">Acted Facial Expressions in the Wild (part of EmotiWChallenge))</a></p>
<p>AVEC challenge datasets</p>
<p>The Interactive Emotional Dyadic Motion Capture (<a href="https://sail.usc.edu/iemocap/" target="_blank" rel="noopener">IEMOCAP</a>)</p>
<p>Persuasive Opinion Multimedia (<a href="https://dl.acm.org/doi/10.1145/2663204.2663260" target="_blank" rel="noopener">POM</a>)</p>
<p>Multimodal Corpus of Sentiment Intensity and Subjectivity Analysis in Online Opinion Videos (<a href="https://docs.google.com/forms/d/e/1FAIpQLSd8LfYr1AZuxeNBNlRUwl8coSoB52qj53Wd9WTwoWEplC4djQ/viewform?c=0&amp;w=1" target="_blank" rel="noopener">MOSI</a>)</p>
<p><a href="https://github.com/A2Zadeh/CMU-MultimodalSDK" target="_blank" rel="noopener">CMU-MOSEI</a>：Multimodal sentiment and emotion recognition</p>
<p><a href="https://github.com/anthonyhu/tumblr-emotions" target="_blank" rel="noopener">Tumblr Dataset: Sentiment and Emotion Analysis </a></p>
<p><a href="http://amhuse.phuselab.di.unimi.it/" target="_blank" rel="noopener">AMHUSE Dataset: Multimodal Humor Sensing</a></p>
<p><a href="https://www.dropbox.com/s/8pl0t9sf1sque03/VideoGameDataset.zip?dl=0" target="_blank" rel="noopener">Video Game Dataset: Multimodal Game Rating</a></p>
<p><a href="https://www.thesocialiq.com/" target="_blank" rel="noopener">Social-IQ</a></p>
<p><a href="https://affective-meld.github.io/" target="_blank" rel="noopener">MELD</a> </p>
<p><a href="https://github.com/soujanyaporia/MUStARD" target="_blank" rel="noopener">MUStARD</a></p>
<p>DEAP </p>
<p><a href="https://mahnob-db.eu/laughter/" target="_blank" rel="noopener">MAHNOB</a></p>
<p>Continuous <a href="https://liris-accede.ec-lyon.fr/" target="_blank" rel="noopener">LIRIS-ACCEDE</a></p>
<p><a href="http://mhug.disi.unitn.it/wp-content/DECAF/DECAF.html#/" target="_blank" rel="noopener">DECAF</a></p>
<p><a href="http://mhug.disi.unitn.it/wp-content/ASCERTAIN/ascertain.html#/" target="_blank" rel="noopener">ASCERTAIN</a></p>
<p><a href="http://www.eecs.qmul.ac.uk/mmv/datasets/amigos/index.html" target="_blank" rel="noopener">amigos</a></p>
<p><a href="https://github.com/rkosti/emotic" target="_blank" rel="noopener">emotic</a></p>
<h3 id="Media-description"><a href="#Media-description" class="headerlink" title="Media description"></a>Media description</h3><p><a href="http://mscoco.org/dataset/" target="_blank" rel="noopener">MS COCO</a></p>
<p><a href="https://www.mpi-inf.mpg.de/departments/computer-vision-and-machine-learning/research/vision-and-language/mpii-movie-description-dataset/" target="_blank" rel="noopener">MPII Movie Description dataset</a></p>
<p><a href="http://www.mila.umontreal.ca/Home/public-datasets/montreal-video-annotation-dataset/" target="_blank" rel="noopener">Montréal Video Annotation dataset</a></p>
<p><a href="https://sites.google.com/site/describingmovies/home" target="_blank" rel="noopener">LSMDC</a></p>
<p><a href="https://prior.allenai.org/projects/charades" target="_blank" rel="noopener">Charades Dataset</a></p>
<p><a href="https://github.com/lichengunc/refer" target="_blank" rel="noopener">Referring Expression datasets</a></p>
<p>Flickr30k Entities</p>
<p><a href="https://github.com/EdinburghNLP/csi-corpus" target="_blank" rel="noopener">CSI Corpus</a></p>
<p><a href="http://mvso.cs.columbia.edu/download.html" target="_blank" rel="noopener">MVSO</a></p>
<p><a href="https://github.com/HMEIatJHU/NeuralWalker" target="_blank" rel="noopener">NeuralWalker</a></p>
<p><a href="https://cs.stanford.edu/people/ranjaykrishna/vrd/" target="_blank" rel="noopener">Visual Relationdataset</a></p>
<p><a href="http://visualgenome.org/" target="_blank" rel="noopener">Visual genome</a></p>
<p>Pinterest</p>
<p><a href="https://nocaps.org/" target="_blank" rel="noopener">nocaps</a></p>
<p><a href="https://github.com/DmZhukov/CrossTask" target="_blank" rel="noopener">CrossTask</a></p>
<p><a href="https://github.com/volkancirik/refer360" target="_blank" rel="noopener">refer360</a></p>
<h3 id="Multimodal-QA"><a href="#Multimodal-QA" class="headerlink" title="Multimodal QA"></a>Multimodal QA</h3><p><a href="https://visualqa.org/" target="_blank" rel="noopener">VQA</a> </p>
<p>VQA v2.0</p>
<p><a href="https://arxiv.org/pdf/1410.0210.pdf" target="_blank" rel="noopener">DAQUAR</a></p>
<p><a href="https://arxiv.org/pdf/1505.02074.pdf" target="_blank" rel="noopener">COCO-QA</a></p>
<p><a href="https://arxiv.org/pdf/1506.00278.pdf" target="_blank" rel="noopener">Visual Madlibs</a></p>
<p><a href="https://allenai.org/data/tqa" target="_blank" rel="noopener">Textbook Question Answering</a></p>
<p><a href="https://arxiv.org/pdf/1511.03416.pdf" target="_blank" rel="noopener">Visual7W</a></p>
<p><a href="https://tvqa.cs.unc.edu/" target="_blank" rel="noopener">TVQA</a></p>
<p><a href="https://visualcommonsense.com/" target="_blank" rel="noopener">VCR</a></p>
<p><a href="https://lil.nlp.cornell.edu/nlvr/" target="_blank" rel="noopener">Cornell NLVR</a></p>
<p><a href="https://cs.stanford.edu/people/jcjohns/clevr/" target="_blank" rel="noopener">CLEVR</a></p>
<p><a href="https://embodiedqa.org/data" target="_blank" rel="noopener">EQA v1.0</a></p>
<p><a href="https://textvqa.org/" target="_blank" rel="noopener">TextVQA</a></p>
<p><a href="https://arxiv.org/pdf/1902.09506.pdf" target="_blank" rel="noopener">GQA</a></p>
<p><a href="https://compguesswhat.github.io/" target="_blank" rel="noopener">CompGuessWhat</a></p>
<h3 id="Multimodal-Navigation"><a href="#Multimodal-Navigation" class="headerlink" title="Multimodal Navigation"></a>Multimodal Navigation</h3><p><a href="https://bringmeaspoon.org/" target="_blank" rel="noopener">Room-2-Room</a></p>
<p><a href="https://github.com/YuankaiQi/REVERIE" target="_blank" rel="noopener">RERERE</a></p>
<p><a href="https://github.com/debadeepta/vnla" target="_blank" rel="noopener">VNLA</a> </p>
<p><a href="https://www.nuscenes.org/" target="_blank" rel="noopener">nuScenes</a></p>
<p><a href="https://waymo.com/open/" target="_blank" rel="noopener">Waymo Open Dataset</a></p>
<p><a href="http://carla.org/" target="_blank" rel="noopener">CARLA</a></p>
<p><a href="https://www.argoverse.org/" target="_blank" rel="noopener">Argoverse</a></p>
<p><a href="https://askforalfred.com/" target="_blank" rel="noopener">ALFRED</a></p>
<h3 id="Multimodal-Dialog"><a href="#Multimodal-Dialog" class="headerlink" title="Multimodal Dialog"></a>Multimodal Dialog</h3><p><a href="https://visualdialog.org/data" target="_blank" rel="noopener">Visual Dialog</a></p>
<p><a href="https://github.com/facebookresearch/talkthewalk" target="_blank" rel="noopener">Talk the Walk</a></p>
<p><a href="https://github.com/mmurray/cvdn" target="_blank" rel="noopener">Cooperative Vision-and-Dialog Navigation</a></p>
<p><a href="https://github.com/satwikkottur/clevr-dialog" target="_blank" rel="noopener">CLEVR-Dialog</a></p>
<p><a href="https://www.spacewu.com/posts/fashion-retrieval/" target="_blank" rel="noopener">Fashion Retrieval</a></p>
<h3 id="Event-detection"><a href="#Event-detection" class="headerlink" title="Event detection"></a>Event detection</h3><p><a href="https://github.com/malmaud/whats_cookin" target="_blank" rel="noopener">What’s Cooking</a></p>
<p><a href="https://www.coli.uni-saarland.de/projects/smile/page.php?id=tacos" target="_blank" rel="noopener">TACoS</a></p>
<p><a href="https://www.mpi-inf.mpg.de/departments/computer-vision-and-machine-learning/research/vision-and-language/tacos-multi-level-corpus/" target="_blank" rel="noopener">TACoS Multi-Level</a></p>
<p><a href="https://www.mpi-inf.mpg.de/departments/computer-vision-and-machine-learning/research/vision-and-language/tacos-multi-level-corpus/" target="_blank" rel="noopener">YouCook</a></p>
<p><a href="https://www.nist.gov/itl/iad/mig/med-2014-evaluation" target="_blank" rel="noopener">Multimedia Event Detection</a></p>
<p><a href="https://github.com/yalesong/tvsum" target="_blank" rel="noopener">Title-based Video Summarization dataset</a></p>
<p><a href="http://multimediaeval.org/mediaeval2015/" target="_blank" rel="noopener">MediaEval</a></p>
<p>CrisisMMD</p>
<h3 id="Multimodal-Retrieval"><a href="#Multimodal-Retrieval" class="headerlink" title="Multimodal Retrieval"></a>Multimodal Retrieval</h3><p><a href="https://github.com/IvonaTau/ikea" target="_blank" rel="noopener">Interior Design Dataset</a></p>
<p><a href="http://press.liacs.nl/mirflickr/" target="_blank" rel="noopener">MIRFLICKR-1M</a></p>
<p><a href="https://lms.comp.nus.edu.sg/wp-content/uploads/2019/research/nuswide/NUS-WIDE.html" target="_blank" rel="noopener">NUS-WIDE dataset</a></p>
<p><a href="http://projects.dfki.uni-kl.de/yfcc100m/" target="_blank" rel="noopener">Yahoo Flickr Creative Commons 100M</a></p>
<h3 id="Other-Multimodal-Datasets"><a href="#Other-Multimodal-Datasets" class="headerlink" title="Other Multimodal Datasets"></a>Other Multimodal Datasets</h3><p><a href="https://research.google.com/youtube8m/" target="_blank" rel="noopener">YouTube 8M</a></p>
<p><a href="https://research.google.com/youtube-bb/" target="_blank" rel="noopener">YouTube Bounding Boxes</a></p>
<p><a href="https://research.googleblog.com/2016/09/introducing-open-images-dataset.html" target="_blank" rel="noopener">YouTube Open Images</a></p>
<p><a href="http://visionandlanguage.net/VIST/" target="_blank" rel="noopener">VIST</a></p>
<p><a href="http://pic2recipe.csail.mit.edu/" target="_blank" rel="noopener">Recipe1M+</a></p>
<p><a href="https://eric-xw.github.io/vatex-website/" target="_blank" rel="noopener">VATEX (G10)</a></p>
<h1 id="Basic-Concepts-–-Neural-Networks"><a href="#Basic-Concepts-–-Neural-Networks" class="headerlink" title="Basic Concepts – Neural Networks"></a>Basic Concepts – Neural Networks</h1><h2 id="Unimodal-Basic-Representations"><a href="#Unimodal-Basic-Representations" class="headerlink" title="Unimodal Basic  Representations"></a>Unimodal Basic  Representations</h2><h3 id="Unimodal-Representation-–-Visual-Modality"><a href="#Unimodal-Representation-–-Visual-Modality" class="headerlink" title="Unimodal Representation – Visual Modality"></a>Unimodal Representation – Visual Modality</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210925130201370.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210925130225265.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210925130303133.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210925130319030.png" alt=""></p>
<h3 id="Unimodal-Representation-–-Language-Modality"><a href="#Unimodal-Representation-–-Language-Modality" class="headerlink" title="Unimodal Representation – Language Modality"></a>Unimodal Representation – Language Modality</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210925130346414.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210925130402432.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210925130415203.png" alt=""></p>
<h3 id="Unimodal-Representation-–-Acoustic-Modality"><a href="#Unimodal-Representation-–-Acoustic-Modality" class="headerlink" title="Unimodal Representation – Acoustic Modality"></a>Unimodal Representation – Acoustic Modality</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210925130436870.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210925130452936.png" alt=""></p>
<h2 id="Other-Unimodal-Representations"><a href="#Other-Unimodal-Representations" class="headerlink" title="Other Unimodal  Representations"></a>Other Unimodal  Representations</h2><h3 id="Unimodal-Representation-–-Sensors"><a href="#Unimodal-Representation-–-Sensors" class="headerlink" title="Unimodal Representation – Sensors"></a>Unimodal Representation – Sensors</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210925130542351.png" alt=""></p>
<p><strong>Lee et al., Making Sense of Vision and Touch: Self-Supervised Learning of Multimodal Representations for Contact-Rich Tasks. ICRA 2019</strong></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210925130554503.png" alt=""></p>
<h3 id="Unimodal-Representation-–-Tables"><a href="#Unimodal-Representation-–-Tables" class="headerlink" title="Unimodal Representation – Tables"></a>Unimodal Representation – Tables</h3><p><strong>Bao et al., Table-to-Text: Describing Table Region with Natural Language. AAAI 2018</strong></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210925130612387.png" alt=""></p>
<h3 id="Unimodal-Representation-–-Graphs"><a href="#Unimodal-Representation-–-Graphs" class="headerlink" title="Unimodal Representation – Graphs"></a>Unimodal Representation – Graphs</h3><p><strong>Hamilton and Tang, Tutorial on Graph Representation Learning. AAAI 2019</strong></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210925130628967.png" alt=""></p>
<h3 id="Unimodal-Representation-–-Sets"><a href="#Unimodal-Representation-–-Sets" class="headerlink" title="Unimodal Representation – Sets"></a>Unimodal Representation – Sets</h3><p>*<em>Zaheer et al., DeepSets. NeurIPS 2017, Li et al., Point Cloud GAN. arxiv 2018 *</em></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210925130650252.png" alt=""></p>
<h2 id="Machine-Learning-–-Basic-Concepts"><a href="#Machine-Learning-–-Basic-Concepts" class="headerlink" title="Machine Learning – Basic Concepts"></a>Machine Learning – Basic Concepts</h2><h3 id="Training-Testing-and-Dataset"><a href="#Training-Testing-and-Dataset" class="headerlink" title="Training, Testing and Dataset"></a>Training, Testing and Dataset</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210925130731492.png" alt=""></p>
<h3 id="Nearest-Neighbor-Classifier"><a href="#Nearest-Neighbor-Classifier" class="headerlink" title="Nearest Neighbor Classifier"></a>Nearest Neighbor Classifier</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210925130804767.png" alt=""></p>
<h3 id="Simple-Classifier-Nearest-Neighbor"><a href="#Simple-Classifier-Nearest-Neighbor" class="headerlink" title="Simple Classifier: Nearest Neighbor"></a>Simple Classifier: Nearest Neighbor</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210925130832036.png" alt=""></p>
<h3 id="Definition-of-K-Nearest-Neighbor"><a href="#Definition-of-K-Nearest-Neighbor" class="headerlink" title="Definition of K-Nearest Neighbor"></a>Definition of K-Nearest Neighbor</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210925130855756.png" alt=""></p>
<h3 id="Data-Driven-Approach"><a href="#Data-Driven-Approach" class="headerlink" title="Data-Driven Approach"></a>Data-Driven Approach</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210925130918277.png" alt=""></p>
<h3 id="Evaluation-methods-for-validation-and-testing"><a href="#Evaluation-methods-for-validation-and-testing" class="headerlink" title="Evaluation methods (for validation and testing)"></a>Evaluation methods (for validation and testing)</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210925130943133.png" alt=""></p>
<h2 id="Linear-Classification-Scores-and-Loss"><a href="#Linear-Classification-Scores-and-Loss" class="headerlink" title="Linear Classification: Scores and Loss"></a>Linear Classification: Scores and Loss</h2><h2 id="Learning-model-parameters"><a href="#Learning-model-parameters" class="headerlink" title="Learning model  parameters"></a>Learning model  parameters</h2><h2 id="Neural-Networks-gradient"><a href="#Neural-Networks-gradient" class="headerlink" title="Neural Networks  gradient"></a>Neural Networks  gradient</h2><h2 id="Gradient-descent"><a href="#Gradient-descent" class="headerlink" title="Gradient descent"></a>Gradient descent</h2><h2 id="Optimization-–-Practical-Guidelines"><a href="#Optimization-–-Practical-Guidelines" class="headerlink" title="Optimization – Practical Guidelines"></a>Optimization – Practical Guidelines</h2><h1 id="CNNs-and-Visual-Representations"><a href="#CNNs-and-Visual-Representations" class="headerlink" title="CNNs and Visual  Representations"></a>CNNs and Visual  Representations</h1><h2 id="Image-Representations"><a href="#Image-Representations" class="headerlink" title="Image  Representations"></a>Image  Representations</h2><h3 id="Object-Based-Visual-Representation"><a href="#Object-Based-Visual-Representation" class="headerlink" title="Object-Based Visual Representation"></a>Object-Based Visual Representation</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210925132045108.png" alt=""></p>
<h3 id="Object-Descriptors"><a href="#Object-Descriptors" class="headerlink" title="Object Descriptors"></a>Object Descriptors</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210925132207164.png" alt=""></p>
<h3 id="Convolution-Kernels"><a href="#Convolution-Kernels" class="headerlink" title="Convolution Kernels"></a>Convolution Kernels</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210925132229123.png" alt=""></p>
<h3 id="Object-Descriptors-1"><a href="#Object-Descriptors-1" class="headerlink" title="Object Descriptors"></a>Object Descriptors</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210925132307884.png" alt=""></p>
<h3 id="Facial-expression-analysis"><a href="#Facial-expression-analysis" class="headerlink" title="Facial expression analysis"></a>Facial expression analysis</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210925132325758.png" alt=""></p>
<h3 id="Articulated-Body-Tracking-OpenPose"><a href="#Articulated-Body-Tracking-OpenPose" class="headerlink" title="Articulated Body Tracking: OpenPose"></a>Articulated Body Tracking: OpenPose</h3><p><a href="https://github.com/CMU-Perceptual-Computing-Lab/openpose" target="_blank" rel="noopener">CMU-Perceptual-Computing-Lab/openpose: OpenPose: Real-time multi-person keypoint detection library for body, face, hands, and foot estimation (github.com)</a></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210925132354601.png" alt=""></p>
<h2 id="Convolutional-Neural-Networks"><a href="#Convolutional-Neural-Networks" class="headerlink" title="Convolutional Neural Networks"></a>Convolutional Neural Networks</h2><h3 id="Convolutional-Neural-Networks-1"><a href="#Convolutional-Neural-Networks-1" class="headerlink" title="Convolutional Neural Networks"></a>Convolutional Neural Networks</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210925132548618.png" alt=""></p>
<h3 id="Translation-Invariance"><a href="#Translation-Invariance" class="headerlink" title="Translation Invariance"></a>Translation Invariance</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210925132651818.png" alt=""></p>
<h3 id="Learned-vs-Predefined-Kernels"><a href="#Learned-vs-Predefined-Kernels" class="headerlink" title="Learned vs Predefined Kernels"></a>Learned vs Predefined Kernels</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210925132730651.png" alt=""></p>
<h2 id="Convolution-Math"><a href="#Convolution-Math" class="headerlink" title="Convolution Math"></a>Convolution Math</h2><h2 id="Convolutional-Neural-Layer"><a href="#Convolutional-Neural-Layer" class="headerlink" title="Convolutional Neural Layer"></a>Convolutional Neural Layer</h2><h2 id="Convolutional-Neural-Network"><a href="#Convolutional-Neural-Network" class="headerlink" title="Convolutional  Neural Network"></a>Convolutional  Neural Network</h2><h2 id="Example-of-CNN-Architectures"><a href="#Example-of-CNN-Architectures" class="headerlink" title="Example of CNN  Architectures"></a>Example of CNN  Architectures</h2><h3 id="Common-architectures"><a href="#Common-architectures" class="headerlink" title="Common architectures"></a>Common architectures</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210925133011496.png" alt=""></p>
<h3 id="VGGNet-model"><a href="#VGGNet-model" class="headerlink" title="VGGNet model"></a>VGGNet model</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210925133033585.png" alt=""></p>
<h3 id="Other-architectures"><a href="#Other-architectures" class="headerlink" title="Other architectures"></a>Other architectures</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210925133323647.png" alt=""></p>
<h3 id="Residual-Networks"><a href="#Residual-Networks" class="headerlink" title="Residual Networks"></a>Residual Networks</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210925133353169.png" alt=""></p>
<h2 id="Visualizing-CNNs"><a href="#Visualizing-CNNs" class="headerlink" title="Visualizing CNNs"></a>Visualizing CNNs</h2><h3 id="Visualizing-the-Last-CNN-Layer-t-sne"><a href="#Visualizing-the-Last-CNN-Layer-t-sne" class="headerlink" title="Visualizing the Last CNN Layer: t-sne"></a>Visualizing the Last CNN Layer: t-sne</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210925133459250.png" alt=""></p>
<h3 id="Deconvolution"><a href="#Deconvolution" class="headerlink" title="Deconvolution"></a>Deconvolution</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210925133516397.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210925133533240.png" alt=""></p>
<h3 id="CAM-Class-Activation-Mapping"><a href="#CAM-Class-Activation-Mapping" class="headerlink" title="CAM: Class Activation Mapping"></a>CAM: Class Activation Mapping</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210925133606290.png" alt=""></p>
<h3 id="Grad-CAM"><a href="#Grad-CAM" class="headerlink" title="Grad-CAM"></a>Grad-CAM</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210925133634235.png" alt=""></p>
<h2 id="Region-based-CNNs"><a href="#Region-based-CNNs" class="headerlink" title="Region-based CNNs"></a>Region-based CNNs</h2><h3 id="Object-Detection-and-Segmentation"><a href="#Object-Detection-and-Segmentation" class="headerlink" title="Object Detection (and Segmentation)"></a>Object Detection (and Segmentation)</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210925145232362.png" alt=""></p>
<h3 id="Selective-Search"><a href="#Selective-Search" class="headerlink" title="Selective Search"></a>Selective Search</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210925145303885.png" alt=""></p>
<h3 id="R-CNN"><a href="#R-CNN" class="headerlink" title="R-CNN"></a>R-CNN</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210925145406387.png" alt=""></p>
<h3 id="Trade-off-Between-Speed-and-Accuracy"><a href="#Trade-off-Between-Speed-and-Accuracy" class="headerlink" title="Trade-off Between Speed and Accuracy"></a>Trade-off Between Speed and Accuracy</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210925145438915.png" alt=""></p>
<h2 id="Sequential-Modeling-with-Convolutional-Networks"><a href="#Sequential-Modeling-with-Convolutional-Networks" class="headerlink" title="Sequential Modeling  with Convolutional  Networks"></a>Sequential Modeling  with Convolutional  Networks</h2><h3 id="3D-CNN"><a href="#3D-CNN" class="headerlink" title="3D CNN"></a>3D CNN</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210925145830857.png" alt=""></p>
<h3 id="Temporal-Convolution-Network-TCN"><a href="#Temporal-Convolution-Network-TCN" class="headerlink" title="Temporal Convolution Network (TCN)"></a>Temporal Convolution Network (TCN)</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210925150140931.png" alt=""></p>
<h2 id="Appendix-Tools-for-Automatic-visual-behavior-analysis"><a href="#Appendix-Tools-for-Automatic-visual-behavior-analysis" class="headerlink" title="Appendix: Tools for  Automatic visual  behavior analysis"></a>Appendix: Tools for  Automatic visual  behavior analysis</h2><p><strong>OpenFace: an open source facial behavior analysis toolkit, T. Baltrušaitis et al., 2016</strong></p>
<p><strong>Image from Hachisu et al (2018). FaceLooks: A Smart Headband for Signaling Face-to-Face Behavior. Sensors.</strong></p>
<h1 id="Language-Representations-and-RNNs"><a href="#Language-Representations-and-RNNs" class="headerlink" title="Language  Representations and RNNs"></a>Language  Representations and RNNs</h1><h2 id="Word-Representations"><a href="#Word-Representations" class="headerlink" title="Word  Representations"></a>Word  Representations</h2><h3 id="How-to-learn-word-features-representations"><a href="#How-to-learn-word-features-representations" class="headerlink" title="How to learn (word) features/representations?"></a>How to learn (word) features/representations?</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210925150848897.png" alt=""></p>
<h3 id="Distance-and-similarity"><a href="#Distance-and-similarity" class="headerlink" title="Distance and similarity"></a>Distance and similarity</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210925151237721.png" alt=""></p>
<h3 id="How-to-learn-word-features-representations-1"><a href="#How-to-learn-word-features-representations-1" class="headerlink" title="How to learn (word) features/representations?"></a>How to learn (word) features/representations?</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210925151720271.png" alt=""></p>
<h3 id="How-to-use-these-word-representations"><a href="#How-to-use-these-word-representations" class="headerlink" title="How to use these word representations"></a>How to use these word representations</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210925152847454.png" alt=""></p>
<h3 id="Vector-space-models-of-words"><a href="#Vector-space-models-of-words" class="headerlink" title="Vector space models of words"></a>Vector space models of words</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210925153012543.png" alt=""></p>
<h2 id="Sentence-Modeling"><a href="#Sentence-Modeling" class="headerlink" title="Sentence Modeling"></a>Sentence Modeling</h2><h3 id="Sentence-Modeling-Sequence-Label-Prediction"><a href="#Sentence-Modeling-Sequence-Label-Prediction" class="headerlink" title="Sentence Modeling: Sequence Label Prediction"></a>Sentence Modeling: Sequence Label Prediction</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210925153315258.png" alt=""></p>
<h3 id="Sentence-Modeling-Sequence-Prediction"><a href="#Sentence-Modeling-Sequence-Prediction" class="headerlink" title="Sentence Modeling: Sequence Prediction"></a>Sentence Modeling: Sequence Prediction</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210925153338031.png" alt=""></p>
<h3 id="Sentence-Modeling-Sequence-Representation"><a href="#Sentence-Modeling-Sequence-Representation" class="headerlink" title="Sentence Modeling: Sequence Representation"></a>Sentence Modeling: Sequence Representation</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210925153510890.png" alt=""></p>
<h3 id="Sentence-Modeling-Language-Model"><a href="#Sentence-Modeling-Language-Model" class="headerlink" title="Sentence Modeling: Language Model"></a>Sentence Modeling: Language Model</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210925153612571.png" alt=""></p>
<h3 id="Language-Model-Application-Language-Generation"><a href="#Language-Model-Application-Language-Generation" class="headerlink" title="Language Model Application: Language Generation"></a>Language Model Application: Language Generation</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210925153648204.png" alt=""></p>
<h3 id="Language-Model-Application-Speech-Recognition"><a href="#Language-Model-Application-Speech-Recognition" class="headerlink" title="Language Model Application: Speech Recognition"></a>Language Model Application: Speech Recognition</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210925153720084.png" alt=""></p>
<h3 id="Challenges-in-Sequence-Modeling"><a href="#Challenges-in-Sequence-Modeling" class="headerlink" title="Challenges in Sequence Modeling"></a>Challenges in Sequence Modeling</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210925153831802.png" alt=""></p>
<h2 id="Recurrent-Neural-Networks"><a href="#Recurrent-Neural-Networks" class="headerlink" title="Recurrent  Neural Networks"></a>Recurrent  Neural Networks</h2><h2 id="Gated-Recurrent-Neural-Networks"><a href="#Gated-Recurrent-Neural-Networks" class="headerlink" title="Gated Recurrent  Neural Networks"></a>Gated Recurrent  Neural Networks</h2><h2 id="Syntax-and-Language-Structure"><a href="#Syntax-and-Language-Structure" class="headerlink" title="Syntax and  Language Structure"></a>Syntax and  Language Structure</h2><h3 id="Syntax-and-Language-Structure-1"><a href="#Syntax-and-Language-Structure-1" class="headerlink" title="Syntax and  Language Structure"></a>Syntax and  Language Structure</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210925154240103.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210925154307159.png" alt=""></p>
<h3 id="Dependency-Grammar"><a href="#Dependency-Grammar" class="headerlink" title="Dependency Grammar"></a>Dependency Grammar</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210925154728307.png" alt=""></p>
<h3 id="Language-Ambiguity"><a href="#Language-Ambiguity" class="headerlink" title="Language Ambiguity"></a>Language Ambiguity</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210925154927405.png" alt=""></p>
<h2 id="Recursive-Neural-Network"><a href="#Recursive-Neural-Network" class="headerlink" title="Recursive Neural  Network"></a>Recursive Neural  Network</h2><p><strong>Socher et al., Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank, EMNLP 2013</strong></p>
<h3 id="Stack-LSTM"><a href="#Stack-LSTM" class="headerlink" title="Stack LSTM"></a>Stack LSTM</h3><p><strong>Dyer et al., Transition-Based Dependency Parsing with Stack Long Short-Term Memory, 2015</strong></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210925155214394.png" alt=""></p>
<h1 id="Multimodal-Representations"><a href="#Multimodal-Representations" class="headerlink" title="Multimodal  Representations"></a>Multimodal  Representations</h1><h2 id="Graph-Representations"><a href="#Graph-Representations" class="headerlink" title="Graph  Representations"></a>Graph  Representations</h2><h3 id="RECAP-Tree-based-RNNs-or-Recursive-Neural-Network"><a href="#RECAP-Tree-based-RNNs-or-Recursive-Neural-Network" class="headerlink" title="RECAP: Tree-based RNNs (or Recursive Neural Network)"></a>RECAP: Tree-based RNNs (or Recursive Neural Network)</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210925155731220.png" alt=""></p>
<h3 id="Graphs-aka-“Networks”"><a href="#Graphs-aka-“Networks”" class="headerlink" title="Graphs (aka “Networks”)"></a>Graphs (aka “Networks”)</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210925160208244.png" alt=""></p>
<h3 id="Graphs-–-Supervised-Task"><a href="#Graphs-–-Supervised-Task" class="headerlink" title="Graphs – Supervised Task"></a>Graphs – Supervised Task</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210925160240282.png" alt=""></p>
<h3 id="Graphs-–-Unsupervised-Task"><a href="#Graphs-–-Unsupervised-Task" class="headerlink" title="Graphs – Unsupervised Task"></a>Graphs – Unsupervised Task</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210925160319402.png" alt=""></p>
<h3 id="Graph-Neural-Nets"><a href="#Graph-Neural-Nets" class="headerlink" title="Graph Neural Nets"></a>Graph Neural Nets</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210925160514799.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210925160529214.png" alt=""></p>
<h3 id="Graph-Neural-Nets-–-Supervised-Training"><a href="#Graph-Neural-Nets-–-Supervised-Training" class="headerlink" title="Graph Neural Nets – Supervised Training"></a>Graph Neural Nets – Supervised Training</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210925160633622.png" alt=""></p>
<h3 id="Graph-Neural-Nets-–-Neighborhood-Aggregation"><a href="#Graph-Neural-Nets-–-Neighborhood-Aggregation" class="headerlink" title="Graph Neural Nets – Neighborhood Aggregation"></a>Graph Neural Nets – Neighborhood Aggregation</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210925160653514.png" alt=""></p>
<p><strong>Kipf et al., 2017. Semi-supervised Classification with Graph  Convolutional Networks. ICLR.</strong></p>
<p><strong>Li et al., 2016. Gated Graph Sequence Neural Networks. ICLR.</strong></p>
<p>*<em>Duvenaud et al. 2016. Convolutional Networks on Graphs for  Learning Molecular Fingerprints. ICML. Li et al. 2016. *</em></p>
<p><strong>Gated Graph Sequence Neural Networks. ICLR.</strong></p>
<h2 id="Multimodal-representations"><a href="#Multimodal-representations" class="headerlink" title="Multimodal representations"></a>Multimodal representations</h2><h3 id="Multimodal-representations-1"><a href="#Multimodal-representations-1" class="headerlink" title="Multimodal representations"></a>Multimodal representations</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210925160956465.png" alt=""></p>
<h2 id="Unsupervised-Joint-representations"><a href="#Unsupervised-Joint-representations" class="headerlink" title="Unsupervised  Joint representations"></a>Unsupervised  Joint representations</h2><h3 id="Unsupervised-representation-learning"><a href="#Unsupervised-representation-learning" class="headerlink" title="Unsupervised representation learning"></a>Unsupervised representation learning</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210925161255268.png" alt=""></p>
<h3 id="Shallow-multimodal-representations"><a href="#Shallow-multimodal-representations" class="headerlink" title="Shallow multimodal representations"></a>Shallow multimodal representations</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210925161428283.png" alt=""></p>
<h3 id="Autoencoders"><a href="#Autoencoders" class="headerlink" title="Autoencoders"></a>Autoencoders</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210925161522115.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210925162316266.png" alt=""></p>
<h3 id="Deep-Multimodal-autoencoders"><a href="#Deep-Multimodal-autoencoders" class="headerlink" title="Deep Multimodal autoencoders"></a>Deep Multimodal autoencoders</h3><p><strong>Ngiam et al., Multimodal Deep Learning, 2011</strong></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210925162451932.png" alt=""></p>
<h3 id="Deep-Multimodal-Boltzmann-machines"><a href="#Deep-Multimodal-Boltzmann-machines" class="headerlink" title="Deep Multimodal Boltzmann machines"></a>Deep Multimodal Boltzmann machines</h3><p><strong>Srivastava and Salakhutdinov, Multimodal Learning with  Deep Boltzmann Machines, 2012, 2014</strong></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210925162811727.png" alt=""></p>
<h2 id="Supervised-Joint-representations"><a href="#Supervised-Joint-representations" class="headerlink" title="Supervised Joint representations"></a>Supervised Joint representations</h2><h3 id="Multimodal-Joint-Representation"><a href="#Multimodal-Joint-Representation" class="headerlink" title="Multimodal Joint Representation"></a>Multimodal Joint Representation</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210925162927697.png" alt=""></p>
<h3 id="Multimodal-Sentiment-Analysis"><a href="#Multimodal-Sentiment-Analysis" class="headerlink" title="Multimodal Sentiment Analysis"></a>Multimodal Sentiment Analysis</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210925162954988.png" alt=""></p>
<h3 id="Unimodal-Bimodal-and-Trimodal-Interactions"><a href="#Unimodal-Bimodal-and-Trimodal-Interactions" class="headerlink" title="Unimodal, Bimodal and Trimodal Interactions"></a>Unimodal, Bimodal and Trimodal Interactions</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210925163158287.png" alt=""></p>
<h3 id="Bilinear-Pooling"><a href="#Bilinear-Pooling" class="headerlink" title="Bilinear Pooling"></a>Bilinear Pooling</h3><p><strong>Tenenbaum and Freeman, 2000</strong></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210925163341818.png" alt=""></p>
<h3 id="Multimodal-Tensor-Fusion-Network-TFN"><a href="#Multimodal-Tensor-Fusion-Network-TFN" class="headerlink" title="Multimodal Tensor Fusion Network (TFN)"></a>Multimodal Tensor Fusion Network (TFN)</h3><p>Zadeh, Jones and Morency, EMNLP 2017</p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210925163555970.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210925163612209.png" alt=""></p>
<h3 id="From-Tensor-Representation-to-Low-rank-Fusion"><a href="#From-Tensor-Representation-to-Low-rank-Fusion" class="headerlink" title="From Tensor Representation to Low-rank Fusion"></a>From Tensor Representation to Low-rank Fusion</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210925164254236.png" alt=""></p>
<h4 id="①-Decomposition-of-weight-tensor-W"><a href="#①-Decomposition-of-weight-tensor-W" class="headerlink" title="① Decomposition of weight tensor W"></a>① Decomposition of weight tensor W</h4><p><img src="/images/loading.gif" data-original="../images/ML/image-20210925164359975.png" alt=""></p>
<h4 id="②-Decomposition-of-Z"><a href="#②-Decomposition-of-Z" class="headerlink" title="② Decomposition of Z"></a>② Decomposition of Z</h4><p><img src="/images/loading.gif" data-original="../images/ML/image-20210925164417293.png" alt=""></p>
<h4 id="③-Rearranging-computation"><a href="#③-Rearranging-computation" class="headerlink" title="③ Rearranging computation"></a>③ Rearranging computation</h4><p><img src="/images/loading.gif" data-original="../images/ML/image-20210925164436854.png" alt=""></p>
<h2 id="Multimodal-LSTM"><a href="#Multimodal-LSTM" class="headerlink" title="Multimodal LSTM"></a>Multimodal LSTM</h2><h3 id="Multimodal-Sequence-Modeling-–-Early-Fusion"><a href="#Multimodal-Sequence-Modeling-–-Early-Fusion" class="headerlink" title="Multimodal Sequence Modeling – Early Fusion"></a>Multimodal Sequence Modeling – Early Fusion</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210925164520479.png" alt=""></p>
<h3 id="Multi-View-Long-Short-Term-Memory-MV-LSTM"><a href="#Multi-View-Long-Short-Term-Memory-MV-LSTM" class="headerlink" title="Multi-View Long Short-Term Memory (MV-LSTM)"></a>Multi-View Long Short-Term Memory (MV-LSTM)</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210925164538641.png" alt=""></p>
<h3 id="Multi-View-Long-Short-Term-Memory"><a href="#Multi-View-Long-Short-Term-Memory" class="headerlink" title="Multi-View Long Short-Term Memory"></a>Multi-View Long Short-Term Memory</h3><p><strong>Shyam, Morency, et al. Extending Long Short-Term Memory for Multi-View Structured Learning, ECCV, 2016</strong></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210925164608815.png" alt=""></p>
<h3 id="Topologies-for-Multi-View-LSTM"><a href="#Topologies-for-Multi-View-LSTM" class="headerlink" title="Topologies for Multi-View LSTM"></a>Topologies for Multi-View LSTM</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210925164629825.png" alt=""></p>
<h2 id="Coordinated-Multimodal-Representations"><a href="#Coordinated-Multimodal-Representations" class="headerlink" title="Coordinated Multimodal  Representations"></a>Coordinated Multimodal  Representations</h2><h3 id="Coordinated-Multimodal-Representations-1"><a href="#Coordinated-Multimodal-Representations-1" class="headerlink" title="Coordinated Multimodal Representations"></a>Coordinated Multimodal Representations</h3><p>Learn (unsupervised) two or more  coordinated representations from  multiple modalities. A loss function  is defined to bring closer these  multiple representations.</p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210925164727194.png" alt=""></p>
<h3 id="Coordinated-Multimodal-Embeddings"><a href="#Coordinated-Multimodal-Embeddings" class="headerlink" title="Coordinated Multimodal Embeddings"></a>Coordinated Multimodal Embeddings</h3><p><strong>Frome et al., DeViSE: A Deep Visual-Semantic Embedding Model, NIPS 2013</strong></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210925165051639.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210925165329609.png" alt=""></p>
<h3 id="Structure-preserving-Loss-–-Multimodal-Embeddings"><a href="#Structure-preserving-Loss-–-Multimodal-Embeddings" class="headerlink" title="Structure-preserving Loss – Multimodal Embeddings"></a>Structure-preserving Loss – Multimodal Embeddings</h3><p><strong>Wang et al., Learning Deep Structure-Preserving Image-Text Embeddings, CVPR 2016</strong></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210925165352742.png" alt=""></p>
<h1 id="Coordinated-Representations"><a href="#Coordinated-Representations" class="headerlink" title="Coordinated  Representations"></a>Coordinated  Representations</h1><h2 id="Quick-Recap"><a href="#Quick-Recap" class="headerlink" title="Quick Recap"></a>Quick Recap</h2><p><img src="/images/loading.gif" data-original="../images/ML/image-20210925171214156.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210925171228809.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210925171238901.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210925171249710.png" alt=""></p>
<h2 id="Coordinated-Multimodal-Representations-2"><a href="#Coordinated-Multimodal-Representations-2" class="headerlink" title="Coordinated Multimodal  Representations"></a>Coordinated Multimodal  Representations</h2><p>Learn (unsupervised) two or more  coordinated representations from  multiple modalities. A loss function  is defined to bring closer these  multiple representations. </p>
<h3 id="Structured-coordinated-embeddings"><a href="#Structured-coordinated-embeddings" class="headerlink" title="Structured coordinated embeddings"></a>Structured coordinated embeddings</h3><p><strong>Vendrov et al., Order-Embeddings of  Images and Language, 2016</strong></p>
<p><strong>Vendrov et al., Order-Embeddings of  Images and Language, 2016</strong></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210925171556819.png" alt=""></p>
<h2 id="Multivariate-Statistical-Analysis"><a href="#Multivariate-Statistical-Analysis" class="headerlink" title="Multivariate  Statistical Analysis"></a>Multivariate  Statistical Analysis</h2><p><img src="/images/loading.gif" data-original="../images/ML/image-20210925172117331.png" alt=""></p>
<h3 id="Random-Variables"><a href="#Random-Variables" class="headerlink" title="Random Variables"></a>Random Variables</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210925174551896.png" alt=""></p>
<h3 id="Definitions"><a href="#Definitions" class="headerlink" title="Definitions"></a>Definitions</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210925175421144.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210925175441878.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210925175504490.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210925175517483.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210925175621900.png" alt=""></p>
<h3 id="Principal-component-analysis"><a href="#Principal-component-analysis" class="headerlink" title="Principal component analysis"></a>Principal component analysis</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210925174903818.png" alt=""></p>
<h3 id="Eigenvalues-and-Eigenvectors"><a href="#Eigenvalues-and-Eigenvectors" class="headerlink" title="Eigenvalues and Eigenvectors"></a>Eigenvalues and Eigenvectors</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210925175739662.png" alt=""></p>
<h3 id="Singular-Value-Decomposition-SVD"><a href="#Singular-Value-Decomposition-SVD" class="headerlink" title="Singular Value Decomposition (SVD)"></a>Singular Value Decomposition (SVD)</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210925175758010.png" alt=""></p>
<h2 id="Canonical-Correlation-Analysis"><a href="#Canonical-Correlation-Analysis" class="headerlink" title="Canonical  Correlation Analysis"></a>Canonical  Correlation Analysis</h2><h3 id="Canonical-Correlation-Analysis-1"><a href="#Canonical-Correlation-Analysis-1" class="headerlink" title="Canonical Correlation Analysis"></a>Canonical Correlation Analysis</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210925175009684.png" alt=""></p>
<h3 id="Correlated-Projection"><a href="#Correlated-Projection" class="headerlink" title="Correlated Projection"></a>Correlated Projection</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210925175041387.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210925180111454.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210925180128991.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210925180152772.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210925180210548.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210925180224010.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210925180234491.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210925180248487.png" alt=""></p>
<h2 id="Exploring-Deep-Correlation-Networks"><a href="#Exploring-Deep-Correlation-Networks" class="headerlink" title="Exploring Deep  Correlation Networks"></a>Exploring Deep  Correlation Networks</h2><h3 id="Deep-Canonical-Correlation-Analysis"><a href="#Deep-Canonical-Correlation-Analysis" class="headerlink" title="Deep Canonical Correlation Analysis"></a>Deep Canonical Correlation Analysis</h3><p><strong>Andrew et al., ICML 2013</strong></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210925180338616.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210925180400091.png" alt=""></p>
<h3 id="Deep-Canonically-Correlated-Autoencoders-DCCAE"><a href="#Deep-Canonically-Correlated-Autoencoders-DCCAE" class="headerlink" title="Deep Canonically Correlated Autoencoders (DCCAE)"></a>Deep Canonically Correlated Autoencoders (DCCAE)</h3><p><strong>Wang et al., ICML 2015</strong></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210925180436454.png" alt=""></p>
<h3 id="Deep-Correlational-Neural-Network"><a href="#Deep-Correlational-Neural-Network" class="headerlink" title="Deep Correlational Neural Network"></a>Deep Correlational Neural Network</h3><p><strong>Chandar et al., Neural Computation, 2015</strong></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210925180503696.png" alt=""></p>
<h2 id="Multi-View-Clustering"><a href="#Multi-View-Clustering" class="headerlink" title="Multi-View  Clustering"></a>Multi-View  Clustering</h2><h3 id="Data-Clustering"><a href="#Data-Clustering" class="headerlink" title="Data Clustering"></a>Data Clustering</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210925181010410.png" alt=""></p>
<h3 id="“Soft”-Clustering-Nonnegative-Matrix-Factorization"><a href="#“Soft”-Clustering-Nonnegative-Matrix-Factorization" class="headerlink" title="“Soft” Clustering: Nonnegative Matrix Factorization"></a>“Soft” Clustering: Nonnegative Matrix Factorization</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210925181035671.png" alt=""></p>
<h3 id="Semi-NMF-and-Other-Extensions"><a href="#Semi-NMF-and-Other-Extensions" class="headerlink" title="Semi-NMF and Other Extensions"></a>Semi-NMF and Other Extensions</h3><p><strong>Ding et al., TPAMI2015</strong></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210925181142837.png" alt=""></p>
<p><strong>Trigerous et al., TPAMI 2015</strong></p>
<h3 id="Principles-of-Multi-View-Clustering"><a href="#Principles-of-Multi-View-Clustering" class="headerlink" title="Principles of Multi-View Clustering"></a>Principles of Multi-View Clustering</h3><p><strong>Yan Yang and Hao Wang, Multi-view Clustering: A Survey, Big data mining and analytics, Volume 1, Number 2, June 2018</strong></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210925181248893.png" alt=""></p>
<h3 id="Multi-view-subspace-clustering"><a href="#Multi-view-subspace-clustering" class="headerlink" title="Multi-view subspace clustering"></a>Multi-view subspace clustering</h3><p><strong>Definition</strong>: learns a unified feature representation from  all the view subspaces by assuming that all views share  this representation</p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210925181323677.png" alt=""></p>
<h3 id="Deep-Matrix-Factorization"><a href="#Deep-Matrix-Factorization" class="headerlink" title="Deep Matrix Factorization"></a>Deep Matrix Factorization</h3><p><strong>Li and Tang, MMML 2015</strong></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210925181348298.png" alt=""></p>
<h3 id="Other-Multi-View-Clustering-Approaches"><a href="#Other-Multi-View-Clustering-Approaches" class="headerlink" title="Other Multi-View Clustering Approaches"></a>Other Multi-View Clustering Approaches</h3><p><strong>Yan Yang and Hao Wang, Multi-view Clustering: A Survey, Big data mining and analytics, Volume 1, Number 2, June 2018</strong></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210925181406285.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210925181419390.png" alt=""></p>
<h2 id="Auto-Encoder-in-Auto-Encoder-Network"><a href="#Auto-Encoder-in-Auto-Encoder-Network" class="headerlink" title="Auto-Encoder in  Auto-Encoder Network"></a>Auto-Encoder in  Auto-Encoder Network</h2><h3 id="Deep-Canonically-Correlated-Autoencoders-DCCAE-1"><a href="#Deep-Canonically-Correlated-Autoencoders-DCCAE-1" class="headerlink" title="Deep Canonically Correlated Autoencoders (DCCAE)"></a>Deep Canonically Correlated Autoencoders (DCCAE)</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210925181451698.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210925181805887.png" alt=""></p>
<h3 id="Multi-view-Latent-“Intact”-Space"><a href="#Multi-view-Latent-“Intact”-Space" class="headerlink" title="Multi-view Latent “Intact” Space"></a>Multi-view Latent “Intact” Space</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210925181721319.png" alt=""></p>
<h1 id="Multimodal-alignment"><a href="#Multimodal-alignment" class="headerlink" title="Multimodal alignment"></a>Multimodal alignment</h1><h2 id="Multimodal-alignment-1"><a href="#Multimodal-alignment-1" class="headerlink" title="Multimodal alignment"></a>Multimodal alignment</h2><h3 id="Explicit-multimodal-alignment"><a href="#Explicit-multimodal-alignment" class="headerlink" title="Explicit multimodal-alignment"></a>Explicit multimodal-alignment</h3><p>Explicit alignment - goal is to find correspondences  between modalities</p>
<p>▪ Aligning speech signal to a transcript</p>
<p>▪ Aligning two out-of sync sequences</p>
<p>▪ Co-referring expressions</p>
<h3 id="Implicit-multimodal-alignment"><a href="#Implicit-multimodal-alignment" class="headerlink" title="Implicit multimodal-alignment"></a>Implicit multimodal-alignment</h3><p>Implicit alignment - uses internal latent alignment of  modalities in order to better solve various problems </p>
<p>▪ Machine Translation </p>
<p>▪ Cross-modal retrieval </p>
<p>▪ Image &amp; Video Captioning </p>
<p>▪ Visual Question Answering</p>
<h2 id="Explicit-alignment"><a href="#Explicit-alignment" class="headerlink" title="Explicit alignment"></a>Explicit alignment</h2><h3 id="Let’s-start-unimodal-–-Dynamic-Time-Warping"><a href="#Let’s-start-unimodal-–-Dynamic-Time-Warping" class="headerlink" title="Let’s start unimodal – Dynamic Time Warping"></a>Let’s start unimodal – Dynamic Time Warping</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210925214543333.png" alt=""></p>
<h3 id="Dynamic-Time-Warping-continued"><a href="#Dynamic-Time-Warping-continued" class="headerlink" title="Dynamic Time Warping continued"></a>Dynamic Time Warping continued</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210925214616472.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210925214740777.png" alt=""></p>
<h3 id="DTW-alternative-formulation"><a href="#DTW-alternative-formulation" class="headerlink" title="DTW alternative formulation"></a>DTW alternative formulation</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210925220019924.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210925220248299.png" alt=""></p>
<h3 id="Canonical-Correlation-Analysis-reminder"><a href="#Canonical-Correlation-Analysis-reminder" class="headerlink" title="Canonical Correlation Analysis reminder"></a>Canonical Correlation Analysis reminder</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210925220343675.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210925220420712.png" alt=""></p>
<h3 id="Canonical-Time-Warping"><a href="#Canonical-Time-Warping" class="headerlink" title="Canonical Time Warping"></a>Canonical Time Warping</h3><p><strong>Canonical Time Warping for Alignment of Human Behavior, Zhou and De la Tore, 2009</strong></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210925220445495.png" alt=""></p>
<p>Optimized by Coordinate-descent –fix one set of parameters, optimize another</p>
<p><strong>Canonical Time Warping for Alignment of Human Behavior, Zhou and De la Tore, 2009, NIPS</strong></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210925220819870.png" alt=""></p>
<p><strong>Generalized Canonical Time Warping, Zhou and De la Tore, 2016, TPAMI</strong></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210925220903492.png" alt=""></p>
<h3 id="Deep-Canonical-Time-Warping"><a href="#Deep-Canonical-Time-Warping" class="headerlink" title="Deep Canonical Time Warping"></a>Deep Canonical Time Warping</h3><p><strong>Deep Canonical Time Warping, Trigeorgis et al., 2016, CVPR</strong></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210925221858298.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210925222124797.png" alt=""></p>
<h2 id="Implicit-alignment"><a href="#Implicit-alignment" class="headerlink" title="Implicit alignment"></a>Implicit alignment</h2><h2 id="Attention-models"><a href="#Attention-models" class="headerlink" title="Attention models"></a>Attention models</h2><p>Recent attention models can be roughly split into three major categories</p>
<ol>
<li><p><strong>Soft attention</strong></p>
<p>Acts like a gate function. Deterministic inference.</p>
</li>
<li><p><strong>Transform network</strong></p>
<p>Warp the input to better align with canonical view。</p>
</li>
<li><p><strong>Hard attention</strong></p>
<p>Includes stochastic processes. Related to reinforcement learning.</p>
</li>
</ol>
<h2 id="Soft-attention"><a href="#Soft-attention" class="headerlink" title="Soft attention"></a>Soft attention</h2><h3 id="Machine-Translation"><a href="#Machine-Translation" class="headerlink" title="Machine Translation"></a>Machine Translation</h3><p>Given a sentence in one language translate it to another</p>
<p>Not exactly multimodal task – but a good start! Each  language can be seen almost as a modality.</p>
<h3 id="Machine-Translation-with-RNNs"><a href="#Machine-Translation-with-RNNs" class="headerlink" title="Machine Translation with RNNs"></a>Machine Translation with RNNs</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210925224510841.png" alt=""></p>
<h3 id="Decoder-–-attention-model"><a href="#Decoder-–-attention-model" class="headerlink" title="Decoder – attention model"></a>Decoder – attention model</h3><p><strong>Bahdanau et al., “Neural Machine Translation  by Jointly Learning to Align and Translate”, ICLR  2015</strong></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210925224628199.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210925224720805.png" alt=""></p>
<h3 id="How-do-we-encode-attention"><a href="#How-do-we-encode-attention" class="headerlink" title="How do we encode attention?"></a>How do we encode attention?</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210925224757919.png" alt=""></p>
<h3 id="MT-with-attention"><a href="#MT-with-attention" class="headerlink" title="MT with attention"></a>MT with attention</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210925224819092.png" alt=""></p>
<h3 id="Visual-captioning-with-soft-attention"><a href="#Visual-captioning-with-soft-attention" class="headerlink" title="Visual captioning with soft attention"></a>Visual captioning with soft attention</h3><p><strong>Show, Attend and Tell: Neural  Image Caption Generation with  Visual Attention, Xu et al., 2015</strong></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210925225231573.png" alt=""></p>
<h3 id="Looking-at-more-fine-grained-feature"><a href="#Looking-at-more-fine-grained-feature" class="headerlink" title="Looking at more fine grained feature"></a>Looking at more fine grained feature</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210925225324464.png" alt=""></p>
<ul>
<li>允许潜在数据对齐</li>
<li>让我们了解网络“看到”的内容</li>
<li>可以使用反向传播进行优化</li>
</ul>
<h2 id="Spatial-Transformer-networks"><a href="#Spatial-Transformer-networks" class="headerlink" title="Spatial Transformer  networks"></a>Spatial Transformer  networks</h2><p><img src="/images/loading.gif" data-original="../images/ML/image-20210925225528663.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210925225617261.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210925225637367.png" alt=""></p>
<h2 id="Glimpse-Network-Hard-Attention"><a href="#Glimpse-Network-Hard-Attention" class="headerlink" title="Glimpse Network (Hard Attention)"></a>Glimpse Network (Hard Attention)</h2><h3 id="Hard-attention"><a href="#Hard-attention" class="headerlink" title="Hard attention"></a>Hard attention</h3><ul>
<li><p>Soft attention requires computing a representation for the whole  image or sentence </p>
</li>
<li><p>Hard attention on the other hand forces looking only at one part </p>
</li>
<li><p>Main motivation was reduced computational cost rather than  improved accuracy (although that happens a bit as well) ▪</p>
</li>
<li><p>Saccade followed by a glimpse – how human visual system  works</p>
</li>
</ul>
<p><strong>Recurrent Models of Visual Attention, Mnih, 2014] [Multiple Object Recognition with Visual Attention,  Ba, 2015</strong></p>
<h3 id="Hard-attention-examples"><a href="#Hard-attention-examples" class="headerlink" title="Hard attention examples"></a>Hard attention examples</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210925230309349.png" alt=""></p>
<h3 id="Glimpse-Sensor"><a href="#Glimpse-Sensor" class="headerlink" title="Glimpse Sensor"></a>Glimpse Sensor</h3><p><strong>Recurrent Models of Visual Attention, Mnih, 2014</strong></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210925230344297.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210925230423534.png" alt=""></p>
<h3 id="Overall-Architecture-Emission-network"><a href="#Overall-Architecture-Emission-network" class="headerlink" title="Overall Architecture - Emission network"></a>Overall Architecture - Emission network</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210925230718138.png" alt=""></p>
<h3 id="Recurrent-model-of-Visual-Attention-RAM"><a href="#Recurrent-model-of-Visual-Attention-RAM" class="headerlink" title="Recurrent model of Visual Attention (RAM)"></a>Recurrent model of Visual Attention (RAM)</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210925230834414.png" alt=""></p>
<h2 id="Multi-modal-alignment-recap"><a href="#Multi-modal-alignment-recap" class="headerlink" title="Multi-modal alignment recap"></a>Multi-modal alignment recap</h2><p>Explicit alignment -aligns two or more modalities (or views) as an actual task. The goal is to find correspondences between modalities</p>
<ul>
<li>Dynamic Time Warping</li>
<li>Canonical Time Warping</li>
<li>Deep Canonical Time Warping</li>
</ul>
<p>Implicit alignment -uses internal latent alignment of modalities in order to better solve various problems</p>
<ul>
<li>Attention models</li>
<li>Soft attention</li>
<li>Spatial transformer networks</li>
<li>Hard attention</li>
</ul>
<h1 id="Alignment-and-Representations"><a href="#Alignment-and-Representations" class="headerlink" title="Alignment and Representations"></a>Alignment and Representations</h1><h2 id="Contextualized-Sequence-Encoding"><a href="#Contextualized-Sequence-Encoding" class="headerlink" title="Contextualized Sequence Encoding"></a>Contextualized Sequence Encoding</h2><h3 id="Sequence-Encoding-Contextualization"><a href="#Sequence-Encoding-Contextualization" class="headerlink" title="Sequence Encoding - Contextualization"></a>Sequence Encoding - Contextualization</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210925231232379.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210925231246965.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210925231259788.png" alt=""></p>
<h2 id="Self-Attention"><a href="#Self-Attention" class="headerlink" title="Self-Attention"></a>Self-Attention</h2><p><img src="/images/loading.gif" data-original="../images/ML/image-20210925231434460.png" alt=""></p>
<h3 id="Transformer-Multi-Head-Self-Attention"><a href="#Transformer-Multi-Head-Self-Attention" class="headerlink" title="Transformer Multi-Head Self-Attention"></a>Transformer Multi-Head Self-Attention</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210925231513569.png" alt=""></p>
<h3 id="Position-embeddings"><a href="#Position-embeddings" class="headerlink" title="Position embeddings"></a>Position embeddings</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210925231634427.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210925231651706.png" alt=""></p>
<h2 id="Sequence-to-Sequence-Using-Transformer"><a href="#Sequence-to-Sequence-Using-Transformer" class="headerlink" title="Sequence-to-Sequence Using Transformer"></a>Sequence-to-Sequence Using Transformer</h2><h3 id="Seq2Seq-with-Transformer-Attentions"><a href="#Seq2Seq-with-Transformer-Attentions" class="headerlink" title="Seq2Seq with Transformer Attentions"></a>Seq2Seq with Transformer Attentions</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210925231753459.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210925231836925.png" alt=""></p>
<h2 id="Contextualized-Multimodal-Embedding"><a href="#Contextualized-Multimodal-Embedding" class="headerlink" title="Contextualized Multimodal Embedding"></a>Contextualized Multimodal Embedding</h2><h3 id="Multimodal-Embeddings"><a href="#Multimodal-Embeddings" class="headerlink" title="Multimodal Embeddings"></a>Multimodal Embeddings</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210925231916772.png" alt=""></p>
<h3 id="Multimodal-Transformer"><a href="#Multimodal-Transformer" class="headerlink" title="Multimodal Transformer"></a>Multimodal Transformer</h3><p><strong>Tsai et al., Multimodal Transformer for Unaligned Multimodal Language Sequences, ACL 2019</strong></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210925232001531.png" alt=""></p>
<h3 id="Cross-Modal-Transformer"><a href="#Cross-Modal-Transformer" class="headerlink" title="Cross-Modal Transformer"></a>Cross-Modal Transformer</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210925232033943.png" alt=""></p>
<h2 id="Language-Pre-training"><a href="#Language-Pre-training" class="headerlink" title="Language Pre-training"></a>Language Pre-training</h2><h3 id="Token-level-and-Sentence-level-Embeddings"><a href="#Token-level-and-Sentence-level-Embeddings" class="headerlink" title="Token-level and Sentence-level Embeddings"></a>Token-level and Sentence-level Embeddings</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210925232125254.png" alt=""></p>
<h3 id="Pre-Training-and-Fine-Tuning"><a href="#Pre-Training-and-Fine-Tuning" class="headerlink" title="Pre-Training and Fine-Tuning"></a>Pre-Training and Fine-Tuning</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210925232148603.png" alt=""></p>
<h3 id="BERT-Bidirectional-Encoder-Representations-from-Transformers"><a href="#BERT-Bidirectional-Encoder-Representations-from-Transformers" class="headerlink" title="BERT:  Bidirectional Encoder Representations from Transformers"></a>BERT:  Bidirectional Encoder Representations from Transformers</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210925232217437.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210925232230673.png" alt=""></p>
<h3 id="Pre-training-BERT-Model"><a href="#Pre-training-BERT-Model" class="headerlink" title="Pre-training BERT Model"></a>Pre-training BERT Model</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210925232300337.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210925232314706.png" alt=""></p>
<h3 id="Three-Embeddings-Token-Position-Sentence"><a href="#Three-Embeddings-Token-Position-Sentence" class="headerlink" title="Three Embeddings: Token + Position + Sentence"></a>Three Embeddings: Token + Position + Sentence</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210925232338271.png" alt=""></p>
<h3 id="Fine-Tuning-BERT"><a href="#Fine-Tuning-BERT" class="headerlink" title="Fine-Tuning BERT"></a>Fine-Tuning BERT</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210925232435913.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210925232448202.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210925232459788.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210925232510710.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210925232527686.png" alt=""></p>
<h2 id="Multimodal-Pre-training"><a href="#Multimodal-Pre-training" class="headerlink" title="Multimodal Pre-training"></a>Multimodal Pre-training</h2><h3 id="VL-BERT"><a href="#VL-BERT" class="headerlink" title="VL-BERT"></a>VL-BERT</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210925232559680.png" alt=""></p>
<h3 id="M-BERT"><a href="#M-BERT" class="headerlink" title="M-BERT"></a>M-BERT</h3><p><a href="https://arxiv.org/pdf/1908.05787.pdf" target="_blank" rel="noopener"><strong>1908.05787.pdf (arxiv.org)</strong></a></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210925232625323.png" alt=""></p>
<h1 id="Alignment-and-Translation"><a href="#Alignment-and-Translation" class="headerlink" title="Alignment and Translation"></a>Alignment and Translation</h1><h2 id="Alignment-for-Speech-Recognition"><a href="#Alignment-for-Speech-Recognition" class="headerlink" title="Alignment for  Speech Recognition"></a>Alignment for  Speech Recognition</h2><h3 id="Architecture-of-Speech-Recognition"><a href="#Architecture-of-Speech-Recognition" class="headerlink" title="Architecture of Speech Recognition"></a>Architecture of Speech Recognition</h3><p><a href="http://slazebni.cs.illinois.edu/spring17/lec26_audio.pdf" target="_blank" rel="noopener"><strong>slazebni.cs.illinois.edu/spring17/lec26_audio.pdf</strong></a></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210925232736243.png" alt=""></p>
<h3 id="Option-1-Sequence-to-Sequence-Seq2Seq"><a href="#Option-1-Sequence-to-Sequence-Seq2Seq" class="headerlink" title="Option 1: Sequence-to-Sequence (Seq2Seq)"></a>Option 1: Sequence-to-Sequence (Seq2Seq)</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210925232842346.png" alt=""></p>
<h3 id="Option-2-Seq2Seq-with-Attention"><a href="#Option-2-Seq2Seq-with-Attention" class="headerlink" title="Option 2: Seq2Seq with Attention"></a>Option 2: Seq2Seq with Attention</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210925232901111.png" alt=""></p>
<h3 id="Option-3-Sequence-Labeling-with-RNN"><a href="#Option-3-Sequence-Labeling-with-RNN" class="headerlink" title="Option 3: Sequence Labeling with RNN"></a>Option 3: Sequence Labeling with RNN</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210925232918619.png" alt=""></p>
<h2 id="Speech-Alignment"><a href="#Speech-Alignment" class="headerlink" title="Speech Alignment"></a>Speech Alignment</h2><h3 id="Connectionist-Temporal-Classification-CTC"><a href="#Connectionist-Temporal-Classification-CTC" class="headerlink" title="Connectionist Temporal Classification (CTC)"></a>Connectionist Temporal Classification (CTC)</h3><p><strong>Amodei, Dario, et al. “Deep speech 2: End-to-end speech recognition in english and mandarin.” (2015)</strong></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210925232958695.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210925233034061.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210925233044732.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210925233116293.png" alt=""></p>
<h3 id="CTC-Optimization"><a href="#CTC-Optimization" class="headerlink" title="CTC Optimization"></a>CTC Optimization</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210925233144982.png" alt=""></p>
<h3 id="Visualizing-CTC-Predictions"><a href="#Visualizing-CTC-Predictions" class="headerlink" title="Visualizing CTC Predictions"></a>Visualizing CTC Predictions</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210925233210284.png" alt=""></p>
<h2 id="Multi-View-Video-Alignment"><a href="#Multi-View-Video-Alignment" class="headerlink" title="Multi-View  Video Alignment"></a>Multi-View  Video Alignment</h2><h3 id="Temporal-Alignment-using-Neural-Representations"><a href="#Temporal-Alignment-using-Neural-Representations" class="headerlink" title="Temporal Alignment using Neural Representations"></a>Temporal Alignment using Neural Representations</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210925233310112.png" alt=""></p>
<h3 id="Temporal-Cycle-Consistency-Learning"><a href="#Temporal-Cycle-Consistency-Learning" class="headerlink" title="Temporal Cycle-Consistency Learning"></a>Temporal Cycle-Consistency Learning</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210925233426698.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210925233511989.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210925233522394.png" alt=""></p>
<h2 id="Multimodal-Translation-Visual-Question-Answering-VQA"><a href="#Multimodal-Translation-Visual-Question-Answering-VQA" class="headerlink" title="Multimodal Translation Visual Question  Answering (VQA)"></a>Multimodal Translation Visual Question  Answering (VQA)</h2><h3 id="VQA-and-Attention"><a href="#VQA-and-Attention" class="headerlink" title="VQA and Attention"></a>VQA and Attention</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210925233622800.png" alt=""></p>
<h3 id="Co-attention"><a href="#Co-attention" class="headerlink" title="Co-attention"></a>Co-attention</h3><p><strong>Lu et al., Hierarchical Question-Image Co-Attention for Visual Question Answering, NIPS 2016</strong></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210925233643418.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210925233720955.png" alt=""></p>
<h3 id="Hierarchical-Co-attention"><a href="#Hierarchical-Co-attention" class="headerlink" title="Hierarchical Co-attention"></a>Hierarchical Co-attention</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210925233741572.png" alt=""></p>
<h3 id="Stacked-Attentions"><a href="#Stacked-Attentions" class="headerlink" title="Stacked Attentions"></a>Stacked Attentions</h3><p><strong>Yang et al., Stacked Attention Networks for Image Question Answering, CVPR 2016</strong></p>
<h2 id="VQA-Neural-Module-Networks"><a href="#VQA-Neural-Module-Networks" class="headerlink" title="VQA: Neural  Module Networks"></a>VQA: Neural  Module Networks</h2><h3 id="Neural-Module-Network"><a href="#Neural-Module-Network" class="headerlink" title="Neural Module Network"></a>Neural Module Network</h3><p><strong>Andreas et al., Deep Compositional Question Answering with Neural Module Networks, 2016</strong></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210925233823008.png" alt=""></p>
<h3 id="Predefined-Set-of-Modules"><a href="#Predefined-Set-of-Modules" class="headerlink" title="Predefined Set of Modules"></a>Predefined Set of Modules</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210925233843784.png" alt=""></p>
<p><strong>Johnson et al., CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning, CVPR 2017</strong></p>
<h3 id="End-to-End-Neural-Module-Network"><a href="#End-to-End-Neural-Module-Network" class="headerlink" title="End-to- End Neural Module Network"></a>End-to- End Neural Module Network</h3><p><strong>Hu et al., Learning to Reason: End-to-End Module Networks for Visual Question Answering, 2017</strong></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210925233908247.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210925233924191.png" alt=""></p>
<h2 id="VQA-Neural-Symbolic-Networks"><a href="#VQA-Neural-Symbolic-Networks" class="headerlink" title="VQA: Neural Symbolic Networks"></a>VQA: Neural Symbolic Networks</h2><h3 id="Neural-symbolic-VQA"><a href="#Neural-symbolic-VQA" class="headerlink" title="Neural-symbolic VQA"></a>Neural-symbolic VQA</h3><p><strong>Kexin Yi, et al. “Neural-Symbolic VQA: Disentangling Reasoning from Vision and Language Understanding.” Neurips 2018</strong></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210925234349915.png" alt=""></p>
<h3 id="The-Neuro-symbolic-Concept-Learner"><a href="#The-Neuro-symbolic-Concept-Learner" class="headerlink" title="The Neuro-symbolic Concept Learner"></a>The Neuro-symbolic Concept Learner</h3><p><strong>Jiayuan Mao , et al. “The Neuro-Symbolic Concept Learner: Interpreting Scenes, Words, and Sentences From Natural  Supervision.” ICLR 2019</strong></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210925234417407.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210925234448680.png" alt=""></p>
<h2 id="Speech-Vision-Translation-Applications"><a href="#Speech-Vision-Translation-Applications" class="headerlink" title="Speech-Vision  Translation:  Applications"></a>Speech-Vision  Translation:  Applications</h2><h3 id="Translation-1-Visually-indicated-sounds"><a href="#Translation-1-Visually-indicated-sounds" class="headerlink" title="Translation 1: Visually indicated sounds"></a>Translation 1: Visually indicated sounds</h3><p><strong>Owens et al. Visually indicated sounds, CVPR, 2016</strong></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210925234551236.png" alt=""></p>
<h3 id="Translation-2-The-Sound-of-Pixels"><a href="#Translation-2-The-Sound-of-Pixels" class="headerlink" title="Translation 2: The Sound of Pixels"></a>Translation 2: The Sound of Pixels</h3><p><strong>Zhao, Hang, et al. “The sound of pixels.”, ECCV 2018</strong></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210925234627602.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210925234643576.png" alt=""></p>
<h3 id="Speech2face"><a href="#Speech2face" class="headerlink" title="Speech2face"></a>Speech2face</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210925234734950.png" alt=""></p>
<h1 id="Generative-Models"><a href="#Generative-Models" class="headerlink" title="Generative Models"></a>Generative Models</h1><h2 id="Probabilistic-Graphical-Models"><a href="#Probabilistic-Graphical-Models" class="headerlink" title="Probabilistic  Graphical Models"></a>Probabilistic  Graphical Models</h2><p><strong>Definition</strong>: A probabilistic graphical model (PGM)  is a graph formalism for compactly modeling joint  probability distributions and dependence structures  over a set of random variables</p>
<h3 id="Inference-for-Known-Joint-Probability-Distribution"><a href="#Inference-for-Known-Joint-Probability-Distribution" class="headerlink" title="Inference for Known Joint Probability Distribution"></a>Inference for Known Joint Probability Distribution</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210925235106472.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210925235144648.png" alt=""></p>
<h2 id="Creating-a-Graphical-Model"><a href="#Creating-a-Graphical-Model" class="headerlink" title="Creating a  Graphical Model"></a>Creating a  Graphical Model</h2><h3 id="Example-Inferring-Emotion-from-Interaction-Logs"><a href="#Example-Inferring-Emotion-from-Interaction-Logs" class="headerlink" title="Example: Inferring Emotion from Interaction Logs"></a>Example: Inferring Emotion from Interaction Logs</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210925235358201.png" alt=""></p>
<h3 id="Example-Bayesian-Network-Representation"><a href="#Example-Bayesian-Network-Representation" class="headerlink" title="Example: Bayesian Network Representation"></a>Example: Bayesian Network Representation</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210925235425007.png" alt=""></p>
<h3 id="Example-Bayesian-Network-Approach"><a href="#Example-Bayesian-Network-Approach" class="headerlink" title="Example: Bayesian Network Approach"></a>Example: Bayesian Network Approach</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210925235520678.png" alt=""></p>
<h3 id="Example-Dynamic-Bayesian-Network-Approach"><a href="#Example-Dynamic-Bayesian-Network-Approach" class="headerlink" title="Example: Dynamic Bayesian Network Approach"></a>Example: Dynamic Bayesian Network Approach</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210925235548382.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210925235605578.png" alt=""></p>
<h2 id="Bayesian-Networks"><a href="#Bayesian-Networks" class="headerlink" title="Bayesian Networks"></a>Bayesian Networks</h2><p><strong>Definition</strong>: A simple, graphical notation for conditional  independence assertions and hence for compact  specification of full joint distributions</p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210925235748296.png" alt=""></p>
<h3 id="Bayesian-Network-BN"><a href="#Bayesian-Network-BN" class="headerlink" title="Bayesian Network (BN)"></a>Bayesian Network (BN)</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210925235817406.png" alt=""></p>
<h3 id="Joint-Probability-in-Graphical-Models"><a href="#Joint-Probability-in-Graphical-Models" class="headerlink" title="Joint Probability in Graphical Models"></a>Joint Probability in Graphical Models</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210925235848900.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210925235909970.png" alt=""></p>
<h3 id="Conditional-Probability-Distribution-CPD"><a href="#Conditional-Probability-Distribution-CPD" class="headerlink" title="Conditional Probability Distribution (CPD)"></a>Conditional Probability Distribution (CPD)</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210926000003390.png" alt=""></p>
<h3 id="Generative-Model-Naive-Bayes-Classifier"><a href="#Generative-Model-Naive-Bayes-Classifier" class="headerlink" title="Generative Model: Naïve Bayes Classifier"></a>Generative Model: Naïve Bayes Classifier</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210926000049051.png" alt=""></p>
<h2 id="Dynamic-Bayesian-Network"><a href="#Dynamic-Bayesian-Network" class="headerlink" title="Dynamic  Bayesian Network"></a>Dynamic  Bayesian Network</h2><ul>
<li>Bayesian network allows to represent sequential  dependencies. </li>
<li>Dynamically changing or evolving over time. </li>
<li>Directed graphical model of stochastic processes. </li>
<li>Especially aiming at time series modeling.</li>
</ul>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210926000334357.png" alt=""></p>
<h3 id="Hidden-Markov-Models"><a href="#Hidden-Markov-Models" class="headerlink" title="Hidden Markov Models"></a>Hidden Markov Models</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210926000353556.png" alt=""></p>
<h3 id="Factorial-HMM"><a href="#Factorial-HMM" class="headerlink" title="Factorial HMM"></a>Factorial HMM</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210926000506396.png" alt=""></p>
<h3 id="The-Boltzmann-Zipper"><a href="#The-Boltzmann-Zipper" class="headerlink" title="The Boltzmann Zipper"></a>The Boltzmann Zipper</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210926000532477.png" alt=""></p>
<h3 id="The-Coupled-HMM"><a href="#The-Coupled-HMM" class="headerlink" title="The Coupled HMM"></a>The Coupled HMM</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210926000554089.png" alt=""></p>
<h2 id="Generating-Data-Using-Neural-Networks"><a href="#Generating-Data-Using-Neural-Networks" class="headerlink" title="Generating Data Using  Neural Networks"></a>Generating Data Using  Neural Networks</h2><h3 id="Variational-Autoencoder"><a href="#Variational-Autoencoder" class="headerlink" title="Variational Autoencoder"></a>Variational Autoencoder</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210926000937631.png" alt=""></p>
<h3 id="Generative-Adversarial-Network-GAN"><a href="#Generative-Adversarial-Network-GAN" class="headerlink" title="Generative Adversarial Network (GAN)"></a>Generative Adversarial Network (GAN)</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210926001153760.png" alt=""></p>
<h3 id="GAN-Training"><a href="#GAN-Training" class="headerlink" title="GAN Training"></a>GAN Training</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210926001217880.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210926001327535.png" alt=""></p>
<h3 id="Example-Audio-to-Scene"><a href="#Example-Audio-to-Scene" class="headerlink" title="Example: Audio to Scene"></a>Example: Audio to Scene</h3><p><strong><a href="https://wjohn1483.github.io/audio_to_scene/index.html" target="_blank" rel="noopener">Audio to Scene Samples - wjohn1483.github.io</a></strong></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210926001402095.png" alt=""></p>
<h3 id="Example-Talking-Head"><a href="#Example-Talking-Head" class="headerlink" title="Example: Talking Head"></a>Example: Talking Head</h3><p><strong><a href="https://arxiv.org/pdf/1905.08233.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1905.08233.pdf</a></strong></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210926001502836.png" alt=""></p>
<h3 id="Bidirectional-GAN"><a href="#Bidirectional-GAN" class="headerlink" title="Bidirectional GAN"></a>Bidirectional GAN</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210926001523490.png" alt=""></p>
<h3 id="cAE-GAN"><a href="#cAE-GAN" class="headerlink" title="cAE-GAN"></a>cAE-GAN</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210926001543273.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210926001602766.png" alt=""></p>
<h3 id="Cycle-GAN"><a href="#Cycle-GAN" class="headerlink" title="Cycle GAN"></a>Cycle GAN</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210926001629810.png" alt=""></p>
<h3 id="BiCycle-GAN"><a href="#BiCycle-GAN" class="headerlink" title="BiCycle GAN"></a>BiCycle GAN</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210926001651194.png" alt=""></p>
<h1 id="Discriminative-Graphical-Models"><a href="#Discriminative-Graphical-Models" class="headerlink" title="Discriminative  Graphical Models"></a>Discriminative  Graphical Models</h1><h2 id="Quick-Recap-1"><a href="#Quick-Recap-1" class="headerlink" title="Quick Recap"></a>Quick Recap</h2><h3 id="Fusion-–-Probabilistic-Graphical-Models"><a href="#Fusion-–-Probabilistic-Graphical-Models" class="headerlink" title="Fusion – Probabilistic Graphical Models"></a>Fusion – Probabilistic Graphical Models</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210926001814077.png" alt=""></p>
<h2 id="Restricted-Boltzmann-Machines"><a href="#Restricted-Boltzmann-Machines" class="headerlink" title="Restricted  Boltzmann Machines"></a>Restricted  Boltzmann Machines</h2><h3 id="Deep-Multimodal-Boltzmann-machines-1"><a href="#Deep-Multimodal-Boltzmann-machines-1" class="headerlink" title="Deep Multimodal Boltzmann machines"></a>Deep Multimodal Boltzmann machines</h3><p><strong>Srivastava and Salakhutdinov,  Multimodal Learning with Deep  Boltzmann Machines, 2012, 2014</strong></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210926101550072.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210926101618346.png" alt=""></p>
<h3 id="Restricted-Boltzmann-Machine-RBM"><a href="#Restricted-Boltzmann-Machine-RBM" class="headerlink" title="Restricted Boltzmann Machine (RBM)"></a>Restricted Boltzmann Machine (RBM)</h3><p><strong>Smolensky, Information Processing in Dynamical Systems:  Foundations of Harmony Theory, 1986</strong></p>
<p>Undirected Graphical Model </p>
<ul>
<li>A generative rather than discriminative model </li>
<li>Connections from every hidden unit to every visible one </li>
<li>No connections across units (hence “Restricted”),  makes it easier to train and run inference</li>
</ul>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210926101810102.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210926101830184.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210926101848541.png" alt=""></p>
<h2 id="Markov-Random-Fields"><a href="#Markov-Random-Fields" class="headerlink" title="Markov Random Fields"></a>Markov Random Fields</h2><p><img src="/images/loading.gif" data-original="../images/ML/image-20210926102214335.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210926102229528.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210926102246512.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210926102305785.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210926102323944.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210926102340221.png" alt=""></p>
<h3 id="Example-Markov-Random-Field-–-Graphical-Model"><a href="#Example-Markov-Random-Field-–-Graphical-Model" class="headerlink" title="Example: Markov Random Field – Graphical Model"></a>Example: Markov Random Field – Graphical Model</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210926102427294.png" alt=""></p>
<h3 id="Example-Markov-Random-Field-–-Factor-Graph"><a href="#Example-Markov-Random-Field-–-Factor-Graph" class="headerlink" title="Example: Markov Random Field – Factor Graph"></a>Example: Markov Random Field – Factor Graph</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210926102638052.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210926102505592.png" alt=""></p>
<h2 id="Conditional-Random-Fields"><a href="#Conditional-Random-Fields" class="headerlink" title="Conditional Random Fields"></a>Conditional Random Fields</h2><h3 id="Conditional-Random-Fields-Factor-Graphs"><a href="#Conditional-Random-Fields-Factor-Graphs" class="headerlink" title="Conditional Random Fields (Factor Graphs)"></a>Conditional Random Fields (Factor Graphs)</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210926102656064.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210926102814265.png" alt=""></p>
<h3 id="Conditional-Random-Fields-Log-linear-Model"><a href="#Conditional-Random-Fields-Log-linear-Model" class="headerlink" title="Conditional Random Fields (Log-linear Model)"></a>Conditional Random Fields (Log-linear Model)</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210926102842567.png" alt=""></p>
<h3 id="Learning-Parameters-of-a-CRF-Model"><a href="#Learning-Parameters-of-a-CRF-Model" class="headerlink" title="Learning Parameters of a CRF Model"></a>Learning Parameters of a CRF Model</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210926102908412.png" alt=""></p>
<h3 id="CRFs-for-Shallow-Parsing"><a href="#CRFs-for-Shallow-Parsing" class="headerlink" title="CRFs for Shallow Parsing"></a>CRFs for Shallow Parsing</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210926102932883.png" alt=""></p>
<h3 id="Latent-Dynamic-CRF"><a href="#Latent-Dynamic-CRF" class="headerlink" title="Latent-Dynamic CRF"></a>Latent-Dynamic CRF</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210926102953709.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210926103010777.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210926103026136.png" alt=""></p>
<h3 id="Hidden-Conditional-Random-Field"><a href="#Hidden-Conditional-Random-Field" class="headerlink" title="Hidden Conditional Random Field"></a>Hidden Conditional Random Field</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210926103127993.png" alt=""></p>
<h3 id="Multi-view-Latent-Variable-Discriminative-Models"><a href="#Multi-view-Latent-Variable-Discriminative-Models" class="headerlink" title="Multi-view Latent Variable Discriminative Models"></a>Multi-view Latent Variable Discriminative Models</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210926103235939.png" alt=""></p>
<h2 id="CRFs-and-Deep-Learning"><a href="#CRFs-and-Deep-Learning" class="headerlink" title="CRFs and  Deep Learning"></a>CRFs and  Deep Learning</h2><h3 id="Conditional-Neural-Fields"><a href="#Conditional-Neural-Fields" class="headerlink" title="Conditional Neural Fields"></a>Conditional Neural Fields</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210926103316655.png" alt=""></p>
<h3 id="Deep-Conditional-Neural-Fields"><a href="#Deep-Conditional-Neural-Fields" class="headerlink" title="Deep Conditional Neural Fields"></a>Deep Conditional Neural Fields</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210926103340931.png" alt=""></p>
<h3 id="CRF-and-Bilinear-LSTM"><a href="#CRF-and-Bilinear-LSTM" class="headerlink" title="CRF and Bilinear LSTM"></a>CRF and Bilinear LSTM</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210926103404858.png" alt=""></p>
<h3 id="CNN-and-CRF-and-Bilinear-LSTM"><a href="#CNN-and-CRF-and-Bilinear-LSTM" class="headerlink" title="CNN and CRF and Bilinear LSTM"></a>CNN and CRF and Bilinear LSTM</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210926104000844.png" alt=""></p>
<h2 id="Continuous-and-Fully-Connected-CRFs"><a href="#Continuous-and-Fully-Connected-CRFs" class="headerlink" title="Continuous and  Fully-Connected CRFs"></a>Continuous and  Fully-Connected CRFs</h2><h3 id="Continuous-Conditional-Neural-Field"><a href="#Continuous-Conditional-Neural-Field" class="headerlink" title="Continuous Conditional Neural Field"></a>Continuous Conditional Neural Field</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210926104033245.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210926104047772.png" alt=""></p>
<h3 id="High-Order-Continuous-Conditional-Neural-Field"><a href="#High-Order-Continuous-Conditional-Neural-Field" class="headerlink" title="High-Order Continuous Conditional Neural Field"></a>High-Order Continuous Conditional Neural Field</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210926104120342.png" alt=""></p>
<h3 id="Fully-Connected-Continuous-Conditional-Neural-Field"><a href="#Fully-Connected-Continuous-Conditional-Neural-Field" class="headerlink" title="Fully-Connected Continuous Conditional Neural Field"></a>Fully-Connected Continuous Conditional Neural Field</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210926104146890.png" alt=""></p>
<h3 id="Fully-Connected-CRF"><a href="#Fully-Connected-CRF" class="headerlink" title="Fully-Connected CRF"></a>Fully-Connected CRF</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210926104225986.png" alt=""></p>
<h3 id="CNN-and-Fully-Connected-CRF"><a href="#CNN-and-Fully-Connected-CRF" class="headerlink" title="CNN and Fully-Connected CRF"></a>CNN and Fully-Connected CRF</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210926104246956.png" alt=""></p>
<h3 id="Fully-Connected-Deep-Structured-Networks"><a href="#Fully-Connected-Deep-Structured-Networks" class="headerlink" title="Fully Connected Deep Structured Networks"></a>Fully Connected Deep Structured Networks</h3><p>Zheng et al., 2015; Schwing and Urtasun, 2015</p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210926104332654.png" alt=""></p>
<p><strong>Sigurdsson et al., Asynchronous Temporal Fields for Action Recognition, CVPR 2017</strong></p>
<h2 id="Soft-Label-Chain-CRF"><a href="#Soft-Label-Chain-CRF" class="headerlink" title="Soft-Label Chain CRF"></a>Soft-Label Chain CRF</h2><h3 id="Phrase-Grounding-by-Soft-Label-Chain-CRF"><a href="#Phrase-Grounding-by-Soft-Label-Chain-CRF" class="headerlink" title="Phrase Grounding by Soft-Label Chain CRF"></a>Phrase Grounding by Soft-Label Chain CRF</h3><p><strong>Liu J, Hockenmaier J. “Phrase Grounding by Soft-Label Chain Conditional Random Field” EMNLP 2019</strong></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210926104500787.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210926104610769.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210926104622845.png" alt=""></p>
<h1 id="Fusion-co-learning-and-new-trends"><a href="#Fusion-co-learning-and-new-trends" class="headerlink" title="Fusion, co-learning  and new trends"></a>Fusion, co-learning  and new trends</h1><h2 id="Quick-Recap-Multimodal-Fusion"><a href="#Quick-Recap-Multimodal-Fusion" class="headerlink" title="Quick Recap: Multimodal Fusion"></a>Quick Recap: Multimodal Fusion</h2><h3 id="Multimodal-fusion"><a href="#Multimodal-fusion" class="headerlink" title="Multimodal fusion"></a>Multimodal fusion</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210926114129153.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210926114224641.png" alt=""></p>
<h3 id="Fusion-–-Probabilistic-Graphical-Models-1"><a href="#Fusion-–-Probabilistic-Graphical-Models-1" class="headerlink" title="Fusion – Probabilistic Graphical Models"></a>Fusion – Probabilistic Graphical Models</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210926114155514.png" alt=""></p>
<h2 id="Model-free-Fusion"><a href="#Model-free-Fusion" class="headerlink" title="Model-free Fusion"></a>Model-free Fusion</h2><h3 id="Model-agnostic-approaches-–-early-fusion"><a href="#Model-agnostic-approaches-–-early-fusion" class="headerlink" title="Model-agnostic approaches – early fusion"></a>Model-agnostic approaches – early fusion</h3><ul>
<li>Easy to implement – just concatenate the features </li>
<li>Exploit dependencies between features </li>
<li>Can end up very high dimensional </li>
<li>More difficult to use if features have different granularities</li>
</ul>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210926123148958.png" alt=""></p>
<h3 id="Model-agnostic-approaches-–-late-fusion"><a href="#Model-agnostic-approaches-–-late-fusion" class="headerlink" title="Model-agnostic approaches – late fusion"></a>Model-agnostic approaches – late fusion</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210926123217119.png" alt=""></p>
<h3 id="Late-Fusion-on-Multi-Layer-Unimodal-Classifiers"><a href="#Late-Fusion-on-Multi-Layer-Unimodal-Classifiers" class="headerlink" title="Late Fusion on Multi-Layer Unimodal Classifiers"></a>Late Fusion on Multi-Layer Unimodal Classifiers</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210926123253870.png" alt=""></p>
<h3 id="Multimodal-Fusion-Architecture-Search-MFAS"><a href="#Multimodal-Fusion-Architecture-Search-MFAS" class="headerlink" title="Multimodal Fusion Architecture Search (MFAS)"></a>Multimodal Fusion Architecture Search (MFAS)</h3><p><strong>“Perez-Rua, Vielzeuf, Pateux, Baccouche, Frederic Jurie,MFAS: Multimodal Fusion Architecture Search,CVPR 2019</strong></p>
<p>Proposed solution: Explore the search space with  Sequential Model-Based Optimization</p>
<ul>
<li>Start with simpler models first (all L=1 models) and  iteratively increase the complexity (L=2, L=3,…) </li>
<li>Use a surrogate function to predict performance  of unseen architectures  <ul>
<li>e.g., the performance of all the L=1 models should give  us an idea of how well the L=2 models will perform</li>
</ul>
</li>
</ul>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210926124349433.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210926124433393.png" alt=""></p>
<h3 id="Memory-Based-Fusion"><a href="#Memory-Based-Fusion" class="headerlink" title="Memory-Based Fusion"></a>Memory-Based Fusion</h3><p><strong>Zadeh et al., Memory Fusion Network for Multi-view Sequential Learning, AAAI 2018</strong></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210926124502441.png" alt=""></p>
<h2 id="Local-Fusion-and-Kernel-Functions"><a href="#Local-Fusion-and-Kernel-Functions" class="headerlink" title="Local Fusion and  Kernel Functions"></a>Local Fusion and  Kernel Functions</h2><h3 id="What-is-a-Kernel-function"><a href="#What-is-a-Kernel-function" class="headerlink" title="What is a Kernel function?"></a>What is a Kernel function?</h3><p><strong>A kernel function</strong>: Acts as a similarity metric between data  points</p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210926124619801.png" alt=""></p>
<h3 id="Non-linearly-separable-data"><a href="#Non-linearly-separable-data" class="headerlink" title="Non-linearly separable data"></a>Non-linearly separable data</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210926124836599.png" alt=""></p>
<h3 id="Radial-Basis-Function-Kernel-RBF"><a href="#Radial-Basis-Function-Kernel-RBF" class="headerlink" title="Radial Basis Function Kernel (RBF)"></a>Radial Basis Function Kernel (RBF)</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210926124924928.png" alt=""></p>
<h3 id="Some-other-kernels"><a href="#Some-other-kernels" class="headerlink" title="Some other kernels"></a>Some other kernels</h3><ul>
<li>Histogram Intersection Kernel ：good for histogram features </li>
<li>String kernels ：specifically for text and sentence features </li>
<li>Proximity distribution kernel </li>
<li>(Spatial) pyramid matching kernel</li>
</ul>
<h3 id="Kernel-CCA"><a href="#Kernel-CCA" class="headerlink" title="Kernel CCA"></a>Kernel CCA</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210926125049439.png" alt=""></p>
<h3 id="Transformer’s-Attention-Function"><a href="#Transformer’s-Attention-Function" class="headerlink" title="Transformer’s Attention Function"></a>Transformer’s Attention Function</h3><p><strong>Tsai et al., Transformer Dissection: An Unified Understanding for Transformer’s Attention via the Lens of Kernel, EMNLP 2019</strong></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210926125152844.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210926125214309.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210926125240104.png" alt=""></p>
<h3 id="Multiple-Kernel-Learning"><a href="#Multiple-Kernel-Learning" class="headerlink" title="Multiple Kernel Learning"></a>Multiple Kernel Learning</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210926125425087.png" alt=""></p>
<h3 id="MKL-in-Unimodal-Case"><a href="#MKL-in-Unimodal-Case" class="headerlink" title="MKL in Unimodal Case"></a>MKL in Unimodal Case</h3><ul>
<li>Pick a family of kernels and learn which  kernels are important for the classification  case </li>
<li>For example a set of RBF and polynomial  kernels</li>
</ul>
<h3 id="MKL-in-Multimodal-Multiview-Case"><a href="#MKL-in-Multimodal-Multiview-Case" class="headerlink" title="MKL in Multimodal/Multiview Case"></a>MKL in Multimodal/Multiview Case</h3><ul>
<li>Pick a family of kernels for  each modality and learn which  kernels are important for the  classification case </li>
<li>Does not need to be different  modalities, often we use  different views of the same  modality (HOG, SIFT, etc.)</li>
</ul>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210926125351892.png" alt=""></p>
<h2 id="Co-Learning"><a href="#Co-Learning" class="headerlink" title="Co-Learning"></a>Co-Learning</h2><h3 id="Co-Learning-The-5th-Multimodal-Challenge"><a href="#Co-Learning-The-5th-Multimodal-Challenge" class="headerlink" title="Co-Learning - The 5th Multimodal Challenge"></a>Co-Learning - The 5th Multimodal Challenge</h3><p><strong>Definition</strong>: Transfer knowledge between modalities, including their  representations and predictive models.</p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210926125535911.png" alt=""></p>
<h3 id="Co-learning-Example-with-Paired-Data"><a href="#Co-learning-Example-with-Paired-Data" class="headerlink" title="Co-learning Example with Paired Data"></a>Co-learning Example with Paired Data</h3><p><strong>ViCo: Word Embeddings from Visual Co-occurrences</strong></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210926125617969.png" alt=""></p>
<h3 id="ViCo-Word-Embeddings-from-Visual-Co-occurrences"><a href="#ViCo-Word-Embeddings-from-Visual-Co-occurrences" class="headerlink" title="ViCo: Word Embeddings from Visual Co-occurrences"></a>ViCo: Word Embeddings from Visual Co-occurrences</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210926125649455.png" alt=""></p>
<h3 id="Co-Learning-with-Paired-Data-Multimodal-Cyclic-Translation"><a href="#Co-Learning-with-Paired-Data-Multimodal-Cyclic-Translation" class="headerlink" title="Co-Learning with Paired Data:  Multimodal Cyclic Translation"></a>Co-Learning with Paired Data:  Multimodal Cyclic Translation</h3><p><strong>Paul Pu Liang<em>, Hai Pham</em>, et al., “Found in Translation: Learning Robust Joint Representations by  Cyclic Translations Between Modalities”, AAAI 2019</strong></p>
<h3 id="Co-Learning-Example-with-Weakly-Paired-Data"><a href="#Co-Learning-Example-with-Weakly-Paired-Data" class="headerlink" title="Co-Learning Example with Weakly Paired Data"></a>Co-Learning Example with Weakly Paired Data</h3><p><strong>End-to-End Learning of Visual Representations from Uncurated Instructional Videos  Antoine Miech, Jean-Baptiste Alayrac, Lucas Smaira, Ivan Laptev, Josef Sivic, and Andrew Zisserman – CVPR 2020</strong></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210926125752029.png" alt=""></p>
<h3 id="Weakly-Paired-Data"><a href="#Weakly-Paired-Data" class="headerlink" title="Weakly Paired Data"></a>Weakly Paired Data</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210926125831774.png" alt=""></p>
<h3 id="Multiple-Instance-Learning-Noise-Contrastive-Estimation"><a href="#Multiple-Instance-Learning-Noise-Contrastive-Estimation" class="headerlink" title="Multiple Instance Learning Noise Contrastive Estimation"></a>Multiple Instance Learning Noise Contrastive Estimation</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210926125858015.png" alt=""></p>
<h2 id="Research-Trend-Few-Shot-Learning-and-Weakly-Supervised"><a href="#Research-Trend-Few-Shot-Learning-and-Weakly-Supervised" class="headerlink" title="Research Trend:  Few-Shot Learning  and Weakly Supervised"></a>Research Trend:  Few-Shot Learning  and Weakly Supervised</h2><h3 id="Few-Shot-Learning-in-RL-Environment"><a href="#Few-Shot-Learning-in-RL-Environment" class="headerlink" title="Few-Shot Learning in RL Environment"></a>Few-Shot Learning in RL Environment</h3><p><strong>Hill et al., Grounded Language Learning Fast and Slow. arXiv 2020</strong></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210926125953105.png" alt=""></p>
<h3 id="Grounded-Language-Learning"><a href="#Grounded-Language-Learning" class="headerlink" title="Grounded Language Learning"></a>Grounded Language Learning</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210926130036857.png" alt=""></p>
<h3 id="Weakly-Supervised-Phrase-Grounding"><a href="#Weakly-Supervised-Phrase-Grounding" class="headerlink" title="Weakly-Supervised Phrase Grounding"></a>Weakly-Supervised Phrase Grounding</h3><p><strong>MAF: Multimodal Alignment Framework for Weakly-Supervised Phrase Grounding, EMNLP 2020</strong></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210926130109942.png" alt=""></p>
<h3 id="Multimodal-Alignment-Framework"><a href="#Multimodal-Alignment-Framework" class="headerlink" title="Multimodal Alignment Framework"></a>Multimodal Alignment Framework</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210926130134473.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210926130201833.png" alt=""></p>
<h1 id="Research-Trends-in-Multimodal-ML"><a href="#Research-Trends-in-Multimodal-ML" class="headerlink" title="Research Trends  in Multimodal ML"></a>Research Trends  in Multimodal ML</h1><ul>
<li>Abstraction and logic  </li>
<li>Multimodal reasoning </li>
<li>Towards causal inference  </li>
<li>Understanding multimodal models </li>
<li>Commonsense and coherence </li>
<li>Social impact - fairness and misinformation </li>
<li>Emotional and engaging interactions  </li>
<li>Multi-lingual multimodal grounding</li>
</ul>
<h2 id="Abstraction-and-Logic"><a href="#Abstraction-and-Logic" class="headerlink" title="Abstraction and Logic"></a>Abstraction and Logic</h2><h3 id="Learning-by-Abstraction-The-Neural-State-Machine"><a href="#Learning-by-Abstraction-The-Neural-State-Machine" class="headerlink" title="Learning by Abstraction: The Neural State Machine"></a>Learning by Abstraction: The Neural State Machine</h3><p><strong>Hudson, Drew, and Christopher D. Manning. “Learning by abstraction: The neural state machine.“ NeurIPS 2019</strong></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210926130359534.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210926130448745.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210926130506478.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210926130522337.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210926130533352.png" alt=""></p>
<h3 id="Learning-by-Abstraction-The-Neural-State-Machine-1"><a href="#Learning-by-Abstraction-The-Neural-State-Machine-1" class="headerlink" title="Learning by Abstraction: The Neural State Machine"></a>Learning by Abstraction: The Neural State Machine</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210926130609904.png" alt=""></p>
<h3 id="VQA-under-the-Lens-of-Logic"><a href="#VQA-under-the-Lens-of-Logic" class="headerlink" title="VQA under the Lens of Logic"></a>VQA under the Lens of Logic</h3><p><strong>Gokhale, Tejas, et al. “VQA-LOL: Visual question answering under the lens of logic.“, ECCV 2020</strong></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210926130647453.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210926130721403.png" alt=""></p>
<h2 id="Multimodal-Reasoning"><a href="#Multimodal-Reasoning" class="headerlink" title="Multimodal Reasoning"></a>Multimodal Reasoning</h2><h3 id="Cross-Modality-Relevance-for-Reasoning-on-Language-and-Vision"><a href="#Cross-Modality-Relevance-for-Reasoning-on-Language-and-Vision" class="headerlink" title="Cross-Modality Relevance  for Reasoning on Language and Vision"></a>Cross-Modality Relevance  for Reasoning on Language and Vision</h3><p><strong>Cross-Modality Relevance for Reasoning on Language and Vision, ACL 2020</strong></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210926130752684.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210926130828908.png" alt=""></p>
<h3 id="Multi-step-Reasoning-via-Recurrent-Dual-Attention-for-Visual-Dialog"><a href="#Multi-step-Reasoning-via-Recurrent-Dual-Attention-for-Visual-Dialog" class="headerlink" title="Multi-step Reasoning  via Recurrent Dual Attention for Visual Dialog"></a>Multi-step Reasoning  via Recurrent Dual Attention for Visual Dialog</h3><p><strong>Gan, Zhe, et al. “Multi-step reasoning via recurrent dual attention for visual dialog.“ ACL 2019</strong></p>
<ul>
<li><strong>Hypothesis</strong>: The failure of visual dialog is caused by the inherent  weakness of single-step reasoning. </li>
<li><strong>Intuition</strong>: Humans take a first glimpse of an image and a dialog  history, before revisiting specific parts of the image/text to understand  the multimodal context. </li>
<li><strong>Proposal</strong>: Apply Multi-step reasoning to visual dialog by using a  recurrent (aka multi-step) version of attention (aka reasoning). This is  done on both text and questions (aka, dual).</li>
</ul>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210926130940299.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210926130953485.png" alt=""></p>
<h2 id="Towards-Causal-Inference"><a href="#Towards-Causal-Inference" class="headerlink" title="Towards  Causal Inference"></a>Towards  Causal Inference</h2><h3 id="Visual-Dialogue-Expressed-with-Causal-Graph"><a href="#Visual-Dialogue-Expressed-with-Causal-Graph" class="headerlink" title="Visual Dialogue Expressed with Causal Graph"></a>Visual Dialogue Expressed with Causal Graph</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210926131034983.png" alt=""></p>
<h3 id="Two-Causal-Principles-for-Improving-Visual-Dialog"><a href="#Two-Causal-Principles-for-Improving-Visual-Dialog" class="headerlink" title="Two Causal Principles for Improving Visual Dialog"></a>Two Causal Principles for Improving Visual Dialog</h3><p><strong>Qi, Jiaxin, et al. “Two causal principles for improving visual dialog.“ CVPR 2020</strong></p>
<p>This paper identifies two causal principles that are holding back VisDial models. </p>
<ol>
<li>Harmful shortcut bias between dialog history (H) and the answer (A) </li>
<li>Unobserved confounder between H, Q and A leading to spurious  correlations.  </li>
</ol>
<p>By identifying and addressing these principles in a model-agnostic  manner, they are able to promote any VisDial model to SOTA levels.</p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210926131140762.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210926131157733.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210926131227950.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210926131241772.png" alt=""></p>
<h3 id="Studying-Biases-in-VQA-Models"><a href="#Studying-Biases-in-VQA-Models" class="headerlink" title="Studying Biases in VQA Models"></a>Studying Biases in VQA Models</h3><p>*<em>Agarwal, Vedika, Rakshith Shetty, and Mario Fritz. “Towards causal vqa: Revealing and reducing spurious correlations  by invariant and covariant semantic editing.” *</em></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210926131258985.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210926131338762.png" alt=""></p>
<h2 id="Understanding-Multimodal-Models"><a href="#Understanding-Multimodal-Models" class="headerlink" title="Understanding  Multimodal Models"></a>Understanding  Multimodal Models</h2><h3 id="Introspecting-VQA-Models-with-Sub-Questions"><a href="#Introspecting-VQA-Models-with-Sub-Questions" class="headerlink" title="Introspecting VQA Models with Sub-Questions"></a>Introspecting VQA Models with Sub-Questions</h3><p><strong>Selvaraju, Ramprasaath R., et al. “SQuINTing at VQA Models: Introspecting VQA Models With Sub-Questions.”, CVPR 2020</strong></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210926131429932.png" alt=""></p>
<h3 id="New-Dataset"><a href="#New-Dataset" class="headerlink" title="New Dataset"></a>New Dataset</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210926131516562.png" alt=""></p>
<h3 id="SQuINTing-Model"><a href="#SQuINTing-Model" class="headerlink" title="SQuINTing Model"></a>SQuINTing Model</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210926131536508.png" alt=""></p>
<h3 id="Training-Multimodal-Networks"><a href="#Training-Multimodal-Networks" class="headerlink" title="Training Multimodal Networks"></a>Training Multimodal Networks</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210926131601617.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210926131611669.png" alt=""></p>
<h2 id="Commonsense-and-Coherence"><a href="#Commonsense-and-Coherence" class="headerlink" title="Commonsense and Coherence"></a>Commonsense and Coherence</h2><h3 id="Emotions-are-Often-Context-Dependent"><a href="#Emotions-are-Often-Context-Dependent" class="headerlink" title="Emotions are Often Context Dependent"></a>Emotions are Often Context Dependent</h3><p><strong>“COSMIC: COmmonSense knowledge for eMotion Identification in Conversations”, Findings of EMNLP 2020</strong></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210926131645471.png" alt=""></p>
<h3 id="Commonsense-and-Emotion-Recognition"><a href="#Commonsense-and-Emotion-Recognition" class="headerlink" title="Commonsense and Emotion Recognition"></a>Commonsense and Emotion Recognition</h3><p>Proposed approach (COSMIC):  </p>
<p>​    For each utterance, try to infer </p>
<ul>
<li>speaker’s intention </li>
<li>effect on the speaker/listener </li>
<li>reaction of the speaker/listener</li>
</ul>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210926131814463.png" alt=""></p>
<h3 id="Proposed-Model-COSMIC"><a href="#Proposed-Model-COSMIC" class="headerlink" title="Proposed Model (COSMIC)"></a>Proposed Model (COSMIC)</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210926131837118.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210926131850610.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210926131906128.png" alt=""></p>
<h3 id="Coherence-and-Commonsense"><a href="#Coherence-and-Commonsense" class="headerlink" title="Coherence and Commonsense"></a>Coherence and Commonsense</h3><p>Coherence relations provide information about how the content of discourse units relate to one another. </p>
<p>They have been used to predict commonsense inference in text.</p>
<h3 id="Cross-modal-Coherence-Modeling-for-Caption-Generation"><a href="#Cross-modal-Coherence-Modeling-for-Caption-Generation" class="headerlink" title="Cross-modal Coherence Modeling  for Caption Generation"></a>Cross-modal Coherence Modeling  for Caption Generation</h3><p><strong>Cross-modal Coherence Modeling for Caption Generation ACL 2020</strong></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210926132006445.png" alt=""></p>
<h2 id="Social-Impact-–-Fairness-and-Misinformation"><a href="#Social-Impact-–-Fairness-and-Misinformation" class="headerlink" title="Social Impact – Fairness and  Misinformation"></a>Social Impact – Fairness and  Misinformation</h2><h3 id="Fair-Representation-Learning"><a href="#Fair-Representation-Learning" class="headerlink" title="Fair Representation Learning"></a>Fair Representation Learning</h3><p><strong>Pena et al., Bias in Multimodal AI: A Testbed for Fair Automatic Recruitment. ICMI 2020</strong></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210926132102573.png" alt=""></p>
<h3 id="Detecting-Cross-Modal-Inconsistency-to-Defend-Against-Neural-Fake-News"><a href="#Detecting-Cross-Modal-Inconsistency-to-Defend-Against-Neural-Fake-News" class="headerlink" title="Detecting Cross-Modal Inconsistency  to Defend Against Neural Fake News"></a>Detecting Cross-Modal Inconsistency  to Defend Against Neural Fake News</h3><p><strong>Detecting Cross-Modal Inconsistency to Defend Against Neural Fake News, EMNLP 2020</strong></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210926132142279.png" alt=""></p>
<h2 id="Emotional-and-Engaging-Interactions"><a href="#Emotional-and-Engaging-Interactions" class="headerlink" title="Emotional and  Engaging Interactions"></a>Emotional and  Engaging Interactions</h2><h3 id="Dialogue-Act-Classification-DAC"><a href="#Dialogue-Act-Classification-DAC" class="headerlink" title="Dialogue Act Classification (DAC)"></a>Dialogue Act Classification (DAC)</h3><p><strong>“Towards Emotion-aided Multi-modal Dialogue Act Classification”, ACL 2020</strong></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210926132225311.png" alt=""></p>
<h3 id="Image-Chat-Engaging-Grounded-Conversations"><a href="#Image-Chat-Engaging-Grounded-Conversations" class="headerlink" title="Image-Chat:  Engaging Grounded Conversations"></a>Image-Chat:  Engaging Grounded Conversations</h3><p><strong>Shuster, Kurt, et al. “Image-chat: Engaging grounded conversations.“ ACL 2020</strong></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210926132255235.png" alt=""></p>
<h2 id="Multi-Lingual-Multimodal-Grounding"><a href="#Multi-Lingual-Multimodal-Grounding" class="headerlink" title="Multi-Lingual Multimodal Grounding"></a>Multi-Lingual Multimodal Grounding</h2><h3 id="Multilingual-Vision-and-Language-Navigation-with-Dense-Spatiotemporal-Grounding"><a href="#Multilingual-Vision-and-Language-Navigation-with-Dense-Spatiotemporal-Grounding" class="headerlink" title="Multilingual Vision-and-Language Navigation  with Dense Spatiotemporal Grounding"></a>Multilingual Vision-and-Language Navigation  with Dense Spatiotemporal Grounding</h3><p><strong>Room-Across-Toom: Multilingual Vision-and-Language Navigation with Dense Spatiotemporal Grounding Alexander Ku, Peter Anderson, Roma Patel, Eugene Ie, and Jason Baldridge – EMNLP 2020</strong></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210926132343193.png" alt=""></p>
<h3 id="Dialog-without-Dialog-Data-Learning-Visual-Dialog-Agents-from-VQA-Data"><a href="#Dialog-without-Dialog-Data-Learning-Visual-Dialog-Agents-from-VQA-Data" class="headerlink" title="Dialog without Dialog Data:  Learning Visual Dialog Agents from VQA Data"></a>Dialog without Dialog Data:  Learning Visual Dialog Agents from VQA Data</h3><p><strong>Cogswell, Michael, et al. “Dialog without Dialog Data: Learning Visual Dialog Agents from VQA Data.”</strong></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210926132422128.png" alt=""></p>
<h1 id="Connecting-Language-to-Actions"><a href="#Connecting-Language-to-Actions" class="headerlink" title="Connecting Language to Actions"></a>Connecting Language to Actions</h1><h2 id="What-does-interaction-mean"><a href="#What-does-interaction-mean" class="headerlink" title="What does interaction mean?"></a>What does interaction mean?</h2><p><img src="/images/loading.gif" data-original="../images/ML/image-20210926132602827.png" alt=""></p>
<h2 id="Sequential-and-Online-Modeling"><a href="#Sequential-and-Online-Modeling" class="headerlink" title="Sequential and Online Modeling"></a>Sequential and Online Modeling</h2><p><img src="/images/loading.gif" data-original="../images/ML/image-20210926132649454.png" alt=""></p>
<h2 id="Planning"><a href="#Planning" class="headerlink" title="Planning"></a>Planning</h2><p><img src="/images/loading.gif" data-original="../images/ML/image-20210926132736278.png" alt=""></p>
<h2 id="V-L-gt-A"><a href="#V-L-gt-A" class="headerlink" title="V+L -> A"></a>V+L -&gt; A</h2><p><img src="/images/loading.gif" data-original="../images/ML/image-20210926132815285.png" alt=""></p>
<h2 id="First-Major-Question-Alignment"><a href="#First-Major-Question-Alignment" class="headerlink" title="First Major Question: Alignment"></a>First Major Question: Alignment</h2><p><strong>Ma et al, “Self-Monitoring Navigation Agent via Auxiliary Progress Estimation” ICLR 2019</strong></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210926132900441.png" alt=""></p>
<h3 id="Alignment"><a href="#Alignment" class="headerlink" title="Alignment"></a>Alignment</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210926133016653.png" alt=""></p>
<h3 id="Lots-of-Data"><a href="#Lots-of-Data" class="headerlink" title="Lots of Data"></a>Lots of Data</h3><p><strong>Ku et al. Room-Across-Room: Multilingual Vision-and-Language Navigation with Dense Spatiotemporal Grounding — EMNLP 2020</strong></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210926133134390.png" alt=""></p>
<h3 id="What-if-you-make-a-mistake"><a href="#What-if-you-make-a-mistake" class="headerlink" title="What if you make a mistake?"></a>What if you make a mistake?</h3><p><strong>Ke 2019, Tactical Rewind: Self-Correction via Backtracking in Vision-and-Language Navigation - CVPR 2019</strong></p>
<h2 id="Why-does-this-question-matter"><a href="#Why-does-this-question-matter" class="headerlink" title="Why does this question matter?"></a>Why does this question matter?</h2><p>Because in general, we can’t supervise everything</p>
<h2 id="Seven-High-level-Tasks"><a href="#Seven-High-level-Tasks" class="headerlink" title="Seven High-level Tasks"></a>Seven High-level Tasks</h2><p><img src="/images/loading.gif" data-original="../images/ML/image-20210926133356316.png" alt=""></p>
<h2 id="Data-collection"><a href="#Data-collection" class="headerlink" title="Data collection"></a>Data collection</h2><p><img src="/images/loading.gif" data-original="../images/ML/image-20210926133427242.png" alt=""></p>
<h2 id="End-to-End-Models"><a href="#End-to-End-Models" class="headerlink" title="End-to-End Models"></a>End-to-End Models</h2><p><img src="/images/loading.gif" data-original="../images/ML/image-20210926133534382.png" alt=""></p>
<h2 id="A-Shared-Semantic-Space"><a href="#A-Shared-Semantic-Space" class="headerlink" title="A Shared Semantic Space"></a>A Shared Semantic Space</h2><p><strong>Paxton et al. Prospection: Interpretable Plans From Language By Predicting the Future ICRA 2019</strong></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210926133636722.png" alt=""></p>
<h2 id="Predicting-the-Future"><a href="#Predicting-the-Future" class="headerlink" title="Predicting the Future"></a>Predicting the Future</h2><p><img src="/images/loading.gif" data-original="../images/ML/image-20210926133709224.png" alt=""></p>
<h2 id="Objectives"><a href="#Objectives" class="headerlink" title="Objectives"></a>Objectives</h2><p><img src="/images/loading.gif" data-original="../images/ML/image-20210926133734621.png" alt=""></p>
<h2 id="Embodiment"><a href="#Embodiment" class="headerlink" title="Embodiment"></a>Embodiment</h2><ul>
<li>Choose your own adventure — Lots of noise </li>
<li>What does it mean to succeed? </li>
<li>Where do concepts come from? </li>
<li>What’s the role of exploration? </li>
<li>Language is woefully underspecified</li>
</ul>
<h1 id="Multimodal-Human-inspired-Language-Learning"><a href="#Multimodal-Human-inspired-Language-Learning" class="headerlink" title="Multimodal Human-inspired  Language Learning"></a>Multimodal Human-inspired  Language Learning</h1><h2 id="Approach"><a href="#Approach" class="headerlink" title="Approach"></a>Approach</h2><p><img src="/images/loading.gif" data-original="../images/ML/image-20210926133935699.png" alt=""></p>
<h2 id="Workflow"><a href="#Workflow" class="headerlink" title="Workflow"></a>Workflow</h2><p><img src="/images/loading.gif" data-original="../images/ML/image-20210926133955623.png" alt=""></p>
<h2 id="Proposed-Model-Vision-language-Pre-training"><a href="#Proposed-Model-Vision-language-Pre-training" class="headerlink" title="Proposed Model: Vision-language Pre-training"></a>Proposed Model: Vision-language Pre-training</h2><p><img src="/images/loading.gif" data-original="../images/ML/image-20210926134046874.png" alt=""></p>
<h2 id="Recognition-Model-using-pre-trained-VLP"><a href="#Recognition-Model-using-pre-trained-VLP" class="headerlink" title="Recognition Model using pre-trained VLP"></a>Recognition Model using pre-trained VLP</h2><p><img src="/images/loading.gif" data-original="../images/ML/image-20210926134107902.png" alt=""></p>
<h2 id="Grounded-Multimodal-Learning"><a href="#Grounded-Multimodal-Learning" class="headerlink" title="Grounded Multimodal Learning"></a>Grounded Multimodal Learning</h2><p>Children learn in a multimodal environment. We investigate human-like learning in  the following perspectives: </p>
<p>● Association of new information to previous (past) knowledge  </p>
<p>● Generalization of learned knowledge to unseen (future) concepts </p>
<p>​    ○ Zero-shot compositionality of the learned concepts </p>
<p>​        ■ Blue + Dog -&gt; Blue dog !?</p>
<h2 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h2><p><img src="/images/loading.gif" data-original="../images/ML/image-20210926134231369.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210926134251111.png" alt=""></p>
<h2 id="New-work-in-progress-Video-Text-Coref"><a href="#New-work-in-progress-Video-Text-Coref" class="headerlink" title="New work in progress: Video-Text Coref"></a>New work in progress: Video-Text Coref</h2><p><img src="/images/loading.gif" data-original="../images/ML/image-20210926134407889.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210926134423430.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210926134431737.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210926134452005.png" alt=""></p>
<h2 id="Two-Approaches-to-Latent-Structure-Learning"><a href="#Two-Approaches-to-Latent-Structure-Learning" class="headerlink" title="Two Approaches to Latent Structure Learning"></a>Two Approaches to Latent Structure Learning</h2><p><img src="/images/loading.gif" data-original="../images/ML/image-20210926134521703.png" alt=""></p>
<h3 id="Latent-Tree-Formalism-Lexicalized-PCFG"><a href="#Latent-Tree-Formalism-Lexicalized-PCFG" class="headerlink" title="Latent Tree Formalism: Lexicalized PCFG"></a>Latent Tree Formalism: Lexicalized PCFG</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210926134542213.png" alt=""></p>
<h3 id="Probabilistic-Model-of-Lexicalized-PCFG"><a href="#Probabilistic-Model-of-Lexicalized-PCFG" class="headerlink" title="Probabilistic Model of Lexicalized PCFG"></a>Probabilistic Model of Lexicalized PCFG</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210926134640132.png" alt=""></p>
<h3 id="Latent-Tree-Learning-Baselines"><a href="#Latent-Tree-Learning-Baselines" class="headerlink" title="Latent Tree Learning: Baselines"></a>Latent Tree Learning: Baselines</h3><p>Baselines: </p>
<p>○ DMV (Klein and Manning, 2004): generative model of dependency structures. </p>
<p>○ Compound PCFG (Kim et al., 2019): neural model to parameterize probabilistic context-free  grammar using sentence-by-sentence parameters and variational training.  </p>
<p>○ Compound PCFG w/ right-headed rule: takes predictions of Compound PCFG and choose the  head of right child as the head of the parent. </p>
<p>○ ON-LSTM (Shen et al., 2019) and PRPN (Shen et al., 2018): two unsupervised constituency  parsing models </p>
<p>○ VGNSL (Shi et al., 2019): unsupervised constituency parsing model with image information</p>
<h3 id="Latent-Template-Learning-Concept"><a href="#Latent-Template-Learning-Concept" class="headerlink" title="Latent Template Learning: Concept"></a>Latent Template Learning: Concept</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210926134802565.png" alt=""></p>
<h3 id="Latent-Template-Learning-Generative-Model"><a href="#Latent-Template-Learning-Generative-Model" class="headerlink" title="Latent Template Learning: Generative Model"></a>Latent Template Learning: Generative Model</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210926134834377.png" alt=""></p>
<h3 id="Learning-of-the-Latent-Template-Model"><a href="#Learning-of-the-Latent-Template-Model" class="headerlink" title="Learning of the Latent Template Model"></a>Learning of the Latent Template Model</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210926134954431.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210926135006747.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210926135017045.png" alt=""></p>
<h3 id="Generation-Conditioned-on-Templates"><a href="#Generation-Conditioned-on-Templates" class="headerlink" title="Generation Conditioned on Templates"></a>Generation Conditioned on Templates</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210926135043849.png" alt=""></p>
<h2 id="Automatic-Speech-Recognition"><a href="#Automatic-Speech-Recognition" class="headerlink" title="Automatic Speech Recognition"></a>Automatic Speech Recognition</h2><p>EESEN: <a href="https://github.com/srvk/eesen" target="_blank" rel="noopener">https://github.com/srvk/eesen</a></p>
<ul>
<li>Pre-trained models from more established Speech  Recognition corpus, (Libri Speech and SwitchBoard in our  case) </li>
<li>3 models: ESP-Net1 , EESEEN-WFST2 , and  EESEN-rnnLM decoding, trained with CTC loss  </li>
<li>Major Challenges: <ul>
<li>fully annotated transcription not available for evaluation </li>
<li>much more noisy than the pre-trained datasets </li>
<li>multiple speakers present</li>
</ul>
</li>
</ul>
<p><strong>ESP-Net architecture  Watanabe et al. 2018</strong> </p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210926135302046.png" alt=""></p>
<h2 id="ASR-Seedling-Dataset-Samples-ESPnet-vs-EESEN"><a href="#ASR-Seedling-Dataset-Samples-ESPnet-vs-EESEN" class="headerlink" title="ASR: Seedling Dataset Samples(ESPnet vs EESEN)"></a>ASR: Seedling Dataset Samples(ESPnet vs EESEN)</h2><p>ESPnet: Hey, do you want to play anything or read a book or anything a book? Okay, which book which book you want to read? The watch one little baby who is born far away. And another who is born on the very next day. And both of these babies as everyone knows.  </p>
<p>Turn the Page. Had Ten Little Fingers ten fingers and ten little toes. There was only there was one little baby who is born in a town and another who is wrapped in either down. And both of these babies as everyone knows add ten little fingers and ten. </p>
<p>Have you any water recently? Get some water, please. Get some water please some water. Yeah water is delicious. Why don’t you have some? Give me some water, please. </p>
<p>There was one little baby who is born in the house and another who Snuffer suffered from sneezes and chills. And both of these babies with everyone knows. at ten little fingers and ten little toes just like</p>
<h2 id="Object-Detection-Methodology"><a href="#Object-Detection-Methodology" class="headerlink" title="Object Detection: Methodology"></a>Object Detection: Methodology</h2><p><img src="/images/loading.gif" data-original="../images/ML/image-20210926135442189.png" alt=""></p>
<h2 id="Multimodal-association-in-SeedlingS-Corpus"><a href="#Multimodal-association-in-SeedlingS-Corpus" class="headerlink" title="Multimodal association in SeedlingS Corpus"></a>Multimodal association in SeedlingS Corpus</h2><p><img src="/images/loading.gif" data-original="../images/ML/image-20210926135508655.png" alt=""></p>
<h1 id="Coherence-and-Grounding-in-Multimodal-Communication"><a href="#Coherence-and-Grounding-in-Multimodal-Communication" class="headerlink" title="Coherence and Grounding  in Multimodal Communication"></a>Coherence and Grounding  in Multimodal Communication</h1><h2 id="Commonsense-and-Coherence-1"><a href="#Commonsense-and-Coherence-1" class="headerlink" title="Commonsense and Coherence"></a>Commonsense and Coherence</h2><h3 id="Architecture"><a href="#Architecture" class="headerlink" title="Architecture"></a>Architecture</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210926135718479.png" alt=""></p>
<h2 id="Grounding"><a href="#Grounding" class="headerlink" title="Grounding"></a>Grounding</h2><p><img src="/images/loading.gif" data-original="../images/ML/image-20210926140309725.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210926140324481.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210926140345114.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210926141221623.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210926141306048.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210926141351294.png" alt=""></p>
<script>
        document.querySelectorAll('.github-emoji')
          .forEach(el => {
            if (!el.dataset.src) { return; }
            const img = document.createElement('img');
            img.style = 'display:none !important;';
            img.src = el.dataset.src;
            img.addEventListener('error', () => {
              img.remove();
              el.style.color = 'inherit';
              el.style.backgroundImage = 'none';
              el.style.background = 'none';
            });
            img.addEventListener('load', () => {
              img.remove();
            });
            document.body.appendChild(img);
          });
      </script>
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://jackhcc.github.io" rel="external nofollow noreferrer">杰克成</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://jackhcc.github.io/posts/Lesson-CS-Multimodal-Machine-Learning.html">https://jackhcc.github.io/posts/Lesson-CS-Multimodal-Machine-Learning.html</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="https://jackhcc.github.io" target="_blank">杰克成</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/tags/Multimodal-Machine-Learning/">
                                    <span class="chip bg-color">Multimodal Machine Learning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/libs/share/css/share.min.css">

<div id="article-share">
    
    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
                <style>
    #reward {
        margin: 40px 0;
        text-align: center;
    }

    #reward .reward-link {
        font-size: 1.4rem;
        line-height: 38px;
    }

    #reward .btn-floating:hover {
        box-shadow: 0 6px 12px rgba(0, 0, 0, 0.2), 0 5px 15px rgba(0, 0, 0, 0.2);
    }

    #rewardModal {
        width: 320px;
        height: 350px;
    }

    #rewardModal .reward-title {
        margin: 15px auto;
        padding-bottom: 5px;
    }

    #rewardModal .modal-content {
        padding: 10px;
    }

    #rewardModal .close {
        position: absolute;
        right: 15px;
        top: 15px;
        color: rgba(0, 0, 0, 0.5);
        font-size: 1.3rem;
        line-height: 20px;
        cursor: pointer;
    }

    #rewardModal .close:hover {
        color: #ef5350;
        transform: scale(1.3);
        -moz-transform:scale(1.3);
        -webkit-transform:scale(1.3);
        -o-transform:scale(1.3);
    }

    #rewardModal .reward-tabs {
        margin: 0 auto;
        width: 210px;
    }

    .reward-tabs .tabs {
        height: 38px;
        margin: 10px auto;
        padding-left: 0;
    }

    .reward-content ul {
        padding-left: 0 !important;
    }

    .reward-tabs .tabs .tab {
        height: 38px;
        line-height: 38px;
    }

    .reward-tabs .tab a {
        color: #fff;
        background-color: #ccc;
    }

    .reward-tabs .tab a:hover {
        background-color: #ccc;
        color: #fff;
    }

    .reward-tabs .wechat-tab .active {
        color: #fff !important;
        background-color: #22AB38 !important;
    }

    .reward-tabs .alipay-tab .active {
        color: #fff !important;
        background-color: #019FE8 !important;
    }

    .reward-tabs .reward-img {
        width: 210px;
        height: 210px;
    }
</style>

<div id="reward">
    <a href="#rewardModal" class="reward-link modal-trigger btn-floating btn-medium waves-effect waves-light red">赏</a>

    <!-- Modal Structure -->
    <div id="rewardModal" class="modal">
        <div class="modal-content">
            <a class="close modal-close"><i class="fas fa-times"></i></a>
            <h4 class="reward-title">你的赏识是我前进的动力</h4>
            <div class="reward-content">
                <div class="reward-tabs">
                    <ul class="tabs row">
                        <li class="tab col s6 alipay-tab waves-effect waves-light"><a href="#alipay">支付宝</a></li>
                        <li class="tab col s6 wechat-tab waves-effect waves-light"><a href="#wechat">微 信</a></li>
                    </ul>
                    <div id="alipay">
                        <img src="/images/loading.gif" data-original="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/medias/reward/aliqr.png" class="reward-img" alt="支付宝打赏二维码">
                    </div>
                    <div id="wechat">
                        <img src="/images/loading.gif" data-original="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/medias/reward/wxqr.png" class="reward-img" alt="微信打赏二维码">
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>

<script>
    $(function () {
        $('.tabs').tabs();
    });
</script>
            
        </div>
    </div>

    
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/libs/gitalk/gitalk.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/css/my-gitalk.css">

<div class="card gitalk-card" data-aos="fade-up">
    <div class="comment_headling" style="font-size: 20px; font-weight: 700; position: relative; left: 20px; top: 15px; padding-bottom: 5px;">
        <i class="fas fa-comments fa-fw" aria-hidden="true"></i>
        <span>评论</span>
    </div>
    <div id="gitalk-container" class="card-content"></div>
</div>

<script src="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/libs/gitalk/gitalk.min.js"></script>
<script>
    let gitalk = new Gitalk({
        clientID: '3821a0bbb773038a51fc',
        clientSecret: '4b30b507d67ec5497ec0e77f43f80cb3e0d7dd3a',
        repo: 'JackHCC.github.io',
        owner: 'JackHCC',
        admin: "JackHCC",
        id: '2021-09-25T09-30-03',
        distractionFreeMode: false  // Facebook-like distraction free mode
    });

    gitalk.render('gitalk-container');
</script>
    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/posts/Embedded-Microprocessor-System.html">
                    <div class="card-image">
                        
                        
                        <img src="/images/loading.gif" data-original="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/medias/featureimages/1.jpg" class="responsive-img" alt="Embedded Microprocessor System笔记">
                        
                        <span class="card-title">Embedded Microprocessor System笔记</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            嵌入式微处理系统学习笔记
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2021-09-26
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/Embedded/" class="post-category">
                                    Embedded
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/Microprocessor/">
                        <span class="chip bg-color">Microprocessor</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/posts/blog-python19.html">
                    <div class="card-image">
                        
                        
                        <img src="/images/loading.gif" data-original="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/medias/featureimages/16.jpg" class="responsive-img" alt="Python-Feather文件操作">
                        
                        <span class="card-title">Python-Feather文件操作</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Feather文件格式详解
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2021-09-24
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/Python/" class="post-category">
                                    Python
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/Feather/">
                        <span class="chip bg-color">Feather</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/libs/codeBlock/codeBlockFuction.js"></script>

<!-- 代码语言 -->

<script type="text/javascript" src="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/libs/codeBlock/codeShrink.js"></script>


<!-- 代码块折行 -->

<style type="text/css">
code[class*="language-"], pre[class*="language-"] { white-space: pre !important; }
</style>


    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('4'),
            headingSelector: 'h1, h2, h3, h4'
        });

        // modify the toc link href to support Chinese.
        let i = 0;
        let tocHeading = 'toc-heading-';
        $('#toc-content a').each(function () {
            $(this).attr('href', '#' + tocHeading + (++i));
        });

        // modify the heading title id to support Chinese.
        i = 0;
        $('#articleContent').children('h1, h2, h3, h4').each(function () {
            $(this).attr('id', tocHeading + (++i));
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>

    

</main>



    <footer class="page-footer bg-color">
    <div class="container row center-align" style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            <span id="year">2020</span>
            <a href="https://jackhcc.github.io" target="_blank">杰克成</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            <br>
            
            &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                class="white-color">3591.2k</span>&nbsp;字
            
            
            
            
            
            
            <span id="busuanzi_container_site_pv">
                |&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;<span id="busuanzi_value_site_pv"
                    class="white-color"></span>&nbsp;次
            </span>
            
            
            <span id="busuanzi_container_site_uv">
                |&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;<span id="busuanzi_value_site_uv"
                    class="white-color"></span>&nbsp;人
            </span>
            
            <br>
            
            <span id="sitetime">载入运行时间...</span>
            <script>
                function siteTime() {
                    var seconds = 1000;
                    var minutes = seconds * 60;
                    var hours = minutes * 60;
                    var days = hours * 24;
                    var years = days * 365;
                    var today = new Date();
                    var startYear = "2020";
                    var startMonth = "2";
                    var startDate = "27";
                    var startHour = "6";
                    var startMinute = "30";
                    var startSecond = "0";
                    var todayYear = today.getFullYear();
                    var todayMonth = today.getMonth() + 1;
                    var todayDate = today.getDate();
                    var todayHour = today.getHours();
                    var todayMinute = today.getMinutes();
                    var todaySecond = today.getSeconds();
                    var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                    var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                    var diff = t2 - t1;
                    var diffYears = Math.floor(diff / years);
                    var diffDays = Math.floor((diff / days) - diffYears * 365);
                    var diffHours = Math.floor((diff - (diffYears * 365 + diffDays) * days) / hours);
                    var diffMinutes = Math.floor((diff - (diffYears * 365 + diffDays) * days - diffHours * hours) /
                        minutes);
                    var diffSeconds = Math.floor((diff - (diffYears * 365 + diffDays) * days - diffHours * hours -
                        diffMinutes * minutes) / seconds);
                    if (startYear == todayYear) {
                        document.getElementById("year").innerHTML = todayYear;
                        document.getElementById("sitetime").innerHTML = "本站已安全运行 " + diffDays + " 天 " + diffHours +
                            " 小时 " + diffMinutes + " 分钟 " + diffSeconds + " 秒";
                    } else {
                        document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                        document.getElementById("sitetime").innerHTML = "本站已安全运行 " + diffYears + " 年 " + diffDays +
                            " 天 " + diffHours + " 小时 " + diffMinutes + " 分钟 " + diffSeconds + " 秒";
                    }
                }
                setInterval(siteTime, 1000);
            </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/JackHCC" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:jackcc0701@163.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>



    <a href="https://www.facebook.com/profile.php?id=100046343443643" class="tooltipped" target="_blank" data-tooltip="关注我的Facebook: https://www.facebook.com/profile.php?id=100046343443643" data-position="top" data-delay="50">
        <i class="fab fa-facebook-f"></i>
    </a>



    <a href="https://twitter.com/JackChe66021834" class="tooltipped" target="_blank" data-tooltip="关注我的Twitter: https://twitter.com/JackChe66021834" data-position="top" data-delay="50">
        <i class="fab fa-twitter"></i>
    </a>



    <a href="tencent://AddContact/?fromId=50&fromSubId=1&subcmd=all&uin=2508074836" class="tooltipped" target="_blank" data-tooltip="QQ联系我: 2508074836" data-position="top" data-delay="50">
        <i class="fab fa-qq"></i>
    </a>



    <a href="https://weibo.com/u/6885584679" class="tooltipped" target="_blank" data-tooltip="关注我的微博: https://weibo.com/u/6885584679" data-position="top" data-delay="50">
        <i class="fab fa-weibo"></i>
    </a>



    <a href="https://www.zhihu.com/people/8f8482f01f0d6a04e844efe32e0f0710" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/8f8482f01f0d6a04e844efe32e0f0710" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script src="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/js/search.js"></script>
<script type="text/javascript">
$(function () {
    searchFunc("https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/" + "search.xml", 'searchInput', 'searchResult');
});
</script>
    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/libs/materialize/materialize.min.js"></script>
    <script src="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/libs/aos/aos.js"></script>
    <script src="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/js/matery.js"></script>

    <canvas class="fireworks" style="position: fixed;left: 0;top: 0;z-index: 1; pointer-events: none;" ></canvas>
    <script type="text/javascript" src="//cdn.bootcss.com/animejs/2.2.0/anime.min.js"></script>
    <script type="text/javascript" src="/js/fireworks.js"></script>

    <script type="text/javascript">
        //只在桌面版网页启用特效
        var windowWidth = $(window).width();
        if (windowWidth > 768) {
            document.write('<script type="text/javascript" src="/js/sakura.js"><\/script>'); }
    </script>

    <!-- weather -->
	<script type="text/javascript">
	WIDGET = {FID: 'TToslpmkVO'}
	</script>
	<script type="text/javascript" src="https://apip.weatherdt.com/float/static/js/r.js?v=1111"></script>


    <!-- Global site tag (gtag.js) - Google Analytics -->


    <!-- Baidu Analytics -->

<script>
    var _hmt = _hmt || [];
    (function () {
        var hm = document.createElement("script");
        hm.src = "https://hm.baidu.com/hm.js?";
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(hm, s);
    })();
</script>

    <!-- Baidu Push -->

    
    
    <script async src="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/libs/others/busuanzi.pure.mini.js"></script>
    

    
        <script src="//code.tidio.co/kqhlkxviiccyoa0czpfpu4ijuey9hfre.js"></script>
        <script> 
            $(document).ready(function () {
                setInterval(change_Tidio, 50);  
                function change_Tidio() { 
                    var tidio=$("#tidio-chat iframe");
                    if(tidio.css("display")=="block"&& $(window).width()>977 ){
                        document.getElementById("tidio-chat-iframe").style.bottom= ($("div#backTop.top-scroll").css("display")=="none" &&$(window).width()>977)>0? "-40px" : ($("div.toc-title").length&&$(window).width()>977)>0?"85px":"20px";   
                        document.getElementById("tidio-chat-iframe").style.right="-15px";   
                        document.getElementById("tidio-chat-iframe").style.height=parseInt(tidio.css("height"))>=520?"520px":tidio.css("height");
                        document.getElementById("tidio-chat-iframe").style.zIndex="997";
                    } 
                    else if(tidio.css("display")=="block"&&$(window).width()>601 &&$(window).width()<992 ){
                        document.getElementById("tidio-chat-iframe").style.bottom= ($("div#backTop.top-scroll").css("display")=="none" && 601< $(window).width()<992)>0? "-40px":"20px" ;   
                        document.getElementById("tidio-chat-iframe").style.right="-15px"; 
                        document.getElementById("tidio-chat-iframe").style.zIndex="997";
                    }
                    else if(tidio.css("display")=="block"&&$(window).width()<601 && parseInt(tidio.css("height"))<230){
                        document.getElementById("tidio-chat-iframe").style.bottom= ($("div#backTop.top-scroll").css("display")=="none" && $(window).width()<601)>0? "-10px":"45px" ;   
                        document.getElementById("tidio-chat-iframe").style.zIndex="997";
                    }
                    if( tidio.css("display")=="block"&&$(window).width()<601 && parseInt(tidio.css("height"))>=230){
                        document.getElementById("tidio-chat-iframe").style.zIndex="998";
                    }
                } 
            }); 
        </script>
    

    

    
    <script type="text/javascript" color="0,0,255"
        pointColor="0,0,255" opacity='0.7'
        zIndex="-1" count="99"
        src="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/libs/background/canvas-nest.js"></script>
    

    

    
    <script type="text/javascript" src="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/libs/background/ribbon-dynamic.js" async="async"></script>
    
    
    
    <script src="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/libs/instantpage/instantpage.js" type="module"></script>
    

        <script src="//cdn.jsdelivr.net/npm/js-base64/base64.min.js"></script>
        <script>
        $('a').each(function() {
          const $this = $(this);
          const href = $this.attr('href');
          if (href && href.match('^((http|https|thunder|qqdl|ed2k|Flashget|qbrowser|ftp|rtsp|mms)://)')) {
            const strs = href.split('/');
            if (strs.length >= 3) {
                const host = strs[2];
                if (host !== 'your_domain' || window.location.host) {
                    $this.attr('href', '/go.html?u='+Base64.encode(href)+'').attr('rel', 'external nofollow noopener noreferrer');
                    if (true) {
                        $this.attr('target', '_blank');
                    }
                }
            }
          }
        });
        </script><script>!function(e){var c=Array.prototype.slice.call(document.querySelectorAll("img[data-original]"));function i(){for(var r=0;r<c.length;r++)t=c[r],0<=(n=t.getBoundingClientRect()).bottom&&0<=n.left&&n.top<=(e.innerHeight||document.documentElement.clientHeight)&&function(){var t,n,e,i,o=c[r];t=o,n=function(){c=c.filter(function(t){return o!==t})},e=new Image,i=t.getAttribute("data-original"),e.onload=function(){t.src=i,n&&n()},e.src=i}();var t,n}i(),e.addEventListener("scroll",function(){var t,n;t=i,n=e,clearTimeout(t.tId),t.tId=setTimeout(function(){t.call(n)},500)})}(this);</script><script>window.addEventListener("load",function(){var t=/\.(gif|jpg|jpeg|tiff|png)$/i,r=/^data:image\/[a-z]+;base64,/;Array.prototype.slice.call(document.querySelectorAll("img[data-original]")).forEach(function(a){var e=a.parentNode;"A"===e.tagName&&(e.href.match(t)||e.href.match(r))&&(e.href=a.dataset.original)})});</script></body>

</html>

