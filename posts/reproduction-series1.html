<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="复现专栏1&gt;Transformer复现, JackHCC">
    <meta name="description" content="手撸Transformer">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <title>复现专栏1&gt;Transformer复现 | JackHCC</title>
    <link rel="icon" type="image/png" href="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/medias/favicon.png">

    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/libs/awesome/css/all.css">
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/libs/materialize/materialize.min.css">
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/libs/aos/aos.css">
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/libs/animate/animate.min.css">
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/libs/lightGallery/css/lightgallery.min.css">
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/css/matery.css">
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/css/my.css">
    
    <script src="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/libs/jquery/jquery.min.js"></script>
    
<meta name="generator" content="Hexo 4.2.0"><link rel="alternate" href="/atom.xml" title="JackHCC" type="application/atom+xml">
<link rel="stylesheet" href="/css/prism-hopscotch.css" type="text/css">
<link rel="stylesheet" href="/css/prism-line-numbers.css" type="text/css"></head>


<body>
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper head-container">
            <div class="brand-logo">
                <a href="/" class="waves-effect waves-light">
                    
                    <img src="/images/loading.gif" data-original="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/medias/me.jpg" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">JackHCC</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="" class="waves-effect waves-light">

      
      <i class="fas fa-list" style="zoom: 0.6;"></i>
      
      <span>Tools</span>
      <i class="fas fa-chevron-down" aria-hidden="true" style="zoom: 0.6;"></i>
    </a>
    <ul class="sub-nav menus_item_child ">
      
      <li>
        <a href="https://creativecc.cn/" target="_blank" rel="noopener">
          
          <i class="fas fa-book" style="margin-top: -20px; zoom: 0.6;"></i>
          
          <span>Creative工具导航</span>
        </a>
      </li>
      
      <li>
        <a href="https://blog.creativecc.cn/Arxiv-NLP-Reporter/" target="_blank" rel="noopener">
          
          <i class="fas fa-film" style="margin-top: -20px; zoom: 0.6;"></i>
          
          <span>NLP每日论文</span>
        </a>
      </li>
      
      <li>
        <a href="http://chat.creativecc.cn/" target="_blank" rel="noopener">
          
          <i class="fas fa-music" style="margin-top: -20px; zoom: 0.6;"></i>
          
          <span>RocketChat聊天室</span>
        </a>
      </li>
      
      <li>
        <a href="/contact">
          
          <i class="fas fa-comments" style="margin-top: -20px; zoom: 0.6;"></i>
          
          <span>Contact留言板</span>
        </a>
      </li>
      
    </ul>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/friends" class="waves-effect waves-light">
      
      <i class="fas fa-address-book" style="zoom: 0.6;"></i>
      
      <span>友情链接</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/about" class="waves-effect waves-light">
      
      <i class="fas fa-user-circle" style="zoom: 0.6;"></i>
      
      <span>关于</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>

<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/images/loading.gif" data-original="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/medias/me.jpg" class="logo-img circle responsive-img">
        
        <div class="logo-name">JackHCC</div>
        <div class="logo-desc">
            
            Make the world betterrrr!!!
            
        </div>
    </div>

    

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="javascript:;">
			
				<i class="fa-fw fas fa-list"></i>
			
			Tools
			<span class="m-icon"><i class="fas fa-chevron-right"></i></span>
		</a>
            <ul  style="background:  ;" >
              
                <li>   
				
                  <a href="https://creativecc.cn/ " target="_blank" rel="noopener" style="margin-left:75px";>
				  
				   <i class="fa fas fa-book" style="position: absolute;left:50px" ></i>
			      
		          <span>Creative工具导航</span>
                  </a>
                </li>
              
                <li>   
				
                  <a href="https://blog.creativecc.cn/Arxiv-NLP-Reporter/ " target="_blank" rel="noopener" style="margin-left:75px";>
				  
				   <i class="fa fas fa-film" style="position: absolute;left:50px" ></i>
			      
		          <span>NLP每日论文</span>
                  </a>
                </li>
              
                <li>   
				
                  <a href="http://chat.creativecc.cn/ " target="_blank" rel="noopener" style="margin-left:75px";>
				  
				   <i class="fa fas fa-music" style="position: absolute;left:50px" ></i>
			      
		          <span>RocketChat聊天室</span>
                  </a>
                </li>
              
                <li>   
				
                  <a href="/contact " style="margin-left:75px";>
				  
				   <i class="fa fas fa-comments" style="position: absolute;left:50px" ></i>
			      
		          <span>Contact留言板</span>
                  </a>
                </li>
               
            </ul>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/friends" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-address-book"></i>
			
			友情链接
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/about" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-user-circle"></i>
			
			关于
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/JackHCC/JackHCC.github.io" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>

        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/JackHCC/JackHCC.github.io" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    
<script src="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/libs/cryptojs/crypto-js.min.js"></script>
<script>
    (function() {
        let pwd = '';
        if (pwd && pwd.length > 0) {
            if (pwd !== CryptoJS.SHA256(prompt('请输入访问本文章的密码')).toString(CryptoJS.enc.Hex)) {
                alert('密码错误，将返回主页！');
                location.href = '/';
            }
        }
    })();
</script>




<div class="bg-cover pd-header post-cover" style="background-image: url('https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/medias/featureimages/16.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">复现专栏1&gt;Transformer复现</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/libs/tocbot/tocbot.css">
<style>
    #articleContent h1::before,
    #articleContent h2::before,
    #articleContent h3::before,
    #articleContent h4::before,
    #articleContent h5::before,
    #articleContent h6::before {
        display: block;
        content: " ";
        height: 100px;
        margin-top: -100px;
        visibility: hidden;
    }

    #articleContent :focus {
        outline: none;
    }

    .toc-fixed {
        position: fixed;
        top: 64px;
    }

    .toc-widget {
        width: 345px;
        padding-left: 20px;
    }

    .toc-widget .toc-title {
        margin: 35px 0 15px 0;
        padding-left: 17px;
        font-size: 1.5rem;
        font-weight: bold;
        line-height: 1.5rem;
    }

    .toc-widget ol {
        padding: 0;
        list-style: none;
    }

    #toc-content {
        height: calc(100vh - 250px);
        overflow: auto;
    }

    #toc-content ol {
        padding-left: 10px;
    }

    #toc-content ol li {
        padding-left: 10px;
    }

    #toc-content .toc-link:hover {
        color: #42b983;
        font-weight: 700;
        text-decoration: underline;
    }

    #toc-content .toc-link::before {
        background-color: transparent;
        max-height: 25px;
    }

    #toc-content .is-active-link {
        color: #42b983;
    }

    #toc-content .is-active-link::before {
        background-color: #42b983;
    }

    #floating-toc-btn {
        position: fixed;
        right: 30px;
        bottom: 146px;
        padding-top: 15px;
        margin-bottom: 0;
        z-index: 998;
    }

    #floating-toc-btn .btn-floating {
        width: 48px;
        height: 48px;
    }

    #floating-toc-btn .btn-floating i {
        line-height: 48px;
        font-size: 1.4rem;
    }
</style>
<div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/tags/Transformer/">
                                <span class="chip bg-color">Transformer</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/categories/Deep-Learning/" class="post-category">
                                Deep Learning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2021-10-28
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2021-11-10
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    7.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    34 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>

        </div>
        <hr class="clearfix">
        <div class="card-content article-card-content">
            <div id="articleContent">
                <h1 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h1><h2 id="1-数据准备"><a href="#1-数据准备" class="headerlink" title="1. 数据准备"></a><strong>1. 数据准备</strong></h2><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> torch
<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>optim <span class="token keyword">as</span> optim
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data <span class="token keyword">as</span> Data

            <span class="token comment" spellcheck="true"># Encoder_input    Decoder_input        Decoder_output</span>
sentences <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token string">'我 是 学 生 P'</span><span class="token punctuation">,</span> <span class="token string">'S I am a student'</span><span class="token punctuation">,</span> <span class="token string">'I am a student E'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>  <span class="token comment" spellcheck="true"># S: 开始符号</span>
             <span class="token punctuation">[</span><span class="token string">'我 喜 欢 学 习'</span><span class="token punctuation">,</span> <span class="token string">'S I like learning P'</span><span class="token punctuation">,</span> <span class="token string">'I like learning P E'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>  <span class="token comment" spellcheck="true"># E: 结束符号</span>
             <span class="token punctuation">[</span><span class="token string">'我 是 男 生 P'</span><span class="token punctuation">,</span> <span class="token string">'S I am a boy'</span><span class="token punctuation">,</span> <span class="token string">'I am a boy E'</span><span class="token punctuation">]</span><span class="token punctuation">]</span>  <span class="token comment" spellcheck="true"># P: 占位符号，如果当前句子不足固定长度用P占位</span>

src_vocab <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token string">'P'</span><span class="token punctuation">:</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token string">'我'</span><span class="token punctuation">:</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token string">'是'</span><span class="token punctuation">:</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token string">'学'</span><span class="token punctuation">:</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token string">'生'</span><span class="token punctuation">:</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token string">'喜'</span><span class="token punctuation">:</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token string">'欢'</span><span class="token punctuation">:</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token string">'习'</span><span class="token punctuation">:</span> <span class="token number">7</span><span class="token punctuation">,</span> <span class="token string">'男'</span><span class="token punctuation">:</span> <span class="token number">8</span><span class="token punctuation">}</span>  <span class="token comment" spellcheck="true"># 词源字典  字：索引</span>
src_idx2word <span class="token operator">=</span> <span class="token punctuation">{</span>src_vocab<span class="token punctuation">[</span>key<span class="token punctuation">]</span><span class="token punctuation">:</span> key <span class="token keyword">for</span> key <span class="token keyword">in</span> src_vocab<span class="token punctuation">}</span>
src_vocab_size <span class="token operator">=</span> len<span class="token punctuation">(</span>src_vocab<span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 字典字的个数</span>
tgt_vocab <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token string">'S'</span><span class="token punctuation">:</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token string">'E'</span><span class="token punctuation">:</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token string">'P'</span><span class="token punctuation">:</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token string">'I'</span><span class="token punctuation">:</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token string">'am'</span><span class="token punctuation">:</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token string">'a'</span><span class="token punctuation">:</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token string">'student'</span><span class="token punctuation">:</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token string">'like'</span><span class="token punctuation">:</span> <span class="token number">7</span><span class="token punctuation">,</span> <span class="token string">'learning'</span><span class="token punctuation">:</span> <span class="token number">8</span><span class="token punctuation">,</span> <span class="token string">'boy'</span><span class="token punctuation">:</span> <span class="token number">9</span><span class="token punctuation">}</span>
idx2word <span class="token operator">=</span> <span class="token punctuation">{</span>tgt_vocab<span class="token punctuation">[</span>key<span class="token punctuation">]</span><span class="token punctuation">:</span> key <span class="token keyword">for</span> key <span class="token keyword">in</span> tgt_vocab<span class="token punctuation">}</span>  <span class="token comment" spellcheck="true"># 把目标字典转换成 索引：字的形式</span>
tgt_vocab_size <span class="token operator">=</span> len<span class="token punctuation">(</span>tgt_vocab<span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 目标字典尺寸</span>
src_len <span class="token operator">=</span> len<span class="token punctuation">(</span>sentences<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">" "</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># Encoder输入的最大长度</span>
tgt_len <span class="token operator">=</span> len<span class="token punctuation">(</span>sentences<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">" "</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># Decoder输入输出最大长度</span>


<span class="token comment" spellcheck="true"># 把sentences 转换成字典索引</span>
<span class="token keyword">def</span> <span class="token function">make_data</span><span class="token punctuation">(</span>sentences<span class="token punctuation">)</span><span class="token punctuation">:</span>
    enc_inputs<span class="token punctuation">,</span> dec_inputs<span class="token punctuation">,</span> dec_outputs <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
    <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>len<span class="token punctuation">(</span>sentences<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        enc_input <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">[</span>src_vocab<span class="token punctuation">[</span>n<span class="token punctuation">]</span> <span class="token keyword">for</span> n <span class="token keyword">in</span> sentences<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">]</span>
        dec_input <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">[</span>tgt_vocab<span class="token punctuation">[</span>n<span class="token punctuation">]</span> <span class="token keyword">for</span> n <span class="token keyword">in</span> sentences<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">]</span>
        dec_output <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">[</span>tgt_vocab<span class="token punctuation">[</span>n<span class="token punctuation">]</span> <span class="token keyword">for</span> n <span class="token keyword">in</span> sentences<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">]</span>
        enc_inputs<span class="token punctuation">.</span>extend<span class="token punctuation">(</span>enc_input<span class="token punctuation">)</span>
        dec_inputs<span class="token punctuation">.</span>extend<span class="token punctuation">(</span>dec_input<span class="token punctuation">)</span>
        dec_outputs<span class="token punctuation">.</span>extend<span class="token punctuation">(</span>dec_output<span class="token punctuation">)</span>
    <span class="token keyword">return</span> torch<span class="token punctuation">.</span>LongTensor<span class="token punctuation">(</span>enc_inputs<span class="token punctuation">)</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>LongTensor<span class="token punctuation">(</span>dec_inputs<span class="token punctuation">)</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>LongTensor<span class="token punctuation">(</span>dec_outputs<span class="token punctuation">)</span>

enc_inputs<span class="token punctuation">,</span> dec_inputs<span class="token punctuation">,</span> dec_outputs <span class="token operator">=</span> make_data<span class="token punctuation">(</span>sentences<span class="token punctuation">)</span>

<span class="token comment" spellcheck="true"># print(enc_inputs, enc_inputs.shape)</span>

<span class="token comment" spellcheck="true"># 自定义数据集函数</span>
<span class="token keyword">class</span> <span class="token class-name">MyDataSet</span><span class="token punctuation">(</span>Data<span class="token punctuation">.</span>Dataset<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> enc_inputs<span class="token punctuation">,</span> dec_inputs<span class="token punctuation">,</span> dec_outputs<span class="token punctuation">)</span><span class="token punctuation">:</span>
        super<span class="token punctuation">(</span>MyDataSet<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>enc_inputs <span class="token operator">=</span> enc_inputs
        self<span class="token punctuation">.</span>dec_inputs <span class="token operator">=</span> dec_inputs
        self<span class="token punctuation">.</span>dec_outputs <span class="token operator">=</span> dec_outputs

    <span class="token keyword">def</span> <span class="token function">__len__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>enc_inputs<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>

    <span class="token keyword">def</span> <span class="token function">__getitem__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> idx<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>enc_inputs<span class="token punctuation">[</span>idx<span class="token punctuation">]</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>dec_inputs<span class="token punctuation">[</span>idx<span class="token punctuation">]</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>dec_outputs<span class="token punctuation">[</span>idx<span class="token punctuation">]</span>


loader <span class="token operator">=</span> Data<span class="token punctuation">.</span>DataLoader<span class="token punctuation">(</span>MyDataSet<span class="token punctuation">(</span>enc_inputs<span class="token punctuation">,</span> dec_inputs<span class="token punctuation">,</span> dec_outputs<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token boolean">True</span><span class="token punctuation">)</span>

<span class="token comment" spellcheck="true"># 参数设置</span>
d_model <span class="token operator">=</span> <span class="token number">512</span>   <span class="token comment" spellcheck="true"># 字 Embedding 的维度</span>
d_ff <span class="token operator">=</span> <span class="token number">2048</span>     <span class="token comment" spellcheck="true"># 前向传播隐藏层维度 d_ff = d_model * 4</span>
d_k <span class="token operator">=</span> d_v <span class="token operator">=</span> <span class="token number">64</span>  <span class="token comment" spellcheck="true"># K(=Q), V的维度</span>
n_layers <span class="token operator">=</span> <span class="token number">6</span>    <span class="token comment" spellcheck="true"># 有多少个encoder和decoder</span>
n_heads <span class="token operator">=</span> <span class="token number">8</span>     <span class="token comment" spellcheck="true"># Multi-Head Attention设置为8  d_k * n_heads = d_model</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>sentences 里一共有三个训练数据，中文-&gt;英文。把Encoder_input、Decoder_input、Decoder_output转换成字典索引，例如”学”-&gt;3、”student”-&gt;6。再把数据转换成batch大小为2的分组数据，3句话一共可以分成两组，一组2句话、一组1句话。src_len表示中文句子固定最大长度，tgt_len 表示英文句子固定最大长度。</p>
<h2 id="2-定义位置信息"><a href="#2-定义位置信息" class="headerlink" title="2. 定义位置信息"></a>2. 定义位置信息</h2><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># Positional Embedding</span>
<span class="token keyword">class</span> <span class="token class-name">PositionalEncoding</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> d_model<span class="token punctuation">,</span> dropout<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">,</span> max_len<span class="token operator">=</span><span class="token number">5000</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        super<span class="token punctuation">(</span>PositionalEncoding<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>dropout <span class="token operator">=</span> nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>p<span class="token operator">=</span>dropout<span class="token punctuation">)</span>
        <span class="token comment" spellcheck="true"># pos_table 第一个维度对应第几个词 pos，第二个维度对应每个词的词嵌入维度 i，[max_len, d_model]</span>
        pos_table <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span>
        <span class="token punctuation">[</span>pos <span class="token operator">/</span> np<span class="token punctuation">.</span>power<span class="token punctuation">(</span><span class="token number">10000</span><span class="token punctuation">,</span> <span class="token number">2</span> <span class="token operator">*</span> i <span class="token operator">/</span> d_model<span class="token punctuation">)</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>d_model<span class="token punctuation">)</span><span class="token punctuation">]</span>    <span class="token comment" spellcheck="true"># i 针对词嵌入的维度</span>
        <span class="token keyword">if</span> pos <span class="token operator">!=</span> <span class="token number">0</span> <span class="token keyword">else</span> np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>d_model<span class="token punctuation">)</span> <span class="token keyword">for</span> pos <span class="token keyword">in</span> range<span class="token punctuation">(</span>max_len<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>      <span class="token comment" spellcheck="true"># pos 针对第几个词</span>
        pos_table<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">:</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span> <span class="token operator">=</span> np<span class="token punctuation">.</span>sin<span class="token punctuation">(</span>pos_table<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">:</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span>                   <span class="token comment" spellcheck="true"># 字嵌入维度为偶数时</span>
        pos_table<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span> <span class="token operator">=</span> np<span class="token punctuation">.</span>cos<span class="token punctuation">(</span>pos_table<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span>                   <span class="token comment" spellcheck="true"># 字嵌入维度为奇数时</span>
        self<span class="token punctuation">.</span>pos_table <span class="token operator">=</span> torch<span class="token punctuation">.</span>FloatTensor<span class="token punctuation">(</span>pos_table<span class="token punctuation">)</span><span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span>                <span class="token comment" spellcheck="true"># enc_inputs: [seq_len, d_model]</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> enc_inputs<span class="token punctuation">)</span><span class="token punctuation">:</span>                                          <span class="token comment" spellcheck="true"># enc_inputs: [batch_size, seq_len, d_model]</span>
        enc_inputs <span class="token operator">+=</span> self<span class="token punctuation">.</span>pos_table<span class="token punctuation">[</span><span class="token punctuation">:</span>enc_inputs<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span>                <span class="token comment" spellcheck="true"># 注意，tensor的size(n)表示第n的维度大小，与numpy的array.size不一样，np只是个数值</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>dropout<span class="token punctuation">(</span>enc_inputs<span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>生成位置信息矩阵pos_table，直接加上输入的enc_inputs上，得到带有位置信息的字向量，pos_table是一个固定值的矩阵。这里矩阵加法利用到了广播机制.</p>
<h2 id="3-Mask掉停用词"><a href="#3-Mask掉停用词" class="headerlink" title="3. Mask掉停用词"></a><strong>3. Mask掉停用词</strong></h2><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># Mask掉停用词</span>
<span class="token keyword">def</span> <span class="token function">get_attn_pad_mask</span><span class="token punctuation">(</span>seq_q<span class="token punctuation">,</span> seq_k<span class="token punctuation">)</span><span class="token punctuation">:</span>                       <span class="token comment" spellcheck="true"># seq_q: [batch_size, seq_len] ,seq_k: [batch_size, seq_len]</span>
    batch_size<span class="token punctuation">,</span> len_q <span class="token operator">=</span> seq_q<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span>
    batch_size<span class="token punctuation">,</span> len_k <span class="token operator">=</span> seq_k<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span>
    pad_attn_mask <span class="token operator">=</span> seq_k<span class="token punctuation">.</span>data<span class="token punctuation">.</span>eq<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>          <span class="token comment" spellcheck="true"># 判断 输入那些含有P(=0),用1标记 ,[batch_size, 1, len_k]</span>
    <span class="token keyword">return</span> pad_attn_mask<span class="token punctuation">.</span>expand<span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span> len_q<span class="token punctuation">,</span> len_k<span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 扩展成多维度, [batch_size, len_q, len_k]</span>


<span class="token comment" spellcheck="true"># Decoder 输入 Mask</span>
<span class="token keyword">def</span> <span class="token function">get_attn_subsequence_mask</span><span class="token punctuation">(</span>seq<span class="token punctuation">)</span><span class="token punctuation">:</span>                               <span class="token comment" spellcheck="true"># seq: [batch_size, tgt_len]</span>
    attn_shape <span class="token operator">=</span> <span class="token punctuation">[</span>seq<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> seq<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> seq<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">]</span>
    subsequence_mask <span class="token operator">=</span> np<span class="token punctuation">.</span>triu<span class="token punctuation">(</span>np<span class="token punctuation">.</span>ones<span class="token punctuation">(</span>attn_shape<span class="token punctuation">)</span><span class="token punctuation">,</span> k<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>          <span class="token comment" spellcheck="true"># 生成上三角矩阵,[batch_size, tgt_len, tgt_len], 数值为1</span>
    subsequence_mask <span class="token operator">=</span> torch<span class="token punctuation">.</span>from_numpy<span class="token punctuation">(</span>subsequence_mask<span class="token punctuation">)</span><span class="token punctuation">.</span>byte<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true">#  [batch_size, tgt_len, tgt_len]</span>
    <span class="token keyword">return</span> subsequence_mask<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<ul>
<li><p>Mask句子中没有实际意义的占位符，例如’我 是 学 生 P’ ，P对应句子没有实际意义，所以需要被Mask，Encoder_input 和Decoder_input占位符都需要被Mask。</p>
</li>
<li><p>用来Mask未来输入信息，返回的是一个上三角矩阵。比如我们在中英文翻译时候，会先把”我是学生”整个句子输入到Encoder中，得到最后一层的输出后，才会在Decoder输入<strong>“S I am a student”</strong>（s表示开始）,但是<strong>“S I am a student”</strong>这个句子我们不会一起输入，而是在<strong>T0</strong>时刻先输入<strong>“S”</strong>预测，预测第一个词<strong>“I”；</strong>在下一个<strong>T1</strong>时刻，同时输入<strong>“S”</strong>和<strong>“I”</strong>到Decoder预测下一个单词<strong>“am”；</strong>然后在<strong>T2</strong>时刻把<strong>“S,I,am”</strong>同时输入到Decoder预测下一个单词<strong>“a”</strong>，依次把整个句子输入到Decoder,预测出<strong>“I am a student E”。</strong></p>
</li>
</ul>
<h2 id="4-计算注意力信息、残差和归一化、前馈神经网络"><a href="#4-计算注意力信息、残差和归一化、前馈神经网络" class="headerlink" title="4. 计算注意力信息、残差和归一化、前馈神经网络"></a><strong>4. 计算注意力信息、残差和归一化、前馈神经网络</strong></h2><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 计算注意力信息、残差和归一化</span>
<span class="token keyword">class</span> <span class="token class-name">ScaledDotProductAttention</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        super<span class="token punctuation">(</span>ScaledDotProductAttention<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> Q<span class="token punctuation">,</span> K<span class="token punctuation">,</span> V<span class="token punctuation">,</span> attn_mask<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment" spellcheck="true"># Q: [batch_size, n_heads, len_q, d_k]</span>
        <span class="token comment" spellcheck="true"># K: [batch_size, n_heads, len_k, d_k]</span>
        <span class="token comment" spellcheck="true"># V: [batch_size, n_heads, len_v(=len_k), d_v]</span>
        <span class="token comment" spellcheck="true"># attn_mask: [batch_size, n_heads, seq_len, seq_len]</span>
        scores <span class="token operator">=</span> torch<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>Q<span class="token punctuation">,</span> K<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">/</span> np<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>d_k<span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># scores : [batch_size, n_heads, len_q, len_k]</span>
        scores<span class="token punctuation">.</span>masked_fill_<span class="token punctuation">(</span>attn_mask<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1e9</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 如果时停用词P就等于 0</span>
        attn <span class="token operator">=</span> nn<span class="token punctuation">.</span>Softmax<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">(</span>scores<span class="token punctuation">)</span>
        context <span class="token operator">=</span> torch<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>attn<span class="token punctuation">,</span> V<span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># [batch_size, n_heads, len_q, d_v]</span>
        <span class="token keyword">return</span> context<span class="token punctuation">,</span> attn


<span class="token keyword">class</span> <span class="token class-name">MultiHeadAttention</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        super<span class="token punctuation">(</span>MultiHeadAttention<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>W_Q <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>d_model<span class="token punctuation">,</span> d_k <span class="token operator">*</span> n_heads<span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>W_K <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>d_model<span class="token punctuation">,</span> d_k <span class="token operator">*</span> n_heads<span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>W_V <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>d_model<span class="token punctuation">,</span> d_v <span class="token operator">*</span> n_heads<span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>fc <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>n_heads <span class="token operator">*</span> d_v<span class="token punctuation">,</span> d_model<span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>     <span class="token comment" spellcheck="true"># multi-head 最后concat后需要乘上一个W_O得到一个 总输出b</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> input_Q<span class="token punctuation">,</span> input_K<span class="token punctuation">,</span> input_V<span class="token punctuation">,</span> attn_mask<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment" spellcheck="true"># input_Q: [batch_size, len_q, d_model]</span>
        <span class="token comment" spellcheck="true"># input_K: [batch_size, len_k, d_model]</span>
        <span class="token comment" spellcheck="true"># input_V: [batch_size, len_v(=len_k), d_model]</span>
        <span class="token comment" spellcheck="true"># attn_mask: [batch_size, seq_len, seq_len]</span>
        residual<span class="token punctuation">,</span> batch_size <span class="token operator">=</span> input_Q<span class="token punctuation">,</span> input_Q<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>
        <span class="token comment" spellcheck="true"># view()函数相当于reshape函数</span>
        Q <span class="token operator">=</span> self<span class="token punctuation">.</span>W_Q<span class="token punctuation">(</span>input_Q<span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> n_heads<span class="token punctuation">,</span> d_k<span class="token punctuation">)</span><span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># Q: [batch_size, n_heads, len_q, d_k]</span>
        K <span class="token operator">=</span> self<span class="token punctuation">.</span>W_K<span class="token punctuation">(</span>input_K<span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> n_heads<span class="token punctuation">,</span> d_k<span class="token punctuation">)</span><span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># K: [batch_size, n_heads, len_k, d_k]</span>
        V <span class="token operator">=</span> self<span class="token punctuation">.</span>W_V<span class="token punctuation">(</span>input_V<span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> n_heads<span class="token punctuation">,</span> d_v<span class="token punctuation">)</span><span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># V: [batch_size, n_heads, len_v(=len_k), d_v]</span>
        attn_mask <span class="token operator">=</span> attn_mask<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>repeat<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> n_heads<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># attn_mask : [batch_size, n_heads, seq_len, seq_len]</span>
        context<span class="token punctuation">,</span> attn <span class="token operator">=</span> ScaledDotProductAttention<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">(</span>Q<span class="token punctuation">,</span> K<span class="token punctuation">,</span> V<span class="token punctuation">,</span> attn_mask<span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># context: [batch_size, n_heads, len_q, d_v]</span>
        <span class="token comment" spellcheck="true"># attn: [batch_size, n_heads, len_q, len_k]</span>
        context <span class="token operator">=</span> context<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span>
                                                  n_heads <span class="token operator">*</span> d_v<span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># context: [batch_size, len_q, n_heads * d_v]</span>
        output <span class="token operator">=</span> self<span class="token punctuation">.</span>fc<span class="token punctuation">(</span>context<span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># [batch_size, len_q, d_model]</span>
        <span class="token keyword">return</span> nn<span class="token punctuation">.</span>LayerNorm<span class="token punctuation">(</span>d_model<span class="token punctuation">)</span><span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">(</span>output <span class="token operator">+</span> residual<span class="token punctuation">)</span><span class="token punctuation">,</span> attn    <span class="token comment" spellcheck="true"># ADD And Layer Norm</span>

<span class="token comment" spellcheck="true"># 前馈神经网络</span>
<span class="token keyword">class</span> <span class="token class-name">PoswiseFeedForwardNet</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        super<span class="token punctuation">(</span>PoswiseFeedForwardNet<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>fc <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>
            nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>d_model<span class="token punctuation">,</span> d_ff<span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>d_ff<span class="token punctuation">,</span> d_model<span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> inputs<span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token comment" spellcheck="true"># inputs: [batch_size, seq_len, d_model]</span>
        residual <span class="token operator">=</span> inputs
        output <span class="token operator">=</span> self<span class="token punctuation">.</span>fc<span class="token punctuation">(</span>inputs<span class="token punctuation">)</span>
        <span class="token keyword">return</span> nn<span class="token punctuation">.</span>LayerNorm<span class="token punctuation">(</span>d_model<span class="token punctuation">)</span><span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">(</span>output <span class="token operator">+</span> residual<span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># [batch_size, seq_len, d_model]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<ul>
<li><p>计算注意力信息，W_Q, W_K, W_V矩阵拆分成8个小矩阵。注意传入的input_Q, input_K, input_V，在Encoder和Decoder的第一次调用传入的三个矩阵是相同的，但Decoder的第二次调用传入的三个矩阵input_Q <strong>等于</strong> input_K <strong>不等于</strong> input_V。</p>
</li>
<li><p>输入inputs ，经过两个全连接成，得到的结果再加上 inputs ，再做LayerNorm归一化。LayerNorm归一化可以理解层是把Batch中每一句话进行归一化。</p>
</li>
</ul>
<h2 id="5-Encoder"><a href="#5-Encoder" class="headerlink" title="5. Encoder"></a><strong>5. Encoder</strong></h2><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 单个Encoder</span>
<span class="token keyword">class</span> <span class="token class-name">EncoderLayer</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        super<span class="token punctuation">(</span>EncoderLayer<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>enc_self_attn <span class="token operator">=</span> MultiHeadAttention<span class="token punctuation">(</span><span class="token punctuation">)</span>                                     <span class="token comment" spellcheck="true"># 多头注意力机制</span>
        self<span class="token punctuation">.</span>pos_ffn <span class="token operator">=</span> PoswiseFeedForwardNet<span class="token punctuation">(</span><span class="token punctuation">)</span>                                        <span class="token comment" spellcheck="true"># 前馈神经网络</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> enc_inputs<span class="token punctuation">,</span> enc_self_attn_mask<span class="token punctuation">)</span><span class="token punctuation">:</span>                                <span class="token comment" spellcheck="true"># enc_inputs: [batch_size, src_len, d_model]</span>
        <span class="token comment" spellcheck="true"># 输入3个enc_inputs分别与W_q、W_k、W_v相乘得到Q、K、V                          # enc_self_attn_mask: [batch_size, src_len, src_len]</span>
        enc_outputs<span class="token punctuation">,</span> attn <span class="token operator">=</span> self<span class="token punctuation">.</span>enc_self_attn<span class="token punctuation">(</span>enc_inputs<span class="token punctuation">,</span> enc_inputs<span class="token punctuation">,</span> enc_inputs<span class="token punctuation">,</span>    <span class="token comment" spellcheck="true"># enc_outputs: [batch_size, src_len, d_model],</span>
                                               enc_self_attn_mask<span class="token punctuation">)</span>                    <span class="token comment" spellcheck="true"># attn: [batch_size, n_heads, src_len, src_len]</span>
        enc_outputs <span class="token operator">=</span> self<span class="token punctuation">.</span>pos_ffn<span class="token punctuation">(</span>enc_outputs<span class="token punctuation">)</span>                                       <span class="token comment" spellcheck="true"># enc_outputs: [batch_size, src_len, d_model]</span>
        <span class="token keyword">return</span> enc_outputs<span class="token punctuation">,</span> attn

<span class="token comment" spellcheck="true"># 整个Encoder</span>
<span class="token keyword">class</span> <span class="token class-name">Encoder</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        super<span class="token punctuation">(</span>Encoder<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>src_emb <span class="token operator">=</span> nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>src_vocab_size<span class="token punctuation">,</span> d_model<span class="token punctuation">)</span>                     <span class="token comment" spellcheck="true"># 把字转换字向量</span>
        self<span class="token punctuation">.</span>pos_emb <span class="token operator">=</span> PositionalEncoding<span class="token punctuation">(</span>d_model<span class="token punctuation">)</span>                               <span class="token comment" spellcheck="true"># 加入位置信息</span>
        self<span class="token punctuation">.</span>layers <span class="token operator">=</span> nn<span class="token punctuation">.</span>ModuleList<span class="token punctuation">(</span><span class="token punctuation">[</span>EncoderLayer<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">for</span> _ <span class="token keyword">in</span> range<span class="token punctuation">(</span>n_layers<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> enc_inputs<span class="token punctuation">)</span><span class="token punctuation">:</span>                                               <span class="token comment" spellcheck="true"># enc_inputs: [batch_size, src_len]</span>
        enc_outputs <span class="token operator">=</span> self<span class="token punctuation">.</span>src_emb<span class="token punctuation">(</span>enc_inputs<span class="token punctuation">)</span>                                   <span class="token comment" spellcheck="true"># enc_outputs: [batch_size, src_len, d_model]</span>
        <span class="token triple-quoted-string string">''' token embedding 与 positional embedding如何混合？ '''</span>
        enc_outputs <span class="token operator">=</span> self<span class="token punctuation">.</span>pos_emb<span class="token punctuation">(</span>enc_outputs<span class="token punctuation">)</span>                                  <span class="token comment" spellcheck="true"># enc_outputs: [batch_size, src_len, d_model]</span>
        enc_self_attn_mask <span class="token operator">=</span> get_attn_pad_mask<span class="token punctuation">(</span>enc_inputs<span class="token punctuation">,</span> enc_inputs<span class="token punctuation">)</span>           <span class="token comment" spellcheck="true"># enc_self_attn_mask: [batch_size, src_len, src_len]</span>
        enc_self_attns <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        <span class="token keyword">for</span> layer <span class="token keyword">in</span> self<span class="token punctuation">.</span>layers<span class="token punctuation">:</span>
            enc_outputs<span class="token punctuation">,</span> enc_self_attn <span class="token operator">=</span> layer<span class="token punctuation">(</span>enc_outputs<span class="token punctuation">,</span> enc_self_attn_mask<span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># enc_outputs :   [batch_size, src_len, d_model],</span>
                                                                                 <span class="token comment" spellcheck="true"># enc_self_attn : [batch_size, n_heads, src_len, src_len]</span>
            enc_self_attns<span class="token punctuation">.</span>append<span class="token punctuation">(</span>enc_self_attn<span class="token punctuation">)</span>
        <span class="token keyword">return</span> enc_outputs<span class="token punctuation">,</span> enc_self_attns<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<ul>
<li>第一步，中文字索引进行Embedding，转换成512维度的字向量。第二步，在子向量上面加上位置信息。第三步，Mask掉句子中的占位符号。第四步，通过6层的encoder（上一层的输出作为下一层的输入）。</li>
</ul>
<h2 id="6-Decoder"><a href="#6-Decoder" class="headerlink" title="6. Decoder"></a><strong>6. Decoder</strong></h2><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 单个decoder</span>
<span class="token keyword">class</span> <span class="token class-name">DecoderLayer</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        super<span class="token punctuation">(</span>DecoderLayer<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>dec_self_attn <span class="token operator">=</span> MultiHeadAttention<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>dec_enc_attn <span class="token operator">=</span> MultiHeadAttention<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>pos_ffn <span class="token operator">=</span> PoswiseFeedForwardNet<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> dec_inputs<span class="token punctuation">,</span> enc_outputs<span class="token punctuation">,</span> dec_self_attn_mask<span class="token punctuation">,</span> dec_enc_attn_mask<span class="token punctuation">)</span><span class="token punctuation">:</span> <span class="token comment" spellcheck="true"># dec_inputs: [batch_size, tgt_len, d_model]</span>
                                                                                       <span class="token comment" spellcheck="true"># enc_outputs: [batch_size, src_len, d_model]</span>
                                                                                       <span class="token comment" spellcheck="true"># dec_self_attn_mask: [batch_size, tgt_len, tgt_len]</span>
                                                                                       <span class="token comment" spellcheck="true"># dec_enc_attn_mask: [batch_size, tgt_len, src_len]</span>
        dec_outputs<span class="token punctuation">,</span> dec_self_attn <span class="token operator">=</span> self<span class="token punctuation">.</span>dec_self_attn<span class="token punctuation">(</span>dec_inputs<span class="token punctuation">,</span> dec_inputs<span class="token punctuation">,</span>
                                                 dec_inputs<span class="token punctuation">,</span> dec_self_attn_mask<span class="token punctuation">)</span>   <span class="token comment" spellcheck="true"># dec_outputs: [batch_size, tgt_len, d_model]</span>
                                                                                   <span class="token comment" spellcheck="true"># dec_self_attn: [batch_size, n_heads, tgt_len, tgt_len]</span>
        dec_outputs<span class="token punctuation">,</span> dec_enc_attn <span class="token operator">=</span> self<span class="token punctuation">.</span>dec_enc_attn<span class="token punctuation">(</span>dec_outputs<span class="token punctuation">,</span> enc_outputs<span class="token punctuation">,</span>
                                                enc_outputs<span class="token punctuation">,</span> dec_enc_attn_mask<span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># dec_outputs: [batch_size, tgt_len, d_model]</span>
                                                                                   <span class="token comment" spellcheck="true"># dec_enc_attn: [batch_size, h_heads, tgt_len, src_len]</span>
        dec_outputs <span class="token operator">=</span> self<span class="token punctuation">.</span>pos_ffn<span class="token punctuation">(</span>dec_outputs<span class="token punctuation">)</span>                                    <span class="token comment" spellcheck="true"># dec_outputs: [batch_size, tgt_len, d_model]</span>
        <span class="token keyword">return</span> dec_outputs<span class="token punctuation">,</span> dec_self_attn<span class="token punctuation">,</span> dec_enc_attn

<span class="token comment" spellcheck="true"># 整个Decoder</span>
<span class="token keyword">class</span> <span class="token class-name">Decoder</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        super<span class="token punctuation">(</span>Decoder<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>tgt_emb <span class="token operator">=</span> nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>tgt_vocab_size<span class="token punctuation">,</span> d_model<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>pos_emb <span class="token operator">=</span> PositionalEncoding<span class="token punctuation">(</span>d_model<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>layers <span class="token operator">=</span> nn<span class="token punctuation">.</span>ModuleList<span class="token punctuation">(</span><span class="token punctuation">[</span>DecoderLayer<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">for</span> _ <span class="token keyword">in</span> range<span class="token punctuation">(</span>n_layers<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> dec_inputs<span class="token punctuation">,</span> enc_inputs<span class="token punctuation">,</span> enc_outputs<span class="token punctuation">)</span><span class="token punctuation">:</span>                               <span class="token comment" spellcheck="true"># dec_inputs: [batch_size, tgt_len]</span>
                                                                                          <span class="token comment" spellcheck="true"># enc_intpus: [batch_size, src_len]</span>
                                                                                          <span class="token comment" spellcheck="true"># enc_outputs: [batsh_size, src_len, d_model]</span>
        dec_outputs <span class="token operator">=</span> self<span class="token punctuation">.</span>tgt_emb<span class="token punctuation">(</span>dec_inputs<span class="token punctuation">)</span>                                            <span class="token comment" spellcheck="true"># [batch_size, tgt_len, d_model]</span>
        dec_outputs <span class="token operator">=</span> self<span class="token punctuation">.</span>pos_emb<span class="token punctuation">(</span>dec_outputs<span class="token punctuation">)</span><span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span>                                    <span class="token comment" spellcheck="true"># [batch_size, tgt_len, d_model]</span>
        dec_self_attn_pad_mask <span class="token operator">=</span> get_attn_pad_mask<span class="token punctuation">(</span>dec_inputs<span class="token punctuation">,</span> dec_inputs<span class="token punctuation">)</span><span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span>         <span class="token comment" spellcheck="true"># [batch_size, tgt_len, tgt_len]</span>
        dec_self_attn_subsequence_mask <span class="token operator">=</span> get_attn_subsequence_mask<span class="token punctuation">(</span>dec_inputs<span class="token punctuation">)</span><span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span>     <span class="token comment" spellcheck="true"># [batch_size, tgt_len, tgt_len]</span>
        <span class="token comment" spellcheck="true"># gt(a, b) a>b , return true</span>
        <span class="token comment" spellcheck="true"># >>> torch.gt(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[1, 1], [4, 4]]))</span>
        <span class="token comment" spellcheck="true"># tensor([[False, True], [False, False]])</span>
        dec_self_attn_mask <span class="token operator">=</span> torch<span class="token punctuation">.</span>gt<span class="token punctuation">(</span><span class="token punctuation">(</span>dec_self_attn_pad_mask <span class="token operator">+</span>
                                       dec_self_attn_subsequence_mask<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span>         <span class="token comment" spellcheck="true"># [batch_size, tgt_len, tgt_len]</span>
        dec_enc_attn_mask <span class="token operator">=</span> get_attn_pad_mask<span class="token punctuation">(</span>dec_inputs<span class="token punctuation">,</span> enc_inputs<span class="token punctuation">)</span>                     <span class="token comment" spellcheck="true"># [batc_size, tgt_len, src_len]</span>
        dec_self_attns<span class="token punctuation">,</span> dec_enc_attns <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        <span class="token keyword">for</span> layer <span class="token keyword">in</span> self<span class="token punctuation">.</span>layers<span class="token punctuation">:</span>                             <span class="token comment" spellcheck="true"># dec_outputs: [batch_size, tgt_len, d_model]</span>
                                                              <span class="token comment" spellcheck="true"># dec_self_attn: [batch_size, n_heads, tgt_len, tgt_len]</span>
                                                              <span class="token comment" spellcheck="true"># dec_enc_attn: [batch_size, h_heads, tgt_len, src_len]</span>
            dec_outputs<span class="token punctuation">,</span> dec_self_attn<span class="token punctuation">,</span> dec_enc_attn <span class="token operator">=</span> layer<span class="token punctuation">(</span>dec_outputs<span class="token punctuation">,</span> enc_outputs<span class="token punctuation">,</span> dec_self_attn_mask<span class="token punctuation">,</span> dec_enc_attn_mask<span class="token punctuation">)</span>
            dec_self_attns<span class="token punctuation">.</span>append<span class="token punctuation">(</span>dec_self_attn<span class="token punctuation">)</span>
            dec_enc_attns<span class="token punctuation">.</span>append<span class="token punctuation">(</span>dec_enc_attn<span class="token punctuation">)</span>
        <span class="token keyword">return</span> dec_outputs<span class="token punctuation">,</span> dec_self_attns<span class="token punctuation">,</span> dec_enc_attns<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<ul>
<li>第一步，英文字索引进行Embedding，转换成512维度的字向量。第二步，在子向量上面加上位置信息。第三步，Mask掉句子中的占位符号和输出顺序细节见步骤3。第四步，通过6层的decoder（上一层的输出作为下一层的输入）。</li>
</ul>
<h2 id="7-Transformer"><a href="#7-Transformer" class="headerlink" title="7. Transformer"></a>7. Transformer</h2><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># Trasformer</span>
<span class="token keyword">class</span> <span class="token class-name">Transformer</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        super<span class="token punctuation">(</span>Transformer<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>Encoder <span class="token operator">=</span> Encoder<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>Decoder <span class="token operator">=</span> Decoder<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>projection <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>d_model<span class="token punctuation">,</span> tgt_vocab_size<span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> enc_inputs<span class="token punctuation">,</span> dec_inputs<span class="token punctuation">)</span><span class="token punctuation">:</span>                         <span class="token comment" spellcheck="true"># enc_inputs: [batch_size, src_len]</span>
                                                                       <span class="token comment" spellcheck="true"># dec_inputs: [batch_size, tgt_len]</span>
        enc_outputs<span class="token punctuation">,</span> enc_self_attns <span class="token operator">=</span> self<span class="token punctuation">.</span>Encoder<span class="token punctuation">(</span>enc_inputs<span class="token punctuation">)</span>         <span class="token comment" spellcheck="true"># enc_outputs: [batch_size, src_len, d_model],</span>
                                                                       <span class="token comment" spellcheck="true"># enc_self_attns: [n_layers, batch_size, n_heads, src_len, src_len]</span>
        dec_outputs<span class="token punctuation">,</span> dec_self_attns<span class="token punctuation">,</span> dec_enc_attns <span class="token operator">=</span> self<span class="token punctuation">.</span>Decoder<span class="token punctuation">(</span>
            dec_inputs<span class="token punctuation">,</span> enc_inputs<span class="token punctuation">,</span> enc_outputs<span class="token punctuation">)</span>                       <span class="token comment" spellcheck="true"># dec_outpus    : [batch_size, tgt_len, d_model],</span>
                                                                       <span class="token comment" spellcheck="true"># dec_self_attns: [n_layers, batch_size, n_heads, tgt_len, tgt_len],</span>
                                                                       <span class="token comment" spellcheck="true"># dec_enc_attn  : [n_layers, batch_size, tgt_len, src_len]</span>
        dec_logits <span class="token operator">=</span> self<span class="token punctuation">.</span>projection<span class="token punctuation">(</span>dec_outputs<span class="token punctuation">)</span>                      <span class="token comment" spellcheck="true"># dec_logits: [batch_size, tgt_len, tgt_vocab_size]</span>
        <span class="token keyword">return</span> dec_logits<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> dec_logits<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> enc_self_attns<span class="token punctuation">,</span> dec_self_attns<span class="token punctuation">,</span> dec_enc_attns<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<ul>
<li>Trasformer的整体结构，输入数据先通过Encoder，再同个Decoder，最后把输出进行多分类，分类数为英文字典长度，也就是判断每一个字的概率。</li>
</ul>
<h2 id="8-定义网络与训练测试"><a href="#8-定义网络与训练测试" class="headerlink" title="8. 定义网络与训练测试"></a><strong>8. 定义网络与训练测试</strong></h2><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 定义网络</span>
model <span class="token operator">=</span> Transformer<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span>
criterion <span class="token operator">=</span> nn<span class="token punctuation">.</span>CrossEntropyLoss<span class="token punctuation">(</span>ignore_index<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>     <span class="token comment" spellcheck="true">#忽略 占位符 索引为0.</span>
optimizer <span class="token operator">=</span> optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span>model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">1e</span><span class="token operator">-</span><span class="token number">3</span><span class="token punctuation">,</span> momentum<span class="token operator">=</span><span class="token number">0.99</span><span class="token punctuation">)</span>

<span class="token comment" spellcheck="true"># 训练Transformer</span>
<span class="token keyword">for</span> epoch <span class="token keyword">in</span> range<span class="token punctuation">(</span><span class="token number">50</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">for</span> enc_inputs<span class="token punctuation">,</span> dec_inputs<span class="token punctuation">,</span> dec_outputs <span class="token keyword">in</span> loader<span class="token punctuation">:</span>  <span class="token comment" spellcheck="true"># enc_inputs : [batch_size, src_len]</span>
        <span class="token comment" spellcheck="true"># dec_inputs : [batch_size, tgt_len]</span>
        <span class="token comment" spellcheck="true"># dec_outputs: [batch_size, tgt_len]</span>

        enc_inputs<span class="token punctuation">,</span> dec_inputs<span class="token punctuation">,</span> dec_outputs <span class="token operator">=</span> enc_inputs<span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> dec_inputs<span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> dec_outputs<span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span>
        outputs<span class="token punctuation">,</span> enc_self_attns<span class="token punctuation">,</span> dec_self_attns<span class="token punctuation">,</span> dec_enc_attns <span class="token operator">=</span> model<span class="token punctuation">(</span>enc_inputs<span class="token punctuation">,</span> dec_inputs<span class="token punctuation">)</span>
        <span class="token comment" spellcheck="true"># outputs: [batch_size * tgt_len, tgt_vocab_size]</span>
        loss <span class="token operator">=</span> criterion<span class="token punctuation">(</span>outputs<span class="token punctuation">,</span> dec_outputs<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'Epoch:'</span><span class="token punctuation">,</span> <span class="token string">'%04d'</span> <span class="token operator">%</span> <span class="token punctuation">(</span>epoch <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">'loss ='</span><span class="token punctuation">,</span> <span class="token string">'{:.6f}'</span><span class="token punctuation">.</span>format<span class="token punctuation">(</span>loss<span class="token punctuation">)</span><span class="token punctuation">)</span>
        optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
        loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
        optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token comment" spellcheck="true"># 测试模型</span>
<span class="token keyword">def</span> <span class="token function">test</span><span class="token punctuation">(</span>model<span class="token punctuation">,</span> enc_input<span class="token punctuation">,</span> start_symbol<span class="token punctuation">)</span><span class="token punctuation">:</span>
    enc_outputs<span class="token punctuation">,</span> enc_self_attns <span class="token operator">=</span> model<span class="token punctuation">.</span>Encoder<span class="token punctuation">(</span>enc_input<span class="token punctuation">)</span>
    dec_input <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> tgt_len<span class="token punctuation">)</span><span class="token punctuation">.</span>type_as<span class="token punctuation">(</span>enc_input<span class="token punctuation">.</span>data<span class="token punctuation">)</span>
    next_symbol <span class="token operator">=</span> start_symbol
    <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> tgt_len<span class="token punctuation">)</span><span class="token punctuation">:</span>
        dec_input<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> next_symbol
        dec_outputs<span class="token punctuation">,</span> _<span class="token punctuation">,</span> _ <span class="token operator">=</span> model<span class="token punctuation">.</span>Decoder<span class="token punctuation">(</span>dec_input<span class="token punctuation">,</span> enc_input<span class="token punctuation">,</span> enc_outputs<span class="token punctuation">)</span>
        projected <span class="token operator">=</span> model<span class="token punctuation">.</span>projection<span class="token punctuation">(</span>dec_outputs<span class="token punctuation">)</span>
        prob <span class="token operator">=</span> projected<span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span>max<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> keepdim<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>
        next_word <span class="token operator">=</span> prob<span class="token punctuation">.</span>data<span class="token punctuation">[</span>i<span class="token punctuation">]</span>
        next_symbol <span class="token operator">=</span> next_word<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> dec_input

enc_inputs<span class="token punctuation">,</span> _<span class="token punctuation">,</span> _ <span class="token operator">=</span> next<span class="token punctuation">(</span>iter<span class="token punctuation">(</span>loader<span class="token punctuation">)</span><span class="token punctuation">)</span>
predict_dec_input <span class="token operator">=</span> test<span class="token punctuation">(</span>model<span class="token punctuation">,</span> enc_inputs<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> start_symbol<span class="token operator">=</span>tgt_vocab<span class="token punctuation">[</span><span class="token string">"S"</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
predict<span class="token punctuation">,</span> _<span class="token punctuation">,</span> _<span class="token punctuation">,</span> _ <span class="token operator">=</span> model<span class="token punctuation">(</span>enc_inputs<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> predict_dec_input<span class="token punctuation">)</span>
predict <span class="token operator">=</span> predict<span class="token punctuation">.</span>data<span class="token punctuation">.</span>max<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> keepdim<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>

<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token punctuation">[</span>src_idx2word<span class="token punctuation">[</span>int<span class="token punctuation">(</span>i<span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> enc_inputs<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token string">'->'</span><span class="token punctuation">,</span>
<span class="token punctuation">[</span>idx2word<span class="token punctuation">[</span>n<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token keyword">for</span> n <span class="token keyword">in</span> predict<span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h2 id="9-一些细节"><a href="#9-一些细节" class="headerlink" title="9. 一些细节"></a>9. 一些细节</h2><ul>
<li>positional embedding在Transformer和bert中方式不一样，positional embbeding的设计是一个课题</li>
<li>注意Multi-Head Self-Attention相比正常的Self-Attention只在最后concat的时候多出参数</li>
<li>Wq，Wk，Wv是要训练的参数，其尺寸取决于输入的embedding向量长度</li>
<li><strong>区分Layer Normalization与Batch Normalization</strong>？</li>
<li>Feed Forward是一个两层的线性层，使用Relu函数</li>
<li>Encoder与Decoder连接的attention，Encoder提供k和v，Decoder提供q</li>
<li>Decoder的Multi-Head Self-Attention带Mask，带Mask是为了并行训练计算，实际测试是依次一个个输入。</li>
</ul>
<h2 id="10-全部代码与测试"><a href="#10-全部代码与测试" class="headerlink" title="10. 全部代码与测试"></a>10. 全部代码与测试</h2><ul>
<li><a href="https://colab.research.google.com/drive/1avycotLAFcgXUP1qTk0bSX_jVGhjtZaV?usp=sharing" target="_blank" rel="noopener">Colab</a></li>
</ul>
<h1 id="其他实现"><a href="#其他实现" class="headerlink" title="其他实现"></a>其他实现</h1><h2 id="1-模型总览"><a href="#1-模型总览" class="headerlink" title="1. 模型总览"></a>1. 模型总览</h2><p>首先，看一下Transformer模型架构图：</p>
<p><img src="/images/loading.gif" data-original="../../../../../Projects/Awesome-DL-Models/Images/Transformer/base_transformer_001.png" alt=""></p>
<ol>
<li>Embedding，对于输入进行Token Embedding并加上Positional Embedding</li>
<li>Encoder：由四个部分组成，包括Multi-Head Attention，Feed Forward，Residual Connection，Layer Normalization</li>
<li>Decoder：由五部分组成，Masked Multi-Head Attention，Encoder-Decoder Attention，Feed Forward，Residual Connection，Layer Normalization</li>
<li>Output：包括Linear和Softmax</li>
</ol>
<p>以上是一个完整的Transformer的部件，下面开始手撸一个Transformer吧！</p>
<h2 id="2-Config"><a href="#2-Config" class="headerlink" title="2. Config"></a>2. Config</h2><p>下面是这个Demo所用的库文件以及一些超参的信息。<strong>单独实现一个Config类保存的原因是，方便日后复用。直接将模型部分复制，所用超参保存在新项目的Config类中即可</strong>。这里不过多赘述。</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> torch
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn
<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
<span class="token keyword">import</span> math


<span class="token keyword">class</span> <span class="token class-name">Config</span><span class="token punctuation">(</span>object<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>vocab_size <span class="token operator">=</span> <span class="token number">6</span>    <span class="token comment" spellcheck="true"># 设置词库大小</span>

        self<span class="token punctuation">.</span>d_model <span class="token operator">=</span> <span class="token number">20</span>    <span class="token comment" spellcheck="true"># 设置Embedding得到向量的维度</span>
        self<span class="token punctuation">.</span>n_heads <span class="token operator">=</span> <span class="token number">2</span>    <span class="token comment" spellcheck="true"># 设置mutil-head数目</span>

        <span class="token keyword">assert</span> self<span class="token punctuation">.</span>d_model <span class="token operator">%</span> self<span class="token punctuation">.</span>n_heads <span class="token operator">==</span> <span class="token number">0</span>
        dim_k  <span class="token operator">=</span> d_model <span class="token operator">%</span> n_heads
        dim_v <span class="token operator">=</span> d_model <span class="token operator">%</span> n_heads

        self<span class="token punctuation">.</span>padding_size <span class="token operator">=</span> <span class="token number">30</span>    <span class="token comment" spellcheck="true"># 统一句子的长度，截长补短</span>
        self<span class="token punctuation">.</span>UNK <span class="token operator">=</span> <span class="token number">5</span>            <span class="token comment" spellcheck="true"># 设置此表中出现OOV情况下的索引，跟vocab_size关联，一般放在词表的最后一项6-1=5，从0开始</span>
        self<span class="token punctuation">.</span>PAD <span class="token operator">=</span> <span class="token number">4</span>            <span class="token comment" spellcheck="true"># 设置padding_idx</span>

        self<span class="token punctuation">.</span>N <span class="token operator">=</span> <span class="token number">6</span>
        self<span class="token punctuation">.</span>p <span class="token operator">=</span> <span class="token number">0.1</span>            <span class="token comment" spellcheck="true"># Dropout比率</span>

config <span class="token operator">=</span> Config<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h2 id="3-Embedding"><a href="#3-Embedding" class="headerlink" title="3. Embedding"></a>3. Embedding</h2><p>Embedding部分接受原始的文本输入(batch_size*seq_len,例:[[1,3,10,5],[3,4,5],[5,3,1,1]])，叠加一个普通的Embedding层以及一个Positional Embedding层，输出最后结果。</p>
<p><img src="/images/loading.gif" data-original="../images/basic/image-20211030210329377.png" alt=""></p>
<p>在这一层中，输入的是一个list: [batch_size * seq_len]，输出的是一个tensor:[batch_size * seq_len * d_model]</p>
<p>普通的 Embedding 层想说两点：</p>
<ul>
<li>采用<code>torch.nn.Embedding</code>实现embedding操作。需要关注的一点是论文中提到的Mask机制，包括padding_mask以及sequence_mask(具体请见论文)。在文本输入之前，我们需要进行padding统一长度，padding_mask的实现可以借助<code>torch.nn.Embedding</code>中的<code>padding_idx</code>参数。</li>
<li>在padding过程中，短补长截</li>
</ul>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Embedding</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>vocab_size<span class="token punctuation">)</span><span class="token punctuation">:</span>
        super<span class="token punctuation">(</span>Embedding<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token comment" spellcheck="true"># 一个普通的 embedding层，我们可以通过设置padding_idx=config.PAD 来实现论文中的 padding_mask</span>
        self<span class="token punctuation">.</span>embedding <span class="token operator">=</span> nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>vocab_size<span class="token punctuation">,</span>config<span class="token punctuation">.</span>d_model<span class="token punctuation">,</span>padding_idx<span class="token operator">=</span>config<span class="token punctuation">.</span>PAD<span class="token punctuation">)</span>


    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment" spellcheck="true"># 根据每个句子的长度，进行padding，短补长截</span>
        <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>len<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token keyword">if</span> len<span class="token punctuation">(</span>x<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">&lt;</span> config<span class="token punctuation">.</span>padding_size<span class="token punctuation">:</span>
                x<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">.</span>extend<span class="token punctuation">(</span><span class="token punctuation">[</span>config<span class="token punctuation">.</span>UNK<span class="token punctuation">]</span> <span class="token operator">*</span> <span class="token punctuation">(</span>config<span class="token punctuation">.</span>padding_size <span class="token operator">-</span> len<span class="token punctuation">(</span>x<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># 注意 UNK是你词表中用来表示oov的token索引，这里进行了简化，直接假设为6</span>
            <span class="token keyword">else</span><span class="token punctuation">:</span>
                x<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> x<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token punctuation">:</span>config<span class="token punctuation">.</span>padding_size<span class="token punctuation">]</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>embedding<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># batch_size * seq_len * d_model</span>
        <span class="token keyword">return</span> x<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>关于Positional Embedding，我们需要参考论文给出的公式。说一句题外话，在作者的实验中对比了Positional Embedding与单独采用一个Embedding训练模型对位置的感知两种方式，模型效果相差无几。</p>
<p><img src="/images/loading.gif" data-original="../images/basic/image-20211030212232076.png" alt=""></p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Positional_Encoding</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>

    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>d_model<span class="token punctuation">)</span><span class="token punctuation">:</span>
        super<span class="token punctuation">(</span>Positional_Encoding<span class="token punctuation">,</span>self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>d_model <span class="token operator">=</span> d_model


    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>seq_len<span class="token punctuation">,</span>embedding_dim<span class="token punctuation">)</span><span class="token punctuation">:</span>
        positional_encoding <span class="token operator">=</span> np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token punctuation">(</span>seq_len<span class="token punctuation">,</span>embedding_dim<span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">for</span> pos <span class="token keyword">in</span> range<span class="token punctuation">(</span>positional_encoding<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>positional_encoding<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
                positional_encoding<span class="token punctuation">[</span>pos<span class="token punctuation">]</span><span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> math<span class="token punctuation">.</span>sin<span class="token punctuation">(</span>pos<span class="token operator">/</span><span class="token punctuation">(</span><span class="token number">10000</span><span class="token operator">**</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token operator">*</span>i<span class="token operator">/</span>self<span class="token punctuation">.</span>d_model<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token keyword">if</span> i <span class="token operator">%</span> <span class="token number">2</span> <span class="token operator">==</span> <span class="token number">0</span> <span class="token keyword">else</span> math<span class="token punctuation">.</span>cos<span class="token punctuation">(</span>pos<span class="token operator">/</span><span class="token punctuation">(</span><span class="token number">10000</span><span class="token operator">**</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token operator">*</span>i<span class="token operator">/</span>self<span class="token punctuation">.</span>d_model<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> torch<span class="token punctuation">.</span>from_numpy<span class="token punctuation">(</span>positional_encoding<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h2 id="4-Encoder"><a href="#4-Encoder" class="headerlink" title="4. Encoder"></a>4. Encoder</h2><h3 id="Muti-head-Attention"><a href="#Muti-head-Attention" class="headerlink" title="Muti_head_Attention"></a>Muti_head_Attention</h3><p>这一部分是模型的核心内容，理论具体参考<a href="https://blog.creativecc.cn/posts/bert.html" target="_blank" rel="noopener">Bert详解</a>。</p>
<p>Encoder 中的 Muti_head_Attention 不需要Mask。</p>
<p>为了避免模型信息泄露的问题，Decoder 中的 Muti_head_Attention 需要Mask。这一节中我们重点讲解Muti_head_Attention中Mask机制的实现。</p>
<ul>
<li><code>forward</code> 函数的参数从 x 变为 x,y：请读者观察模型架构，Decoder需要接受Encoder的输入作为公式中的<code>V</code>,即我们参数中的y。在普通的自注意力机制中，我们在调用中设置<code>y=x</code>即可。</li>
<li>requires_mask： 是否采用Mask机制，在Decoder中设置为True</li>
</ul>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Mutihead_Attention</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>d_model<span class="token punctuation">,</span>dim_k<span class="token punctuation">,</span>dim_v<span class="token punctuation">,</span>n_heads<span class="token punctuation">)</span><span class="token punctuation">:</span>
        super<span class="token punctuation">(</span>Mutihead_Attention<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>dim_v <span class="token operator">=</span> dim_v    <span class="token comment" spellcheck="true"># value</span>
        self<span class="token punctuation">.</span>dim_k <span class="token operator">=</span> dim_k    <span class="token comment" spellcheck="true"># key</span>
        self<span class="token punctuation">.</span>n_heads <span class="token operator">=</span> n_heads    <span class="token comment" spellcheck="true"># multi-head nums</span>

        self<span class="token punctuation">.</span>q <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>d_model<span class="token punctuation">,</span>dim_k<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>k <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>d_model<span class="token punctuation">,</span>dim_k<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>v <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>d_model<span class="token punctuation">,</span>dim_v<span class="token punctuation">)</span>

        self<span class="token punctuation">.</span>o <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>dim_v<span class="token punctuation">,</span>d_model<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>norm_fact <span class="token operator">=</span> <span class="token number">1</span> <span class="token operator">/</span> math<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>d_model<span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># </span>

    <span class="token keyword">def</span> <span class="token function">generate_mask</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>dim<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment" spellcheck="true"># 此处是 sequence mask ，防止 decoder窥视后面时间步的信息。</span>
        <span class="token comment" spellcheck="true"># padding mask 在数据输入模型之前完成。</span>
        matirx <span class="token operator">=</span> np<span class="token punctuation">.</span>ones<span class="token punctuation">(</span><span class="token punctuation">(</span>dim<span class="token punctuation">,</span>dim<span class="token punctuation">)</span><span class="token punctuation">)</span>
        mask <span class="token operator">=</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">(</span>np<span class="token punctuation">.</span>tril<span class="token punctuation">(</span>matirx<span class="token punctuation">)</span><span class="token punctuation">)</span>

        <span class="token keyword">return</span> mask<span class="token operator">==</span><span class="token number">1</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>x<span class="token punctuation">,</span>y<span class="token punctuation">,</span>requires_mask<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">assert</span> self<span class="token punctuation">.</span>dim_k <span class="token operator">%</span> self<span class="token punctuation">.</span>n_heads <span class="token operator">==</span> <span class="token number">0</span> <span class="token operator">and</span> self<span class="token punctuation">.</span>dim_v <span class="token operator">%</span> self<span class="token punctuation">.</span>n_heads <span class="token operator">==</span> <span class="token number">0</span>
        <span class="token comment" spellcheck="true"># size of x : [batch_size * seq_len * batch_size]</span>
        <span class="token comment" spellcheck="true"># 对 x 进行自注意力</span>
        Q <span class="token operator">=</span> self<span class="token punctuation">.</span>q<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span>x<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>x<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span>self<span class="token punctuation">.</span>dim_k <span class="token operator">//</span> self<span class="token punctuation">.</span>n_heads<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># n_heads * batch_size * seq_len * dim_k</span>
        K <span class="token operator">=</span> self<span class="token punctuation">.</span>k<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span>x<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>x<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span>self<span class="token punctuation">.</span>dim_k <span class="token operator">//</span> self<span class="token punctuation">.</span>n_heads<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># n_heads * batch_size * seq_len * dim_k</span>
        V <span class="token operator">=</span> self<span class="token punctuation">.</span>v<span class="token punctuation">(</span>y<span class="token punctuation">)</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span>y<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>y<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span>self<span class="token punctuation">.</span>dim_v <span class="token operator">//</span> self<span class="token punctuation">.</span>n_heads<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># n_heads * batch_size * seq_len * dim_v</span>
        <span class="token comment" spellcheck="true"># print("Attention V shape : {}".format(V.shape))</span>
        attention_score <span class="token operator">=</span> torch<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>Q<span class="token punctuation">,</span>K<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">*</span> self<span class="token punctuation">.</span>norm_fact
        <span class="token keyword">if</span> requires_mask<span class="token punctuation">:</span>
            mask <span class="token operator">=</span> self<span class="token punctuation">.</span>generate_mask<span class="token punctuation">(</span>x<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
            attention_score<span class="token punctuation">.</span>masked_fill<span class="token punctuation">(</span>mask<span class="token punctuation">,</span>value<span class="token operator">=</span>float<span class="token punctuation">(</span><span class="token string">"-inf"</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># 注意这里的小Trick，不需要将Q,K,V 分别MASK,只MASKSoftmax之前的结果就好了</span>
        output <span class="token operator">=</span> torch<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>attention_score<span class="token punctuation">,</span>V<span class="token punctuation">)</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>y<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>y<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
        <span class="token comment" spellcheck="true"># print("Attention output shape : {}".format(output.shape))</span>

        output <span class="token operator">=</span> self<span class="token punctuation">.</span>o<span class="token punctuation">(</span>output<span class="token punctuation">)</span>
        <span class="token keyword">return</span> output<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h3 id="Feed-Forward"><a href="#Feed-Forward" class="headerlink" title="Feed Forward"></a>Feed Forward</h3><p><img src="/images/loading.gif" data-original="../images/basic/image-20211030213350610.png" alt=""></p>
<p>这一部分实现很简单，两个Linear中连接Relu即可，目的是为模型增添非线性信息，提高模型的拟合能力。</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Feed_Forward</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>input_dim<span class="token punctuation">,</span>hidden_dim<span class="token operator">=</span><span class="token number">2048</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        super<span class="token punctuation">(</span>Feed_Forward<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>L1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>input_dim<span class="token punctuation">,</span>hidden_dim<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>L2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>hidden_dim<span class="token punctuation">,</span>input_dim<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        output <span class="token operator">=</span> nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>L1<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        output <span class="token operator">=</span> self<span class="token punctuation">.</span>L2<span class="token punctuation">(</span>output<span class="token punctuation">)</span>
        <span class="token keyword">return</span> output<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h3 id="Add-amp-LayerNorm"><a href="#Add-amp-LayerNorm" class="headerlink" title="Add &amp; LayerNorm"></a>Add &amp; LayerNorm</h3><p>这一节我们实现论文中提出的残差连接以及LayerNorm。</p>
<p>论文中关于这部分给出公式：</p>
<p>代码中的dropout，在论文中也有所解释，对输入layer_norm的tensor进行dropout，对模型的性能影响还是蛮大的。</p>
<p>代码中的参数<code>sub_layer</code> ，可以是Feed Forward，也可以是Muti_head_Attention。</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Add_Norm</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>dropout <span class="token operator">=</span> nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>config<span class="token punctuation">.</span>p<span class="token punctuation">)</span>
        super<span class="token punctuation">(</span>Add_Norm<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>x<span class="token punctuation">,</span>sub_layer<span class="token punctuation">,</span><span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span>
        sub_output <span class="token operator">=</span> sub_layer<span class="token punctuation">(</span>x<span class="token punctuation">,</span><span class="token operator">**</span>kwargs<span class="token punctuation">)</span>
        <span class="token comment" spellcheck="true"># print("{} output : {}".format(sub_layer,sub_output.size()))</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>dropout<span class="token punctuation">(</span>x <span class="token operator">+</span> sub_output<span class="token punctuation">)</span>

        layer_norm <span class="token operator">=</span> nn<span class="token punctuation">.</span>LayerNorm<span class="token punctuation">(</span>x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
        out <span class="token operator">=</span> layer_norm<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        <span class="token keyword">return</span> out<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>OK，Encoder中所有模块我们已经讲解完毕，接下来我们将其拼接作为Encoder</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Encoder</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        super<span class="token punctuation">(</span>Encoder<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>positional_encoding <span class="token operator">=</span> Positional_Encoding<span class="token punctuation">(</span>config<span class="token punctuation">.</span>d_model<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>muti_atten <span class="token operator">=</span> Mutihead_Attention<span class="token punctuation">(</span>config<span class="token punctuation">.</span>d_model<span class="token punctuation">,</span>config<span class="token punctuation">.</span>dim_k<span class="token punctuation">,</span>config<span class="token punctuation">.</span>dim_v<span class="token punctuation">,</span>config<span class="token punctuation">.</span>n_heads<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>feed_forward <span class="token operator">=</span> Feed_Forward<span class="token punctuation">(</span>config<span class="token punctuation">.</span>d_model<span class="token punctuation">)</span>

        self<span class="token punctuation">.</span>add_norm <span class="token operator">=</span> Add_Norm<span class="token punctuation">(</span><span class="token punctuation">)</span>


    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span> <span class="token comment" spellcheck="true"># batch_size * seq_len 并且 x 的类型不是tensor，是普通list</span>

        x <span class="token operator">+=</span> self<span class="token punctuation">.</span>positional_encoding<span class="token punctuation">(</span>x<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span>config<span class="token punctuation">.</span>d_model<span class="token punctuation">)</span>
        <span class="token comment" spellcheck="true"># print("After positional_encoding: {}".format(x.size()))</span>
        output <span class="token operator">=</span> self<span class="token punctuation">.</span>add_norm<span class="token punctuation">(</span>x<span class="token punctuation">,</span>self<span class="token punctuation">.</span>muti_atten<span class="token punctuation">,</span>y<span class="token operator">=</span>x<span class="token punctuation">)</span>
        output <span class="token operator">=</span> self<span class="token punctuation">.</span>add_norm<span class="token punctuation">(</span>output<span class="token punctuation">,</span>self<span class="token punctuation">.</span>feed_forward<span class="token punctuation">)</span>

        <span class="token keyword">return</span> output<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h2 id="5-Decoder"><a href="#5-Decoder" class="headerlink" title="5. Decoder"></a>5. Decoder</h2><p>在 Encoder 部分的讲解中，我们已经实现了大部分Decoder的模块。Decoder的Muti_head_Attention引入了Mask机制，Decoder与Encoder 中模块的拼接方式不同。以上两点读者在Coding的时候需要注意。</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Decoder</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        super<span class="token punctuation">(</span>Decoder<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>positional_encoding <span class="token operator">=</span> Positional_Encoding<span class="token punctuation">(</span>config<span class="token punctuation">.</span>d_model<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>muti_atten <span class="token operator">=</span> Mutihead_Attention<span class="token punctuation">(</span>config<span class="token punctuation">.</span>d_model<span class="token punctuation">,</span>config<span class="token punctuation">.</span>dim_k<span class="token punctuation">,</span>config<span class="token punctuation">.</span>dim_v<span class="token punctuation">,</span>config<span class="token punctuation">.</span>n_heads<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>feed_forward <span class="token operator">=</span> Feed_Forward<span class="token punctuation">(</span>config<span class="token punctuation">.</span>d_model<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>add_norm <span class="token operator">=</span> Add_Norm<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>x<span class="token punctuation">,</span>encoder_output<span class="token punctuation">)</span><span class="token punctuation">:</span> <span class="token comment" spellcheck="true"># batch_size * seq_len 并且 x 的类型不是tensor，是普通list</span>
        <span class="token comment" spellcheck="true"># print(x.size())</span>
        x <span class="token operator">+=</span> self<span class="token punctuation">.</span>positional_encoding<span class="token punctuation">(</span>x<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span>config<span class="token punctuation">.</span>d_model<span class="token punctuation">)</span>
        <span class="token comment" spellcheck="true"># print(x.size())</span>
        <span class="token comment" spellcheck="true"># 第一个 sub_layer</span>
        output <span class="token operator">=</span> self<span class="token punctuation">.</span>add_norm<span class="token punctuation">(</span>x<span class="token punctuation">,</span>self<span class="token punctuation">.</span>muti_atten<span class="token punctuation">,</span>y<span class="token operator">=</span>x<span class="token punctuation">,</span>requires_mask<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
        <span class="token comment" spellcheck="true"># 第二个 sub_layer</span>
        output <span class="token operator">=</span> self<span class="token punctuation">.</span>add_norm<span class="token punctuation">(</span>output<span class="token punctuation">,</span>self<span class="token punctuation">.</span>muti_atten<span class="token punctuation">,</span>y<span class="token operator">=</span>encoder_output<span class="token punctuation">,</span>requires_mask<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
        <span class="token comment" spellcheck="true"># 第三个 sub_layer</span>
        output <span class="token operator">=</span> self<span class="token punctuation">.</span>add_norm<span class="token punctuation">(</span>output<span class="token punctuation">,</span>self<span class="token punctuation">.</span>feed_forward<span class="token punctuation">)</span>


        <span class="token keyword">return</span> output<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h2 id="6-Transformer"><a href="#6-Transformer" class="headerlink" title="6. Transformer"></a>6. Transformer</h2><p>至此，所有内容已经铺垫完毕，我们开始组装Transformer模型。论文中提到，Transformer中堆叠了6个我们上文中实现的Encoder 和 Decoder。这里笔者采用<code>nn.Sequential</code>实现了堆叠操作。</p>
<p>Output模块的 Linear 和 Softmax 的实现也包含在下面的代码中</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Transformer_layer</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        super<span class="token punctuation">(</span>Transformer_layer<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>encoder <span class="token operator">=</span> Encoder<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>decoder <span class="token operator">=</span> Decoder<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        x_input<span class="token punctuation">,</span>x_output <span class="token operator">=</span> x
        encoder_output <span class="token operator">=</span> self<span class="token punctuation">.</span>encoder<span class="token punctuation">(</span>x_input<span class="token punctuation">)</span>
        decoder_output <span class="token operator">=</span> self<span class="token punctuation">.</span>decoder<span class="token punctuation">(</span>x_output<span class="token punctuation">,</span>encoder_output<span class="token punctuation">)</span>
        <span class="token keyword">return</span> <span class="token punctuation">(</span>encoder_output<span class="token punctuation">,</span>decoder_output<span class="token punctuation">)</span>


<span class="token keyword">class</span> <span class="token class-name">Transformer</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>N<span class="token punctuation">,</span>vocab_size<span class="token punctuation">,</span>output_dim<span class="token punctuation">)</span><span class="token punctuation">:</span>
        super<span class="token punctuation">(</span>Transformer<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>embedding_input <span class="token operator">=</span> Embedding<span class="token punctuation">(</span>vocab_size<span class="token operator">=</span>vocab_size<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>embedding_output <span class="token operator">=</span> Embedding<span class="token punctuation">(</span>vocab_size<span class="token operator">=</span>vocab_size<span class="token punctuation">)</span>

        self<span class="token punctuation">.</span>output_dim <span class="token operator">=</span> output_dim
        self<span class="token punctuation">.</span>linear <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>config<span class="token punctuation">.</span>d_model<span class="token punctuation">,</span>output_dim<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>softmax <span class="token operator">=</span> nn<span class="token punctuation">.</span>Softmax<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>model <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span><span class="token operator">*</span><span class="token punctuation">[</span>Transformer_layer<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">for</span> _ <span class="token keyword">in</span> range<span class="token punctuation">(</span>N<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>


    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        x_input <span class="token punctuation">,</span> x_output <span class="token operator">=</span> x
        x_input <span class="token operator">=</span> self<span class="token punctuation">.</span>embedding_input<span class="token punctuation">(</span>x_input<span class="token punctuation">)</span>
        x_output <span class="token operator">=</span> self<span class="token punctuation">.</span>embedding_output<span class="token punctuation">(</span>x_output<span class="token punctuation">)</span>

        _ <span class="token punctuation">,</span> output <span class="token operator">=</span> self<span class="token punctuation">.</span>model<span class="token punctuation">(</span><span class="token punctuation">(</span>x_input<span class="token punctuation">,</span>x_output<span class="token punctuation">)</span><span class="token punctuation">)</span>

        output <span class="token operator">=</span> self<span class="token punctuation">.</span>linear<span class="token punctuation">(</span>output<span class="token punctuation">)</span>
        output <span class="token operator">=</span> self<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>output<span class="token punctuation">)</span>

        <span class="token keyword">return</span> output<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h2 id="7-完整代码"><a href="#7-完整代码" class="headerlink" title="7. 完整代码"></a>7. 完整代码</h2><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> torch
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn
<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
<span class="token keyword">import</span> math


<span class="token keyword">class</span> <span class="token class-name">Config</span><span class="token punctuation">(</span>object<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>vocab_size <span class="token operator">=</span> <span class="token number">6</span>

        self<span class="token punctuation">.</span>d_model <span class="token operator">=</span> <span class="token number">20</span>
        self<span class="token punctuation">.</span>n_heads <span class="token operator">=</span> <span class="token number">2</span>

        <span class="token keyword">assert</span> self<span class="token punctuation">.</span>d_model <span class="token operator">%</span> self<span class="token punctuation">.</span>n_heads <span class="token operator">==</span> <span class="token number">0</span>
        dim_k  <span class="token operator">=</span> self<span class="token punctuation">.</span>d_model <span class="token operator">//</span> self<span class="token punctuation">.</span>n_heads
        dim_v <span class="token operator">=</span> self<span class="token punctuation">.</span>d_model <span class="token operator">//</span> self<span class="token punctuation">.</span>n_heads



        self<span class="token punctuation">.</span>padding_size <span class="token operator">=</span> <span class="token number">30</span>
        self<span class="token punctuation">.</span>UNK <span class="token operator">=</span> <span class="token number">5</span>
        self<span class="token punctuation">.</span>PAD <span class="token operator">=</span> <span class="token number">4</span>

        self<span class="token punctuation">.</span>N <span class="token operator">=</span> <span class="token number">6</span>
        self<span class="token punctuation">.</span>p <span class="token operator">=</span> <span class="token number">0.1</span>

config <span class="token operator">=</span> Config<span class="token punctuation">(</span><span class="token punctuation">)</span>


<span class="token keyword">class</span> <span class="token class-name">Embedding</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>vocab_size<span class="token punctuation">)</span><span class="token punctuation">:</span>
        super<span class="token punctuation">(</span>Embedding<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token comment" spellcheck="true"># 一个普通的 embedding层，我们可以通过设置padding_idx=config.PAD 来实现论文中的 padding_mask</span>
        self<span class="token punctuation">.</span>embedding <span class="token operator">=</span> nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>vocab_size<span class="token punctuation">,</span>config<span class="token punctuation">.</span>d_model<span class="token punctuation">,</span>padding_idx<span class="token operator">=</span>config<span class="token punctuation">.</span>PAD<span class="token punctuation">)</span>


    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment" spellcheck="true"># 根据每个句子的长度，进行padding，短补长截</span>
        <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>len<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token keyword">if</span> len<span class="token punctuation">(</span>x<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">&lt;</span> config<span class="token punctuation">.</span>padding_size<span class="token punctuation">:</span>
                x<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">.</span>extend<span class="token punctuation">(</span><span class="token punctuation">[</span>config<span class="token punctuation">.</span>UNK<span class="token punctuation">]</span> <span class="token operator">*</span> <span class="token punctuation">(</span>config<span class="token punctuation">.</span>padding_size <span class="token operator">-</span> len<span class="token punctuation">(</span>x<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># 注意 UNK是你词表中用来表示oov的token索引，这里进行了简化，直接假设为6</span>
            <span class="token keyword">else</span><span class="token punctuation">:</span>
                x<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> x<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token punctuation">:</span>config<span class="token punctuation">.</span>padding_size<span class="token punctuation">]</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>embedding<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># batch_size * seq_len * d_model</span>
        <span class="token keyword">return</span> x



<span class="token keyword">class</span> <span class="token class-name">Positional_Encoding</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>

    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>d_model<span class="token punctuation">)</span><span class="token punctuation">:</span>
        super<span class="token punctuation">(</span>Positional_Encoding<span class="token punctuation">,</span>self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>d_model <span class="token operator">=</span> d_model


    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>seq_len<span class="token punctuation">,</span>embedding_dim<span class="token punctuation">)</span><span class="token punctuation">:</span>
        positional_encoding <span class="token operator">=</span> np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token punctuation">(</span>seq_len<span class="token punctuation">,</span>embedding_dim<span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">for</span> pos <span class="token keyword">in</span> range<span class="token punctuation">(</span>positional_encoding<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>positional_encoding<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
                positional_encoding<span class="token punctuation">[</span>pos<span class="token punctuation">]</span><span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> math<span class="token punctuation">.</span>sin<span class="token punctuation">(</span>pos<span class="token operator">/</span><span class="token punctuation">(</span><span class="token number">10000</span><span class="token operator">**</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token operator">*</span>i<span class="token operator">/</span>self<span class="token punctuation">.</span>d_model<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token keyword">if</span> i <span class="token operator">%</span> <span class="token number">2</span> <span class="token operator">==</span> <span class="token number">0</span> <span class="token keyword">else</span> math<span class="token punctuation">.</span>cos<span class="token punctuation">(</span>pos<span class="token operator">/</span><span class="token punctuation">(</span><span class="token number">10000</span><span class="token operator">**</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token operator">*</span>i<span class="token operator">/</span>self<span class="token punctuation">.</span>d_model<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> torch<span class="token punctuation">.</span>from_numpy<span class="token punctuation">(</span>positional_encoding<span class="token punctuation">)</span>


<span class="token keyword">class</span> <span class="token class-name">Mutihead_Attention</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>d_model<span class="token punctuation">,</span>dim_k<span class="token punctuation">,</span>dim_v<span class="token punctuation">,</span>n_heads<span class="token punctuation">)</span><span class="token punctuation">:</span>
        super<span class="token punctuation">(</span>Mutihead_Attention<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>dim_v <span class="token operator">=</span> dim_v
        self<span class="token punctuation">.</span>dim_k <span class="token operator">=</span> dim_k
        self<span class="token punctuation">.</span>n_heads <span class="token operator">=</span> n_heads

        self<span class="token punctuation">.</span>q <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>d_model<span class="token punctuation">,</span>dim_k<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>k <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>d_model<span class="token punctuation">,</span>dim_k<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>v <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>d_model<span class="token punctuation">,</span>dim_v<span class="token punctuation">)</span>

        self<span class="token punctuation">.</span>o <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>dim_v<span class="token punctuation">,</span>d_model<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>norm_fact <span class="token operator">=</span> <span class="token number">1</span> <span class="token operator">/</span> math<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>d_model<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">generate_mask</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>dim<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment" spellcheck="true"># 此处是 sequence mask ，防止 decoder窥视后面时间步的信息。</span>
        <span class="token comment" spellcheck="true"># padding mask 在数据输入模型之前完成。</span>
        matirx <span class="token operator">=</span> np<span class="token punctuation">.</span>ones<span class="token punctuation">(</span><span class="token punctuation">(</span>dim<span class="token punctuation">,</span>dim<span class="token punctuation">)</span><span class="token punctuation">)</span>
        mask <span class="token operator">=</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">(</span>np<span class="token punctuation">.</span>tril<span class="token punctuation">(</span>matirx<span class="token punctuation">)</span><span class="token punctuation">)</span>

        <span class="token keyword">return</span> mask<span class="token operator">==</span><span class="token number">1</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>x<span class="token punctuation">,</span>y<span class="token punctuation">,</span>requires_mask<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">assert</span> self<span class="token punctuation">.</span>dim_k <span class="token operator">%</span> self<span class="token punctuation">.</span>n_heads <span class="token operator">==</span> <span class="token number">0</span> <span class="token operator">and</span> self<span class="token punctuation">.</span>dim_v <span class="token operator">%</span> self<span class="token punctuation">.</span>n_heads <span class="token operator">==</span> <span class="token number">0</span>
        <span class="token comment" spellcheck="true"># size of x : [batch_size * seq_len * batch_size]</span>
        <span class="token comment" spellcheck="true"># 对 x 进行自注意力</span>
        Q <span class="token operator">=</span> self<span class="token punctuation">.</span>q<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span>x<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>x<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span>self<span class="token punctuation">.</span>dim_k <span class="token operator">//</span> self<span class="token punctuation">.</span>n_heads<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># n_heads * batch_size * seq_len * dim_k</span>
        K <span class="token operator">=</span> self<span class="token punctuation">.</span>k<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span>x<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>x<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span>self<span class="token punctuation">.</span>dim_k <span class="token operator">//</span> self<span class="token punctuation">.</span>n_heads<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># n_heads * batch_size * seq_len * dim_k</span>
        V <span class="token operator">=</span> self<span class="token punctuation">.</span>v<span class="token punctuation">(</span>y<span class="token punctuation">)</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span>y<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>y<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span>self<span class="token punctuation">.</span>dim_v <span class="token operator">//</span> self<span class="token punctuation">.</span>n_heads<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># n_heads * batch_size * seq_len * dim_v</span>
        <span class="token comment" spellcheck="true"># print("Attention V shape : {}".format(V.shape))</span>
        attention_score <span class="token operator">=</span> torch<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>Q<span class="token punctuation">,</span>K<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">*</span> self<span class="token punctuation">.</span>norm_fact
        <span class="token keyword">if</span> requires_mask<span class="token punctuation">:</span>
            mask <span class="token operator">=</span> self<span class="token punctuation">.</span>generate_mask<span class="token punctuation">(</span>x<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
            <span class="token comment" spellcheck="true"># masked_fill 函数中，对Mask位置为True的部分进行Mask</span>
            attention_score<span class="token punctuation">.</span>masked_fill<span class="token punctuation">(</span>mask<span class="token punctuation">,</span>value<span class="token operator">=</span>float<span class="token punctuation">(</span><span class="token string">"-inf"</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># 注意这里的小Trick，不需要将Q,K,V 分别MASK,只MASKSoftmax之前的结果就好了</span>
        output <span class="token operator">=</span> torch<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>attention_score<span class="token punctuation">,</span>V<span class="token punctuation">)</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>y<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>y<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
        <span class="token comment" spellcheck="true"># print("Attention output shape : {}".format(output.shape))</span>

        output <span class="token operator">=</span> self<span class="token punctuation">.</span>o<span class="token punctuation">(</span>output<span class="token punctuation">)</span>
        <span class="token keyword">return</span> output


<span class="token keyword">class</span> <span class="token class-name">Feed_Forward</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>input_dim<span class="token punctuation">,</span>hidden_dim<span class="token operator">=</span><span class="token number">2048</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        super<span class="token punctuation">(</span>Feed_Forward<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>L1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>input_dim<span class="token punctuation">,</span>hidden_dim<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>L2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>hidden_dim<span class="token punctuation">,</span>input_dim<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        output <span class="token operator">=</span> nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>L1<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>
        output <span class="token operator">=</span> self<span class="token punctuation">.</span>L2<span class="token punctuation">(</span>output<span class="token punctuation">)</span>
        <span class="token keyword">return</span> output

<span class="token keyword">class</span> <span class="token class-name">Add_Norm</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>dropout <span class="token operator">=</span> nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>config<span class="token punctuation">.</span>p<span class="token punctuation">)</span>
        super<span class="token punctuation">(</span>Add_Norm<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>x<span class="token punctuation">,</span>sub_layer<span class="token punctuation">,</span><span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span>
        sub_output <span class="token operator">=</span> sub_layer<span class="token punctuation">(</span>x<span class="token punctuation">,</span><span class="token operator">**</span>kwargs<span class="token punctuation">)</span>
        <span class="token comment" spellcheck="true"># print("{} output : {}".format(sub_layer,sub_output.size()))</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>dropout<span class="token punctuation">(</span>x <span class="token operator">+</span> sub_output<span class="token punctuation">)</span>

        layer_norm <span class="token operator">=</span> nn<span class="token punctuation">.</span>LayerNorm<span class="token punctuation">(</span>x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
        out <span class="token operator">=</span> layer_norm<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        <span class="token keyword">return</span> out


<span class="token keyword">class</span> <span class="token class-name">Encoder</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        super<span class="token punctuation">(</span>Encoder<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>positional_encoding <span class="token operator">=</span> Positional_Encoding<span class="token punctuation">(</span>config<span class="token punctuation">.</span>d_model<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>muti_atten <span class="token operator">=</span> Mutihead_Attention<span class="token punctuation">(</span>config<span class="token punctuation">.</span>d_model<span class="token punctuation">,</span>config<span class="token punctuation">.</span>dim_k<span class="token punctuation">,</span>config<span class="token punctuation">.</span>dim_v<span class="token punctuation">,</span>config<span class="token punctuation">.</span>n_heads<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>feed_forward <span class="token operator">=</span> Feed_Forward<span class="token punctuation">(</span>config<span class="token punctuation">.</span>d_model<span class="token punctuation">)</span>

        self<span class="token punctuation">.</span>add_norm <span class="token operator">=</span> Add_Norm<span class="token punctuation">(</span><span class="token punctuation">)</span>


    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span> <span class="token comment" spellcheck="true"># batch_size * seq_len 并且 x 的类型不是tensor，是普通list</span>

        x <span class="token operator">+=</span> self<span class="token punctuation">.</span>positional_encoding<span class="token punctuation">(</span>x<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span>config<span class="token punctuation">.</span>d_model<span class="token punctuation">)</span>
        <span class="token comment" spellcheck="true"># print("After positional_encoding: {}".format(x.size()))</span>
        output <span class="token operator">=</span> self<span class="token punctuation">.</span>add_norm<span class="token punctuation">(</span>x<span class="token punctuation">,</span>self<span class="token punctuation">.</span>muti_atten<span class="token punctuation">,</span>y<span class="token operator">=</span>x<span class="token punctuation">)</span>
        output <span class="token operator">=</span> self<span class="token punctuation">.</span>add_norm<span class="token punctuation">(</span>output<span class="token punctuation">,</span>self<span class="token punctuation">.</span>feed_forward<span class="token punctuation">)</span>

        <span class="token keyword">return</span> output

<span class="token comment" spellcheck="true"># 在 Decoder 中，Encoder的输出作为Query和KEy输出的那个东西。即 Decoder的Input作为V。此时是可行的</span>
<span class="token comment" spellcheck="true"># 因为在输入过程中，我们有一个padding操作，将Inputs和Outputs的seq_len这个维度都拉成一样的了</span>
<span class="token comment" spellcheck="true"># 我们知道，QK那个过程得到的结果是 batch_size * seq_len * seq_len .既然 seq_len 一样，那么我们可以这样操作</span>
<span class="token comment" spellcheck="true"># 这样操作的意义是，Outputs 中的 token 分别对于 Inputs 中的每个token作注意力</span>

<span class="token keyword">class</span> <span class="token class-name">Decoder</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        super<span class="token punctuation">(</span>Decoder<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>positional_encoding <span class="token operator">=</span> Positional_Encoding<span class="token punctuation">(</span>config<span class="token punctuation">.</span>d_model<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>muti_atten <span class="token operator">=</span> Mutihead_Attention<span class="token punctuation">(</span>config<span class="token punctuation">.</span>d_model<span class="token punctuation">,</span>config<span class="token punctuation">.</span>dim_k<span class="token punctuation">,</span>config<span class="token punctuation">.</span>dim_v<span class="token punctuation">,</span>config<span class="token punctuation">.</span>n_heads<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>feed_forward <span class="token operator">=</span> Feed_Forward<span class="token punctuation">(</span>config<span class="token punctuation">.</span>d_model<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>add_norm <span class="token operator">=</span> Add_Norm<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>x<span class="token punctuation">,</span>encoder_output<span class="token punctuation">)</span><span class="token punctuation">:</span> <span class="token comment" spellcheck="true"># batch_size * seq_len 并且 x 的类型不是tensor，是普通list</span>
        <span class="token comment" spellcheck="true"># print(x.size())</span>
        x <span class="token operator">+=</span> self<span class="token punctuation">.</span>positional_encoding<span class="token punctuation">(</span>x<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span>config<span class="token punctuation">.</span>d_model<span class="token punctuation">)</span>
        <span class="token comment" spellcheck="true"># print(x.size())</span>
        <span class="token comment" spellcheck="true"># 第一个 sub_layer</span>
        output <span class="token operator">=</span> self<span class="token punctuation">.</span>add_norm<span class="token punctuation">(</span>x<span class="token punctuation">,</span>self<span class="token punctuation">.</span>muti_atten<span class="token punctuation">,</span>y<span class="token operator">=</span>x<span class="token punctuation">,</span>requires_mask<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
        <span class="token comment" spellcheck="true"># 第二个 sub_layer</span>
        output <span class="token operator">=</span> self<span class="token punctuation">.</span>add_norm<span class="token punctuation">(</span>x<span class="token punctuation">,</span>self<span class="token punctuation">.</span>muti_atten<span class="token punctuation">,</span>y<span class="token operator">=</span>encoder_output<span class="token punctuation">,</span>requires_mask<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
        <span class="token comment" spellcheck="true"># 第三个 sub_layer</span>
        output <span class="token operator">=</span> self<span class="token punctuation">.</span>add_norm<span class="token punctuation">(</span>output<span class="token punctuation">,</span>self<span class="token punctuation">.</span>feed_forward<span class="token punctuation">)</span>
        <span class="token keyword">return</span> output

<span class="token keyword">class</span> <span class="token class-name">Transformer_layer</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        super<span class="token punctuation">(</span>Transformer_layer<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>encoder <span class="token operator">=</span> Encoder<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>decoder <span class="token operator">=</span> Decoder<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        x_input<span class="token punctuation">,</span>x_output <span class="token operator">=</span> x
        encoder_output <span class="token operator">=</span> self<span class="token punctuation">.</span>encoder<span class="token punctuation">(</span>x_input<span class="token punctuation">)</span>
        decoder_output <span class="token operator">=</span> self<span class="token punctuation">.</span>decoder<span class="token punctuation">(</span>x_output<span class="token punctuation">,</span>encoder_output<span class="token punctuation">)</span>
        <span class="token keyword">return</span> <span class="token punctuation">(</span>encoder_output<span class="token punctuation">,</span>decoder_output<span class="token punctuation">)</span>

<span class="token keyword">class</span> <span class="token class-name">Transformer</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>N<span class="token punctuation">,</span>vocab_size<span class="token punctuation">,</span>output_dim<span class="token punctuation">)</span><span class="token punctuation">:</span>
        super<span class="token punctuation">(</span>Transformer<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>embedding_input <span class="token operator">=</span> Embedding<span class="token punctuation">(</span>vocab_size<span class="token operator">=</span>vocab_size<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>embedding_output <span class="token operator">=</span> Embedding<span class="token punctuation">(</span>vocab_size<span class="token operator">=</span>vocab_size<span class="token punctuation">)</span>

        self<span class="token punctuation">.</span>output_dim <span class="token operator">=</span> output_dim
        self<span class="token punctuation">.</span>linear <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>config<span class="token punctuation">.</span>d_model<span class="token punctuation">,</span>output_dim<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>softmax <span class="token operator">=</span> nn<span class="token punctuation">.</span>Softmax<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>model <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span><span class="token operator">*</span><span class="token punctuation">[</span>Transformer_layer<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">for</span> _ <span class="token keyword">in</span> range<span class="token punctuation">(</span>N<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>


    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        x_input <span class="token punctuation">,</span> x_output <span class="token operator">=</span> x
        x_input <span class="token operator">=</span> self<span class="token punctuation">.</span>embedding_input<span class="token punctuation">(</span>x_input<span class="token punctuation">)</span>
        x_output <span class="token operator">=</span> self<span class="token punctuation">.</span>embedding_output<span class="token punctuation">(</span>x_output<span class="token punctuation">)</span>

        _ <span class="token punctuation">,</span> output <span class="token operator">=</span> self<span class="token punctuation">.</span>model<span class="token punctuation">(</span><span class="token punctuation">(</span>x_input<span class="token punctuation">,</span>x_output<span class="token punctuation">)</span><span class="token punctuation">)</span>

        output <span class="token operator">=</span> self<span class="token punctuation">.</span>linear<span class="token punctuation">(</span>output<span class="token punctuation">)</span>
        output <span class="token operator">=</span> self<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>output<span class="token punctuation">)</span>

        <span class="token keyword">return</span> output<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p>更多实现可参考：</p>
<ul>
<li><a href="http://nlp.seas.harvard.edu/2018/04/03/attention.html" target="_blank" rel="noopener">http://nlp.seas.harvard.edu/2018/04/03/attention.html</a></li>
</ul>
<h1 id="Transformer-Q-amp-A"><a href="#Transformer-Q-amp-A" class="headerlink" title="Transformer Q&amp;A"></a>Transformer Q&amp;A</h1><h3 id="Transformer如何解决梯度消失问题？"><a href="#Transformer如何解决梯度消失问题？" class="headerlink" title="Transformer如何解决梯度消失问题？"></a>Transformer如何解决梯度消失问题？</h3><p>残差</p>
<h3 id="为何Transformer中使用LN而不用BN"><a href="#为何Transformer中使用LN而不用BN" class="headerlink" title="为何Transformer中使用LN而不用BN?"></a>为何Transformer中使用LN而不用BN?</h3><p>BatchNorm是对一个batch-size样本内的每个特征做归一化，LayerNorm是对每个样本的所有特征做归一化。</p>
<p>形象点来说，假设有一个二维矩阵。行为batch-size，列为样本特征。那么BN就是竖着归一化，LN就是横着归一化。</p>
<p>它们的出发点都是让该层参数稳定下来，避免梯度消失或者梯度爆炸，方便后续的学习。但是也有侧重点。</p>
<p>一般来说，如果你的特征依赖于不同样本间的统计参数，那BN更有效。因为它抹杀了不同特征之间的大小关系，但是保留了不同样本间的大小关系。（CV领域）</p>
<p>而在NLP领域，LN就更加合适。因为它抹杀了不同样本间的大小关系，但是保留了一个样本内不同特征之间的大小关系。对于NLP或者序列任务来说，一条样本的不同特征，其实就是时序上字符取值的变化，样本内的特征关系是非常紧密的。</p>
<h3 id="LN的作用是什么？"><a href="#LN的作用是什么？" class="headerlink" title="LN的作用是什么？"></a>LN的作用是什么？</h3><p>允许使用更大的学习率，加速训练。有一定的抗过拟合作用，使训练过程更加平稳</p>
<h3 id="多头自注意力层中的“多头”如何理解，有什么作用？"><a href="#多头自注意力层中的“多头”如何理解，有什么作用？" class="headerlink" title="多头自注意力层中的“多头”如何理解，有什么作用？"></a>多头自注意力层中的“多头”如何理解，有什么作用？</h3><p>有点类似于CNN的多个卷积核。通过三个线性层的映射，不同头中的Q、K、V是不一样的，而这三个线性层的权重是先初始化后续通过学习得到的。不同的权重可以捕捉到序列中不同的相关性。</p>
<h3 id="Transformer是自回归模型还是自编码模型？"><a href="#Transformer是自回归模型还是自编码模型？" class="headerlink" title="Transformer是自回归模型还是自编码模型？"></a>Transformer是自回归模型还是自编码模型？</h3><p>自回归模型。</p>
<p>所谓自回归，即使用当前自己预测的字符再去预测接下来的信息。Transformer在预测阶段（机器翻译任务）会先预测第一个字，然后在第一个预测的字的基础上接下来再去预测后面的字，是典型的自回归模型。Bert中的Mask任务是典型的自编码模型，即根据上下文字符来预测当前信息。</p>
<h3 id="原论文中Q、K矩阵相乘为什么最后要除以√d-k"><a href="#原论文中Q、K矩阵相乘为什么最后要除以√d-k" class="headerlink" title="原论文中Q、K矩阵相乘为什么最后要除以√d_k"></a>原论文中Q、K矩阵相乘为什么最后要除以√d_k</h3><p>当√d_k特别小的时候，其实除不除无所谓。无论编码器还是解码器Q、K矩阵其实本质是一个相同的矩阵。Q、K相乘其实相等于Q乘以Q的转置，这样造成结果会很大或者很小。小了还好说，大的话会使得后续做softmax继续被放大造成梯度消失，不利于梯度反向传播。</p>
<h3 id="原论中编码器与解码器的Embedding层的权重为什么要乘以√d-model"><a href="#原论中编码器与解码器的Embedding层的权重为什么要乘以√d-model" class="headerlink" title="原论中编码器与解码器的Embedding层的权重为什么要乘以√d_model"></a>原论中编码器与解码器的Embedding层的权重为什么要乘以√d_model</h3><p>为了让embedding层的权重值不至于过小，乘以√d_model 后与位置编码的值差不多，可以保护原有向量空间不被破坏。</p>
<h3 id="Transformer在训练与验证的时候有什么不同"><a href="#Transformer在训练与验证的时候有什么不同" class="headerlink" title="Transformer在训练与验证的时候有什么不同"></a>Transformer在训练与验证的时候有什么不同</h3><p>Transformer在训练的时候是并行的，在验证的时候是串行的。这个问题与Transformer是否是自回归模型考察的是同一个知识点。</p>
<h3 id="Transformer模型的计算复杂度是多少？"><a href="#Transformer模型的计算复杂度是多少？" class="headerlink" title="Transformer模型的计算复杂度是多少？"></a>Transformer模型的计算复杂度是多少？</h3><p>n是序列长度，d是embedding的长度。Transformer中最大的计算量就是多头自注意力层，这里的计算量主要就是QK相乘再乘上V，即两次矩阵相乘。QK相乘是矩阵【n d】乘以【d n】，这个复杂度就是d*n^2</p>
<h3 id="Transformer中三个多头自注意力层分别有什么意义与作用？"><a href="#Transformer中三个多头自注意力层分别有什么意义与作用？" class="headerlink" title="Transformer中三个多头自注意力层分别有什么意义与作用？"></a>Transformer中三个多头自注意力层分别有什么意义与作用？</h3><p>Transformer中有三个多头自注意力层，编码器中有一个，解码器中有两个。</p>
<p>编码器中的多头自注意力层的作用是将原始文本序列信息做整合，转换后的文本序列中每个字符都与整个文本序列的信息相关（这也是Transformer中最创新的思想，尽管根据最新的综述研究表明，Transformer的效果非常好其实多头自注意力层并不占据绝大贡献）。示意图如下：</p>
<img src="/images/loading.gif" data-original="../images/language/image-20211110193809154.png" style="zoom:67%;">

<p>解码器的第一个多头自注意力层比较特殊，原论文给其起名叫Masked Multi-Head-Attention。其一方面也有上图介绍的作用，即对输入文本做整合（对与翻译任务来说，编码器的输入是翻译前的文本，解码器的输入是翻译后的文本）。另一个任务是做掩码，防止信息泄露。拓展解释一下就是在做信息整合的时候，第一个字符其实不应该看到后面的字符，第二个字符也只能看到第一个、第二个字符的信息，以此类推。</p>
<p>解码器的第二个多头自注意力层与编码器的第一个多头自注意力层功能是完全一样的。不过输入需要额外强调下，我们都知道多头自注意力层是通过计算QKV三个矩阵最后完成信息整合的。在这里，Q是解码器整合后的信息，KV两个矩阵是编码器整合后的信息，是两个完全相同的矩阵。QKV矩阵相乘后，翻译前与翻译后的文本也做了充分的交互整合。至此最终得到的向量矩阵用来做后续下游工作。</p>
<h3 id="Transformer中的mask机制有什么作用"><a href="#Transformer中的mask机制有什么作用" class="headerlink" title="Transformer中的mask机制有什么作用"></a>Transformer中的mask机制有什么作用</h3><p>有两个作用。</p>
<ol>
<li>对不等长的序列做padding补齐</li>
<li>掩码防止信息泄露</li>
</ol>
<script>
        document.querySelectorAll('.github-emoji')
          .forEach(el => {
            if (!el.dataset.src) { return; }
            const img = document.createElement('img');
            img.style = 'display:none !important;';
            img.src = el.dataset.src;
            img.addEventListener('error', () => {
              img.remove();
              el.style.color = 'inherit';
              el.style.backgroundImage = 'none';
              el.style.background = 'none';
            });
            img.addEventListener('load', () => {
              img.remove();
            });
            document.body.appendChild(img);
          });
      </script>
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://jackhcc.github.io" rel="external nofollow noreferrer">杰克成</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://jackhcc.github.io/posts/reproduction-series1.html">https://jackhcc.github.io/posts/reproduction-series1.html</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="https://jackhcc.github.io" target="_blank">杰克成</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/tags/Transformer/">
                                    <span class="chip bg-color">Transformer</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/libs/share/css/share.min.css">

<div id="article-share">
    
    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
                <style>
    #reward {
        margin: 40px 0;
        text-align: center;
    }

    #reward .reward-link {
        font-size: 1.4rem;
        line-height: 38px;
    }

    #reward .btn-floating:hover {
        box-shadow: 0 6px 12px rgba(0, 0, 0, 0.2), 0 5px 15px rgba(0, 0, 0, 0.2);
    }

    #rewardModal {
        width: 320px;
        height: 350px;
    }

    #rewardModal .reward-title {
        margin: 15px auto;
        padding-bottom: 5px;
    }

    #rewardModal .modal-content {
        padding: 10px;
    }

    #rewardModal .close {
        position: absolute;
        right: 15px;
        top: 15px;
        color: rgba(0, 0, 0, 0.5);
        font-size: 1.3rem;
        line-height: 20px;
        cursor: pointer;
    }

    #rewardModal .close:hover {
        color: #ef5350;
        transform: scale(1.3);
        -moz-transform:scale(1.3);
        -webkit-transform:scale(1.3);
        -o-transform:scale(1.3);
    }

    #rewardModal .reward-tabs {
        margin: 0 auto;
        width: 210px;
    }

    .reward-tabs .tabs {
        height: 38px;
        margin: 10px auto;
        padding-left: 0;
    }

    .reward-content ul {
        padding-left: 0 !important;
    }

    .reward-tabs .tabs .tab {
        height: 38px;
        line-height: 38px;
    }

    .reward-tabs .tab a {
        color: #fff;
        background-color: #ccc;
    }

    .reward-tabs .tab a:hover {
        background-color: #ccc;
        color: #fff;
    }

    .reward-tabs .wechat-tab .active {
        color: #fff !important;
        background-color: #22AB38 !important;
    }

    .reward-tabs .alipay-tab .active {
        color: #fff !important;
        background-color: #019FE8 !important;
    }

    .reward-tabs .reward-img {
        width: 210px;
        height: 210px;
    }
</style>

<div id="reward">
    <a href="#rewardModal" class="reward-link modal-trigger btn-floating btn-medium waves-effect waves-light red">赏</a>

    <!-- Modal Structure -->
    <div id="rewardModal" class="modal">
        <div class="modal-content">
            <a class="close modal-close"><i class="fas fa-times"></i></a>
            <h4 class="reward-title">你的赏识是我前进的动力</h4>
            <div class="reward-content">
                <div class="reward-tabs">
                    <ul class="tabs row">
                        <li class="tab col s6 alipay-tab waves-effect waves-light"><a href="#alipay">支付宝</a></li>
                        <li class="tab col s6 wechat-tab waves-effect waves-light"><a href="#wechat">微 信</a></li>
                    </ul>
                    <div id="alipay">
                        <img src="/images/loading.gif" data-original="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/medias/reward/aliqr.png" class="reward-img" alt="支付宝打赏二维码">
                    </div>
                    <div id="wechat">
                        <img src="/images/loading.gif" data-original="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/medias/reward/wxqr.png" class="reward-img" alt="微信打赏二维码">
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>

<script>
    $(function () {
        $('.tabs').tabs();
    });
</script>
            
        </div>
    </div>

    
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/libs/gitalk/gitalk.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/css/my-gitalk.css">

<div class="card gitalk-card" data-aos="fade-up">
    <div class="comment_headling" style="font-size: 20px; font-weight: 700; position: relative; left: 20px; top: 15px; padding-bottom: 5px;">
        <i class="fas fa-comments fa-fw" aria-hidden="true"></i>
        <span>评论</span>
    </div>
    <div id="gitalk-container" class="card-content"></div>
</div>

<script src="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/libs/gitalk/gitalk.min.js"></script>
<script>
    let gitalk = new Gitalk({
        clientID: '3821a0bbb773038a51fc',
        clientSecret: '4b30b507d67ec5497ec0e77f43f80cb3e0d7dd3a',
        repo: 'JackHCC.github.io',
        owner: 'JackHCC',
        admin: "JackHCC",
        id: '2021-10-28T22-56-44',
        distractionFreeMode: false  // Facebook-like distraction free mode
    });

    gitalk.render('gitalk-container');
</script>
    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/posts/dl-series16.html">
                    <div class="card-image">
                        
                        
                        <img src="/images/loading.gif" data-original="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/medias/featureimages/22.jpg" class="responsive-img" alt="DL专栏16-计算机视觉技巧">
                        
                        <span class="card-title">DL专栏16-计算机视觉技巧</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            一些计算机视觉的方法汇总
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2021-10-29
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/Deep-Learning/" class="post-category">
                                    Deep Learning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/CV/">
                        <span class="chip bg-color">CV</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/posts/Open-Projects.html">
                    <div class="card-image">
                        
                        
                        <img src="/images/loading.gif" data-original="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/medias/featureimages/11.jpg" class="responsive-img" alt="Awesome Open Projects">
                        
                        <span class="card-title">Awesome Open Projects</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            开源优质项目汇总
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2021-10-27
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/Awesome/" class="post-category">
                                    Awesome
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/project/">
                        <span class="chip bg-color">project</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/libs/codeBlock/codeBlockFuction.js"></script>

<!-- 代码语言 -->

<script type="text/javascript" src="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/libs/codeBlock/codeShrink.js"></script>


<!-- 代码块折行 -->

<style type="text/css">
code[class*="language-"], pre[class*="language-"] { white-space: pre !important; }
</style>


    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('4'),
            headingSelector: 'h1, h2, h3, h4'
        });

        // modify the toc link href to support Chinese.
        let i = 0;
        let tocHeading = 'toc-heading-';
        $('#toc-content a').each(function () {
            $(this).attr('href', '#' + tocHeading + (++i));
        });

        // modify the heading title id to support Chinese.
        i = 0;
        $('#articleContent').children('h1, h2, h3, h4').each(function () {
            $(this).attr('id', tocHeading + (++i));
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>

    

</main>



    <footer class="page-footer bg-color">
    <div class="container row center-align" style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            <span id="year">2020</span>
            <a href="https://jackhcc.github.io" target="_blank">杰克成</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            <br>
            
            &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                class="white-color">3591.2k</span>&nbsp;字
            
            
            
            
            
            
            <span id="busuanzi_container_site_pv">
                |&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;<span id="busuanzi_value_site_pv"
                    class="white-color"></span>&nbsp;次
            </span>
            
            
            <span id="busuanzi_container_site_uv">
                |&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;<span id="busuanzi_value_site_uv"
                    class="white-color"></span>&nbsp;人
            </span>
            
            <br>
            
            <span id="sitetime">载入运行时间...</span>
            <script>
                function siteTime() {
                    var seconds = 1000;
                    var minutes = seconds * 60;
                    var hours = minutes * 60;
                    var days = hours * 24;
                    var years = days * 365;
                    var today = new Date();
                    var startYear = "2020";
                    var startMonth = "2";
                    var startDate = "27";
                    var startHour = "6";
                    var startMinute = "30";
                    var startSecond = "0";
                    var todayYear = today.getFullYear();
                    var todayMonth = today.getMonth() + 1;
                    var todayDate = today.getDate();
                    var todayHour = today.getHours();
                    var todayMinute = today.getMinutes();
                    var todaySecond = today.getSeconds();
                    var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                    var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                    var diff = t2 - t1;
                    var diffYears = Math.floor(diff / years);
                    var diffDays = Math.floor((diff / days) - diffYears * 365);
                    var diffHours = Math.floor((diff - (diffYears * 365 + diffDays) * days) / hours);
                    var diffMinutes = Math.floor((diff - (diffYears * 365 + diffDays) * days - diffHours * hours) /
                        minutes);
                    var diffSeconds = Math.floor((diff - (diffYears * 365 + diffDays) * days - diffHours * hours -
                        diffMinutes * minutes) / seconds);
                    if (startYear == todayYear) {
                        document.getElementById("year").innerHTML = todayYear;
                        document.getElementById("sitetime").innerHTML = "本站已安全运行 " + diffDays + " 天 " + diffHours +
                            " 小时 " + diffMinutes + " 分钟 " + diffSeconds + " 秒";
                    } else {
                        document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                        document.getElementById("sitetime").innerHTML = "本站已安全运行 " + diffYears + " 年 " + diffDays +
                            " 天 " + diffHours + " 小时 " + diffMinutes + " 分钟 " + diffSeconds + " 秒";
                    }
                }
                setInterval(siteTime, 1000);
            </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/JackHCC" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:jackcc0701@163.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>



    <a href="https://www.facebook.com/profile.php?id=100046343443643" class="tooltipped" target="_blank" data-tooltip="关注我的Facebook: https://www.facebook.com/profile.php?id=100046343443643" data-position="top" data-delay="50">
        <i class="fab fa-facebook-f"></i>
    </a>



    <a href="https://twitter.com/JackChe66021834" class="tooltipped" target="_blank" data-tooltip="关注我的Twitter: https://twitter.com/JackChe66021834" data-position="top" data-delay="50">
        <i class="fab fa-twitter"></i>
    </a>



    <a href="tencent://AddContact/?fromId=50&fromSubId=1&subcmd=all&uin=2508074836" class="tooltipped" target="_blank" data-tooltip="QQ联系我: 2508074836" data-position="top" data-delay="50">
        <i class="fab fa-qq"></i>
    </a>



    <a href="https://weibo.com/u/6885584679" class="tooltipped" target="_blank" data-tooltip="关注我的微博: https://weibo.com/u/6885584679" data-position="top" data-delay="50">
        <i class="fab fa-weibo"></i>
    </a>



    <a href="https://www.zhihu.com/people/8f8482f01f0d6a04e844efe32e0f0710" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/8f8482f01f0d6a04e844efe32e0f0710" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script src="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/js/search.js"></script>
<script type="text/javascript">
$(function () {
    searchFunc("https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/" + "search.xml", 'searchInput', 'searchResult');
});
</script>
    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/libs/materialize/materialize.min.js"></script>
    <script src="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/libs/aos/aos.js"></script>
    <script src="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/js/matery.js"></script>

    <canvas class="fireworks" style="position: fixed;left: 0;top: 0;z-index: 1; pointer-events: none;" ></canvas>
    <script type="text/javascript" src="//cdn.bootcss.com/animejs/2.2.0/anime.min.js"></script>
    <script type="text/javascript" src="/js/fireworks.js"></script>

    <script type="text/javascript">
        //只在桌面版网页启用特效
        var windowWidth = $(window).width();
        if (windowWidth > 768) {
            document.write('<script type="text/javascript" src="/js/sakura.js"><\/script>'); }
    </script>

    <!-- weather -->
	<script type="text/javascript">
	WIDGET = {FID: 'TToslpmkVO'}
	</script>
	<script type="text/javascript" src="https://apip.weatherdt.com/float/static/js/r.js?v=1111"></script>


    <!-- Global site tag (gtag.js) - Google Analytics -->


    <!-- Baidu Analytics -->

<script>
    var _hmt = _hmt || [];
    (function () {
        var hm = document.createElement("script");
        hm.src = "https://hm.baidu.com/hm.js?";
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(hm, s);
    })();
</script>

    <!-- Baidu Push -->

    
    
    <script async src="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/libs/others/busuanzi.pure.mini.js"></script>
    

    
        <script src="//code.tidio.co/kqhlkxviiccyoa0czpfpu4ijuey9hfre.js"></script>
        <script> 
            $(document).ready(function () {
                setInterval(change_Tidio, 50);  
                function change_Tidio() { 
                    var tidio=$("#tidio-chat iframe");
                    if(tidio.css("display")=="block"&& $(window).width()>977 ){
                        document.getElementById("tidio-chat-iframe").style.bottom= ($("div#backTop.top-scroll").css("display")=="none" &&$(window).width()>977)>0? "-40px" : ($("div.toc-title").length&&$(window).width()>977)>0?"85px":"20px";   
                        document.getElementById("tidio-chat-iframe").style.right="-15px";   
                        document.getElementById("tidio-chat-iframe").style.height=parseInt(tidio.css("height"))>=520?"520px":tidio.css("height");
                        document.getElementById("tidio-chat-iframe").style.zIndex="997";
                    } 
                    else if(tidio.css("display")=="block"&&$(window).width()>601 &&$(window).width()<992 ){
                        document.getElementById("tidio-chat-iframe").style.bottom= ($("div#backTop.top-scroll").css("display")=="none" && 601< $(window).width()<992)>0? "-40px":"20px" ;   
                        document.getElementById("tidio-chat-iframe").style.right="-15px"; 
                        document.getElementById("tidio-chat-iframe").style.zIndex="997";
                    }
                    else if(tidio.css("display")=="block"&&$(window).width()<601 && parseInt(tidio.css("height"))<230){
                        document.getElementById("tidio-chat-iframe").style.bottom= ($("div#backTop.top-scroll").css("display")=="none" && $(window).width()<601)>0? "-10px":"45px" ;   
                        document.getElementById("tidio-chat-iframe").style.zIndex="997";
                    }
                    if( tidio.css("display")=="block"&&$(window).width()<601 && parseInt(tidio.css("height"))>=230){
                        document.getElementById("tidio-chat-iframe").style.zIndex="998";
                    }
                } 
            }); 
        </script>
    

    

    
    <script type="text/javascript" color="0,0,255"
        pointColor="0,0,255" opacity='0.7'
        zIndex="-1" count="99"
        src="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/libs/background/canvas-nest.js"></script>
    

    

    
    <script type="text/javascript" src="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/libs/background/ribbon-dynamic.js" async="async"></script>
    
    
    
    <script src="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/libs/instantpage/instantpage.js" type="module"></script>
    

        <script src="//cdn.jsdelivr.net/npm/js-base64/base64.min.js"></script>
        <script>
        $('a').each(function() {
          const $this = $(this);
          const href = $this.attr('href');
          if (href && href.match('^((http|https|thunder|qqdl|ed2k|Flashget|qbrowser|ftp|rtsp|mms)://)')) {
            const strs = href.split('/');
            if (strs.length >= 3) {
                const host = strs[2];
                if (host !== 'your_domain' || window.location.host) {
                    $this.attr('href', '/go.html?u='+Base64.encode(href)+'').attr('rel', 'external nofollow noopener noreferrer');
                    if (true) {
                        $this.attr('target', '_blank');
                    }
                }
            }
          }
        });
        </script><script>!function(e){var c=Array.prototype.slice.call(document.querySelectorAll("img[data-original]"));function i(){for(var r=0;r<c.length;r++)t=c[r],0<=(n=t.getBoundingClientRect()).bottom&&0<=n.left&&n.top<=(e.innerHeight||document.documentElement.clientHeight)&&function(){var t,n,e,i,o=c[r];t=o,n=function(){c=c.filter(function(t){return o!==t})},e=new Image,i=t.getAttribute("data-original"),e.onload=function(){t.src=i,n&&n()},e.src=i}();var t,n}i(),e.addEventListener("scroll",function(){var t,n;t=i,n=e,clearTimeout(t.tId),t.tId=setTimeout(function(){t.call(n)},500)})}(this);</script><script>window.addEventListener("load",function(){var t=/\.(gif|jpg|jpeg|tiff|png)$/i,r=/^data:image\/[a-z]+;base64,/;Array.prototype.slice.call(document.querySelectorAll("img[data-original]")).forEach(function(a){var e=a.parentNode;"A"===e.tagName&&(e.href.match(t)||e.href.match(r))&&(e.href=a.dataset.original)})});</script></body>

</html>

