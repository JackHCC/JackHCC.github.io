<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="DL专栏1-PyTorch Tricks, JackHCC">
    <meta name="description" content="PyTorch使用及技巧">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <title>DL专栏1-PyTorch Tricks | JackHCC</title>
    <link rel="icon" type="image/png" href="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/medias/favicon.png">

    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/libs/awesome/css/all.css">
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/libs/materialize/materialize.min.css">
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/libs/aos/aos.css">
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/libs/animate/animate.min.css">
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/libs/lightGallery/css/lightgallery.min.css">
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/css/matery.css">
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/css/my.css">
    
    <script src="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/libs/jquery/jquery.min.js"></script>
    
<meta name="generator" content="Hexo 4.2.0"><link rel="alternate" href="/atom.xml" title="JackHCC" type="application/atom+xml">
<link rel="stylesheet" href="/css/prism-hopscotch.css" type="text/css">
<link rel="stylesheet" href="/css/prism-line-numbers.css" type="text/css"></head>


<body>
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper head-container">
            <div class="brand-logo">
                <a href="/" class="waves-effect waves-light">
                    
                    <img src="/images/loading.gif" data-original="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/medias/me.jpg" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">JackHCC</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="" class="waves-effect waves-light">

      
      <i class="fas fa-list" style="zoom: 0.6;"></i>
      
      <span>Tools</span>
      <i class="fas fa-chevron-down" aria-hidden="true" style="zoom: 0.6;"></i>
    </a>
    <ul class="sub-nav menus_item_child ">
      
      <li>
        <a href="https://creativecc.cn/" target="_blank" rel="noopener">
          
          <i class="fas fa-book" style="margin-top: -20px; zoom: 0.6;"></i>
          
          <span>Creative工具导航</span>
        </a>
      </li>
      
      <li>
        <a href="https://blog.creativecc.cn/Arxiv-NLP-Reporter/" target="_blank" rel="noopener">
          
          <i class="fas fa-film" style="margin-top: -20px; zoom: 0.6;"></i>
          
          <span>NLP每日论文</span>
        </a>
      </li>
      
      <li>
        <a href="http://chat.creativecc.cn/" target="_blank" rel="noopener">
          
          <i class="fas fa-music" style="margin-top: -20px; zoom: 0.6;"></i>
          
          <span>RocketChat聊天室</span>
        </a>
      </li>
      
      <li>
        <a href="/contact">
          
          <i class="fas fa-comments" style="margin-top: -20px; zoom: 0.6;"></i>
          
          <span>Contact留言板</span>
        </a>
      </li>
      
    </ul>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/friends" class="waves-effect waves-light">
      
      <i class="fas fa-address-book" style="zoom: 0.6;"></i>
      
      <span>友情链接</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/about" class="waves-effect waves-light">
      
      <i class="fas fa-user-circle" style="zoom: 0.6;"></i>
      
      <span>关于</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>

<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/images/loading.gif" data-original="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/medias/me.jpg" class="logo-img circle responsive-img">
        
        <div class="logo-name">JackHCC</div>
        <div class="logo-desc">
            
            Make the world betterrrr!!!
            
        </div>
    </div>

    

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="javascript:;">
			
				<i class="fa-fw fas fa-list"></i>
			
			Tools
			<span class="m-icon"><i class="fas fa-chevron-right"></i></span>
		</a>
            <ul  style="background:  ;" >
              
                <li>   
				
                  <a href="https://creativecc.cn/ " target="_blank" rel="noopener" style="margin-left:75px";>
				  
				   <i class="fa fas fa-book" style="position: absolute;left:50px" ></i>
			      
		          <span>Creative工具导航</span>
                  </a>
                </li>
              
                <li>   
				
                  <a href="https://blog.creativecc.cn/Arxiv-NLP-Reporter/ " target="_blank" rel="noopener" style="margin-left:75px";>
				  
				   <i class="fa fas fa-film" style="position: absolute;left:50px" ></i>
			      
		          <span>NLP每日论文</span>
                  </a>
                </li>
              
                <li>   
				
                  <a href="http://chat.creativecc.cn/ " target="_blank" rel="noopener" style="margin-left:75px";>
				  
				   <i class="fa fas fa-music" style="position: absolute;left:50px" ></i>
			      
		          <span>RocketChat聊天室</span>
                  </a>
                </li>
              
                <li>   
				
                  <a href="/contact " style="margin-left:75px";>
				  
				   <i class="fa fas fa-comments" style="position: absolute;left:50px" ></i>
			      
		          <span>Contact留言板</span>
                  </a>
                </li>
               
            </ul>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/friends" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-address-book"></i>
			
			友情链接
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/about" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-user-circle"></i>
			
			关于
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/JackHCC/JackHCC.github.io" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>

        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/JackHCC/JackHCC.github.io" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    
<script src="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/libs/cryptojs/crypto-js.min.js"></script>
<script>
    (function() {
        let pwd = '';
        if (pwd && pwd.length > 0) {
            if (pwd !== CryptoJS.SHA256(prompt('请输入访问本文章的密码')).toString(CryptoJS.enc.Hex)) {
                alert('密码错误，将返回主页！');
                location.href = '/';
            }
        }
    })();
</script>




<div class="bg-cover pd-header post-cover" style="background-image: url('https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/medias/featureimages/5.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">DL专栏1-PyTorch Tricks</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/libs/tocbot/tocbot.css">
<style>
    #articleContent h1::before,
    #articleContent h2::before,
    #articleContent h3::before,
    #articleContent h4::before,
    #articleContent h5::before,
    #articleContent h6::before {
        display: block;
        content: " ";
        height: 100px;
        margin-top: -100px;
        visibility: hidden;
    }

    #articleContent :focus {
        outline: none;
    }

    .toc-fixed {
        position: fixed;
        top: 64px;
    }

    .toc-widget {
        width: 345px;
        padding-left: 20px;
    }

    .toc-widget .toc-title {
        margin: 35px 0 15px 0;
        padding-left: 17px;
        font-size: 1.5rem;
        font-weight: bold;
        line-height: 1.5rem;
    }

    .toc-widget ol {
        padding: 0;
        list-style: none;
    }

    #toc-content {
        height: calc(100vh - 250px);
        overflow: auto;
    }

    #toc-content ol {
        padding-left: 10px;
    }

    #toc-content ol li {
        padding-left: 10px;
    }

    #toc-content .toc-link:hover {
        color: #42b983;
        font-weight: 700;
        text-decoration: underline;
    }

    #toc-content .toc-link::before {
        background-color: transparent;
        max-height: 25px;
    }

    #toc-content .is-active-link {
        color: #42b983;
    }

    #toc-content .is-active-link::before {
        background-color: #42b983;
    }

    #floating-toc-btn {
        position: fixed;
        right: 30px;
        bottom: 146px;
        padding-top: 15px;
        margin-bottom: 0;
        z-index: 998;
    }

    #floating-toc-btn .btn-floating {
        width: 48px;
        height: 48px;
    }

    #floating-toc-btn .btn-floating i {
        line-height: 48px;
        font-size: 1.4rem;
    }
</style>
<div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/tags/PyTorch/">
                                <span class="chip bg-color">PyTorch</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/categories/Deep-Learning/" class="post-category">
                                Deep Learning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2021-10-04
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2021-11-18
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    17.6k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    81 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>

        </div>
        <hr class="clearfix">
        <div class="card-content article-card-content">
            <div id="articleContent">
                <h1 id="PyTorch训练代码模板"><a href="#PyTorch训练代码模板" class="headerlink" title="PyTorch训练代码模板"></a>PyTorch训练代码模板</h1><p>从参数定义，到网络模型定义，再到训练步骤，验证步骤，测试步骤，总结了一套较为直观的模板。目录如下：</p>
<ol>
<li>导入包以及设置随机种子</li>
<li>以类的方式定义超参数</li>
<li>定义自己的模型</li>
<li>定义早停类(此步骤可以省略)</li>
<li>定义自己的数据集Dataset,DataLoader</li>
<li>实例化模型，设置loss，优化器等</li>
<li>开始训练以及调整lr</li>
<li>绘图</li>
<li>预测</li>
</ol>
<h2 id="一、导入包以及设置随机种子"><a href="#一、导入包以及设置随机种子" class="headerlink" title="一、导入包以及设置随机种子"></a>一、导入包以及设置随机种子</h2><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
<span class="token keyword">import</span> torch
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn
<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
<span class="token keyword">import</span> pandas <span class="token keyword">as</span> pd
<span class="token keyword">from</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data <span class="token keyword">import</span> DataLoader<span class="token punctuation">,</span> Dataset
<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>model_selection <span class="token keyword">import</span> train_test_split
<span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> plt

<span class="token keyword">import</span> random
seed <span class="token operator">=</span> <span class="token number">42</span>
torch<span class="token punctuation">.</span>manual_seed<span class="token punctuation">(</span>seed<span class="token punctuation">)</span>
np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>seed<span class="token punctuation">(</span>seed<span class="token punctuation">)</span>
random<span class="token punctuation">.</span>seed<span class="token punctuation">(</span>seed<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h2 id="二、以类的方式定义超参数"><a href="#二、以类的方式定义超参数" class="headerlink" title="二、以类的方式定义超参数"></a>二、以类的方式定义超参数</h2><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">argparse</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">pass</span>

args <span class="token operator">=</span> argparse<span class="token punctuation">(</span><span class="token punctuation">)</span>
args<span class="token punctuation">.</span>epochs<span class="token punctuation">,</span> args<span class="token punctuation">.</span>learning_rate<span class="token punctuation">,</span> args<span class="token punctuation">.</span>patience <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">30</span><span class="token punctuation">,</span> <span class="token number">0.001</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">]</span>
args<span class="token punctuation">.</span>hidden_size<span class="token punctuation">,</span> args<span class="token punctuation">.</span>input_size<span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">40</span><span class="token punctuation">,</span> <span class="token number">30</span><span class="token punctuation">]</span>
args<span class="token punctuation">.</span>device<span class="token punctuation">,</span> <span class="token operator">=</span> <span class="token punctuation">[</span>torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">"cuda:0"</span> <span class="token keyword">if</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>is_available<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">else</span> <span class="token string">"cpu"</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h2 id="三、定义自己的模型"><a href="#三、定义自己的模型" class="headerlink" title="三、定义自己的模型"></a>三、定义自己的模型</h2><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Your_model</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        super<span class="token punctuation">(</span>Your_model<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token keyword">pass</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">pass</span>
        <span class="token keyword">return</span> x<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h2 id="四、定义早停类-此步骤可以省略"><a href="#四、定义早停类-此步骤可以省略" class="headerlink" title="四、定义早停类(此步骤可以省略)"></a>四、定义早停类(此步骤可以省略)</h2><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">EarlyStopping</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>patience<span class="token operator">=</span><span class="token number">7</span><span class="token punctuation">,</span>verbose<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>delta<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>patience <span class="token operator">=</span> patience
        self<span class="token punctuation">.</span>verbose <span class="token operator">=</span> verbose
        self<span class="token punctuation">.</span>counter <span class="token operator">=</span> <span class="token number">0</span>
        self<span class="token punctuation">.</span>best_score <span class="token operator">=</span> None
        self<span class="token punctuation">.</span>early_stop <span class="token operator">=</span> <span class="token boolean">False</span>
        self<span class="token punctuation">.</span>val_loss_min <span class="token operator">=</span> np<span class="token punctuation">.</span>Inf
        self<span class="token punctuation">.</span>delta <span class="token operator">=</span> delta
    <span class="token keyword">def</span> <span class="token function">__call__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>val_loss<span class="token punctuation">,</span>model<span class="token punctuation">,</span>path<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"val_loss={}"</span><span class="token punctuation">.</span>format<span class="token punctuation">(</span>val_loss<span class="token punctuation">)</span><span class="token punctuation">)</span>
        score <span class="token operator">=</span> <span class="token operator">-</span>val_loss
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>best_score <span class="token keyword">is</span> None<span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>best_score <span class="token operator">=</span> score
            self<span class="token punctuation">.</span>save_checkpoint<span class="token punctuation">(</span>val_loss<span class="token punctuation">,</span>model<span class="token punctuation">,</span>path<span class="token punctuation">)</span>
        <span class="token keyword">elif</span> score <span class="token operator">&lt;</span> self<span class="token punctuation">.</span>best_score<span class="token operator">+</span>self<span class="token punctuation">.</span>delta<span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>counter<span class="token operator">+=</span><span class="token number">1</span>
            <span class="token keyword">print</span><span class="token punctuation">(</span>f<span class="token string">'EarlyStopping counter: {self.counter} out of {self.patience}'</span><span class="token punctuation">)</span>
            <span class="token keyword">if</span> self<span class="token punctuation">.</span>counter<span class="token operator">>=</span>self<span class="token punctuation">.</span>patience<span class="token punctuation">:</span>
                self<span class="token punctuation">.</span>early_stop <span class="token operator">=</span> <span class="token boolean">True</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>best_score <span class="token operator">=</span> score
            self<span class="token punctuation">.</span>save_checkpoint<span class="token punctuation">(</span>val_loss<span class="token punctuation">,</span>model<span class="token punctuation">,</span>path<span class="token punctuation">)</span>
            self<span class="token punctuation">.</span>counter <span class="token operator">=</span> <span class="token number">0</span>
    <span class="token keyword">def</span> <span class="token function">save_checkpoint</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>val_loss<span class="token punctuation">,</span>model<span class="token punctuation">,</span>path<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>verbose<span class="token punctuation">:</span>
            <span class="token keyword">print</span><span class="token punctuation">(</span>
                f<span class="token string">'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...'</span><span class="token punctuation">)</span>
        torch<span class="token punctuation">.</span>save<span class="token punctuation">(</span>model<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> path<span class="token operator">+</span><span class="token string">'/'</span><span class="token operator">+</span><span class="token string">'model_checkpoint.pth'</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>val_loss_min <span class="token operator">=</span> val_loss<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h2 id="五、定义自己的数据集Dataset-DataLoader"><a href="#五、定义自己的数据集Dataset-DataLoader" class="headerlink" title="五、定义自己的数据集Dataset,DataLoader"></a>五、定义自己的数据集Dataset,DataLoader</h2><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Dataset_name</span><span class="token punctuation">(</span>Dataset<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> flag<span class="token operator">=</span><span class="token string">'train'</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">assert</span> flag <span class="token keyword">in</span> <span class="token punctuation">[</span><span class="token string">'train'</span><span class="token punctuation">,</span> <span class="token string">'test'</span><span class="token punctuation">,</span> <span class="token string">'valid'</span><span class="token punctuation">]</span>
        self<span class="token punctuation">.</span>flag <span class="token operator">=</span> flag
        self<span class="token punctuation">.</span>__load_data__<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">__getitem__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> index<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">pass</span>
    <span class="token keyword">def</span> <span class="token function">__len__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">pass</span>

    <span class="token keyword">def</span> <span class="token function">__load_data__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> csv_paths<span class="token punctuation">:</span> list<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">pass</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span>
            <span class="token string">"train_X.shape:{}\ntrain_Y.shape:{}\nvalid_X.shape:{}\nvalid_Y.shape:{}\n"</span>
            <span class="token punctuation">.</span>format<span class="token punctuation">(</span>self<span class="token punctuation">.</span>train_X<span class="token punctuation">.</span>shape<span class="token punctuation">,</span> self<span class="token punctuation">.</span>train_Y<span class="token punctuation">.</span>shape<span class="token punctuation">,</span> self<span class="token punctuation">.</span>valid_X<span class="token punctuation">.</span>shape<span class="token punctuation">,</span> self<span class="token punctuation">.</span>valid_Y<span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token punctuation">)</span>

train_dataset <span class="token operator">=</span> Dataset_name<span class="token punctuation">(</span>flag<span class="token operator">=</span><span class="token string">'train'</span><span class="token punctuation">)</span>
train_dataloader <span class="token operator">=</span> DataLoader<span class="token punctuation">(</span>dataset<span class="token operator">=</span>train_dataset<span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">64</span><span class="token punctuation">,</span> shuffle<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
valid_dataset <span class="token operator">=</span> Dataset_name<span class="token punctuation">(</span>flag<span class="token operator">=</span><span class="token string">'valid'</span><span class="token punctuation">)</span>
valid_dataloader <span class="token operator">=</span> DataLoader<span class="token punctuation">(</span>dataset<span class="token operator">=</span>valid_dataset<span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">64</span><span class="token punctuation">,</span> shuffle<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h2 id="六、实例化模型，设置loss，优化器等"><a href="#六、实例化模型，设置loss，优化器等" class="headerlink" title="六、实例化模型，设置loss，优化器等"></a>六、实例化模型，设置loss，优化器等</h2><pre class="line-numbers language-python"><code class="language-python">model <span class="token operator">=</span> Your_model<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>args<span class="token punctuation">.</span>device<span class="token punctuation">)</span>
criterion <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>MSELoss<span class="token punctuation">(</span><span class="token punctuation">)</span>
optimizer <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span>Your_model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>lr<span class="token operator">=</span>args<span class="token punctuation">.</span>learning_rate<span class="token punctuation">)</span>

train_loss <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
valid_loss <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
train_epochs_loss <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
valid_epochs_loss <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>

early_stopping <span class="token operator">=</span> EarlyStopping<span class="token punctuation">(</span>patience<span class="token operator">=</span>args<span class="token punctuation">.</span>patience<span class="token punctuation">,</span>verbose<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h2 id="七、开始训练以及调整lr"><a href="#七、开始训练以及调整lr" class="headerlink" title="七、开始训练以及调整lr"></a>七、开始训练以及调整lr</h2><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">for</span> epoch <span class="token keyword">in</span> range<span class="token punctuation">(</span>args<span class="token punctuation">.</span>epochs<span class="token punctuation">)</span><span class="token punctuation">:</span>
    Your_model<span class="token punctuation">.</span>train<span class="token punctuation">(</span><span class="token punctuation">)</span>
    train_epoch_loss <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
    <span class="token keyword">for</span> idx<span class="token punctuation">,</span><span class="token punctuation">(</span>data_x<span class="token punctuation">,</span>data_y<span class="token punctuation">)</span> <span class="token keyword">in</span> enumerate<span class="token punctuation">(</span>train_dataloader<span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        data_x <span class="token operator">=</span> data_x<span class="token punctuation">.</span>to<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>float32<span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>args<span class="token punctuation">.</span>device<span class="token punctuation">)</span>
        data_y <span class="token operator">=</span> data_y<span class="token punctuation">.</span>to<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>float32<span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>args<span class="token punctuation">.</span>device<span class="token punctuation">)</span>
        outputs <span class="token operator">=</span> Your_model<span class="token punctuation">(</span>data_x<span class="token punctuation">)</span>
        optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
        loss <span class="token operator">=</span> criterion<span class="token punctuation">(</span>data_y<span class="token punctuation">,</span>outputs<span class="token punctuation">)</span>
        loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
        optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>
        train_epoch_loss<span class="token punctuation">.</span>append<span class="token punctuation">(</span>loss<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        train_loss<span class="token punctuation">.</span>append<span class="token punctuation">(</span>loss<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">if</span> idx<span class="token operator">%</span><span class="token punctuation">(</span>len<span class="token punctuation">(</span>train_dataloader<span class="token punctuation">)</span><span class="token operator">//</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token operator">==</span><span class="token number">0</span><span class="token punctuation">:</span>
            <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"epoch={}/{},{}/{}of train, loss={}"</span><span class="token punctuation">.</span>format<span class="token punctuation">(</span>
                epoch<span class="token punctuation">,</span> args<span class="token punctuation">.</span>epochs<span class="token punctuation">,</span> idx<span class="token punctuation">,</span> len<span class="token punctuation">(</span>train_dataloader<span class="token punctuation">)</span><span class="token punctuation">,</span>loss<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    train_epochs_loss<span class="token punctuation">.</span>append<span class="token punctuation">(</span>np<span class="token punctuation">.</span>average<span class="token punctuation">(</span>train_epoch_loss<span class="token punctuation">)</span><span class="token punctuation">)</span>

    <span class="token comment" spellcheck="true">#=====================valid============================</span>
    Your_model<span class="token punctuation">.</span>eval<span class="token punctuation">(</span><span class="token punctuation">)</span>
    valid_epoch_loss <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
    <span class="token keyword">for</span> idx<span class="token punctuation">,</span><span class="token punctuation">(</span>data_x<span class="token punctuation">,</span>data_y<span class="token punctuation">)</span> <span class="token keyword">in</span> enumerate<span class="token punctuation">(</span>valid_dataloader<span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        data_x <span class="token operator">=</span> data_x<span class="token punctuation">.</span>to<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>float32<span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>args<span class="token punctuation">.</span>device<span class="token punctuation">)</span>
        data_y <span class="token operator">=</span> data_y<span class="token punctuation">.</span>to<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>float32<span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>args<span class="token punctuation">.</span>device<span class="token punctuation">)</span>
        outputs <span class="token operator">=</span> Your_model<span class="token punctuation">(</span>data_x<span class="token punctuation">)</span>
        loss <span class="token operator">=</span> criterion<span class="token punctuation">(</span>outputs<span class="token punctuation">,</span>data_y<span class="token punctuation">)</span>
        valid_epoch_loss<span class="token punctuation">.</span>append<span class="token punctuation">(</span>loss<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        valid_loss<span class="token punctuation">.</span>append<span class="token punctuation">(</span>loss<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    valid_epochs_loss<span class="token punctuation">.</span>append<span class="token punctuation">(</span>np<span class="token punctuation">.</span>average<span class="token punctuation">(</span>valid_epoch_loss<span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token comment" spellcheck="true">#==================early stopping======================</span>
    early_stopping<span class="token punctuation">(</span>valid_epochs_loss<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span>model<span class="token operator">=</span>Your_model<span class="token punctuation">,</span>path<span class="token operator">=</span>r<span class="token string">'c:\\your_model_to_save'</span><span class="token punctuation">)</span>
    <span class="token keyword">if</span> early_stopping<span class="token punctuation">.</span>early_stop<span class="token punctuation">:</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Early stopping"</span><span class="token punctuation">)</span>
        <span class="token keyword">break</span>
    <span class="token comment" spellcheck="true">#====================adjust lr========================</span>
    lr_adjust <span class="token operator">=</span> <span class="token punctuation">{</span>
            <span class="token number">2</span><span class="token punctuation">:</span> <span class="token number">5e</span><span class="token operator">-</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">:</span> <span class="token number">1e</span><span class="token operator">-</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">:</span> <span class="token number">5e</span><span class="token operator">-</span><span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">:</span> <span class="token number">1e</span><span class="token operator">-</span><span class="token number">6</span><span class="token punctuation">,</span>
            <span class="token number">10</span><span class="token punctuation">:</span> <span class="token number">5e</span><span class="token operator">-</span><span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">15</span><span class="token punctuation">:</span> <span class="token number">1e</span><span class="token operator">-</span><span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">20</span><span class="token punctuation">:</span> <span class="token number">5e</span><span class="token operator">-</span><span class="token number">8</span>
        <span class="token punctuation">}</span>
    <span class="token keyword">if</span> epoch <span class="token keyword">in</span> lr_adjust<span class="token punctuation">.</span>keys<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        lr <span class="token operator">=</span> lr_adjust<span class="token punctuation">[</span>epoch<span class="token punctuation">]</span>
        <span class="token keyword">for</span> param_group <span class="token keyword">in</span> optimizer<span class="token punctuation">.</span>param_groups<span class="token punctuation">:</span>
            param_group<span class="token punctuation">[</span><span class="token string">'lr'</span><span class="token punctuation">]</span> <span class="token operator">=</span> lr
        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'Updating learning rate to {}'</span><span class="token punctuation">.</span>format<span class="token punctuation">(</span>lr<span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h2 id="八、绘图"><a href="#八、绘图" class="headerlink" title="八、绘图"></a>八、绘图</h2><pre class="line-numbers language-python"><code class="language-python">plt<span class="token punctuation">.</span>figure<span class="token punctuation">(</span>figsize<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">12</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>subplot<span class="token punctuation">(</span><span class="token number">121</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>train_loss<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>title<span class="token punctuation">(</span><span class="token string">"train_loss"</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>subplot<span class="token punctuation">(</span><span class="token number">122</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>train_epochs_loss<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token string">'-o'</span><span class="token punctuation">,</span>label<span class="token operator">=</span><span class="token string">"train_loss"</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>valid_epochs_loss<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token string">'-o'</span><span class="token punctuation">,</span>label<span class="token operator">=</span><span class="token string">"valid_loss"</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>title<span class="token punctuation">(</span><span class="token string">"epochs_loss"</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>legend<span class="token punctuation">(</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h2 id="九、预测"><a href="#九、预测" class="headerlink" title="九、预测"></a>九、预测</h2><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 此处可定义一个预测集的Dataloader。也可以直接将你的预测数据reshape,添加batch_size=1</span>
Your_model<span class="token punctuation">.</span>eval<span class="token punctuation">(</span><span class="token punctuation">)</span>
predict <span class="token operator">=</span> Your_model<span class="token punctuation">(</span>data<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>
<h1 id="PyTorch-常用代码段"><a href="#PyTorch-常用代码段" class="headerlink" title="PyTorch 常用代码段"></a>PyTorch 常用代码段</h1><h2 id="1-基本配置"><a href="#1-基本配置" class="headerlink" title="1. 基本配置"></a>1. 基本配置</h2><h3 id="导入包和版本查询"><a href="#导入包和版本查询" class="headerlink" title="导入包和版本查询"></a>导入包和版本查询</h3><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> torch
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn
<span class="token keyword">import</span> torchvision
<span class="token keyword">print</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>__version__<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>version<span class="token punctuation">.</span>cuda<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>backends<span class="token punctuation">.</span>cudnn<span class="token punctuation">.</span>version<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>get_device_name<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h3 id="可复现性"><a href="#可复现性" class="headerlink" title="可复现性"></a>可复现性</h3><p>在硬件设备（CPU、GPU）不同时，完全的可复现性无法保证，即使随机种子相同。但是，在同一个设备上，应该保证可复现性。具体做法是，在程序开始的时候固定torch的随机种子，同时也把numpy的随机种子固定。</p>
<pre class="line-numbers language-python"><code class="language-python">np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>seed<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>
torch<span class="token punctuation">.</span>manual_seed<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>
torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>manual_seed_all<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>

torch<span class="token punctuation">.</span>backends<span class="token punctuation">.</span>cudnn<span class="token punctuation">.</span>deterministic <span class="token operator">=</span> <span class="token boolean">True</span>
torch<span class="token punctuation">.</span>backends<span class="token punctuation">.</span>cudnn<span class="token punctuation">.</span>benchmark <span class="token operator">=</span> <span class="token boolean">False</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h3 id="显卡设置"><a href="#显卡设置" class="headerlink" title="显卡设置"></a>显卡设置</h3><p>如果只需要一张显卡</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># Device configuration</span>
device <span class="token operator">=</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">'cuda'</span> <span class="token keyword">if</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>is_available<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">else</span> <span class="token string">'cpu'</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>
<p>如果需要指定多张显卡，比如0，1号显卡。</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> os
os<span class="token punctuation">.</span>environ<span class="token punctuation">[</span><span class="token string">'CUDA_VISIBLE_DEVICES'</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token string">'0,1'</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>
<p>也可以在命令行运行代码时设置显卡：</p>
<pre class="line-numbers language-text"><code class="language-text">CUDA_VISIBLE_DEVICES=0,1 python train.py<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<p>清除显存</p>
<pre class="line-numbers language-python"><code class="language-python">torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>empty_cache<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<p>也可以使用在命令行重置GPU的指令</p>
<pre class="line-numbers language-text"><code class="language-text">nvidia-smi --gpu-reset -i [gpu_id]<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<h2 id="2-张量-Tensor-处理"><a href="#2-张量-Tensor-处理" class="headerlink" title="2. 张量(Tensor)处理"></a>2. 张量(Tensor)处理</h2><h3 id="张量的数据类型"><a href="#张量的数据类型" class="headerlink" title="张量的数据类型"></a>张量的数据类型</h3><p>PyTorch有10种CPU张量类型和10种GPU张量类型。</p>
<table>
<thead>
<tr>
<th>Data type</th>
<th>dtype</th>
<th>CPU tensor</th>
<th>GPU tensor</th>
</tr>
</thead>
<tbody><tr>
<td>32-bit floating point</td>
<td><code>torch.float32</code> or <code>torch.float</code></td>
<td><code>torch.FloatTensor</code></td>
<td><code>torch.cuda.FloatTensor</code></td>
</tr>
<tr>
<td>64-bit floating point</td>
<td><code>torch.float64</code> or <code>torch.double</code></td>
<td><code>torch.DoubleTensor</code></td>
<td><code>torch.cuda.DoubleTensor</code></td>
</tr>
<tr>
<td>16-bit floating point</td>
<td><code>torch.float16</code> or <code>torch.half</code></td>
<td><code>torch.HalfTensor</code></td>
<td><code>torch.cuda.HalfTensor</code></td>
</tr>
<tr>
<td>16-bit floating point</td>
<td><code>torch.bfloat16</code></td>
<td><code>torch.BFloat16Tensor</code></td>
<td><code>torch.cuda.BFloat16Tensor</code></td>
</tr>
<tr>
<td>32-bit complex</td>
<td><code>torch.complex32</code></td>
<td></td>
<td></td>
</tr>
<tr>
<td>64-bit complex</td>
<td><code>torch.complex64</code></td>
<td></td>
<td></td>
</tr>
<tr>
<td>128-bit complex</td>
<td><code>torch.complex128</code> or <code>torch.cdouble</code></td>
<td></td>
<td></td>
</tr>
<tr>
<td>8-bit integer (unsigned)</td>
<td><code>torch.uint8</code></td>
<td><code>torch.ByteTensor</code></td>
<td><code>torch.cuda.ByteTensor</code></td>
</tr>
<tr>
<td>8-bit integer (signed)</td>
<td><code>torch.int8</code></td>
<td><code>torch.CharTensor</code></td>
<td><code>torch.cuda.CharTensor</code></td>
</tr>
<tr>
<td>16-bit integer (signed)</td>
<td><code>torch.int16</code> or <code>torch.short</code></td>
<td><code>torch.ShortTensor</code></td>
<td><code>torch.cuda.ShortTensor</code></td>
</tr>
<tr>
<td>32-bit integer (signed)</td>
<td><code>torch.int32</code> or <code>torch.int</code></td>
<td><code>torch.IntTensor</code></td>
<td><code>torch.cuda.IntTensor</code></td>
</tr>
<tr>
<td>64-bit integer (signed)</td>
<td><code>torch.int64</code> or <code>torch.long</code></td>
<td><code>torch.LongTensor</code></td>
<td><code>torch.cuda.LongTensor</code></td>
</tr>
<tr>
<td>Boolean</td>
<td><code>torch.bool</code></td>
<td><code>torch.BoolTensor</code></td>
<td><code>torch.cuda.BoolTensor</code></td>
</tr>
<tr>
<td>quantized 8-bit integer (unsigned)</td>
<td><code>torch.quint8</code></td>
<td><code>torch.ByteTensor</code></td>
<td>/</td>
</tr>
<tr>
<td>quantized 8-bit integer (signed)</td>
<td><code>torch.qint8</code></td>
<td><code>torch.CharTensor</code></td>
<td>/</td>
</tr>
<tr>
<td>quantized 32-bit integer (signed)</td>
<td><code>torch.qfint32</code></td>
<td><code>torch.IntTensor</code></td>
<td>/</td>
</tr>
<tr>
<td>quantized 4-bit integer (unsigned)</td>
<td><code>torch.quint4x2</code></td>
<td><code>torch.ByteTensor</code></td>
<td>/</td>
</tr>
</tbody></table>
<h3 id="张量基本信息"><a href="#张量基本信息" class="headerlink" title="张量基本信息"></a>张量基本信息</h3><pre class="line-numbers language-python"><code class="language-python">tensor <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>tensor<span class="token punctuation">.</span>type<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 数据类型</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>tensor<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 张量的shape，是个元组</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>tensor<span class="token punctuation">.</span>dim<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>   <span class="token comment" spellcheck="true"># 维度的数量</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>
<h3 id="命名张量"><a href="#命名张量" class="headerlink" title="命名张量"></a>命名张量</h3><p>张量命名是一个非常有用的方法，这样可以方便地使用维度的名字来做索引或其他操作，大大提高了可读性、易用性，防止出错。</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 在PyTorch 1.3之前，需要使用注释</span>
<span class="token comment" spellcheck="true"># Tensor[N, C, H, W]</span>
images <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">56</span><span class="token punctuation">,</span> <span class="token number">56</span><span class="token punctuation">)</span>
images<span class="token punctuation">.</span>sum<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
images<span class="token punctuation">.</span>select<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> index<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>

<span class="token comment" spellcheck="true"># PyTorch 1.3之后</span>
NCHW <span class="token operator">=</span> <span class="token punctuation">[</span>‘N’<span class="token punctuation">,</span> ‘C’<span class="token punctuation">,</span> ‘H’<span class="token punctuation">,</span> ‘W’<span class="token punctuation">]</span>
images <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">56</span><span class="token punctuation">,</span> <span class="token number">56</span><span class="token punctuation">,</span> names<span class="token operator">=</span>NCHW<span class="token punctuation">)</span>
images<span class="token punctuation">.</span>sum<span class="token punctuation">(</span><span class="token string">'C'</span><span class="token punctuation">)</span>
images<span class="token punctuation">.</span>select<span class="token punctuation">(</span><span class="token string">'C'</span><span class="token punctuation">,</span> index<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>
<span class="token comment" spellcheck="true"># 也可以这么设置</span>
tensor <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span>names<span class="token operator">=</span><span class="token punctuation">(</span><span class="token string">'C'</span><span class="token punctuation">,</span> <span class="token string">'N'</span><span class="token punctuation">,</span> <span class="token string">'H'</span><span class="token punctuation">,</span> <span class="token string">'W'</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token comment" spellcheck="true"># 使用align_to可以对维度方便地排序</span>
tensor <span class="token operator">=</span> tensor<span class="token punctuation">.</span>align_to<span class="token punctuation">(</span><span class="token string">'N'</span><span class="token punctuation">,</span> <span class="token string">'C'</span><span class="token punctuation">,</span> <span class="token string">'H'</span><span class="token punctuation">,</span> <span class="token string">'W'</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h3 id="数据类型转换"><a href="#数据类型转换" class="headerlink" title="数据类型转换"></a>数据类型转换</h3><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 设置默认类型，pytorch中的FloatTensor远远快于DoubleTensor</span>
torch<span class="token punctuation">.</span>set_default_tensor_type<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>FloatTensor<span class="token punctuation">)</span>

<span class="token comment" spellcheck="true"># 类型转换</span>
tensor <span class="token operator">=</span> tensor<span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span>
tensor <span class="token operator">=</span> tensor<span class="token punctuation">.</span>cpu<span class="token punctuation">(</span><span class="token punctuation">)</span>
tensor <span class="token operator">=</span> tensor<span class="token punctuation">.</span>float<span class="token punctuation">(</span><span class="token punctuation">)</span>
tensor <span class="token operator">=</span> tensor<span class="token punctuation">.</span>long<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h3 id="torch-Tensor与np-ndarray转换"><a href="#torch-Tensor与np-ndarray转换" class="headerlink" title="torch.Tensor与np.ndarray转换"></a><strong>torch.Tensor与np.ndarray转换</strong></h3><p>除了CharTensor，其他所有CPU上的张量都支持转换为numpy格式然后再转换回来。</p>
<pre class="line-numbers language-python"><code class="language-python">ndarray <span class="token operator">=</span> tensor<span class="token punctuation">.</span>cpu<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span>
tensor <span class="token operator">=</span> torch<span class="token punctuation">.</span>from_numpy<span class="token punctuation">(</span>ndarray<span class="token punctuation">)</span><span class="token punctuation">.</span>float<span class="token punctuation">(</span><span class="token punctuation">)</span>
tensor <span class="token operator">=</span> torch<span class="token punctuation">.</span>from_numpy<span class="token punctuation">(</span>ndarray<span class="token punctuation">.</span>copy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>float<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># If ndarray has negative stride.</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>
<h3 id="Torch-tensor与PIL-Image转换"><a href="#Torch-tensor与PIL-Image转换" class="headerlink" title="Torch.tensor与PIL.Image转换"></a><strong>Torch.tensor与PIL.Image转换</strong></h3><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># pytorch中的张量默认采用[N, C, H, W]的顺序，并且数据范围在[0,1]，需要进行转置和规范化</span>
<span class="token comment" spellcheck="true"># torch.Tensor -> PIL.Image</span>
image <span class="token operator">=</span> PIL<span class="token punctuation">.</span>Image<span class="token punctuation">.</span>fromarray<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>clamp<span class="token punctuation">(</span>tensor<span class="token operator">*</span><span class="token number">255</span><span class="token punctuation">,</span> min<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> max<span class="token operator">=</span><span class="token number">255</span><span class="token punctuation">)</span><span class="token punctuation">.</span>byte<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span>cpu<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
image <span class="token operator">=</span> torchvision<span class="token punctuation">.</span>transforms<span class="token punctuation">.</span>functional<span class="token punctuation">.</span>to_pil_image<span class="token punctuation">(</span>tensor<span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># Equivalently way</span>

<span class="token comment" spellcheck="true"># PIL.Image -> torch.Tensor</span>
path <span class="token operator">=</span> r<span class="token string">'./figure.jpg'</span>
tensor <span class="token operator">=</span> torch<span class="token punctuation">.</span>from_numpy<span class="token punctuation">(</span>np<span class="token punctuation">.</span>asarray<span class="token punctuation">(</span>PIL<span class="token punctuation">.</span>Image<span class="token punctuation">.</span>open<span class="token punctuation">(</span>path<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>float<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token number">255</span>
tensor <span class="token operator">=</span> torchvision<span class="token punctuation">.</span>transforms<span class="token punctuation">.</span>functional<span class="token punctuation">.</span>to_tensor<span class="token punctuation">(</span>PIL<span class="token punctuation">.</span>Image<span class="token punctuation">.</span>open<span class="token punctuation">(</span>path<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># Equivalently way</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h3 id="np-ndarray与PIL-Image的转换"><a href="#np-ndarray与PIL-Image的转换" class="headerlink" title="np.ndarray与PIL.Image的转换"></a><strong>np.ndarray与PIL.Image的转换</strong></h3><pre class="line-numbers language-python"><code class="language-python">image <span class="token operator">=</span> PIL<span class="token punctuation">.</span>Image<span class="token punctuation">.</span>fromarray<span class="token punctuation">(</span>ndarray<span class="token punctuation">.</span>astype<span class="token punctuation">(</span>np<span class="token punctuation">.</span>uint8<span class="token punctuation">)</span><span class="token punctuation">)</span>

ndarray <span class="token operator">=</span> np<span class="token punctuation">.</span>asarray<span class="token punctuation">(</span>PIL<span class="token punctuation">.</span>Image<span class="token punctuation">.</span>open<span class="token punctuation">(</span>path<span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>
<h3 id="从只包含一个元素的张量中提取值"><a href="#从只包含一个元素的张量中提取值" class="headerlink" title="从只包含一个元素的张量中提取值"></a><strong>从只包含一个元素的张量中提取值</strong></h3><pre class="line-numbers language-python"><code class="language-python">value <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<h3 id="张量形变"><a href="#张量形变" class="headerlink" title="张量形变"></a><strong>张量形变</strong></h3><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 在将卷积层输入全连接层的情况下通常需要对张量做形变处理，</span>
<span class="token comment" spellcheck="true"># 相比torch.view，torch.reshape可以自动处理输入张量不连续的情况。</span>
tensor <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">)</span>
shape <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span>
tensor <span class="token operator">=</span> torch<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>tensor<span class="token punctuation">,</span> shape<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h3 id="打乱顺序"><a href="#打乱顺序" class="headerlink" title="打乱顺序"></a><strong>打乱顺序</strong></h3><pre class="line-numbers language-python"><code class="language-python">tensor <span class="token operator">=</span> tensor<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>randperm<span class="token punctuation">(</span>tensor<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">]</span>  <span class="token comment" spellcheck="true"># 打乱第一个维度</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<h3 id="水平翻转"><a href="#水平翻转" class="headerlink" title="水平翻转"></a><strong>水平翻转</strong></h3><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># pytorch不支持tensor[::-1]这样的负步长操作，水平翻转可以通过张量索引实现</span>
<span class="token comment" spellcheck="true"># 假设张量的维度为[N, D, H, W].</span>
tensor <span class="token operator">=</span> tensor<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token punctuation">,</span>torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span>tensor<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">)</span> <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>long<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>
<h3 id="复制张量"><a href="#复制张量" class="headerlink" title="复制张量"></a><strong>复制张量</strong></h3><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># Operation                 |  New/Shared memory | Still in computation graph |</span>
tensor<span class="token punctuation">.</span>clone<span class="token punctuation">(</span><span class="token punctuation">)</span>            <span class="token comment" spellcheck="true"># |        New         |          Yes               |</span>
tensor<span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span>           <span class="token comment" spellcheck="true"># |      Shared        |          No                |</span>
tensor<span class="token punctuation">.</span>detach<span class="token punctuation">.</span>clone<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">(</span><span class="token punctuation">)</span>   <span class="token comment" spellcheck="true"># |        New         |          No                |</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>
<h3 id="张量拼接"><a href="#张量拼接" class="headerlink" title="张量拼接"></a><strong>张量拼接</strong></h3><pre class="line-numbers language-python"><code class="language-python"><span class="token triple-quoted-string string">'''
注意torch.cat和torch.stack的区别在于torch.cat沿着给定的维度拼接，
而torch.stack会新增一维。例如当参数是3个10x5的张量，torch.cat的结果是30x5的张量，
而torch.stack的结果是3x10x5的张量。
'''</span>
tensor <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span>list_of_tensors<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>
tensor <span class="token operator">=</span> torch<span class="token punctuation">.</span>stack<span class="token punctuation">(</span>list_of_tensors<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h3 id="将整数标签转为one-hot编码"><a href="#将整数标签转为one-hot编码" class="headerlink" title="将整数标签转为one-hot编码"></a><strong>将整数标签转为one-hot编码</strong></h3><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># pytorch的标记默认从0开始</span>
tensor <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
N <span class="token operator">=</span> tensor<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>
num_classes <span class="token operator">=</span> <span class="token number">4</span>
one_hot <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>N<span class="token punctuation">,</span> num_classes<span class="token punctuation">)</span><span class="token punctuation">.</span>long<span class="token punctuation">(</span><span class="token punctuation">)</span>
one_hot<span class="token punctuation">.</span>scatter_<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> index<span class="token operator">=</span>torch<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span>tensor<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> src<span class="token operator">=</span>torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span>N<span class="token punctuation">,</span> num_classes<span class="token punctuation">)</span><span class="token punctuation">.</span>long<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h3 id="得到非零元素"><a href="#得到非零元素" class="headerlink" title="得到非零元素"></a><strong>得到非零元素</strong></h3><pre class="line-numbers language-python"><code class="language-python">torch<span class="token punctuation">.</span>nonzero<span class="token punctuation">(</span>tensor<span class="token punctuation">)</span>               <span class="token comment" spellcheck="true"># index of non-zero elements</span>
torch<span class="token punctuation">.</span>nonzero<span class="token punctuation">(</span>tensor<span class="token operator">==</span><span class="token number">0</span><span class="token punctuation">)</span>            <span class="token comment" spellcheck="true"># index of zero elements</span>
torch<span class="token punctuation">.</span>nonzero<span class="token punctuation">(</span>tensor<span class="token punctuation">)</span><span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>       <span class="token comment" spellcheck="true"># number of non-zero elements</span>
torch<span class="token punctuation">.</span>nonzero<span class="token punctuation">(</span>tensor <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># number of zero elements</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>
<h3 id="判断两个张量相等"><a href="#判断两个张量相等" class="headerlink" title="判断两个张量相等"></a><strong>判断两个张量相等</strong></h3><pre class="line-numbers language-python"><code class="language-python">torch<span class="token punctuation">.</span>allclose<span class="token punctuation">(</span>tensor1<span class="token punctuation">,</span> tensor2<span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># float tensor</span>
torch<span class="token punctuation">.</span>equal<span class="token punctuation">(</span>tensor1<span class="token punctuation">,</span> tensor2<span class="token punctuation">)</span>     <span class="token comment" spellcheck="true"># int tensor</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>
<h3 id="张量扩展"><a href="#张量扩展" class="headerlink" title="张量扩展"></a><strong>张量扩展</strong></h3><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># Expand tensor of shape 64*512 to shape 64*512*7*7.</span>
tensor <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">,</span><span class="token number">512</span><span class="token punctuation">)</span>
torch<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>tensor<span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">,</span> <span class="token number">512</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>expand<span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">,</span> <span class="token number">512</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>
<h3 id="矩阵乘法"><a href="#矩阵乘法" class="headerlink" title="矩阵乘法"></a><strong>矩阵乘法</strong></h3><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># Matrix multiplcation: (m*n) * (n*p) * -> (m*p).</span>
result <span class="token operator">=</span> torch<span class="token punctuation">.</span>mm<span class="token punctuation">(</span>tensor1<span class="token punctuation">,</span> tensor2<span class="token punctuation">)</span>

<span class="token comment" spellcheck="true"># Batch matrix multiplication: (b*m*n) * (b*n*p) -> (b*m*p)</span>
result <span class="token operator">=</span> torch<span class="token punctuation">.</span>bmm<span class="token punctuation">(</span>tensor1<span class="token punctuation">,</span> tensor2<span class="token punctuation">)</span>

<span class="token comment" spellcheck="true"># Element-wise multiplication.</span>
result <span class="token operator">=</span> tensor1 <span class="token operator">*</span> tensor2<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h3 id="计算两组数据之间的两两欧式距离"><a href="#计算两组数据之间的两两欧式距离" class="headerlink" title="计算两组数据之间的两两欧式距离"></a><strong>计算两组数据之间的两两欧式距离</strong></h3><p>利用broadcast机制</p>
<pre class="line-numbers language-python"><code class="language-python">dist <span class="token operator">=</span> torch<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>sum<span class="token punctuation">(</span><span class="token punctuation">(</span>X1<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span>None<span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token operator">-</span> X2<span class="token punctuation">)</span> <span class="token operator">**</span> <span class="token number">2</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<h2 id="3-模型定义和操作"><a href="#3-模型定义和操作" class="headerlink" title="3. 模型定义和操作"></a>3. 模型定义和操作</h2><h3 id="一个简单两层卷积网络的示例"><a href="#一个简单两层卷积网络的示例" class="headerlink" title="一个简单两层卷积网络的示例"></a>一个简单两层卷积网络的示例</h3><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># convolutional neural network (2 convolutional layers)</span>
<span class="token keyword">class</span> <span class="token class-name">ConvNet</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> num_classes<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        super<span class="token punctuation">(</span>ConvNet<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>layer1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>
            nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">16</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span><span class="token number">16</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>MaxPool2d<span class="token punctuation">(</span>kernel_size<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>layer2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>
            nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">16</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>MaxPool2d<span class="token punctuation">(</span>kernel_size<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>fc <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">7</span><span class="token operator">*</span><span class="token number">7</span><span class="token operator">*</span><span class="token number">32</span><span class="token punctuation">,</span> num_classes<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        out <span class="token operator">=</span> self<span class="token punctuation">.</span>layer1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        out <span class="token operator">=</span> self<span class="token punctuation">.</span>layer2<span class="token punctuation">(</span>out<span class="token punctuation">)</span>
        out <span class="token operator">=</span> out<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>out<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
        out <span class="token operator">=</span> self<span class="token punctuation">.</span>fc<span class="token punctuation">(</span>out<span class="token punctuation">)</span>
        <span class="token keyword">return</span> out

model <span class="token operator">=</span> ConvNet<span class="token punctuation">(</span>num_classes<span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>卷积层的计算和展示可以用<a href="https://ezyang.github.io/convolution-visualizer/index.html" target="_blank" rel="noopener">这个网站</a>辅助。</p>
<h3 id="双线性汇合（bilinear-pooling）"><a href="#双线性汇合（bilinear-pooling）" class="headerlink" title="双线性汇合（bilinear pooling）"></a><strong>双线性汇合（bilinear pooling）</strong></h3><pre class="line-numbers language-python"><code class="language-python">X <span class="token operator">=</span> torch<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>N<span class="token punctuation">,</span> D<span class="token punctuation">,</span> H <span class="token operator">*</span> W<span class="token punctuation">)</span>                        <span class="token comment" spellcheck="true"># Assume X has shape N*D*H*W</span>
X <span class="token operator">=</span> torch<span class="token punctuation">.</span>bmm<span class="token punctuation">(</span>X<span class="token punctuation">,</span> torch<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span>X<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token punctuation">(</span>H <span class="token operator">*</span> W<span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># Bilinear pooling</span>
<span class="token keyword">assert</span> X<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token punctuation">(</span>N<span class="token punctuation">,</span> D<span class="token punctuation">,</span> D<span class="token punctuation">)</span>
X <span class="token operator">=</span> torch<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>X<span class="token punctuation">,</span> <span class="token punctuation">(</span>N<span class="token punctuation">,</span> D <span class="token operator">*</span> D<span class="token punctuation">)</span><span class="token punctuation">)</span>
X <span class="token operator">=</span> torch<span class="token punctuation">.</span>sign<span class="token punctuation">(</span>X<span class="token punctuation">)</span> <span class="token operator">*</span> torch<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>abs<span class="token punctuation">(</span>X<span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token number">1e</span><span class="token operator">-</span><span class="token number">5</span><span class="token punctuation">)</span>   <span class="token comment" spellcheck="true"># Signed-sqrt normalization</span>
X <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional<span class="token punctuation">.</span>normalize<span class="token punctuation">(</span>X<span class="token punctuation">)</span>                  <span class="token comment" spellcheck="true"># L2 normalization</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h3 id="多卡同步-BN（Batch-normalization）"><a href="#多卡同步-BN（Batch-normalization）" class="headerlink" title="多卡同步 BN（Batch normalization）"></a><strong>多卡同步 BN（Batch normalization）</strong></h3><p>当使用 torch.nn.DataParallel 将代码运行在多张 GPU 卡上时，PyTorch 的 BN 层默认操作是各卡上数据独立地计算均值和标准差，同步 BN 使用所有卡上的数据一起计算 BN 层的均值和标准差，缓解了当批量大小（batch size）比较小时对均值和标准差估计不准的情况，是在目标检测等任务中一个有效的提升性能的技巧。</p>
<pre class="line-numbers language-python"><code class="language-python">sync_bn <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>SyncBatchNorm<span class="token punctuation">(</span>num_features<span class="token punctuation">,</span> eps<span class="token operator">=</span><span class="token number">1e</span><span class="token operator">-</span><span class="token number">05</span><span class="token punctuation">,</span> momentum<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">,</span> affine<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> 
                                 track_running_stats<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>
<h3 id="将已有网络的所有BN层改为同步BN层"><a href="#将已有网络的所有BN层改为同步BN层" class="headerlink" title="将已有网络的所有BN层改为同步BN层"></a>将已有网络的所有BN层改为同步BN层</h3><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">convertBNtoSyncBN</span><span class="token punctuation">(</span>module<span class="token punctuation">,</span> process_group<span class="token operator">=</span>None<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">'''Recursively replace all BN layers to SyncBN layer.

    Args:
        module[torch.nn.Module]. Network
    '''</span>
    <span class="token keyword">if</span> isinstance<span class="token punctuation">(</span>module<span class="token punctuation">,</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>modules<span class="token punctuation">.</span>batchnorm<span class="token punctuation">.</span>_BatchNorm<span class="token punctuation">)</span><span class="token punctuation">:</span>
        sync_bn <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>SyncBatchNorm<span class="token punctuation">(</span>module<span class="token punctuation">.</span>num_features<span class="token punctuation">,</span> module<span class="token punctuation">.</span>eps<span class="token punctuation">,</span> module<span class="token punctuation">.</span>momentum<span class="token punctuation">,</span> 
                                         module<span class="token punctuation">.</span>affine<span class="token punctuation">,</span> module<span class="token punctuation">.</span>track_running_stats<span class="token punctuation">,</span> process_group<span class="token punctuation">)</span>
        sync_bn<span class="token punctuation">.</span>running_mean <span class="token operator">=</span> module<span class="token punctuation">.</span>running_mean
        sync_bn<span class="token punctuation">.</span>running_var <span class="token operator">=</span> module<span class="token punctuation">.</span>running_var
        <span class="token keyword">if</span> module<span class="token punctuation">.</span>affine<span class="token punctuation">:</span>
            sync_bn<span class="token punctuation">.</span>weight <span class="token operator">=</span> module<span class="token punctuation">.</span>weight<span class="token punctuation">.</span>clone<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span>
            sync_bn<span class="token punctuation">.</span>bias <span class="token operator">=</span> module<span class="token punctuation">.</span>bias<span class="token punctuation">.</span>clone<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> sync_bn
    <span class="token keyword">else</span><span class="token punctuation">:</span>
        <span class="token keyword">for</span> name<span class="token punctuation">,</span> child_module <span class="token keyword">in</span> module<span class="token punctuation">.</span>named_children<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            setattr<span class="token punctuation">(</span>module<span class="token punctuation">,</span> name<span class="token punctuation">)</span> <span class="token operator">=</span> convert_syncbn_model<span class="token punctuation">(</span>child_module<span class="token punctuation">,</span> process_group<span class="token operator">=</span>process_group<span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> module<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h3 id="类似-BN-滑动平均"><a href="#类似-BN-滑动平均" class="headerlink" title="类似 BN 滑动平均"></a><strong>类似 BN 滑动平均</strong></h3><p>如果要实现类似 BN 滑动平均的操作，在 forward 函数中要使用原地（inplace）操作给滑动平均赋值。</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">BN</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>
        self<span class="token punctuation">.</span>register_buffer<span class="token punctuation">(</span><span class="token string">'running_mean'</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>num_features<span class="token punctuation">)</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> X<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>
        self<span class="token punctuation">.</span>running_mean <span class="token operator">+=</span> momentum <span class="token operator">*</span> <span class="token punctuation">(</span>current <span class="token operator">-</span> self<span class="token punctuation">.</span>running_mean<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h3 id="计算模型整体参数量"><a href="#计算模型整体参数量" class="headerlink" title="计算模型整体参数量"></a><strong>计算模型整体参数量</strong></h3><pre class="line-numbers language-python"><code class="language-python">num_parameters <span class="token operator">=</span> sum<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>numel<span class="token punctuation">(</span>parameter<span class="token punctuation">)</span> <span class="token keyword">for</span> parameter <span class="token keyword">in</span> model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<h3 id="查看网络中的参数"><a href="#查看网络中的参数" class="headerlink" title="查看网络中的参数"></a><strong>查看网络中的参数</strong></h3><p>可以通过model.state_dict()或者model.named_parameters()函数查看现在的全部可训练参数（包括通过继承得到的父类中的参数）</p>
<pre class="line-numbers language-python"><code class="language-python">params <span class="token operator">=</span> list<span class="token punctuation">(</span>model<span class="token punctuation">.</span>named_parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token punctuation">(</span>name<span class="token punctuation">,</span> param<span class="token punctuation">)</span> <span class="token operator">=</span> params<span class="token punctuation">[</span><span class="token number">28</span><span class="token punctuation">]</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>name<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>param<span class="token punctuation">.</span>grad<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'-------------------------------------------------'</span><span class="token punctuation">)</span>
<span class="token punctuation">(</span>name2<span class="token punctuation">,</span> param2<span class="token punctuation">)</span> <span class="token operator">=</span> params<span class="token punctuation">[</span><span class="token number">29</span><span class="token punctuation">]</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>name2<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>param2<span class="token punctuation">.</span>grad<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'----------------------------------------------------'</span><span class="token punctuation">)</span>
<span class="token punctuation">(</span>name1<span class="token punctuation">,</span> param1<span class="token punctuation">)</span> <span class="token operator">=</span> params<span class="token punctuation">[</span><span class="token number">30</span><span class="token punctuation">]</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>name1<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>param1<span class="token punctuation">.</span>grad<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h3 id="模型可视化（使用pytorchviz）"><a href="#模型可视化（使用pytorchviz）" class="headerlink" title="模型可视化（使用pytorchviz）"></a>模型可视化（使用pytorchviz）</h3><p><a href="https://github.com/szagoruyko/pytorchviz" target="_blank" rel="noopener">szagoruyko/pytorchviz</a></p>
<h3 id="类似-Keras-的-model-summary-输出模型信息（使用pytorch-summary-）"><a href="#类似-Keras-的-model-summary-输出模型信息（使用pytorch-summary-）" class="headerlink" title="类似 Keras 的 model.summary() 输出模型信息（使用pytorch-summary ）"></a><strong>类似 Keras 的 model.summary() 输出模型信息（</strong>使用pytorch-summary <strong>）</strong></h3><p><a href="https://github.com/sksq96/pytorch-summary" target="_blank" rel="noopener">sksq96/pytorch-summary</a></p>
<p><strong>模型权重初始化</strong></p>
<p>注意 model.modules() 和 model.children() 的区别：model.modules() 会迭代地遍历模型的所有子层，而 model.children() 只会遍历模型下的一层。</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># Common practise for initialization.</span>
<span class="token keyword">for</span> layer <span class="token keyword">in</span> model<span class="token punctuation">.</span>modules<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">if</span> isinstance<span class="token punctuation">(</span>layer<span class="token punctuation">,</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">)</span><span class="token punctuation">:</span>
        torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>kaiming_normal_<span class="token punctuation">(</span>layer<span class="token punctuation">.</span>weight<span class="token punctuation">,</span> mode<span class="token operator">=</span><span class="token string">'fan_out'</span><span class="token punctuation">,</span>
                                      nonlinearity<span class="token operator">=</span><span class="token string">'relu'</span><span class="token punctuation">)</span>
        <span class="token keyword">if</span> layer<span class="token punctuation">.</span>bias <span class="token keyword">is</span> <span class="token operator">not</span> None<span class="token punctuation">:</span>
            torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>constant_<span class="token punctuation">(</span>layer<span class="token punctuation">.</span>bias<span class="token punctuation">,</span> val<span class="token operator">=</span><span class="token number">0.0</span><span class="token punctuation">)</span>
    <span class="token keyword">elif</span> isinstance<span class="token punctuation">(</span>layer<span class="token punctuation">,</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">)</span><span class="token punctuation">:</span>
        torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>constant_<span class="token punctuation">(</span>layer<span class="token punctuation">.</span>weight<span class="token punctuation">,</span> val<span class="token operator">=</span><span class="token number">1.0</span><span class="token punctuation">)</span>
        torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>constant_<span class="token punctuation">(</span>layer<span class="token punctuation">.</span>bias<span class="token punctuation">,</span> val<span class="token operator">=</span><span class="token number">0.0</span><span class="token punctuation">)</span>
    <span class="token keyword">elif</span> isinstance<span class="token punctuation">(</span>layer<span class="token punctuation">,</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">)</span><span class="token punctuation">:</span>
        torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>xavier_normal_<span class="token punctuation">(</span>layer<span class="token punctuation">.</span>weight<span class="token punctuation">)</span>
        <span class="token keyword">if</span> layer<span class="token punctuation">.</span>bias <span class="token keyword">is</span> <span class="token operator">not</span> None<span class="token punctuation">:</span>
            torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>constant_<span class="token punctuation">(</span>layer<span class="token punctuation">.</span>bias<span class="token punctuation">,</span> val<span class="token operator">=</span><span class="token number">0.0</span><span class="token punctuation">)</span>

<span class="token comment" spellcheck="true"># Initialization with given tensor.</span>
layer<span class="token punctuation">.</span>weight <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>tensor<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h3 id="提取模型中的某一层"><a href="#提取模型中的某一层" class="headerlink" title="提取模型中的某一层"></a><strong>提取模型中的某一层</strong></h3><p>modules()会返回模型中所有模块的迭代器，它能够访问到最内层，比如self.layer1.conv1这个模块，还有一个与它们相对应的是name_children()属性以及named_modules(),这两个不仅会返回模块的迭代器，还会返回网络层的名字。</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 取模型中的前两层</span>
new_model <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span><span class="token operator">*</span>list<span class="token punctuation">(</span>model<span class="token punctuation">.</span>children<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span> 
<span class="token comment" spellcheck="true"># 如果希望提取出模型中的所有卷积层，可以像下面这样操作：</span>
<span class="token keyword">for</span> layer <span class="token keyword">in</span> model<span class="token punctuation">.</span>named_modules<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">if</span> isinstance<span class="token punctuation">(</span>layer<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span>nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">)</span><span class="token punctuation">:</span>
         conv_model<span class="token punctuation">.</span>add_module<span class="token punctuation">(</span>layer<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>layer<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h3 id="部分层使用预训练模型"><a href="#部分层使用预训练模型" class="headerlink" title="部分层使用预训练模型"></a><strong>部分层使用预训练模型</strong></h3><p>注意如果保存的模型是 torch.nn.DataParallel，则当前的模型也需要是</p>
<pre class="line-numbers language-python"><code class="language-python">model<span class="token punctuation">.</span>load_state_dict<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token string">'model.pth'</span><span class="token punctuation">)</span><span class="token punctuation">,</span> strict<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<h3 id="将在-GPU-保存的模型加载到-CPU"><a href="#将在-GPU-保存的模型加载到-CPU" class="headerlink" title="将在 GPU 保存的模型加载到 CPU"></a><strong>将在 GPU 保存的模型加载到 CPU</strong></h3><pre class="line-numbers language-python"><code class="language-python">model<span class="token punctuation">.</span>load_state_dict<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token string">'model.pth'</span><span class="token punctuation">,</span> map_location<span class="token operator">=</span><span class="token string">'cpu'</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<h2 id="4-数据处理"><a href="#4-数据处理" class="headerlink" title="4. 数据处理"></a><strong>4. 数据处理</strong></h2><h3 id="计算数据集的均值和标准差"><a href="#计算数据集的均值和标准差" class="headerlink" title="计算数据集的均值和标准差"></a><strong>计算数据集的均值和标准差</strong></h3><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> os
<span class="token keyword">import</span> cv2
<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
<span class="token keyword">from</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data <span class="token keyword">import</span> Dataset
<span class="token keyword">from</span> PIL <span class="token keyword">import</span> Image

<span class="token keyword">def</span> <span class="token function">compute_mean_and_std</span><span class="token punctuation">(</span>dataset<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment" spellcheck="true"># 输入PyTorch的dataset，输出均值和标准差</span>
    mean_r <span class="token operator">=</span> <span class="token number">0</span>
    mean_g <span class="token operator">=</span> <span class="token number">0</span>
    mean_b <span class="token operator">=</span> <span class="token number">0</span>

    <span class="token keyword">for</span> img<span class="token punctuation">,</span> _ <span class="token keyword">in</span> dataset<span class="token punctuation">:</span>
        img <span class="token operator">=</span> np<span class="token punctuation">.</span>asarray<span class="token punctuation">(</span>img<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># change PIL Image to numpy array</span>
        mean_b <span class="token operator">+=</span> np<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>img<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
        mean_g <span class="token operator">+=</span> np<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>img<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
        mean_r <span class="token operator">+=</span> np<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>img<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

    mean_b <span class="token operator">/=</span> len<span class="token punctuation">(</span>dataset<span class="token punctuation">)</span>
    mean_g <span class="token operator">/=</span> len<span class="token punctuation">(</span>dataset<span class="token punctuation">)</span>
    mean_r <span class="token operator">/=</span> len<span class="token punctuation">(</span>dataset<span class="token punctuation">)</span>

    diff_r <span class="token operator">=</span> <span class="token number">0</span>
    diff_g <span class="token operator">=</span> <span class="token number">0</span>
    diff_b <span class="token operator">=</span> <span class="token number">0</span>

    N <span class="token operator">=</span> <span class="token number">0</span>

    <span class="token keyword">for</span> img<span class="token punctuation">,</span> _ <span class="token keyword">in</span> dataset<span class="token punctuation">:</span>
        img <span class="token operator">=</span> np<span class="token punctuation">.</span>asarray<span class="token punctuation">(</span>img<span class="token punctuation">)</span>

        diff_b <span class="token operator">+=</span> np<span class="token punctuation">.</span>sum<span class="token punctuation">(</span>np<span class="token punctuation">.</span>power<span class="token punctuation">(</span>img<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">-</span> mean_b<span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        diff_g <span class="token operator">+=</span> np<span class="token punctuation">.</span>sum<span class="token punctuation">(</span>np<span class="token punctuation">.</span>power<span class="token punctuation">(</span>img<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">-</span> mean_g<span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        diff_r <span class="token operator">+=</span> np<span class="token punctuation">.</span>sum<span class="token punctuation">(</span>np<span class="token punctuation">.</span>power<span class="token punctuation">(</span>img<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span> <span class="token operator">-</span> mean_r<span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

        N <span class="token operator">+=</span> np<span class="token punctuation">.</span>prod<span class="token punctuation">(</span>img<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>shape<span class="token punctuation">)</span>

    std_b <span class="token operator">=</span> np<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>diff_b <span class="token operator">/</span> N<span class="token punctuation">)</span>
    std_g <span class="token operator">=</span> np<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>diff_g <span class="token operator">/</span> N<span class="token punctuation">)</span>
    std_r <span class="token operator">=</span> np<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>diff_r <span class="token operator">/</span> N<span class="token punctuation">)</span>

    mean <span class="token operator">=</span> <span class="token punctuation">(</span>mean_b<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token number">255.0</span><span class="token punctuation">,</span> mean_g<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token number">255.0</span><span class="token punctuation">,</span> mean_r<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token number">255.0</span><span class="token punctuation">)</span>
    std <span class="token operator">=</span> <span class="token punctuation">(</span>std_b<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token number">255.0</span><span class="token punctuation">,</span> std_g<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token number">255.0</span><span class="token punctuation">,</span> std_r<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token number">255.0</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> mean<span class="token punctuation">,</span> std<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h3 id="得到视频数据基本信息"><a href="#得到视频数据基本信息" class="headerlink" title="得到视频数据基本信息"></a><strong>得到视频数据基本信息</strong></h3><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> cv2
video <span class="token operator">=</span> cv2<span class="token punctuation">.</span>VideoCapture<span class="token punctuation">(</span>mp4_path<span class="token punctuation">)</span>
height <span class="token operator">=</span> int<span class="token punctuation">(</span>video<span class="token punctuation">.</span>get<span class="token punctuation">(</span>cv2<span class="token punctuation">.</span>CAP_PROP_FRAME_HEIGHT<span class="token punctuation">)</span><span class="token punctuation">)</span>
width <span class="token operator">=</span> int<span class="token punctuation">(</span>video<span class="token punctuation">.</span>get<span class="token punctuation">(</span>cv2<span class="token punctuation">.</span>CAP_PROP_FRAME_WIDTH<span class="token punctuation">)</span><span class="token punctuation">)</span>
num_frames <span class="token operator">=</span> int<span class="token punctuation">(</span>video<span class="token punctuation">.</span>get<span class="token punctuation">(</span>cv2<span class="token punctuation">.</span>CAP_PROP_FRAME_COUNT<span class="token punctuation">)</span><span class="token punctuation">)</span>
fps <span class="token operator">=</span> int<span class="token punctuation">(</span>video<span class="token punctuation">.</span>get<span class="token punctuation">(</span>cv2<span class="token punctuation">.</span>CAP_PROP_FPS<span class="token punctuation">)</span><span class="token punctuation">)</span>
video<span class="token punctuation">.</span>release<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h3 id="TSN-每段（segment）采样一帧视频"><a href="#TSN-每段（segment）采样一帧视频" class="headerlink" title="TSN 每段（segment）采样一帧视频"></a><strong>TSN 每段（segment）采样一帧视频</strong></h3><pre class="line-numbers language-python"><code class="language-python">K <span class="token operator">=</span> self<span class="token punctuation">.</span>_num_segments
<span class="token keyword">if</span> is_train<span class="token punctuation">:</span>
    <span class="token keyword">if</span> num_frames <span class="token operator">></span> K<span class="token punctuation">:</span>
        <span class="token comment" spellcheck="true"># Random index for each segment.</span>
        frame_indices <span class="token operator">=</span> torch<span class="token punctuation">.</span>randint<span class="token punctuation">(</span>
            high<span class="token operator">=</span>num_frames <span class="token operator">//</span> K<span class="token punctuation">,</span> size<span class="token operator">=</span><span class="token punctuation">(</span>K<span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>long<span class="token punctuation">)</span>
        frame_indices <span class="token operator">+=</span> num_frames <span class="token operator">//</span> K <span class="token operator">*</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span>K<span class="token punctuation">)</span>
    <span class="token keyword">else</span><span class="token punctuation">:</span>
        frame_indices <span class="token operator">=</span> torch<span class="token punctuation">.</span>randint<span class="token punctuation">(</span>
            high<span class="token operator">=</span>num_frames<span class="token punctuation">,</span> size<span class="token operator">=</span><span class="token punctuation">(</span>K <span class="token operator">-</span> num_frames<span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>long<span class="token punctuation">)</span>
        frame_indices <span class="token operator">=</span> torch<span class="token punctuation">.</span>sort<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">(</span>
            torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span>num_frames<span class="token punctuation">)</span><span class="token punctuation">,</span> frame_indices<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
<span class="token keyword">else</span><span class="token punctuation">:</span>
    <span class="token keyword">if</span> num_frames <span class="token operator">></span> K<span class="token punctuation">:</span>
        <span class="token comment" spellcheck="true"># Middle index for each segment.</span>
        frame_indices <span class="token operator">=</span> num_frames <span class="token operator">/</span> K <span class="token operator">//</span> <span class="token number">2</span>
        frame_indices <span class="token operator">+=</span> num_frames <span class="token operator">//</span> K <span class="token operator">*</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span>K<span class="token punctuation">)</span>
    <span class="token keyword">else</span><span class="token punctuation">:</span>
        frame_indices <span class="token operator">=</span> torch<span class="token punctuation">.</span>sort<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">(</span>                              
            torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span>num_frames<span class="token punctuation">)</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span>K <span class="token operator">-</span> num_frames<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
<span class="token keyword">assert</span> frame_indices<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token punctuation">(</span>K<span class="token punctuation">,</span><span class="token punctuation">)</span>
<span class="token keyword">return</span> <span class="token punctuation">[</span>frame_indices<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>K<span class="token punctuation">)</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h3 id="常用训练和验证数据预处理"><a href="#常用训练和验证数据预处理" class="headerlink" title="常用训练和验证数据预处理"></a><strong>常用训练和验证数据预处理</strong></h3><p>其中 ToTensor 操作会将 PIL.Image 或形状为 H×W×D，数值范围为 [0, 255] 的 np.ndarray 转换为形状为 D×H×W，数值范围为 [0.0, 1.0] 的 torch.Tensor。</p>
<pre class="line-numbers language-python"><code class="language-python">train_transform <span class="token operator">=</span> torchvision<span class="token punctuation">.</span>transforms<span class="token punctuation">.</span>Compose<span class="token punctuation">(</span><span class="token punctuation">[</span>
    torchvision<span class="token punctuation">.</span>transforms<span class="token punctuation">.</span>RandomResizedCrop<span class="token punctuation">(</span>size<span class="token operator">=</span><span class="token number">224</span><span class="token punctuation">,</span>
                                             scale<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">0.08</span><span class="token punctuation">,</span> <span class="token number">1.0</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    torchvision<span class="token punctuation">.</span>transforms<span class="token punctuation">.</span>RandomHorizontalFlip<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    torchvision<span class="token punctuation">.</span>transforms<span class="token punctuation">.</span>ToTensor<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    torchvision<span class="token punctuation">.</span>transforms<span class="token punctuation">.</span>Normalize<span class="token punctuation">(</span>mean<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">0.485</span><span class="token punctuation">,</span> <span class="token number">0.456</span><span class="token punctuation">,</span> <span class="token number">0.406</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                                     std<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">0.229</span><span class="token punctuation">,</span> <span class="token number">0.224</span><span class="token punctuation">,</span> <span class="token number">0.225</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
 <span class="token punctuation">]</span><span class="token punctuation">)</span>
 val_transform <span class="token operator">=</span> torchvision<span class="token punctuation">.</span>transforms<span class="token punctuation">.</span>Compose<span class="token punctuation">(</span><span class="token punctuation">[</span>
    torchvision<span class="token punctuation">.</span>transforms<span class="token punctuation">.</span>Resize<span class="token punctuation">(</span><span class="token number">256</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    torchvision<span class="token punctuation">.</span>transforms<span class="token punctuation">.</span>CenterCrop<span class="token punctuation">(</span><span class="token number">224</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    torchvision<span class="token punctuation">.</span>transforms<span class="token punctuation">.</span>ToTensor<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    torchvision<span class="token punctuation">.</span>transforms<span class="token punctuation">.</span>Normalize<span class="token punctuation">(</span>mean<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">0.485</span><span class="token punctuation">,</span> <span class="token number">0.456</span><span class="token punctuation">,</span> <span class="token number">0.406</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                                     std<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">0.229</span><span class="token punctuation">,</span> <span class="token number">0.224</span><span class="token punctuation">,</span> <span class="token number">0.225</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
<span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h2 id="5-模型训练和测试"><a href="#5-模型训练和测试" class="headerlink" title="5. 模型训练和测试"></a>5. 模型训练和测试</h2><h3 id="分类模型训练代码"><a href="#分类模型训练代码" class="headerlink" title="分类模型训练代码"></a>分类模型训练代码</h3><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># Loss and optimizer</span>
criterion <span class="token operator">=</span> nn<span class="token punctuation">.</span>CrossEntropyLoss<span class="token punctuation">(</span><span class="token punctuation">)</span>
optimizer <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span>model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span>learning_rate<span class="token punctuation">)</span>

<span class="token comment" spellcheck="true"># Train the model</span>
total_step <span class="token operator">=</span> len<span class="token punctuation">(</span>train_loader<span class="token punctuation">)</span>
<span class="token keyword">for</span> epoch <span class="token keyword">in</span> range<span class="token punctuation">(</span>num_epochs<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">for</span> i <span class="token punctuation">,</span><span class="token punctuation">(</span>images<span class="token punctuation">,</span> labels<span class="token punctuation">)</span> <span class="token keyword">in</span> enumerate<span class="token punctuation">(</span>train_loader<span class="token punctuation">)</span><span class="token punctuation">:</span>
        images <span class="token operator">=</span> images<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>
        labels <span class="token operator">=</span> labels<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>

        <span class="token comment" spellcheck="true"># Forward pass</span>
        outputs <span class="token operator">=</span> model<span class="token punctuation">(</span>images<span class="token punctuation">)</span>
        loss <span class="token operator">=</span> criterion<span class="token punctuation">(</span>outputs<span class="token punctuation">,</span> labels<span class="token punctuation">)</span>

        <span class="token comment" spellcheck="true"># Backward and optimizer</span>
        optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
        loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
        optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>

        <span class="token keyword">if</span> <span class="token punctuation">(</span>i<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">%</span> <span class="token number">100</span> <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>
            <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'Epoch: [{}/{}], Step: [{}/{}], Loss: {}'</span>
                  <span class="token punctuation">.</span>format<span class="token punctuation">(</span>epoch<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">,</span> num_epochs<span class="token punctuation">,</span> i<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">,</span> total_step<span class="token punctuation">,</span> loss<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h3 id="分类模型测试代码"><a href="#分类模型测试代码" class="headerlink" title="分类模型测试代码"></a>分类模型测试代码</h3><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># Test the model</span>
model<span class="token punctuation">.</span>eval<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># eval mode(batch norm uses moving mean/variance </span>
              <span class="token comment" spellcheck="true">#instead of mini-batch mean/variance)</span>
<span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    correct <span class="token operator">=</span> <span class="token number">0</span>
    total <span class="token operator">=</span> <span class="token number">0</span>
    <span class="token keyword">for</span> images<span class="token punctuation">,</span> labels <span class="token keyword">in</span> test_loader<span class="token punctuation">:</span>
        images <span class="token operator">=</span> images<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>
        labels <span class="token operator">=</span> labels<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>
        outputs <span class="token operator">=</span> model<span class="token punctuation">(</span>images<span class="token punctuation">)</span>
        _<span class="token punctuation">,</span> predicted <span class="token operator">=</span> torch<span class="token punctuation">.</span>max<span class="token punctuation">(</span>outputs<span class="token punctuation">.</span>data<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
        total <span class="token operator">+=</span> labels<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>
        correct <span class="token operator">+=</span> <span class="token punctuation">(</span>predicted <span class="token operator">==</span> labels<span class="token punctuation">)</span><span class="token punctuation">.</span>sum<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'Test accuracy of the model on the 10000 test images: {} %'</span>
          <span class="token punctuation">.</span>format<span class="token punctuation">(</span><span class="token number">100</span> <span class="token operator">*</span> correct <span class="token operator">/</span> total<span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h3 id="自定义loss"><a href="#自定义loss" class="headerlink" title="自定义loss"></a>自定义loss</h3><p>继承torch.nn.Module类写自己的loss。</p>
<pre class="line-numbers language-python"><code class="language-python">pythonclass MyLoss<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Moudle<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        super<span class="token punctuation">(</span>MyLoss<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">,</span> y<span class="token punctuation">)</span><span class="token punctuation">:</span>
        loss <span class="token operator">=</span> torch<span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token punctuation">(</span>x <span class="token operator">-</span> y<span class="token punctuation">)</span> <span class="token operator">**</span> <span class="token number">2</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> losspython<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h3 id="标签平滑（label-smoothing）"><a href="#标签平滑（label-smoothing）" class="headerlink" title="标签平滑（label smoothing）"></a><strong>标签平滑（label smoothing）</strong></h3><p>写一个label_smoothing.py的文件，然后在训练代码里引用，用LSR代替交叉熵损失即可。label_smoothing.py内容如下：</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> torch
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn

<span class="token keyword">class</span> <span class="token class-name">LSR</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>

    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> e<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">,</span> reduction<span class="token operator">=</span><span class="token string">'mean'</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        super<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>

        self<span class="token punctuation">.</span>log_softmax <span class="token operator">=</span> nn<span class="token punctuation">.</span>LogSoftmax<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>e <span class="token operator">=</span> e
        self<span class="token punctuation">.</span>reduction <span class="token operator">=</span> reduction

    <span class="token keyword">def</span> <span class="token function">_one_hot</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> labels<span class="token punctuation">,</span> classes<span class="token punctuation">,</span> value<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""
            Convert labels to one hot vectors

        Args:
            labels: torch tensor in format [label1, label2, label3, ...]
            classes: int, number of classes
            value: label value in one hot vector, default to 1

        Returns:
            return one hot format labels in shape [batchsize, classes]
        """</span>

        one_hot <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>labels<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> classes<span class="token punctuation">)</span>

        <span class="token comment" spellcheck="true">#labels and value_added  size must match</span>
        labels <span class="token operator">=</span> labels<span class="token punctuation">.</span>view<span class="token punctuation">(</span>labels<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
        value_added <span class="token operator">=</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">(</span>labels<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>fill_<span class="token punctuation">(</span>value<span class="token punctuation">)</span>

        value_added <span class="token operator">=</span> value_added<span class="token punctuation">.</span>to<span class="token punctuation">(</span>labels<span class="token punctuation">.</span>device<span class="token punctuation">)</span>
        one_hot <span class="token operator">=</span> one_hot<span class="token punctuation">.</span>to<span class="token punctuation">(</span>labels<span class="token punctuation">.</span>device<span class="token punctuation">)</span>

        one_hot<span class="token punctuation">.</span>scatter_add_<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> labels<span class="token punctuation">,</span> value_added<span class="token punctuation">)</span>

        <span class="token keyword">return</span> one_hot

    <span class="token keyword">def</span> <span class="token function">_smooth_label</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> target<span class="token punctuation">,</span> length<span class="token punctuation">,</span> smooth_factor<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""convert targets to one-hot format, and smooth
        them.
        Args:
            target: target in form with [label1, label2, label_batchsize]
            length: length of one-hot format(number of classes)
            smooth_factor: smooth factor for label smooth

        Returns:
            smoothed labels in one hot format
        """</span>
        one_hot <span class="token operator">=</span> self<span class="token punctuation">.</span>_one_hot<span class="token punctuation">(</span>target<span class="token punctuation">,</span> length<span class="token punctuation">,</span> value<span class="token operator">=</span><span class="token number">1</span> <span class="token operator">-</span> smooth_factor<span class="token punctuation">)</span>
        one_hot <span class="token operator">+=</span> smooth_factor <span class="token operator">/</span> <span class="token punctuation">(</span>length <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">)</span>

        <span class="token keyword">return</span> one_hot<span class="token punctuation">.</span>to<span class="token punctuation">(</span>target<span class="token punctuation">.</span>device<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">,</span> target<span class="token punctuation">)</span><span class="token punctuation">:</span>

        <span class="token keyword">if</span> x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span> <span class="token operator">!=</span> target<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token keyword">raise</span> ValueError<span class="token punctuation">(</span><span class="token string">'Expected input batchsize ({}) to match target batch_size({})'</span>
                    <span class="token punctuation">.</span>format<span class="token punctuation">(</span>x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> target<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

        <span class="token keyword">if</span> x<span class="token punctuation">.</span>dim<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">&lt;</span> <span class="token number">2</span><span class="token punctuation">:</span>
            <span class="token keyword">raise</span> ValueError<span class="token punctuation">(</span><span class="token string">'Expected input tensor to have least 2 dimensions(got {})'</span>
                    <span class="token punctuation">.</span>format<span class="token punctuation">(</span>x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

        <span class="token keyword">if</span> x<span class="token punctuation">.</span>dim<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">!=</span> <span class="token number">2</span><span class="token punctuation">:</span>
            <span class="token keyword">raise</span> ValueError<span class="token punctuation">(</span><span class="token string">'Only 2 dimension tensor are implemented, (got {})'</span>
                    <span class="token punctuation">.</span>format<span class="token punctuation">(</span>x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

        smoothed_target <span class="token operator">=</span> self<span class="token punctuation">.</span>_smooth_label<span class="token punctuation">(</span>target<span class="token punctuation">,</span> x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>e<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>log_softmax<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        loss <span class="token operator">=</span> torch<span class="token punctuation">.</span>sum<span class="token punctuation">(</span><span class="token operator">-</span> x <span class="token operator">*</span> smoothed_target<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>

        <span class="token keyword">if</span> self<span class="token punctuation">.</span>reduction <span class="token operator">==</span> <span class="token string">'none'</span><span class="token punctuation">:</span>
            <span class="token keyword">return</span> loss

        <span class="token keyword">elif</span> self<span class="token punctuation">.</span>reduction <span class="token operator">==</span> <span class="token string">'sum'</span><span class="token punctuation">:</span>
            <span class="token keyword">return</span> torch<span class="token punctuation">.</span>sum<span class="token punctuation">(</span>loss<span class="token punctuation">)</span>

        <span class="token keyword">elif</span> self<span class="token punctuation">.</span>reduction <span class="token operator">==</span> <span class="token string">'mean'</span><span class="token punctuation">:</span>
            <span class="token keyword">return</span> torch<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>loss<span class="token punctuation">)</span>

        <span class="token keyword">else</span><span class="token punctuation">:</span>
            <span class="token keyword">raise</span> ValueError<span class="token punctuation">(</span><span class="token string">'unrecognized option, expect reduction to be one of none, mean, sum'</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>或者直接在训练文件里做label smoothing</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">for</span> images<span class="token punctuation">,</span> labels <span class="token keyword">in</span> train_loader<span class="token punctuation">:</span>
    images<span class="token punctuation">,</span> labels <span class="token operator">=</span> images<span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> labels<span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span>
    N <span class="token operator">=</span> labels<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>
    <span class="token comment" spellcheck="true"># C is the number of classes.</span>
    smoothed_labels <span class="token operator">=</span> torch<span class="token punctuation">.</span>full<span class="token punctuation">(</span>size<span class="token operator">=</span><span class="token punctuation">(</span>N<span class="token punctuation">,</span> C<span class="token punctuation">)</span><span class="token punctuation">,</span> fill_value<span class="token operator">=</span><span class="token number">0.1</span> <span class="token operator">/</span> <span class="token punctuation">(</span>C <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span>
    smoothed_labels<span class="token punctuation">.</span>scatter_<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> index<span class="token operator">=</span>torch<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span>labels<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> value<span class="token operator">=</span><span class="token number">0.9</span><span class="token punctuation">)</span>

    score <span class="token operator">=</span> model<span class="token punctuation">(</span>images<span class="token punctuation">)</span>
    log_prob <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional<span class="token punctuation">.</span>log_softmax<span class="token punctuation">(</span>score<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
    loss <span class="token operator">=</span> <span class="token operator">-</span>torch<span class="token punctuation">.</span>sum<span class="token punctuation">(</span>log_prob <span class="token operator">*</span> smoothed_labels<span class="token punctuation">)</span> <span class="token operator">/</span> N
    optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
    loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
    optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h3 id="Mixup训练"><a href="#Mixup训练" class="headerlink" title="Mixup训练"></a>Mixup训练</h3><pre class="line-numbers language-python"><code class="language-python">beta_distribution <span class="token operator">=</span> torch<span class="token punctuation">.</span>distributions<span class="token punctuation">.</span>beta<span class="token punctuation">.</span>Beta<span class="token punctuation">(</span>alpha<span class="token punctuation">,</span> alpha<span class="token punctuation">)</span>
<span class="token keyword">for</span> images<span class="token punctuation">,</span> labels <span class="token keyword">in</span> train_loader<span class="token punctuation">:</span>
    images<span class="token punctuation">,</span> labels <span class="token operator">=</span> images<span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> labels<span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token comment" spellcheck="true"># Mixup images and labels.</span>
    lambda_ <span class="token operator">=</span> beta_distribution<span class="token punctuation">.</span>sample<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>
    index <span class="token operator">=</span> torch<span class="token punctuation">.</span>randperm<span class="token punctuation">(</span>images<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span>
    mixed_images <span class="token operator">=</span> lambda_ <span class="token operator">*</span> images <span class="token operator">+</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">-</span> lambda_<span class="token punctuation">)</span> <span class="token operator">*</span> images<span class="token punctuation">[</span>index<span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span>
    label_a<span class="token punctuation">,</span> label_b <span class="token operator">=</span> labels<span class="token punctuation">,</span> labels<span class="token punctuation">[</span>index<span class="token punctuation">]</span>

    <span class="token comment" spellcheck="true"># Mixup loss.</span>
    scores <span class="token operator">=</span> model<span class="token punctuation">(</span>mixed_images<span class="token punctuation">)</span>
    loss <span class="token operator">=</span> <span class="token punctuation">(</span>lambda_ <span class="token operator">*</span> loss_function<span class="token punctuation">(</span>scores<span class="token punctuation">,</span> label_a<span class="token punctuation">)</span>
            <span class="token operator">+</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">-</span> lambda_<span class="token punctuation">)</span> <span class="token operator">*</span> loss_function<span class="token punctuation">(</span>scores<span class="token punctuation">,</span> label_b<span class="token punctuation">)</span><span class="token punctuation">)</span>
    optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
    loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
    optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h3 id="L1-正则化"><a href="#L1-正则化" class="headerlink" title="L1 正则化"></a><strong>L1 正则化</strong></h3><pre class="line-numbers language-python"><code class="language-python">l1_regularization <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>L1Loss<span class="token punctuation">(</span>reduction<span class="token operator">=</span><span class="token string">'sum'</span><span class="token punctuation">)</span>
loss <span class="token operator">=</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>  <span class="token comment" spellcheck="true"># Standard cross-entropy loss</span>
<span class="token keyword">for</span> param <span class="token keyword">in</span> model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    loss <span class="token operator">+=</span> torch<span class="token punctuation">.</span>sum<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>abs<span class="token punctuation">(</span>param<span class="token punctuation">)</span><span class="token punctuation">)</span>
loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h3 id="不对偏置项进行权重衰减（weight-decay）"><a href="#不对偏置项进行权重衰减（weight-decay）" class="headerlink" title="不对偏置项进行权重衰减（weight decay）"></a><strong>不对偏置项进行权重衰减（weight decay）</strong></h3><p>pytorch里的weight decay相当于l2正则</p>
<pre class="line-numbers language-python"><code class="language-python">bias_list <span class="token operator">=</span> <span class="token punctuation">(</span>param <span class="token keyword">for</span> name<span class="token punctuation">,</span> param <span class="token keyword">in</span> model<span class="token punctuation">.</span>named_parameters<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">if</span> name<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">4</span><span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token operator">==</span> <span class="token string">'bias'</span><span class="token punctuation">)</span>
others_list <span class="token operator">=</span> <span class="token punctuation">(</span>param <span class="token keyword">for</span> name<span class="token punctuation">,</span> param <span class="token keyword">in</span> model<span class="token punctuation">.</span>named_parameters<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">if</span> name<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">4</span><span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token operator">!=</span> <span class="token string">'bias'</span><span class="token punctuation">)</span>
parameters <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">{</span><span class="token string">'parameters'</span><span class="token punctuation">:</span> bias_list<span class="token punctuation">,</span> <span class="token string">'weight_decay'</span><span class="token punctuation">:</span> <span class="token number">0</span><span class="token punctuation">}</span><span class="token punctuation">,</span>                
              <span class="token punctuation">{</span><span class="token string">'parameters'</span><span class="token punctuation">:</span> others_list<span class="token punctuation">}</span><span class="token punctuation">]</span>
optimizer <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span>parameters<span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">1e</span><span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">,</span> momentum<span class="token operator">=</span><span class="token number">0.9</span><span class="token punctuation">,</span> weight_decay<span class="token operator">=</span><span class="token number">1e</span><span class="token operator">-</span><span class="token number">4</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h3 id="梯度裁剪（gradient-clipping）"><a href="#梯度裁剪（gradient-clipping）" class="headerlink" title="梯度裁剪（gradient clipping）"></a><strong>梯度裁剪（gradient clipping）</strong></h3><pre class="line-numbers language-python"><code class="language-python">torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>clip_grad_norm_<span class="token punctuation">(</span>model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> max_norm<span class="token operator">=</span><span class="token number">20</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<h3 id="得到当前学习率"><a href="#得到当前学习率" class="headerlink" title="得到当前学习率"></a><strong>得到当前学习率</strong></h3><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># If there is one global learning rate (which is the common case).</span>
lr <span class="token operator">=</span> next<span class="token punctuation">(</span>iter<span class="token punctuation">(</span>optimizer<span class="token punctuation">.</span>param_groups<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token string">'lr'</span><span class="token punctuation">]</span>

<span class="token comment" spellcheck="true"># If there are multiple learning rates for different layers.</span>
all_lr <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
<span class="token keyword">for</span> param_group <span class="token keyword">in</span> optimizer<span class="token punctuation">.</span>param_groups<span class="token punctuation">:</span>
    all_lr<span class="token punctuation">.</span>append<span class="token punctuation">(</span>param_group<span class="token punctuation">[</span><span class="token string">'lr'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>另一种方法，在一个batch训练代码里，当前的lr是optimizer.param_groups[0][‘lr’]</p>
<h3 id="学习率衰减"><a href="#学习率衰减" class="headerlink" title="学习率衰减"></a><strong>学习率衰减</strong></h3><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># Reduce learning rate when validation accuarcy plateau.</span>
scheduler <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>lr_scheduler<span class="token punctuation">.</span>ReduceLROnPlateau<span class="token punctuation">(</span>optimizer<span class="token punctuation">,</span> mode<span class="token operator">=</span><span class="token string">'max'</span><span class="token punctuation">,</span> patience<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">,</span> verbose<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
<span class="token keyword">for</span> t <span class="token keyword">in</span> range<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">80</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    train<span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span>
    val<span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span>
    scheduler<span class="token punctuation">.</span>step<span class="token punctuation">(</span>val_acc<span class="token punctuation">)</span>

<span class="token comment" spellcheck="true"># Cosine annealing learning rate.</span>
scheduler <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>lr_scheduler<span class="token punctuation">.</span>CosineAnnealingLR<span class="token punctuation">(</span>optimizer<span class="token punctuation">,</span> T_max<span class="token operator">=</span><span class="token number">80</span><span class="token punctuation">)</span>
<span class="token comment" spellcheck="true"># Reduce learning rate by 10 at given epochs.</span>
scheduler <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>lr_scheduler<span class="token punctuation">.</span>MultiStepLR<span class="token punctuation">(</span>optimizer<span class="token punctuation">,</span> milestones<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">50</span><span class="token punctuation">,</span> <span class="token number">70</span><span class="token punctuation">]</span><span class="token punctuation">,</span> gamma<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">)</span>
<span class="token keyword">for</span> t <span class="token keyword">in</span> range<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">80</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    scheduler<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>    
    train<span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span>
    val<span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span>

<span class="token comment" spellcheck="true"># Learning rate warmup by 10 epochs.</span>
scheduler <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>lr_scheduler<span class="token punctuation">.</span>LambdaLR<span class="token punctuation">(</span>optimizer<span class="token punctuation">,</span> lr_lambda<span class="token operator">=</span><span class="token keyword">lambda</span> t<span class="token punctuation">:</span> t <span class="token operator">/</span> <span class="token number">10</span><span class="token punctuation">)</span>
<span class="token keyword">for</span> t <span class="token keyword">in</span> range<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    scheduler<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>
    train<span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span>
    val<span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h3 id="优化器链式更新"><a href="#优化器链式更新" class="headerlink" title="优化器链式更新"></a>优化器链式更新</h3><p>从1.4版本开始，torch.optim.lr_scheduler 支持链式更新（chaining），即用户可以定义两个 schedulers，并交替在训练中使用。</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> torch
<span class="token keyword">from</span> torch<span class="token punctuation">.</span>optim <span class="token keyword">import</span> SGD
<span class="token keyword">from</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>lr_scheduler <span class="token keyword">import</span> ExponentialLR<span class="token punctuation">,</span> StepLR
model <span class="token operator">=</span> <span class="token punctuation">[</span>torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">]</span>
optimizer <span class="token operator">=</span> SGD<span class="token punctuation">(</span>model<span class="token punctuation">,</span> <span class="token number">0.1</span><span class="token punctuation">)</span>
scheduler1 <span class="token operator">=</span> ExponentialLR<span class="token punctuation">(</span>optimizer<span class="token punctuation">,</span> gamma<span class="token operator">=</span><span class="token number">0.9</span><span class="token punctuation">)</span>
scheduler2 <span class="token operator">=</span> StepLR<span class="token punctuation">(</span>optimizer<span class="token punctuation">,</span> step_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> gamma<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">)</span>
<span class="token keyword">for</span> epoch <span class="token keyword">in</span> range<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>epoch<span class="token punctuation">,</span> scheduler2<span class="token punctuation">.</span>get_last_lr<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
    optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>
    scheduler1<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>
    scheduler2<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h3 id="模型训练可视化"><a href="#模型训练可视化" class="headerlink" title="模型训练可视化"></a>模型训练可视化</h3><p>PyTorch可以使用tensorboard来可视化训练过程。</p>
<p>安装和运行TensorBoard。</p>
<pre class="line-numbers language-python"><code class="language-python">pip install tensorboard
tensorboard <span class="token operator">-</span><span class="token operator">-</span>logdir<span class="token operator">=</span>runs<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>
<p>使用SummaryWriter类来收集和可视化相应的数据，放了方便查看，可以使用不同的文件夹，比如’Loss/train’和’Loss/test’。</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">from</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>tensorboard <span class="token keyword">import</span> SummaryWriter
<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np

writer <span class="token operator">=</span> SummaryWriter<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token keyword">for</span> n_iter <span class="token keyword">in</span> range<span class="token punctuation">(</span><span class="token number">100</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    writer<span class="token punctuation">.</span>add_scalar<span class="token punctuation">(</span><span class="token string">'Loss/train'</span><span class="token punctuation">,</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>random<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> n_iter<span class="token punctuation">)</span>
    writer<span class="token punctuation">.</span>add_scalar<span class="token punctuation">(</span><span class="token string">'Loss/test'</span><span class="token punctuation">,</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>random<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> n_iter<span class="token punctuation">)</span>
    writer<span class="token punctuation">.</span>add_scalar<span class="token punctuation">(</span><span class="token string">'Accuracy/train'</span><span class="token punctuation">,</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>random<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> n_iter<span class="token punctuation">)</span>
    writer<span class="token punctuation">.</span>add_scalar<span class="token punctuation">(</span><span class="token string">'Accuracy/test'</span><span class="token punctuation">,</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>random<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> n_iter<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h3 id="保存与加载断点"><a href="#保存与加载断点" class="headerlink" title="保存与加载断点"></a><strong>保存与加载断点</strong></h3><p>注意为了能够恢复训练，我们需要同时保存模型和优化器的状态，以及当前的训练轮数。</p>
<pre class="line-numbers language-python"><code class="language-python">start_epoch <span class="token operator">=</span> <span class="token number">0</span>
<span class="token comment" spellcheck="true"># Load checkpoint.</span>
<span class="token keyword">if</span> resume<span class="token punctuation">:</span> <span class="token comment" spellcheck="true"># resume为参数，第一次训练时设为0，中断再训练时设为1</span>
    model_path <span class="token operator">=</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>join<span class="token punctuation">(</span><span class="token string">'model'</span><span class="token punctuation">,</span> <span class="token string">'best_checkpoint.pth.tar'</span><span class="token punctuation">)</span>
    <span class="token keyword">assert</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>isfile<span class="token punctuation">(</span>model_path<span class="token punctuation">)</span>
    checkpoint <span class="token operator">=</span> torch<span class="token punctuation">.</span>load<span class="token punctuation">(</span>model_path<span class="token punctuation">)</span>
    best_acc <span class="token operator">=</span> checkpoint<span class="token punctuation">[</span><span class="token string">'best_acc'</span><span class="token punctuation">]</span>
    start_epoch <span class="token operator">=</span> checkpoint<span class="token punctuation">[</span><span class="token string">'epoch'</span><span class="token punctuation">]</span>
    model<span class="token punctuation">.</span>load_state_dict<span class="token punctuation">(</span>checkpoint<span class="token punctuation">[</span><span class="token string">'model'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
    optimizer<span class="token punctuation">.</span>load_state_dict<span class="token punctuation">(</span>checkpoint<span class="token punctuation">[</span><span class="token string">'optimizer'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'Load checkpoint at epoch {}.'</span><span class="token punctuation">.</span>format<span class="token punctuation">(</span>start_epoch<span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'Best accuracy so far {}.'</span><span class="token punctuation">.</span>format<span class="token punctuation">(</span>best_acc<span class="token punctuation">)</span><span class="token punctuation">)</span>

<span class="token comment" spellcheck="true"># Train the model</span>
<span class="token keyword">for</span> epoch <span class="token keyword">in</span> range<span class="token punctuation">(</span>start_epoch<span class="token punctuation">,</span> num_epochs<span class="token punctuation">)</span><span class="token punctuation">:</span> 
    <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span> 

    <span class="token comment" spellcheck="true"># Test the model</span>
    <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>

    <span class="token comment" spellcheck="true"># save checkpoint</span>
    is_best <span class="token operator">=</span> current_acc <span class="token operator">></span> best_acc
    best_acc <span class="token operator">=</span> max<span class="token punctuation">(</span>current_acc<span class="token punctuation">,</span> best_acc<span class="token punctuation">)</span>
    checkpoint <span class="token operator">=</span> <span class="token punctuation">{</span>
        <span class="token string">'best_acc'</span><span class="token punctuation">:</span> best_acc<span class="token punctuation">,</span>
        <span class="token string">'epoch'</span><span class="token punctuation">:</span> epoch <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">,</span>
        <span class="token string">'model'</span><span class="token punctuation">:</span> model<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
        <span class="token string">'optimizer'</span><span class="token punctuation">:</span> optimizer<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    <span class="token punctuation">}</span>
    model_path <span class="token operator">=</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>join<span class="token punctuation">(</span><span class="token string">'model'</span><span class="token punctuation">,</span> <span class="token string">'checkpoint.pth.tar'</span><span class="token punctuation">)</span>
    best_model_path <span class="token operator">=</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>join<span class="token punctuation">(</span><span class="token string">'model'</span><span class="token punctuation">,</span> <span class="token string">'best_checkpoint.pth.tar'</span><span class="token punctuation">)</span>
    torch<span class="token punctuation">.</span>save<span class="token punctuation">(</span>checkpoint<span class="token punctuation">,</span> model_path<span class="token punctuation">)</span>
    <span class="token keyword">if</span> is_best<span class="token punctuation">:</span>
        shutil<span class="token punctuation">.</span>copy<span class="token punctuation">(</span>model_path<span class="token punctuation">,</span> best_model_path<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h3 id="提取-ImageNet-预训练模型某层的卷积特征"><a href="#提取-ImageNet-预训练模型某层的卷积特征" class="headerlink" title="提取 ImageNet 预训练模型某层的卷积特征"></a><strong>提取 ImageNet 预训练模型某层的卷积特征</strong></h3><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># VGG-16 relu5-3 feature.</span>
model <span class="token operator">=</span> torchvision<span class="token punctuation">.</span>models<span class="token punctuation">.</span>vgg16<span class="token punctuation">(</span>pretrained<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">.</span>features<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span>
<span class="token comment" spellcheck="true"># VGG-16 pool5 feature.</span>
model <span class="token operator">=</span> torchvision<span class="token punctuation">.</span>models<span class="token punctuation">.</span>vgg16<span class="token punctuation">(</span>pretrained<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">.</span>features
<span class="token comment" spellcheck="true"># VGG-16 fc7 feature.</span>
model <span class="token operator">=</span> torchvision<span class="token punctuation">.</span>models<span class="token punctuation">.</span>vgg16<span class="token punctuation">(</span>pretrained<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
model<span class="token punctuation">.</span>classifier <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span><span class="token operator">*</span>list<span class="token punctuation">(</span>model<span class="token punctuation">.</span>classifier<span class="token punctuation">.</span>children<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token comment" spellcheck="true"># ResNet GAP feature.</span>
model <span class="token operator">=</span> torchvision<span class="token punctuation">.</span>models<span class="token punctuation">.</span>resnet18<span class="token punctuation">(</span>pretrained<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
model <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>collections<span class="token punctuation">.</span>OrderedDict<span class="token punctuation">(</span>
    list<span class="token punctuation">(</span>model<span class="token punctuation">.</span>named_children<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

<span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    model<span class="token punctuation">.</span>eval<span class="token punctuation">(</span><span class="token punctuation">)</span>
    conv_representation <span class="token operator">=</span> model<span class="token punctuation">(</span>image<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h3 id="提取-ImageNet-预训练模型多层的卷积特征"><a href="#提取-ImageNet-预训练模型多层的卷积特征" class="headerlink" title="提取 ImageNet 预训练模型多层的卷积特征"></a><strong>提取 ImageNet 预训练模型多层的卷积特征</strong></h3><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">FeatureExtractor</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""Helper class to extract several convolution features from the given
    pre-trained model.

    Attributes:
        _model, torch.nn.Module.
        _layers_to_extract, list&lt;str> or set&lt;str>

    Example:
        >>> model = torchvision.models.resnet152(pretrained=True)
        >>> model = torch.nn.Sequential(collections.OrderedDict(
                list(model.named_children())[:-1]))
        >>> conv_representation = FeatureExtractor(
                pretrained_model=model,
                layers_to_extract={'layer1', 'layer2', 'layer3', 'layer4'})(image)
    """</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> pretrained_model<span class="token punctuation">,</span> layers_to_extract<span class="token punctuation">)</span><span class="token punctuation">:</span>
        torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">.</span>__init__<span class="token punctuation">(</span>self<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>_model <span class="token operator">=</span> pretrained_model
        self<span class="token punctuation">.</span>_model<span class="token punctuation">.</span>eval<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>_layers_to_extract <span class="token operator">=</span> set<span class="token punctuation">(</span>layers_to_extract<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            conv_representation <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
            <span class="token keyword">for</span> name<span class="token punctuation">,</span> layer <span class="token keyword">in</span> self<span class="token punctuation">.</span>_model<span class="token punctuation">.</span>named_children<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
                x <span class="token operator">=</span> layer<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
                <span class="token keyword">if</span> name <span class="token keyword">in</span> self<span class="token punctuation">.</span>_layers_to_extract<span class="token punctuation">:</span>
                    conv_representation<span class="token punctuation">.</span>append<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
            <span class="token keyword">return</span> conv_representation<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h3 id="微调全连接层"><a href="#微调全连接层" class="headerlink" title="微调全连接层"></a><strong>微调全连接层</strong></h3><pre class="line-numbers language-python"><code class="language-python">model <span class="token operator">=</span> torchvision<span class="token punctuation">.</span>models<span class="token punctuation">.</span>resnet18<span class="token punctuation">(</span>pretrained<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
<span class="token keyword">for</span> param <span class="token keyword">in</span> model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    param<span class="token punctuation">.</span>requires_grad <span class="token operator">=</span> <span class="token boolean">False</span>
model<span class="token punctuation">.</span>fc <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">512</span><span class="token punctuation">,</span> <span class="token number">100</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># Replace the last fc layer</span>
optimizer <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span>model<span class="token punctuation">.</span>fc<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">1e</span><span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">,</span> momentum<span class="token operator">=</span><span class="token number">0.9</span><span class="token punctuation">,</span> weight_decay<span class="token operator">=</span><span class="token number">1e</span><span class="token operator">-</span><span class="token number">4</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h3 id="以较大学习率微调全连接层，较小学习率微调卷积层"><a href="#以较大学习率微调全连接层，较小学习率微调卷积层" class="headerlink" title="以较大学习率微调全连接层，较小学习率微调卷积层"></a><strong>以较大学习率微调全连接层，较小学习率微调卷积层</strong></h3><pre class="line-numbers language-python"><code class="language-python">model <span class="token operator">=</span> torchvision<span class="token punctuation">.</span>models<span class="token punctuation">.</span>resnet18<span class="token punctuation">(</span>pretrained<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
finetuned_parameters <span class="token operator">=</span> list<span class="token punctuation">(</span>map<span class="token punctuation">(</span>id<span class="token punctuation">,</span> model<span class="token punctuation">.</span>fc<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
conv_parameters <span class="token operator">=</span> <span class="token punctuation">(</span>p <span class="token keyword">for</span> p <span class="token keyword">in</span> model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">if</span> id<span class="token punctuation">(</span>p<span class="token punctuation">)</span> <span class="token operator">not</span> <span class="token keyword">in</span> finetuned_parameters<span class="token punctuation">)</span>
parameters <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">{</span><span class="token string">'params'</span><span class="token punctuation">:</span> conv_parameters<span class="token punctuation">,</span> <span class="token string">'lr'</span><span class="token punctuation">:</span> <span class="token number">1e</span><span class="token operator">-</span><span class="token number">3</span><span class="token punctuation">}</span><span class="token punctuation">,</span> 
              <span class="token punctuation">{</span><span class="token string">'params'</span><span class="token punctuation">:</span> model<span class="token punctuation">.</span>fc<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">}</span><span class="token punctuation">]</span>
optimizer <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span>parameters<span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">1e</span><span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">,</span> momentum<span class="token operator">=</span><span class="token number">0.9</span><span class="token punctuation">,</span> weight_decay<span class="token operator">=</span><span class="token number">1e</span><span class="token operator">-</span><span class="token number">4</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h2 id="6-其他注意事项"><a href="#6-其他注意事项" class="headerlink" title="6. 其他注意事项"></a>6. 其他注意事项</h2><ul>
<li><p>不要使用太大的线性层。因为nn.Linear(m,n)使用的是的内存，线性层太大很容易超出现有显存。</p>
</li>
<li><p>不要在太长的序列上使用RNN。因为RNN反向传播使用的是BPTT算法，其需要的内存和输入序列的长度呈线性关系。</p>
</li>
<li><p>model(x) 前用 model.train() 和 model.eval() 切换网络状态。</p>
</li>
<li><p>不需要计算梯度的代码块用 with torch.no_grad() 包含起来。</p>
</li>
<li><p>model.eval() 和 torch.no_grad() 的区别在于，model.eval() 是将网络切换为测试状态，例如 BN 和dropout在训练和测试阶段使用不同的计算方法。torch.no_grad() 是关闭 PyTorch 张量的自动求导机制，以减少存储使用和加速计算，得到的结果无法进行loss.backward()。</p>
</li>
<li><p>model.zero_grad()会把整个模型的参数的梯度都归零, 而optimizer.zero_grad()只会把传入其中的参数的梯度归零.</p>
</li>
<li><p>torch.nn.CrossEntropyLoss 的输入不需要经过 Softmax。torch.nn.CrossEntropyLoss 等价于 torch.nn.functional.log_softmax + torch.nn.NLLLoss。</p>
</li>
<li><p>loss.backward() 前用 optimizer.zero_grad() 清除累积梯度。</p>
</li>
<li><p>torch.utils.data.DataLoader 中尽量设置 pin_memory=True，对特别小的数据集如 MNIST 设置 pin_memory=False 反而更快一些。num_workers 的设置需要在实验中找到最快的取值。</p>
</li>
<li><p>用 del 及时删除不用的中间变量，节约 GPU 存储。</p>
</li>
<li><p>使用 inplace 操作可节约 GPU 存储，如<br>x = torch.nn.functional.relu(x, inplace=True)</p>
</li>
<li><p>减少 CPU 和 GPU 之间的数据传输。例如如果你想知道一个 epoch 中每个 mini-batch 的 loss 和准确率，先将它们累积在 GPU 中等一个 epoch 结束之后一起传输回 CPU 会比每个 mini-batch 都进行一次 GPU 到 CPU 的传输更快。</p>
</li>
<li><p>使用半精度浮点数 half() 会有一定的速度提升，具体效率依赖于 GPU 型号。需要小心数值精度过低带来的稳定性问题。</p>
</li>
<li><p>时常使用 assert tensor.size() == (N, D, H, W) 作为调试手段，确保张量维度和你设想中一致。</p>
</li>
<li><p>除了标记 y 外，尽量少使用一维张量，使用 n*1 的二维张量代替，可以避免一些意想不到的一维张量计算结果。</p>
</li>
<li><p>统计代码各部分耗时<br>with torch.autograd.profiler.profile(enabled=True, use_cuda=False) as profile:<br>…<br>print(profile)<br>或者在命令行运行<br>python -m torch.utils.bottleneck main.py</p>
</li>
<li><p>使用TorchSnooper来调试PyTorch代码，程序在执行的时候，就会自动 print 出来每一行的执行结果的 tensor 的形状、数据类型、设备、是否需要梯度的信息。<br>pip install torchsnooper<br>import torchsnooper<br>对于函数，使用修饰器<br>@torchsnooper.snoop()<br>如果不是函数，使用 with 语句来激活 TorchSnooper，把训练的那个循环装进 with 语句中去。<br>with torchsnooper.snoop():<br>原本的代码</p>
</li>
</ul>
<p><a href="https://link.zhihu.com/?target=https%3A//github.com/zasdfgbnm/TorchSnooper">https://github.com/zasdfgbnm/TorchSnooper</a></p>
<ul>
<li>模型可解释性，使用captum库</li>
</ul>
<p><a href="https://link.zhihu.com/?target=https%3A//captum.ai/">https://captum.ai/</a></p>
<h1 id="PyTorch使用技巧"><a href="#PyTorch使用技巧" class="headerlink" title="PyTorch使用技巧"></a>PyTorch使用技巧</h1><h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a><strong>目录</strong></h2><p>1、指定GPU编号</p>
<p>2、查看模型每层输出详情</p>
<p>3、梯度裁剪</p>
<p>4、扩展单张图片维度</p>
<p>5、one hot编码</p>
<p>6、防止验证模型时爆显存</p>
<p>7、学习率衰减</p>
<p>8、冻结某些层的参数</p>
<p>9、对不同层使用不同学习率</p>
<p>10、模型相关操作</p>
<p>11、Pytorch内置one hot函数</p>
<p>12、网络参数初始化</p>
<p>13、加载内置预训练模型</p>
<p>14、Pytorch编写代码基本步骤思想</p>
<h2 id="指定GPU编号"><a href="#指定GPU编号" class="headerlink" title="指定GPU编号"></a><strong>指定GPU编号</strong></h2><ul>
<li>设置当前使用的GPU设备仅为0号设备，设备名称为 <code>/gpu:0</code>：<code>os.environ["CUDA_VISIBLE_DEVICES"] = "0"</code></li>
<li>设置当前使用的GPU设备为0,1号两个设备，名称依次为 <code>/gpu:0</code>、<code>/gpu:1</code>：<code>os.environ["CUDA_VISIBLE_DEVICES"] = "0,1"</code> ，根据顺序表示优先使用0号设备,然后使用1号设备。</li>
</ul>
<p>指定GPU的命令需要放在和神经网络相关的一系列操作的前面。</p>
<h2 id="查看模型每层输出详情"><a href="#查看模型每层输出详情" class="headerlink" title="查看模型每层输出详情"></a><strong>查看模型每层输出详情</strong></h2><p>Keras有一个简洁的API来查看模型的每一层输出尺寸，这在调试网络时非常有用。现在在PyTorch中也可以实现这个功能。</p>
<p>使用很简单，如下用法：</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">from</span> torchsummary <span class="token keyword">import</span> summary
summary<span class="token punctuation">(</span>your_model<span class="token punctuation">,</span> input_size<span class="token operator">=</span><span class="token punctuation">(</span>channels<span class="token punctuation">,</span> H<span class="token punctuation">,</span> W<span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>
<p><code>input_size</code> 是根据你自己的网络模型的输入尺寸进行设置。</p>
<h2 id="梯度裁剪（Gradient-Clipping）"><a href="#梯度裁剪（Gradient-Clipping）" class="headerlink" title="梯度裁剪（Gradient Clipping）"></a><strong>梯度裁剪（Gradient Clipping）</strong></h2><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn

outputs <span class="token operator">=</span> model<span class="token punctuation">(</span>data<span class="token punctuation">)</span>
loss<span class="token operator">=</span> loss_fn<span class="token punctuation">(</span>outputs<span class="token punctuation">,</span> target<span class="token punctuation">)</span>
optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
nn<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>clip_grad_norm_<span class="token punctuation">(</span>model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> max_norm<span class="token operator">=</span><span class="token number">20</span><span class="token punctuation">,</span> norm_type<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>
optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p><code>nn.utils.clip_grad_norm_</code> 的参数：</p>
<ul>
<li><strong>parameters</strong> – 一个基于变量的迭代器，会进行梯度归一化</li>
<li><strong>max_norm</strong> – 梯度的最大范数</li>
<li><strong>norm_type</strong> – 规定范数的类型，默认为L2</li>
</ul>
<h2 id="扩展单张图片维度"><a href="#扩展单张图片维度" class="headerlink" title="扩展单张图片维度"></a><strong>扩展单张图片维度</strong></h2><p>因为在训练时的数据维度一般都是 (batch_size, c, h, w)，而在测试时只输入一张图片，所以需要扩展维度，扩展维度有多个方法：</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> cv2
<span class="token keyword">import</span> torch

image <span class="token operator">=</span> cv2<span class="token punctuation">.</span>imread<span class="token punctuation">(</span>img_path<span class="token punctuation">)</span>
image <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>image<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>image<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
img <span class="token operator">=</span> image<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">*</span>image<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>img<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token comment" spellcheck="true"># output:</span>
<span class="token comment" spellcheck="true"># torch.Size([h, w, c])</span>
<span class="token comment" spellcheck="true"># torch.Size([1, h, w, c])</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>或</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> cv2
<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
image <span class="token operator">=</span> cv2<span class="token punctuation">.</span>imread<span class="token punctuation">(</span>img_path<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>image<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
img <span class="token operator">=</span> image<span class="token punctuation">[</span>np<span class="token punctuation">.</span>newaxis<span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>img<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
<span class="token comment" spellcheck="true"># output:</span>
<span class="token comment" spellcheck="true"># (h, w, c)</span>
<span class="token comment" spellcheck="true"># (1, h, w, c)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>或</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> cv2
<span class="token keyword">import</span> torch

image <span class="token operator">=</span> cv2<span class="token punctuation">.</span>imread<span class="token punctuation">(</span>img_path<span class="token punctuation">)</span>
image <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>image<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>image<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

img <span class="token operator">=</span> image<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>  
<span class="token keyword">print</span><span class="token punctuation">(</span>img<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

img <span class="token operator">=</span> img<span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>img<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token comment" spellcheck="true"># output:</span>
<span class="token comment" spellcheck="true"># torch.Size([(h, w, c)])</span>
<span class="token comment" spellcheck="true"># torch.Size([1, h, w, c])</span>
<span class="token comment" spellcheck="true"># torch.Size([h, w, c])</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p><code>tensor.unsqueeze(dim)</code>：扩展维度，dim指定扩展哪个维度。</p>
<p><code>tensor.squeeze(dim)</code>：去除dim指定的且size为1的维度，维度大于1时，squeeze()不起作用，不指定dim时，去除所有size为1的维度。</p>
<h2 id="独热编码"><a href="#独热编码" class="headerlink" title="独热编码"></a><strong>独热编码</strong></h2><p>在PyTorch中使用交叉熵损失函数的时候会自动把label转化成onehot，所以不用手动转化，而使用MSE需要手动转化成onehot编码。</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> torch
class_num <span class="token operator">=</span> <span class="token number">8</span>
batch_size <span class="token operator">=</span> <span class="token number">4</span>
<span class="token keyword">def</span> <span class="token function">one_hot</span><span class="token punctuation">(</span>label<span class="token punctuation">)</span><span class="token punctuation">:</span>    
    <span class="token triple-quoted-string string">"""    将一维列表转换为独热编码    """</span>    
    label <span class="token operator">=</span> label<span class="token punctuation">.</span>resize_<span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>    
    m_zeros <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span> class_num<span class="token punctuation">)</span>    
    <span class="token comment" spellcheck="true"># 从 value 中取值，然后根据 dim 和 index 给相应位置赋值    </span>
    onehot <span class="token operator">=</span> m_zeros<span class="token punctuation">.</span>scatter_<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> label<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># (dim,index,value)</span>
    <span class="token keyword">return</span> onehot<span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># Tensor -> Numpy</span>

label <span class="token operator">=</span> torch<span class="token punctuation">.</span>LongTensor<span class="token punctuation">(</span>batch_size<span class="token punctuation">)</span><span class="token punctuation">.</span>random_<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">%</span> class_num  <span class="token comment" spellcheck="true"># 对随机数取余print(one_hot(label))</span>
<span class="token comment" spellcheck="true"># output:</span>
<span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">.</span> <span class="token number">0</span><span class="token punctuation">.</span> <span class="token number">0</span><span class="token punctuation">.</span> <span class="token number">1</span><span class="token punctuation">.</span> <span class="token number">0</span><span class="token punctuation">.</span> <span class="token number">0</span><span class="token punctuation">.</span> <span class="token number">0</span><span class="token punctuation">.</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">]</span> 
 <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">.</span> <span class="token number">0</span><span class="token punctuation">.</span> <span class="token number">0</span><span class="token punctuation">.</span> <span class="token number">0</span><span class="token punctuation">.</span> <span class="token number">1</span><span class="token punctuation">.</span> <span class="token number">0</span><span class="token punctuation">.</span> <span class="token number">0</span><span class="token punctuation">.</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">]</span> 
 <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">.</span> <span class="token number">0</span><span class="token punctuation">.</span> <span class="token number">1</span><span class="token punctuation">.</span> <span class="token number">0</span><span class="token punctuation">.</span> <span class="token number">0</span><span class="token punctuation">.</span> <span class="token number">0</span><span class="token punctuation">.</span> <span class="token number">0</span><span class="token punctuation">.</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">]</span> 
 <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">.</span> <span class="token number">1</span><span class="token punctuation">.</span> <span class="token number">0</span><span class="token punctuation">.</span> <span class="token number">0</span><span class="token punctuation">.</span> <span class="token number">0</span><span class="token punctuation">.</span> <span class="token number">0</span><span class="token punctuation">.</span> <span class="token number">0</span><span class="token punctuation">.</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h2 id="防止验证模型时爆显存"><a href="#防止验证模型时爆显存" class="headerlink" title="防止验证模型时爆显存"></a><strong>防止验证模型时爆显存</strong></h2><p>验证模型时不需要求导，即不需要梯度计算，关闭autograd，可以提高速度，节约内存。如果不关闭可能会爆显存。</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    
    <span class="token comment" spellcheck="true"># 使用model进行预测的代码    </span>
    <span class="token keyword">pass</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>
<blockquote>
<p>Pytorch 训练时无用的临时变量可能会越来越多，导致 <code>out of memory</code> ，可以使用下面语句来清理这些不需要的变量。</p>
</blockquote>
<p>官网 上的解释为：</p>
<blockquote>
<p>Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible innvidia-smi. <code>torch.cuda.empty_cache()</code></p>
</blockquote>
<p>意思就是PyTorch的缓存分配器会事先分配一些固定的显存，即使实际上tensors并没有使用完这些显存，这些显存也不能被其他应用使用。这个分配过程由第一次CUDA内存访问触发的。</p>
<p>而 <code>torch.cuda.empty_cache()</code> 的作用就是释放缓存分配器当前持有的且未占用的缓存显存，以便这些显存可以被其他GPU应用程序中使用，并且通过 <code>nvidia-smi</code>命令可见。注意使用此命令不会释放tensors占用的显存。</p>
<p>对于不用的数据变量，Pytorch 可以自动进行回收从而释放相应的显存。</p>
<p>更详细的优化可以查看 优化显存使用 和 显存利用问题。</p>
<h2 id="学习率衰减-1"><a href="#学习率衰减-1" class="headerlink" title="学习率衰减"></a><strong>学习率衰减</strong></h2><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token punctuation">.</span>optim <span class="token keyword">as</span> optim
<span class="token keyword">from</span> torch<span class="token punctuation">.</span>optim <span class="token keyword">import</span> lr_scheduler

<span class="token comment" spellcheck="true"># 训练前的初始化</span>
optimizer <span class="token operator">=</span> optim<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span>net<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">0.001</span><span class="token punctuation">)</span>
scheduler <span class="token operator">=</span> lr_scheduler<span class="token punctuation">.</span>StepLR<span class="token punctuation">(</span>optimizer<span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">0.1</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 每过10个epoch，学习率乘以0.1</span>
<span class="token comment" spellcheck="true"># 训练过程中</span>
<span class="token keyword">for</span> n <span class="token keyword">in</span> n_epoch<span class="token punctuation">:</span>    
    scheduler<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>    
    <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>可以随时查看学习率的值：<code>optimizer.param_groups[0]['lr']</code>。</p>
<p>还有其他学习率更新的方式：</p>
<p>1、自定义更新公式：</p>
<pre class="line-numbers language-python"><code class="language-python">scheduler <span class="token operator">=</span> lr_scheduler<span class="token punctuation">.</span>LambdaLR<span class="token punctuation">(</span>optimizer<span class="token punctuation">,</span> lr_lambda<span class="token operator">=</span><span class="token keyword">lambda</span> epoch<span class="token punctuation">:</span><span class="token number">1</span><span class="token operator">/</span><span class="token punctuation">(</span>epoch<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<p>2、不依赖epoch更新学习率：</p>
<p><code>lr_scheduler.ReduceLROnPlateau()</code>提供了基于训练中某些测量值使学习率动态下降的方法，它的参数说明到处都可以查到。<br>提醒一点就是参数 mode=’min’ 还是’max’，取决于优化的的损失还是准确率，即使用 <code>scheduler.step(loss)</code>还是<code>scheduler.step(acc)</code> 。</p>
<h2 id="冻结某些层的参数"><a href="#冻结某些层的参数" class="headerlink" title="冻结某些层的参数"></a><strong>冻结某些层的参数</strong></h2><p>参考：<a href="https://www.zhihu.com/question/311095447/answer/589307812" target="_blank" rel="noopener">https://www.zhihu.com/question/311095447/answer/589307812</a></p>
<p>在加载预训练模型的时候，我们有时想冻结前面几层，使其参数在训练过程中不发生变化。</p>
<p>我们需要先知道每一层的名字，通过如下代码打印：</p>
<pre class="line-numbers language-python"><code class="language-python">net <span class="token operator">=</span> Network<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 获取自定义网络结构</span>
<span class="token keyword">for</span> name<span class="token punctuation">,</span> value <span class="token keyword">in</span> net<span class="token punctuation">.</span>named_parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'name: {0},\t grad: {1}'</span><span class="token punctuation">.</span>format<span class="token punctuation">(</span>name<span class="token punctuation">,</span> value<span class="token punctuation">.</span>requires_grad<span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>
<p>假设前几层信息如下：</p>
<pre><code>name: cnn.VGG_16.convolution1_1.weight, grad: True
name: cnn.VGG_16.convolution1_1.bias, grad: True
name: cnn.VGG_16.convolution1_2.weight, grad: True
name: cnn.VGG_16.convolution1_2.bias, grad: True
name: cnn.VGG_16.convolution2_1.weight, grad: True
name: cnn.VGG_16.convolution2_1.bias, grad: True
name: cnn.VGG_16.convolution2_2.weight, grad: True
name: cnn.VGG_16.convolution2_2.bias, grad: True</code></pre><p>后面的True表示该层的参数可训练，然后我们定义一个要冻结的层的列表：</p>
<pre><code>no_grad = [    
    'cnn.VGG_16.convolution1_1.weight',    
    'cnn.VGG_16.convolution1_1.bias',    
    'cnn.VGG_16.convolution1_2.weight',    
    'cnn.VGG_16.convolution1_2.bias']</code></pre><p>冻结方法如下：</p>
<pre class="line-numbers language-python"><code class="language-python">net <span class="token operator">=</span> Net<span class="token punctuation">.</span>CTPN<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 获取网络结构</span>
<span class="token keyword">for</span> name<span class="token punctuation">,</span> value <span class="token keyword">in</span> net<span class="token punctuation">.</span>named_parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    
    <span class="token keyword">if</span> name <span class="token keyword">in</span> no_grad<span class="token punctuation">:</span>        
        value<span class="token punctuation">.</span>requires_grad <span class="token operator">=</span> <span class="token boolean">False</span>    
    <span class="token keyword">else</span><span class="token punctuation">:</span>        
        value<span class="token punctuation">.</span>requires_grad <span class="token operator">=</span> <span class="token boolean">True</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>冻结后我们再打印每层的信息：</p>
<pre><code>name: cnn.VGG_16.convolution1_1.weight, grad: False
name: cnn.VGG_16.convolution1_1.bias, grad: False
name: cnn.VGG_16.convolution1_2.weight, grad: False
name: cnn.VGG_16.convolution1_2.bias, grad: False
name: cnn.VGG_16.convolution2_1.weight, grad: True
name: cnn.VGG_16.convolution2_1.bias, grad: True
name: cnn.VGG_16.convolution2_2.weight, grad: True
name: cnn.VGG_16.convolution2_2.bias, grad: True</code></pre><p>可以看到前两层的weight和bias的requires_grad都为False，表示它们不可训练。</p>
<p>最后在定义优化器时，只对requires_grad为True的层的参数进行更新。</p>
<pre class="line-numbers language-python"><code class="language-python">optimizer <span class="token operator">=</span> optim<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span>filter<span class="token punctuation">(</span><span class="token keyword">lambda</span> p<span class="token punctuation">:</span> p<span class="token punctuation">.</span>requires_grad<span class="token punctuation">,</span> net<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">0.01</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<h2 id="对不同层使用不同学习率"><a href="#对不同层使用不同学习率" class="headerlink" title="对不同层使用不同学习率"></a><strong>对不同层使用不同学习率</strong></h2><p>我们对模型的不同层使用不同的学习率。</p>
<p>还是使用这个模型作为例子：</p>
<pre><code>net = Network()  # 获取自定义网络结构
for name, value in net.named_parameters():    
    print('name: {}'.format(name))

# 输出：
# name: cnn.VGG_16.convolution1_1.weight
# name: cnn.VGG_16.convolution1_1.bias
# name: cnn.VGG_16.convolution1_2.weight
# name: cnn.VGG_16.convolution1_2.bias
# name: cnn.VGG_16.convolution2_1.weight
# name: cnn.VGG_16.convolution2_1.bias
# name: cnn.VGG_16.convolution2_2.weight
# name: cnn.VGG_16.convolution2_2.bias</code></pre><p>对 convolution1 和 convolution2 设置不同的学习率，首先将它们分开，即放到不同的列表里：</p>
<pre class="line-numbers language-python"><code class="language-python">conv1_params <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
conv2_params <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>

<span class="token keyword">for</span> name<span class="token punctuation">,</span> parms <span class="token keyword">in</span> net<span class="token punctuation">.</span>named_parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    
    <span class="token keyword">if</span> <span class="token string">"convolution1"</span> <span class="token keyword">in</span> name<span class="token punctuation">:</span>        
        conv1_params <span class="token operator">+=</span> <span class="token punctuation">[</span>parms<span class="token punctuation">]</span>    
    <span class="token keyword">else</span><span class="token punctuation">:</span>        
        conv2_params <span class="token operator">+=</span> <span class="token punctuation">[</span>parms<span class="token punctuation">]</span>

<span class="token comment" spellcheck="true"># 然后在优化器中进行如下操作：</span>
optimizer <span class="token operator">=</span> optim<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span>    
    <span class="token punctuation">[</span>        
        <span class="token punctuation">{</span><span class="token string">"params"</span><span class="token punctuation">:</span> conv1_params<span class="token punctuation">,</span> <span class="token string">'lr'</span><span class="token punctuation">:</span> <span class="token number">0.01</span><span class="token punctuation">}</span><span class="token punctuation">,</span>        
        <span class="token punctuation">{</span><span class="token string">"params"</span><span class="token punctuation">:</span> conv2_params<span class="token punctuation">,</span> <span class="token string">'lr'</span><span class="token punctuation">:</span> <span class="token number">0.001</span><span class="token punctuation">}</span><span class="token punctuation">,</span>    
    <span class="token punctuation">]</span><span class="token punctuation">,</span>    
    weight_decay<span class="token operator">=</span><span class="token number">1e</span><span class="token operator">-</span><span class="token number">3</span><span class="token punctuation">,</span>
<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>我们将模型划分为两部分，存放到一个列表里，每部分就对应上面的一个字典，在字典里设置不同的学习率。当这两部分有相同的其他参数时，就将该参数放到列表外面作为全局参数，如上面的<code>weight_decay</code>。</p>
<p>也可以在列表外设置一个全局学习率，当各部分字典里设置了局部学习率时，就使用该学习率，否则就使用列表外的全局学习率。</p>
<h2 id="模型相关操作"><a href="#模型相关操作" class="headerlink" title="模型相关操作"></a><strong>模型相关操作</strong></h2><h3 id="保存加载模型基本用法"><a href="#保存加载模型基本用法" class="headerlink" title="保存加载模型基本用法"></a>保存加载模型基本用法</h3><p><strong>1、保存加载整个模型</strong></p>
<p>保存整个网络模型（网络结构+权重参数）。</p>
<pre class="line-numbers language-python"><code class="language-python">torch<span class="token punctuation">.</span>save<span class="token punctuation">(</span>model<span class="token punctuation">,</span> <span class="token string">'net.pkl'</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<p>直接加载整个网络模型（可能比较耗时）。</p>
<pre class="line-numbers language-python"><code class="language-python">model <span class="token operator">=</span> torch<span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token string">'net.pkl'</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<p><strong>2、只保存加载模型参数</strong></p>
<p>只保存模型的权重参数（速度快，占内存少）。</p>
<pre class="line-numbers language-python"><code class="language-python">torch<span class="token punctuation">.</span>save<span class="token punctuation">(</span>model<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">'net_params.pkl'</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<p>因为我们只保存了模型的参数，所以需要先定义一个网络对象，然后再加载模型参数。</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 构建一个网络结构</span>
model <span class="token operator">=</span> ClassNet<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token comment" spellcheck="true"># 将模型参数加载到新模型中</span>
state_dict <span class="token operator">=</span> torch<span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token string">'net_params.pkl'</span><span class="token punctuation">)</span>
model<span class="token punctuation">.</span>load_state_dict<span class="token punctuation">(</span>state_dict<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>保存模型进行推理测试时，只需保存训练好的模型的权重参数，即推荐第二种方法。</p>
<p>主要用法就是上面这些，接下来讲一下PyTorch中保存加载模型内部的一些原理，以及我们可能会遇到的一些特殊的需求。</p>
<h3 id="保存加载自定义模型"><a href="#保存加载自定义模型" class="headerlink" title="保存加载自定义模型"></a><strong>保存加载自定义模型</strong></h3><p>上面保存加载的 <code>net.pkl</code> 其实一个字典，通常包含如下内容：</p>
<ol>
<li><strong>网络结构</strong>：输入尺寸、输出尺寸以及隐藏层信息，以便能够在加载时重建模型。</li>
<li><strong>模型的权重参数</strong>：包含各网络层训练后的可学习参数，可以在模型实例上调用 <code>state_dict()</code> 方法来获取，比如前面介绍只保存模型权重参数时用到的 <code>model.state_dict()</code>。</li>
<li><strong>优化器参数</strong>：有时保存模型的参数需要稍后接着训练，那么就必须保存优化器的状态和所其使用的超参数，也是在优化器实例上调用 <code>state_dict()</code> 方法来获取这些参数。</li>
<li>其他信息：有时我们需要保存一些其他的信息，比如 <code>epoch</code>，<code>batch_size</code> 等超参数。</li>
</ol>
<p>知道了这些，那么我们就可以自定义需要保存的内容，比如：</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># saving a checkpoint assuming the network class named ClassNet</span>
checkpoint <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token string">'model'</span><span class="token punctuation">:</span> ClassNet<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
              <span class="token string">'model_state_dict'</span><span class="token punctuation">:</span> model<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
              <span class="token string">'optimizer_state_dict'</span><span class="token punctuation">:</span> optimizer<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
              <span class="token string">'epoch'</span><span class="token punctuation">:</span> epoch<span class="token punctuation">}</span>

torch<span class="token punctuation">.</span>save<span class="token punctuation">(</span>checkpoint<span class="token punctuation">,</span> <span class="token string">'checkpoint.pkl'</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>上面的 checkpoint 是个字典，里面有4个键值对，分别表示网络模型的不同信息。</p>
<p>然后我们要加载上面保存的自定义的模型：</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">load_checkpoint</span><span class="token punctuation">(</span>filepath<span class="token punctuation">)</span><span class="token punctuation">:</span>
    checkpoint <span class="token operator">=</span> torch<span class="token punctuation">.</span>load<span class="token punctuation">(</span>filepath<span class="token punctuation">)</span>
    model <span class="token operator">=</span> checkpoint<span class="token punctuation">[</span><span class="token string">'model'</span><span class="token punctuation">]</span>  <span class="token comment" spellcheck="true"># 提取网络结构</span>
    model<span class="token punctuation">.</span>load_state_dict<span class="token punctuation">(</span>checkpoint<span class="token punctuation">[</span><span class="token string">'model_state_dict'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 加载网络权重参数</span>
    optimizer <span class="token operator">=</span> TheOptimizerClass<span class="token punctuation">(</span><span class="token punctuation">)</span>
    optimizer<span class="token punctuation">.</span>load_state_dict<span class="token punctuation">(</span>checkpoint<span class="token punctuation">[</span><span class="token string">'optimizer_state_dict'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 加载优化器参数</span>

    <span class="token keyword">for</span> parameter <span class="token keyword">in</span> model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        parameter<span class="token punctuation">.</span>requires_grad <span class="token operator">=</span> <span class="token boolean">False</span>
    model<span class="token punctuation">.</span>eval<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token keyword">return</span> model

model <span class="token operator">=</span> load_checkpoint<span class="token punctuation">(</span><span class="token string">'checkpoint.pkl'</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>如果加载模型只是为了进行推理测试，则将每一层的 <code>requires_grad</code> 置为 <code>False</code>，即固定这些权重参数；还需要调用 <code>model.eval()</code> 将模型置为测试模式，主要是将 <code>dropout</code> 和 <code>batch normalization</code> 层进行固定，否则模型的预测结果每次都会不同。</p>
<p>如果希望继续训练，则调用 <code>model.train()</code>，以确保网络模型处于训练模式。</p>
<p><code>state_dict()</code> 也是一个Python字典对象，<code>model.state_dict()</code> 将每一层的可学习参数映射为参数矩阵，其中只包含具有可学习参数的层(卷积层、全连接层等)。</p>
<p>比如下面这个例子：</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># Define model</span>
<span class="token keyword">class</span> <span class="token class-name">TheModelClass</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        super<span class="token punctuation">(</span>TheModelClass<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>conv1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>bn <span class="token operator">=</span> nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span><span class="token number">8</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>conv2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">16</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>pool <span class="token operator">=</span> nn<span class="token punctuation">.</span>MaxPool2d<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>fc1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">16</span> <span class="token operator">*</span> <span class="token number">5</span> <span class="token operator">*</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">120</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>fc2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">120</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>pool<span class="token punctuation">(</span>F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>conv1<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>bn<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>pool<span class="token punctuation">(</span>F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>conv2<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        x <span class="token operator">=</span> x<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">16</span> <span class="token operator">*</span> <span class="token number">5</span> <span class="token operator">*</span> <span class="token number">5</span><span class="token punctuation">)</span>
        x <span class="token operator">=</span> F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>fc1<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>
        x <span class="token operator">=</span> F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>fc2<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>fc3<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        <span class="token keyword">return</span> x

    <span class="token comment" spellcheck="true"># Initialize model</span>
    model <span class="token operator">=</span> TheModelClass<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token comment" spellcheck="true"># Initialize optimizer</span>
    optimizer <span class="token operator">=</span> optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span>model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">0.001</span><span class="token punctuation">,</span> momentum<span class="token operator">=</span><span class="token number">0.9</span><span class="token punctuation">)</span>

    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Model's state_dict:"</span><span class="token punctuation">)</span>
    <span class="token keyword">for</span> param_tensor <span class="token keyword">in</span> model<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span>param_tensor<span class="token punctuation">,</span> <span class="token string">"\t"</span><span class="token punctuation">,</span> model<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">[</span>param_tensor<span class="token punctuation">]</span><span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Optimizer's state_dict:"</span><span class="token punctuation">)</span>
    <span class="token keyword">for</span> var_name <span class="token keyword">in</span> optimizer<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span>var_name<span class="token punctuation">,</span> <span class="token string">"\t"</span><span class="token punctuation">,</span> optimizer<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">[</span>var_name<span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>输出为：</p>
<pre class="line-numbers language-python"><code class="language-python">Model's state_dict<span class="token punctuation">:</span>
conv1<span class="token punctuation">.</span>weight            torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
conv1<span class="token punctuation">.</span>bias              torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
bn<span class="token punctuation">.</span>weight               torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
bn<span class="token punctuation">.</span>bias                 torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
bn<span class="token punctuation">.</span>running_mean         torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
bn<span class="token punctuation">.</span>running_var          torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
bn<span class="token punctuation">.</span>num_batches_tracked  torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
conv2<span class="token punctuation">.</span>weight            torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">16</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
conv2<span class="token punctuation">.</span>bias              torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">16</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
fc1<span class="token punctuation">.</span>weight              torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">120</span><span class="token punctuation">,</span> <span class="token number">400</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
fc1<span class="token punctuation">.</span>bias                torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">120</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
fc2<span class="token punctuation">.</span>weight              torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">120</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
fc2<span class="token punctuation">.</span>bias                torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">10</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
Optimizer's state_dict<span class="token punctuation">:</span>
state            <span class="token punctuation">{</span><span class="token punctuation">}</span>
param_groups     <span class="token punctuation">[</span><span class="token punctuation">{</span><span class="token string">'lr'</span><span class="token punctuation">:</span> <span class="token number">0.001</span><span class="token punctuation">,</span> <span class="token string">'momentum'</span><span class="token punctuation">:</span> <span class="token number">0.9</span><span class="token punctuation">,</span> <span class="token string">'dampening'</span><span class="token punctuation">:</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token string">'weight_decay'</span><span class="token punctuation">:</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token string">'nesterov'</span><span class="token punctuation">:</span> <span class="token boolean">False</span><span class="token punctuation">,</span> <span class="token string">'params'</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token number">139805696932024</span><span class="token punctuation">,</span> <span class="token number">139805483616008</span><span class="token punctuation">,</span> <span class="token number">139805483616080</span><span class="token punctuation">,</span> <span class="token number">139805483616152</span><span class="token punctuation">,</span> <span class="token number">139805483616440</span><span class="token punctuation">,</span> <span class="token number">139805483616512</span><span class="token punctuation">,</span> <span class="token number">139805483616584</span><span class="token punctuation">,</span> <span class="token number">139805483616656</span><span class="token punctuation">,</span> <span class="token number">139805483616728</span><span class="token punctuation">,</span> <span class="token number">139805483616800</span><span class="token punctuation">]</span><span class="token punctuation">}</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>可以看到 <code>model.state_dict()</code> 保存了卷积层，BatchNorm层和最大池化层的信息；而 <code>optimizer.state_dict()</code> 则保存的优化器的状态和相关的超参数。</p>
<h3 id="跨设备保存加载模型"><a href="#跨设备保存加载模型" class="headerlink" title="跨设备保存加载模型"></a>跨设备保存加载模型</h3><p>1、<strong>在 CPU 上加载在 GPU 上训练并保存的模型（Save on GPU, Load on CPU）：</strong></p>
<pre class="line-numbers language-python"><code class="language-python">device <span class="token operator">=</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">'cpu'</span><span class="token punctuation">)</span>
model <span class="token operator">=</span> TheModelClass<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token comment" spellcheck="true"># Load all tensors onto the CPU device</span>
model<span class="token punctuation">.</span>load_state_dict<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token string">'net_params.pkl'</span><span class="token punctuation">,</span> map_location<span class="token operator">=</span>device<span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>
<p><code>map_location</code>：a function, torch.device, string or a dict specifying how to remap storage locations</p>
<p>令 <code>torch.load()</code> 函数的 <code>map_location</code> 参数等于 <code>torch.device('cpu')</code> 即可。 这里令 <code>map_location</code> 参数等于 <code>'cpu'</code> 也同样可以。</p>
<p><strong>2、在 GPU 上加载在 GPU 上训练并保存的模型（Save on GPU, Load on GPU）：</strong></p>
<pre class="line-numbers language-python"><code class="language-python">device <span class="token operator">=</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">"cuda"</span><span class="token punctuation">)</span>
model <span class="token operator">=</span> TheModelClass<span class="token punctuation">(</span><span class="token punctuation">)</span>
model<span class="token punctuation">.</span>load_state_dict<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token string">'net_params.pkl'</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
model<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>
<p>在这里使用 <code>map_location</code> 参数不起作用，要使用 <code>model.to(torch.device("cuda"))</code> 将模型转换为CUDA优化的模型。</p>
<p>还需要对将要输入模型的数据调用 <code>data = data.to(device)</code>，即将数据从CPU转移到GPU。请注意，调用 <code>my_tensor.to(device)</code> 会返回一个 <code>my_tensor</code> 在 GPU 上的副本，它不会覆盖 <code>my_tensor</code>。因此需要手动覆盖张量：<code>my_tensor = my_tensor.to(device)</code>。</p>
<p><strong>3、在 GPU 上加载在 GPU 上训练并保存的模型（Save on CPU, Load on GPU）</strong></p>
<pre class="line-numbers language-python"><code class="language-python">device <span class="token operator">=</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">"cuda"</span><span class="token punctuation">)</span>
model <span class="token operator">=</span> TheModelClass<span class="token punctuation">(</span><span class="token punctuation">)</span>
model<span class="token punctuation">.</span>load_state_dict<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token string">'net_params.pkl'</span><span class="token punctuation">,</span> map_location<span class="token operator">=</span><span class="token string">"cuda:0"</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
model<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>
<p>当加载包含GPU tensors的模型时，这些tensors 会被默认加载到GPU上，不过是同一个GPU设备。</p>
<p>当有多个GPU设备时，可以通过将 <code>map_location</code> 设定为 <code>*cuda:device_id*</code> 来指定使用哪一个GPU设备，上面例子是指定编号为0的GPU设备。</p>
<p>其实也可以将 <code>torch.device("cuda")</code> 改为 <code>torch.device("cuda:0")</code> 来指定编号为0的GPU设备。</p>
<p>最后调用 <code>model.to(torch.device('cuda'))</code> 来将模型的tensors转换为 CUDA tensors。</p>
<p>下面是PyTorch官方文档上的用法，可以进行参考：</p>
<pre class="line-numbers language-python3"><code class="language-python3">>>> torch.load('tensors.pt')
# Load all tensors onto the CPU
>>> torch.load('tensors.pt', map_location=torch.device('cpu'))
# Load all tensors onto the CPU, using a function
>>> torch.load('tensors.pt', map_location=lambda storage, loc: storage)
# Load all tensors onto GPU 1
>>> torch.load('tensors.pt', map_location=lambda storage, loc: storage.cuda(1))
# Map tensors from GPU 1 to GPU 0
>>> torch.load('tensors.pt', map_location={'cuda:1':'cuda:0'})<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h3 id="CUDA-的用法"><a href="#CUDA-的用法" class="headerlink" title="CUDA 的用法"></a>CUDA 的用法</h3><p>在PyTorch中和GPU相关的几个函数：</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> torch

<span class="token comment" spellcheck="true"># 判断cuda是否可用；</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>is_available<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

<span class="token comment" spellcheck="true"># 获取gpu数量；</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>device_count<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

<span class="token comment" spellcheck="true"># 获取gpu名字；</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>get_device_name<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

<span class="token comment" spellcheck="true"># 返回当前gpu设备索引，默认从0开始；</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>current_device<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

<span class="token comment" spellcheck="true"># 查看tensor或者model在哪块GPU上</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span>get_device<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>我的电脑输出为：</p>
<pre class="line-numbers language-text"><code class="language-text">True
1
GeForce RTX 2080 Ti
0<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>
<p>有时我们需要把数据和模型从cpu移到gpu中，有以下两种方法：</p>
<pre class="line-numbers language-python"><code class="language-python">use_cuda <span class="token operator">=</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>is_available<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token comment" spellcheck="true"># 方法一：</span>
<span class="token keyword">if</span> use_cuda<span class="token punctuation">:</span>
    data <span class="token operator">=</span> data<span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span>
    model<span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token comment" spellcheck="true"># 方法二：</span>
device <span class="token operator">=</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">"cuda"</span> <span class="token keyword">if</span> use_cuda <span class="token keyword">else</span> <span class="token string">"cpu"</span><span class="token punctuation">)</span>
data <span class="token operator">=</span> data<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>
model<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>个人比较习惯第二种方法，可以少一个 if 语句。而且该方法还可以通过设备号指定使用哪个GPU设备，比如使用0号设备：</p>
<pre><code>device = torch.device("cuda:0" if use_cuda else "cpu")</code></pre><h2 id="Pytorch内置one-hot函数"><a href="#Pytorch内置one-hot函数" class="headerlink" title="Pytorch内置one_hot函数"></a><strong>Pytorch内置one_hot函数</strong></h2><p>感谢@yangyangyang 补充：Pytorch 1.1后，one_hot可以直接用<code>torch.nn.functional.one_hot</code>。</p>
<p>然后我将Pytorch升级到1.2版本，试用了下 one_hot 函数，确实很方便。</p>
<p>具体用法如下：</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional <span class="token keyword">as</span> F
<span class="token keyword">import</span> torch
tensor <span class="token operator">=</span>  torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span> <span class="token operator">%</span> <span class="token number">3</span>  <span class="token comment" spellcheck="true"># tensor([0, 1, 2, 0, 1])</span>
one_hot <span class="token operator">=</span> F<span class="token punctuation">.</span>one_hot<span class="token punctuation">(</span>tensor<span class="token punctuation">)</span>

<span class="token comment" spellcheck="true"># 输出：</span>
<span class="token comment" spellcheck="true"># tensor([[1, 0, 0],</span>
<span class="token comment" spellcheck="true">#         [0, 1, 0],</span>
<span class="token comment" spellcheck="true">#         [0, 0, 1],</span>
<span class="token comment" spellcheck="true">#         [1, 0, 0],</span>
<span class="token comment" spellcheck="true">#         [0, 1, 0]])</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p><code>F.one_hot</code>会自己检测不同类别个数，生成对应独热编码。我们也可以自己指定类别数：</p>
<pre class="line-numbers language-python"><code class="language-python">tensor <span class="token operator">=</span>  torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span> <span class="token operator">%</span> <span class="token number">3</span>  <span class="token comment" spellcheck="true"># tensor([0, 1, 2, 0, 1])</span>
one_hot <span class="token operator">=</span> F<span class="token punctuation">.</span>one_hot<span class="token punctuation">(</span>tensor<span class="token punctuation">,</span> num_classes<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">)</span>

<span class="token comment" spellcheck="true"># 输出：</span>
<span class="token comment" spellcheck="true"># tensor([[1, 0, 0, 0, 0],</span>
<span class="token comment" spellcheck="true">#         [0, 1, 0, 0, 0],</span>
<span class="token comment" spellcheck="true">#         [0, 0, 1, 0, 0],</span>
<span class="token comment" spellcheck="true">#         [1, 0, 0, 0, 0],</span>
<span class="token comment" spellcheck="true">#         [0, 1, 0, 0, 0]])</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>升级 Pytorch (cpu版本)的命令：<code>conda install pytorch torchvision \-c pytorch</code></p>
<p>（希望Pytorch升级不会影响项目代码）</p>
<h2 id="网络参数初始化"><a href="#网络参数初始化" class="headerlink" title="网络参数初始化"></a><strong>网络参数初始化</strong></h2><p>神经网络的初始化是训练流程的重要基础环节，会对模型的性能、收敛性、收敛速度等产生重要的影响。</p>
<p>以下介绍两种常用的初始化操作。</p>
<p>(1) 使用pytorch内置的torch.nn.init方法。</p>
<p>常用的初始化操作，例如正态分布、均匀分布、xavier初始化、kaiming初始化等都已经实现，可以直接使用。具体详见PyTorch 中 torch.nn.init 中文文档。</p>
<pre class="line-numbers language-python"><code class="language-python">init<span class="token punctuation">.</span>xavier_uniform<span class="token punctuation">(</span>net1<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>weight<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<p>(2) 对于一些更加灵活的初始化方法，可以借助numpy。</p>
<p>对于自定义的初始化方法，有时tensor的功能不如numpy强大灵活，故可以借助numpy实现初始化方法，再转换到tensor上使用。</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">for</span> layer <span class="token keyword">in</span> net1<span class="token punctuation">.</span>modules<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    
    <span class="token keyword">if</span> isinstance<span class="token punctuation">(</span>layer<span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">)</span><span class="token punctuation">:</span> <span class="token comment" spellcheck="true"># 判断是否是线性层        </span>
        param_shape <span class="token operator">=</span> layer<span class="token punctuation">.</span>weight<span class="token punctuation">.</span>shape        
        layer<span class="token punctuation">.</span>weight<span class="token punctuation">.</span>data <span class="token operator">=</span> torch<span class="token punctuation">.</span>from_numpy<span class="token punctuation">(</span>np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>normal<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0.5</span><span class="token punctuation">,</span> size<span class="token operator">=</span>param_shape<span class="token punctuation">)</span><span class="token punctuation">)</span>         
        <span class="token comment" spellcheck="true"># 定义为均值为 0，方差为 0.5 的正态分布</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h2 id="加载内置预训练模型"><a href="#加载内置预训练模型" class="headerlink" title="加载内置预训练模型"></a><strong>加载内置预训练模型</strong></h2><p><code>torchvision.models</code>模块的子模块中包含以下模型：</p>
<ul>
<li>AlexNet</li>
<li>VGG</li>
<li>ResNet</li>
<li>SqueezeNet</li>
<li>DenseNet</li>
</ul>
<p>导入这些模型的方法为：</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> torchvision<span class="token punctuation">.</span>models <span class="token keyword">as</span> models
resnet18 <span class="token operator">=</span> models<span class="token punctuation">.</span>resnet18<span class="token punctuation">(</span><span class="token punctuation">)</span>
alexnet <span class="token operator">=</span> models<span class="token punctuation">.</span>alexnet<span class="token punctuation">(</span><span class="token punctuation">)</span>
vgg16 <span class="token operator">=</span> models<span class="token punctuation">.</span>vgg16<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>
<p>有一个很重要的参数为<code>pretrained</code>，默认为<code>False</code>，表示只导入模型的结构，其中的权重是随机初始化的。</p>
<p>如果<code>pretrained</code> 为 <code>True</code>，表示导入的是在<code>ImageNet</code>数据集上预训练的模型。</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> torchvision<span class="token punctuation">.</span>models <span class="token keyword">as</span> models
resnet18 <span class="token operator">=</span> models<span class="token punctuation">.</span>resnet18<span class="token punctuation">(</span>pretrained<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
alexnet <span class="token operator">=</span> models<span class="token punctuation">.</span>alexnet<span class="token punctuation">(</span>pretrained<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
vgg16 <span class="token operator">=</span> models<span class="token punctuation">.</span>vgg16<span class="token punctuation">(</span>pretrained<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>
<h2 id="Pytorch编写代码基本步骤思想"><a href="#Pytorch编写代码基本步骤思想" class="headerlink" title="Pytorch编写代码基本步骤思想"></a>Pytorch编写代码基本步骤思想</h2><p>分为四大步骤：</p>
<p><strong>1、输入处理模块</strong> (X 输入数据，变成网络能够处理的Tensor类型)</p>
<p><strong>2、模型构建模块</strong> (主要负责从输入的数据，得到预测的y^, 这就是我们经常说的前向过程)</p>
<p><strong>3、定义代价函数和优化器模块</strong> (注意，前向过程只会得到模型预测的结果，并不会自动求导和更新，是由这个模块进行处理)</p>
<p><strong>4、构建训练过程</strong> （迭代训练过程，就是上图表情包的训练迭代过程）</p>
<p>这几个模块分别与上图的数字标号1，2，3，4进行一一对应！</p>
<p>知道了上面的宏观思想之后，后面给出每个模块稍微具体一点的解释和具体一个例子，再帮助大家熟悉对应的代码！</p>
<p><strong>1.数据处理</strong></p>
<p>对于数据处理，最为简单的⽅式就是将数据组织成为⼀个 。但许多训练需要⽤到mini-batch，直 接组织成Tensor不便于我们操作。pytorch为我们提供了Dataset和Dataloader两个类来方便的构建。</p>
<p>torch.utils.data.Dataset</p>
<p>继承Dataset 类需要override 以下⽅法：</p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20211007140654126.png" alt=""></p>
<p><strong>torch.utils.data.DataLoader</strong></p>
<pre><code>torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=False)</code></pre><p>DataLoader Batch。如果选择shuffle = True，每⼀个epoch 后，mini-Batch batch_size 常⻅的使⽤⽅法如下：</p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20211007140714730.png" alt=""></p>
<p><strong>2. 模型构建</strong></p>
<p>所有的模型都需要继承torch.nn.Module ， 需要实现以下⽅法：</p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20211007140738054.png" alt=""></p>
<p>其中forward() ⽅法是前向传播的过程。在实现模型时，我们不需要考虑反向传播。</p>
<p><strong>3. 定义代价函数和优化器</strong></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20211007140756578.png" alt=""></p>
<p><strong>4、构建训练过程</strong></p>
<p>pytorch的训练循环⼤致如下：</p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20211007140812602.png" alt=""></p>
<p><strong>5、总结</strong></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20211007140900845.png" alt=""></p>
<h1 id="13个PyTorch特性"><a href="#13个PyTorch特性" class="headerlink" title="13个PyTorch特性"></a>13个PyTorch特性</h1><h2 id="1-DatasetFolder"><a href="#1-DatasetFolder" class="headerlink" title="1 DatasetFolder"></a><strong>1 DatasetFolder</strong></h2><p>当学习PyTorch时，人们首先要做的事情之一是实现自己的某种Dataset 。这是一个低级错误，没有必要浪费时间写这样的东西。通常，数据集要么是数据列表(或者是numpy数组)，要么磁盘上的文件。所以，把数据在磁盘上组织好，要比写一个自定义的Dataset来加载某种奇怪的格式更好。</p>
<p>分类器最常见的数据格式之一，是有一个带有子文件夹的目录，子文件夹表示类，子文件夹中的文件表示样本，如下所示。</p>
<pre><code>folder/class_0/file1.txt
folder/class_0/file2.txt
folder/class_0/...

folder/class_1/file3.txt
folder/class_1/file4.txt

folder/class_2/file5.txt
folder/class_2/...</code></pre><p>有一个内置的方式来加载这类数据集，不管你的数据是图像，文本文件或其他什么，只要使用’DatasetFolder就可以了。令人惊讶的是，这个类是torchvision包的一部分，而不是核心PyTorch。这个类非常全面，你可以从文件夹中过滤文件，使用自定义代码加载它们，并动态转换原始文件。例子:</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">from</span> torchvision<span class="token punctuation">.</span>datasets <span class="token keyword">import</span> DatasetFolder
<span class="token keyword">from</span> pathlib <span class="token keyword">import</span> Path
<span class="token comment" spellcheck="true"># I have text files in this folder</span>
ds <span class="token operator">=</span> DatasetFolder<span class="token punctuation">(</span><span class="token string">"/Users/marcin/Dev/tmp/my_text_dataset"</span><span class="token punctuation">,</span> 
    loader<span class="token operator">=</span><span class="token keyword">lambda</span> path<span class="token punctuation">:</span> Path<span class="token punctuation">(</span>path<span class="token punctuation">)</span><span class="token punctuation">.</span>read_text<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    extensions<span class="token operator">=</span><span class="token punctuation">(</span><span class="token string">".txt"</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token comment" spellcheck="true">#only load .txt files</span>
    transform<span class="token operator">=</span><span class="token keyword">lambda</span> text<span class="token punctuation">:</span> text<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">100</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token comment" spellcheck="true"># only take first 100 characters</span>
<span class="token punctuation">)</span>

<span class="token comment" spellcheck="true"># Everything you need is already there</span>
len<span class="token punctuation">(</span>ds<span class="token punctuation">)</span><span class="token punctuation">,</span> ds<span class="token punctuation">.</span>classes<span class="token punctuation">,</span> ds<span class="token punctuation">.</span>class_to_idx
<span class="token punctuation">(</span><span class="token number">20</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token string">'novels'</span><span class="token punctuation">,</span> <span class="token string">'thrillers'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">{</span><span class="token string">'novels'</span><span class="token punctuation">:</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token string">'thrillers'</span><span class="token punctuation">:</span> <span class="token number">1</span><span class="token punctuation">}</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>如果你在处理图像，还有一个torchvision.datasets.ImageFolder类，它基于DatasetLoader，它被预先配置为加载图像。</p>
<h2 id="2-尽量少用-to-device-，用zeros-like-ones-like之类的代替"><a href="#2-尽量少用-to-device-，用zeros-like-ones-like之类的代替" class="headerlink" title="2 尽量少用.to(device)，用zeros_like/ones_like之类的代替"></a><strong>2 尽量少用.to(device)，用zeros_like/ones_like之类的代替</strong></h2><p>我读过很多来自GitHub仓库的PyTorch代码。最让我恼火的是，几乎在每个repo中都有许多*.to(device)行，它们将数据从CPU或GPU转移到其他地方。这样的语句通常会出现在大量的repos或初学者教程中。我强烈建议尽可能少地实现这类操作，并依赖内置的PyTorch功能自动实现这类操作。到处使用.to(device)通常会导致性能下降，还会出现异常：</p>
<p>Expected object of device type cuda but got device type cpu</p>
<p>显然，有些情况下你无法回避它，但大多数情况(如果不是全部)都在这里。其中一种情况是初始化一个全0或全1的张量，这在深度神经网络计算损失的的时候是经常发生的，模型的输出已经在cuda上了，你需要另外的tensor也是在cuda上，这时，你可以使用*_like操作符：</p>
<pre class="line-numbers language-python"><code class="language-python">my_output <span class="token comment" spellcheck="true"># on any device, if it's cuda then my_zeros will also be on cudamy_zeros = torch.zeros_like(my_output_from_model)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<p>在内部，PyTorch所做的是调用以下操作：</p>
<pre class="line-numbers language-python"><code class="language-python">my_zeros <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>my_output<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>my_output<span class="token punctuation">.</span>dtype<span class="token punctuation">,</span> layout<span class="token operator">=</span>my_output<span class="token punctuation">.</span>layout<span class="token punctuation">,</span> device<span class="token operator">=</span>my_output<span class="token punctuation">.</span>device<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<p>所以所有的设置都是正确的，这样就减少了代码中出现错误的概率。类似的操作包括：</p>
<pre class="line-numbers language-python"><code class="language-python">torch<span class="token punctuation">.</span>zeros_like<span class="token punctuation">(</span><span class="token punctuation">)</span>
torch<span class="token punctuation">.</span>ones_like<span class="token punctuation">(</span><span class="token punctuation">)</span>
torch<span class="token punctuation">.</span>rand_like<span class="token punctuation">(</span><span class="token punctuation">)</span>
torch<span class="token punctuation">.</span>randn_like<span class="token punctuation">(</span><span class="token punctuation">)</span>
torch<span class="token punctuation">.</span>randint_like<span class="token punctuation">(</span><span class="token punctuation">)</span>
torch<span class="token punctuation">.</span>empty_like<span class="token punctuation">(</span><span class="token punctuation">)</span>
torch<span class="token punctuation">.</span>full_like<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h2 id="3-Register-Buffer-nn-Module-register-buffer"><a href="#3-Register-Buffer-nn-Module-register-buffer" class="headerlink" title="3 Register Buffer ( nn.Module.register_buffer)"></a><strong>3 Register Buffer ( nn.Module.register_buffer)</strong></h2><p>这将是我劝人们不要到处使用 .to(device) 的下一步。有时，你的模型或损失函数需要有预先设置的参数，并在调用forward时使用，例如，它可以是一个“权重”参数，它可以缩放损失或一些固定张量，它不会改变，但每次都使用。对于这种情况，请使用nn.Module.register_buffer 方法，它告诉PyTorch将传递给它的值存储在模块中，并将这些值随模块一起移动。如果你初始化你的模块，然后将它移动到GPU，这些值也会自动移动。此外，如果你保存模块的状态，buffers也会被保存！</p>
<p>一旦注册，这些值就可以在forward函数中访问，就像其他模块的属性一样</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">from</span> torch <span class="token keyword">import</span> nn
<span class="token keyword">import</span> torch

<span class="token keyword">class</span> <span class="token class-name">ModuleWithCustomValues</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> weights<span class="token punctuation">,</span> alpha<span class="token punctuation">)</span><span class="token punctuation">:</span>
        super<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>register_buffer<span class="token punctuation">(</span><span class="token string">"weights"</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>weights<span class="token punctuation">)</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>register_buffer<span class="token punctuation">(</span><span class="token string">"alpha"</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>alpha<span class="token punctuation">)</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> x <span class="token operator">*</span> self<span class="token punctuation">.</span>weights <span class="token operator">+</span> self<span class="token punctuation">.</span>alpha

m <span class="token operator">=</span> ModuleWithCustomValues<span class="token punctuation">(</span>
    weights<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">1.0</span><span class="token punctuation">,</span> <span class="token number">2.0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> alpha<span class="token operator">=</span><span class="token number">1e</span><span class="token operator">-</span><span class="token number">4</span>
<span class="token punctuation">)</span>
m<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1.23</span><span class="token punctuation">,</span> <span class="token number">4.56</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1.2301</span><span class="token punctuation">,</span> <span class="token number">9.1201</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h2 id="4-Built-in-Identity"><a href="#4-Built-in-Identity" class="headerlink" title="4 Built-in Identity()"></a><strong>4 Built-in Identity()</strong></h2><p>有时候，当你使用迁移学习时，你需要用1:1的映射替换一些层，可以用nn.Module来实现这个目的，只返回输入值。PyTorch内置了这个类。</p>
<p>例子，你想要在分类层之前从一个预训练过的ResNet50获取图像表示。以下是如何做到这一点：</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">from</span> torchvision<span class="token punctuation">.</span>models <span class="token keyword">import</span> resnet50
model <span class="token operator">=</span> resnet50<span class="token punctuation">(</span>pretrained<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
model<span class="token punctuation">.</span>fc <span class="token operator">=</span> nn<span class="token punctuation">.</span>Identity<span class="token punctuation">(</span><span class="token punctuation">)</span>
last_layer_output <span class="token operator">=</span> model<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">224</span><span class="token punctuation">,</span> <span class="token number">224</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
last_layer_output<span class="token punctuation">.</span>shape
torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2048</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h2 id="5-Pairwise-distances-torch-cdist"><a href="#5-Pairwise-distances-torch-cdist" class="headerlink" title="5 Pairwise distances: torch.cdist"></a><strong>5 Pairwise distances: torch.cdist</strong></h2><p>下次当你遇到计算两个张量之间的欧几里得距离(或者一般来说:p范数)的问题时，请记住torch.cdist。它确实做到了这一点，并且在使用欧几里得距离时还自动使用矩阵乘法，从而提高了性能。</p>
<pre class="line-numbers language-python"><code class="language-python">points1 <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0.0</span><span class="token punctuation">,</span> <span class="token number">0.0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">1.0</span><span class="token punctuation">,</span> <span class="token number">1.0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">2.0</span><span class="token punctuation">,</span> <span class="token number">2.0</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
points2 <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0.0</span><span class="token punctuation">,</span> <span class="token number">0.0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1.0</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1.0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">2.0</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">2.0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">3.0</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">3.0</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># batches don't have to be equal</span>
torch<span class="token punctuation">.</span>cdist<span class="token punctuation">(</span>points1<span class="token punctuation">,</span> points2<span class="token punctuation">,</span> p<span class="token operator">=</span><span class="token number">2.0</span><span class="token punctuation">)</span>
tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0.0000</span><span class="token punctuation">,</span> <span class="token number">1.4142</span><span class="token punctuation">,</span> <span class="token number">2.8284</span><span class="token punctuation">,</span> <span class="token number">4.2426</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">1.4142</span><span class="token punctuation">,</span> <span class="token number">2.8284</span><span class="token punctuation">,</span> <span class="token number">4.2426</span><span class="token punctuation">,</span> <span class="token number">5.6569</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">2.8284</span><span class="token punctuation">,</span> <span class="token number">4.2426</span><span class="token punctuation">,</span> <span class="token number">5.6569</span><span class="token punctuation">,</span> <span class="token number">7.0711</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>没有矩阵乘法或有矩阵乘法的性能，在我的机器上使用mm时，速度快了2倍以上。</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token operator">%</span><span class="token operator">%</span>timeit
points1 <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">512</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
points2 <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">512</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
torch<span class="token punctuation">.</span>cdist<span class="token punctuation">(</span>points1<span class="token punctuation">,</span> points2<span class="token punctuation">,</span> p<span class="token operator">=</span><span class="token number">2.0</span><span class="token punctuation">,</span> compute_mode<span class="token operator">=</span><span class="token string">"donot_use_mm_for_euclid_dist"</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>
<p>867µs±142µs per loop (mean±std. dev. of 7 run, 1000 loop each)</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token operator">%</span><span class="token operator">%</span>timeit
points1 <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">512</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
points2 <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">512</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
torch<span class="token punctuation">.</span>cdist<span class="token punctuation">(</span>points1<span class="token punctuation">,</span> points2<span class="token punctuation">,</span> p<span class="token operator">=</span><span class="token number">2.0</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>
<p>417µs±52.9µs per loop (mean±std. dev. of 7 run, 1000 loop each)</p>
<h2 id="6-Cosine-similarity-F-cosine-similarity"><a href="#6-Cosine-similarity-F-cosine-similarity" class="headerlink" title="6 Cosine similarity: F.cosine_similarity"></a><strong>6 Cosine similarity: F.cosine_similarity</strong></h2><p>与上一点相同，计算欧几里得距离并不总是你需要的东西。当处理向量时，通常余弦相似度是选择的度量。PyTorch也有一个内置的余弦相似度实现。</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional <span class="token keyword">as</span> F
vector1 <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0.0</span><span class="token punctuation">,</span> <span class="token number">1.0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
vector2 <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0.05</span><span class="token punctuation">,</span> <span class="token number">1.0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>F<span class="token punctuation">.</span>cosine_similarity<span class="token punctuation">(</span>vector1<span class="token punctuation">,</span> vector2<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
vector3 <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0.0</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1.0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>F<span class="token punctuation">.</span>cosine_similarity<span class="token punctuation">(</span>vector1<span class="token punctuation">,</span> vector3<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
tensor<span class="token punctuation">(</span><span class="token number">0.9988</span><span class="token punctuation">)</span>
tensor<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p><strong>PyTorch中批量计算余弦距离</strong></p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional <span class="token keyword">as</span> F
batch_of_vectors <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
similarity_matrix <span class="token operator">=</span> F<span class="token punctuation">.</span>cosine_similarity<span class="token punctuation">(</span>batch_of_vectors<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> batch_of_vectors<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>
similarity_matrix
tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1.0000</span><span class="token punctuation">,</span> <span class="token number">0.6922</span><span class="token punctuation">,</span> <span class="token number">0.6480</span><span class="token punctuation">,</span> <span class="token number">0.6789</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">0.6922</span><span class="token punctuation">,</span> <span class="token number">1.0000</span><span class="token punctuation">,</span> <span class="token number">0.7143</span><span class="token punctuation">,</span> <span class="token number">0.7172</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">0.6480</span><span class="token punctuation">,</span> <span class="token number">0.7143</span><span class="token punctuation">,</span> <span class="token number">1.0000</span><span class="token punctuation">,</span> <span class="token number">0.7312</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">0.6789</span><span class="token punctuation">,</span> <span class="token number">0.7172</span><span class="token punctuation">,</span> <span class="token number">0.7312</span><span class="token punctuation">,</span> <span class="token number">1.0000</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h2 id="7-归一化向量-F-normalize"><a href="#7-归一化向量-F-normalize" class="headerlink" title="7 归一化向量: F.normalize"></a><strong>7 归一化向量: F.normalize</strong></h2><p>最后一点仍然与向量和距离有松散的联系，那就是归一化：通常是通过改变向量的大小来提高计算的稳定性。最常用的归一化是L2，可以在PyTorch中按如下方式应用:</p>
<pre class="line-numbers language-python"><code class="language-python">vector <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">99.0</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">512.0</span><span class="token punctuation">,</span> <span class="token number">123.0</span><span class="token punctuation">,</span> <span class="token number">0.1</span><span class="token punctuation">,</span> <span class="token number">6.66</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
normalized_vector <span class="token operator">=</span> F<span class="token punctuation">.</span>normalize<span class="token punctuation">(</span>vector<span class="token punctuation">,</span> p<span class="token operator">=</span><span class="token number">2.0</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>
normalized_vector
tensor<span class="token punctuation">(</span><span class="token punctuation">[</span> <span class="token number">1.8476e-01</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">9.5552e-01</span><span class="token punctuation">,</span>  <span class="token number">2.2955e-01</span><span class="token punctuation">,</span>  <span class="token number">1.8662e-04</span><span class="token punctuation">,</span>  <span class="token number">1.2429e-02</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>
<p>在PyTorch中执行归一化的旧方法是：</p>
<pre class="line-numbers language-python"><code class="language-python">vector <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">99.0</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">512.0</span><span class="token punctuation">,</span> <span class="token number">123.0</span><span class="token punctuation">,</span> <span class="token number">0.1</span><span class="token punctuation">,</span> <span class="token number">6.66</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
normalized_vector <span class="token operator">=</span> vector <span class="token operator">/</span> torch<span class="token punctuation">.</span>norm<span class="token punctuation">(</span>vector<span class="token punctuation">,</span> p<span class="token operator">=</span><span class="token number">2.0</span><span class="token punctuation">)</span>
normalized_vector
tensor<span class="token punctuation">(</span><span class="token punctuation">[</span> <span class="token number">1.8476e-01</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">9.5552e-01</span><span class="token punctuation">,</span>  <span class="token number">2.2955e-01</span><span class="token punctuation">,</span>  <span class="token number">1.8662e-04</span><span class="token punctuation">,</span>  <span class="token number">1.2429e-02</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>
<p>在PyTorch中批量进行L2归一化</p>
<pre class="line-numbers language-python"><code class="language-python">batch_of_vectors <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
normalized_batch_of_vectors <span class="token operator">=</span> F<span class="token punctuation">.</span>normalize<span class="token punctuation">(</span>batch_of_vectors<span class="token punctuation">,</span> p<span class="token operator">=</span><span class="token number">2.0</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
normalized_batch_of_vectors<span class="token punctuation">.</span>shape<span class="token punctuation">,</span> torch<span class="token punctuation">.</span>norm<span class="token punctuation">(</span>normalized_batch_of_vectors<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span> 
<span class="token comment" spellcheck="true"># all vectors will have length of 1.0(torch.Size([4, 64]), tensor([1.0000, 1.0000, 1.0000, 1.0000]))</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>
<h2 id="8-线性层-分块技巧-torch-chunk"><a href="#8-线性层-分块技巧-torch-chunk" class="headerlink" title="8 线性层 + 分块技巧 (torch.chunk)"></a><strong>8 线性层 + 分块技巧 (torch.chunk)</strong></h2><p>这是我最近发现的一个有创意的技巧。假设你想把你的输入映射到N个不同的线性投影中。你可以通过创建N个nn.Linear来做到这一点。或者你也可以创建一个单一的线性层，做一个向前传递，然后将输出分成N块。这种方法通常会带来更高的性能，所以这是一个值得记住的技巧。</p>
<pre class="line-numbers language-python"><code class="language-python">d <span class="token operator">=</span> <span class="token number">1024</span>
batch <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">8</span><span class="token punctuation">,</span> d<span class="token punctuation">)</span><span class="token punctuation">)</span>
layers <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>d<span class="token punctuation">,</span> <span class="token number">128</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>d<span class="token punctuation">,</span> <span class="token number">128</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>d<span class="token punctuation">,</span> <span class="token number">128</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
one_layer <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>d<span class="token punctuation">,</span> <span class="token number">128</span> <span class="token operator">*</span> <span class="token number">3</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
<span class="token operator">%</span><span class="token operator">%</span>timeit
o1 <span class="token operator">=</span> layers<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">(</span>batch<span class="token punctuation">)</span>
o2 <span class="token operator">=</span> layers<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">(</span>batch<span class="token punctuation">)</span>
o3 <span class="token operator">=</span> layers<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">(</span>batch<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>289 µs ± 30.8 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token operator">%</span><span class="token operator">%</span>timeit
o1<span class="token punctuation">,</span> o2<span class="token punctuation">,</span> o3 <span class="token operator">=</span> torch<span class="token punctuation">.</span>chunk<span class="token punctuation">(</span>one_layer<span class="token punctuation">(</span>batch<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>
<p>202 µs ± 8.09 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)</p>
<h2 id="9-Masked-select-torch-masked-select"><a href="#9-Masked-select-torch-masked-select" class="headerlink" title="9 Masked select (torch.masked_select)"></a><strong>9 Masked select (torch.masked_select)</strong></h2><p>有时你只需要对输入张量的一部分进行计算。给你一个例子：你想计算的损失只在满足某些条件的张量上。为了做到这一点，你可以使用torch.masked_select，注意，当需要梯度时也可以使用这个操作。</p>
<pre class="line-numbers language-python"><code class="language-python">data <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>requires_grad_<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>data<span class="token punctuation">)</span>
mask <span class="token operator">=</span> data <span class="token operator">></span> data<span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>mask<span class="token punctuation">)</span>
torch<span class="token punctuation">.</span>masked_select<span class="token punctuation">(</span>data<span class="token punctuation">,</span> mask<span class="token punctuation">)</span>
tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0.0582</span><span class="token punctuation">,</span> <span class="token number">0.7170</span><span class="token punctuation">,</span> <span class="token number">0.7713</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">0.9458</span><span class="token punctuation">,</span> <span class="token number">0.2597</span><span class="token punctuation">,</span> <span class="token number">0.6711</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">0.2828</span><span class="token punctuation">,</span> <span class="token number">0.2232</span><span class="token punctuation">,</span> <span class="token number">0.1981</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token boolean">False</span><span class="token punctuation">,</span>  <span class="token boolean">True</span><span class="token punctuation">,</span>  <span class="token boolean">True</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span> <span class="token boolean">True</span><span class="token punctuation">,</span> <span class="token boolean">False</span><span class="token punctuation">,</span>  <span class="token boolean">True</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token boolean">False</span><span class="token punctuation">,</span> <span class="token boolean">False</span><span class="token punctuation">,</span> <span class="token boolean">False</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0.7170</span><span class="token punctuation">,</span> <span class="token number">0.7713</span><span class="token punctuation">,</span> <span class="token number">0.9458</span><span class="token punctuation">,</span> <span class="token number">0.6711</span><span class="token punctuation">]</span><span class="token punctuation">,</span> grad_fn<span class="token operator">=</span><span class="token operator">&lt;</span>MaskedSelectBackward<span class="token operator">></span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p><strong>直接在tensor上应用mask</strong></p>
<p>类似的行为可以通过使用mask作为输入张量的 “indexer”来实现。</p>
<pre class="line-numbers language-python"><code class="language-python">data<span class="token punctuation">[</span>mask<span class="token punctuation">]</span>
tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0.7170</span><span class="token punctuation">,</span> <span class="token number">0.7713</span><span class="token punctuation">,</span> <span class="token number">0.9458</span><span class="token punctuation">,</span> <span class="token number">0.6711</span><span class="token punctuation">]</span><span class="token punctuation">,</span> grad_fn<span class="token operator">=</span><span class="token operator">&lt;</span>IndexBackward<span class="token operator">></span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>
<p>有时，一个理想的解决方案是用0填充mask中所有的False值，可以这样做:</p>
<pre class="line-numbers language-python"><code class="language-python">data <span class="token operator">*</span> mask
tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0.0000</span><span class="token punctuation">,</span> <span class="token number">0.7170</span><span class="token punctuation">,</span> <span class="token number">0.7713</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">0.9458</span><span class="token punctuation">,</span> <span class="token number">0.0000</span><span class="token punctuation">,</span> <span class="token number">0.6711</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">0.0000</span><span class="token punctuation">,</span> <span class="token number">0.0000</span><span class="token punctuation">,</span> <span class="token number">0.0000</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span> grad_fn<span class="token operator">=</span><span class="token operator">&lt;</span>MulBackward0<span class="token operator">></span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>
<h2 id="10-使用-torch-where来对tensors加条件"><a href="#10-使用-torch-where来对tensors加条件" class="headerlink" title="10 使用 torch.where来对tensors加条件"></a><strong>10 使用 torch.where来对tensors加条件</strong></h2><p>当你想把两个张量结合在一个条件下这个函数很有用，如果条件是真，那么从第一个张量中取元素，如果条件是假，从第二个张量中取元素。</p>
<pre class="line-numbers language-python"><code class="language-python">x <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1.0</span><span class="token punctuation">,</span> <span class="token number">2.0</span><span class="token punctuation">,</span> <span class="token number">3.0</span><span class="token punctuation">,</span> <span class="token number">4.0</span><span class="token punctuation">,</span> <span class="token number">5.0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
y <span class="token operator">=</span> <span class="token operator">-</span>x
condition_or_mask <span class="token operator">=</span> x <span class="token operator">&lt;=</span> <span class="token number">3.0</span>
torch<span class="token punctuation">.</span>where<span class="token punctuation">(</span>condition_or_mask<span class="token punctuation">,</span> x<span class="token punctuation">,</span> y<span class="token punctuation">)</span>
tensor<span class="token punctuation">(</span><span class="token punctuation">[</span> <span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span>  <span class="token number">2</span><span class="token punctuation">.</span><span class="token punctuation">,</span>  <span class="token number">3</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">4</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">5</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span> grad_fn<span class="token operator">=</span><span class="token operator">&lt;</span>SWhereBackward<span class="token operator">></span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h2 id="11-在给定的位置给张量填入值-Tensor-scatter"><a href="#11-在给定的位置给张量填入值-Tensor-scatter" class="headerlink" title="11 在给定的位置给张量填入值(Tensor.scatter)"></a><strong>11 在给定的位置给张量填入值(Tensor.scatter)</strong></h2><p>这个函数的用例如下，你想用给定位置下另一个张量的值填充一个张量。一维张量更容易理解，所以我将先展示它，然后继续更高级的例子。</p>
<pre class="line-numbers language-python"><code class="language-python">data <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
index <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
values <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">5</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
data<span class="token punctuation">.</span>scatter<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> index<span class="token punctuation">,</span> values<span class="token punctuation">)</span>
tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">,</span>  <span class="token number">3</span><span class="token punctuation">,</span>  <span class="token number">4</span><span class="token punctuation">,</span>  <span class="token number">5</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>上面的例子很简单，但是现在看看如果将index改为index = torch.tensor([0, 1, 4])会发生什么：</p>
<pre class="line-numbers language-python"><code class="language-python">data <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
index <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
values <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">5</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
data<span class="token punctuation">.</span>scatter<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> index<span class="token punctuation">,</span> values<span class="token punctuation">)</span>
tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">,</span>  <span class="token number">3</span><span class="token punctuation">,</span>  <span class="token number">4</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>为什么最后一个值是-3，这是反直觉的，对吧？这是PyTorch scatter函数的中心思想。index变量表示data张量的第i个值应该放在values张量的哪个位置。我希望下面的简单python版的这个操作能让你更明白：</p>
<pre class="line-numbers language-python"><code class="language-python">data_orig <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
index <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
values <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">5</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
scattered <span class="token operator">=</span> data_orig<span class="token punctuation">.</span>scatter<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> index<span class="token punctuation">,</span> values<span class="token punctuation">)</span>

data <span class="token operator">=</span> data_orig<span class="token punctuation">.</span>clone<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token keyword">for</span> idx_in_values<span class="token punctuation">,</span> where_to_put_the_value <span class="token keyword">in</span> enumerate<span class="token punctuation">(</span>index<span class="token punctuation">)</span><span class="token punctuation">:</span>
    what_value_to_put <span class="token operator">=</span> values<span class="token punctuation">[</span>idx_in_values<span class="token punctuation">]</span>
    data<span class="token punctuation">[</span>where_to_put_the_value<span class="token punctuation">]</span> <span class="token operator">=</span> what_value_to_put
data<span class="token punctuation">,</span> scattered
<span class="token punctuation">(</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">,</span>  <span class="token number">3</span><span class="token punctuation">,</span>  <span class="token number">4</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span> tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">,</span>  <span class="token number">3</span><span class="token punctuation">,</span>  <span class="token number">4</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p><strong>2D数据的PyTorch scatter例子</strong></p>
<p>始终记住，index的形状与values的形状相关，而index中的值对应于data中的位置。</p>
<pre class="line-numbers language-python"><code class="language-python">data <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>float<span class="token punctuation">(</span><span class="token punctuation">)</span>
index <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span>
    <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    <span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span>
<span class="token punctuation">]</span><span class="token punctuation">)</span>
values <span class="token operator">=</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">9</span><span class="token punctuation">)</span><span class="token punctuation">.</span>float<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>
values<span class="token punctuation">,</span> data<span class="token punctuation">.</span>scatter<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> index<span class="token punctuation">,</span> values<span class="token punctuation">)</span>
<span class="token punctuation">(</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">5</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">7</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">5</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h2 id="12-在网络中进行图像插值-F-interpolate"><a href="#12-在网络中进行图像插值-F-interpolate" class="headerlink" title="12 在网络中进行图像插值 (F.interpolate)"></a><strong>12 在网络中进行图像插值 (F.interpolate)</strong></h2><p>当我学习PyTorch时，我惊讶地发现，实际上可以在前向传递中调整图像(或任何中间张量)，并保持梯度流。这种方法在使用CNN和GANs时特别有用。</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># image from https://commons.wikimedia.org/wiki/File:A_female_British_Shorthair_at_the_age_of_20_months.jpg</span>
img <span class="token operator">=</span> Image<span class="token punctuation">.</span>open<span class="token punctuation">(</span><span class="token string">"./cat.jpg"</span><span class="token punctuation">)</span>
img<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>
<pre class="line-numbers language-python"><code class="language-python">to_pil_image<span class="token punctuation">(</span>
    F<span class="token punctuation">.</span>interpolate<span class="token punctuation">(</span>to_tensor<span class="token punctuation">(</span>img<span class="token punctuation">)</span><span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span>  <span class="token comment" spellcheck="true"># batch of size 1</span>
                  mode<span class="token operator">=</span><span class="token string">"bilinear"</span><span class="token punctuation">,</span> 
                  scale_factor<span class="token operator">=</span><span class="token number">2.0</span><span class="token punctuation">,</span> 
                  align_corners<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># remove batch dimension</span>
<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>看看梯度流是如何保存的：</p>
<pre><code>F.interpolate(to_tensor(img).unsqueeze(0).requires_grad_(),
                  mode="bicubic", 
                  scale_factor=2.0, 
                  align_corners=False)
tensor([[[[0.9216, 0.9216, 0.9216,  ..., 0.8361, 0.8272, 0.8219],
    [0.9214, 0.9214, 0.9214,  ..., 0.8361, 0.8272, 0.8219],
    [0.9212, 0.9212, 0.9212,  ..., 0.8361, 0.8272, 0.8219],
    ...,
    [0.9098, 0.9098, 0.9098,  ..., 0.3592, 0.3486, 0.3421],
    [0.9098, 0.9098, 0.9098,  ..., 0.3566, 0.3463, 0.3400],
    [0.9098, 0.9098, 0.9098,  ..., 0.3550, 0.3449, 0.3387]],

    [[0.6627, 0.6627, 0.6627,  ..., 0.5380, 0.5292, 0.5238],
    [0.6626, 0.6626, 0.6626,  ..., 0.5380, 0.5292, 0.5238],
    [0.6623, 0.6623, 0.6623,  ..., 0.5380, 0.5292, 0.5238],
    ...,
    [0.6196, 0.6196, 0.6196,  ..., 0.3631, 0.3525, 0.3461],
    [0.6196, 0.6196, 0.6196,  ..., 0.3605, 0.3502, 0.3439],
    [0.6196, 0.6196, 0.6196,  ..., 0.3589, 0.3488, 0.3426]],

    [[0.4353, 0.4353, 0.4353,  ..., 0.1913, 0.1835, 0.1787],
    [0.4352, 0.4352, 0.4352,  ..., 0.1913, 0.1835, 0.1787],
    [0.4349, 0.4349, 0.4349,  ..., 0.1913, 0.1835, 0.1787],
    ...,
    [0.3333, 0.3333, 0.3333,  ..., 0.3827, 0.3721, 0.3657],
    [0.3333, 0.3333, 0.3333,  ..., 0.3801, 0.3698, 0.3635],
    [0.3333, 0.3333, 0.3333,  ..., 0.3785, 0.3684, 0.3622]]]],
grad_fn=&lt;UpsampleBicubic2DBackward1&gt;)</code></pre><h2 id="13-将图像做成网格-torchvision-utils-make-grid"><a href="#13-将图像做成网格-torchvision-utils-make-grid" class="headerlink" title="13 将图像做成网格 (torchvision.utils.make_grid)"></a><strong>13 将图像做成网格 (torchvision.utils.make_grid)</strong></h2><p>当使用PyTorch和torchvision时，不需要使用matplotlib或一些外部库来复制粘贴代码来显示图像网格。只要使用torchvision.utils.make_grid就行了。</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">from</span> torchvision<span class="token punctuation">.</span>utils <span class="token keyword">import</span> make_grid
<span class="token keyword">from</span> torchvision<span class="token punctuation">.</span>transforms<span class="token punctuation">.</span>functional <span class="token keyword">import</span> to_tensor<span class="token punctuation">,</span> to_pil_image
<span class="token keyword">from</span> PIL <span class="token keyword">import</span> Image
img <span class="token operator">=</span> Image<span class="token punctuation">.</span>open<span class="token punctuation">(</span><span class="token string">"./cat.jpg"</span><span class="token punctuation">)</span>
to_pil_image<span class="token punctuation">(</span>
    make_grid<span class="token punctuation">(</span>
        <span class="token punctuation">[</span>to_tensor<span class="token punctuation">(</span>i<span class="token punctuation">)</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token punctuation">[</span>img<span class="token punctuation">,</span> img<span class="token punctuation">,</span> img<span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
         nrow<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token comment" spellcheck="true"># number of images in single row</span>
         padding<span class="token operator">=</span><span class="token number">5</span> <span class="token comment" spellcheck="true"># "frame" size</span>
     <span class="token punctuation">)</span>
<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20211022093246273.png" alt=""></p>
<h1 id="深度学习调参技巧"><a href="#深度学习调参技巧" class="headerlink" title="深度学习调参技巧"></a>深度学习调参技巧</h1><ol>
<li>不管什么模型，先在一个较小的训练集上train和test，看看它能不能过拟合。如果不能过拟合，可能是学习率太大，或者代码写错了。先调小学习率试一下，如果还不行就去检查代码，先看dataloader输出的数据对不对，再看模型每一步的size是否符合自己期待。</li>
<li>看train/eval的loss曲线，正常的情况应该是train loss呈log状一直下降最后趋于稳定，eval loss开始时一直下降到某一个epoch之后开始趋于稳定或开始上升，这时候可以用early stopping保存eval loss最低的那个模型。如果loss曲线非常不正常，很有可能是数据处理出了问题，比如label对应错了，回去检查代码。</li>
<li>不要一开始就用大数据集，先在一个大概2w训练集，2k测试集的小数据集上调参。</li>
<li>尽量不要自己从头搭架子（新手和半新手）。找一个已经明确没有bug能跑通的其它任务的架子，在它的基础上修改。否则debug过程非常艰难，因为有时候是版本迭代产生的问题，修改起来很麻烦。</li>
<li>优化器优先用adam，学习率设1e-3或1e-4，再试Radam（LiyuanLucasLiu/RAdam）。不推荐sgdm，因为很慢。</li>
<li>lrscheduler用torch.optim.lr_scheduler.CosineAnnealingLR，T_max设32或64，几个任务上试效果都不错。（用这个lr_scheduler加上adam系的optimizer基本就不用怎么调学习率了）</li>
<li>有一些任务（尤其是有RNN的）要做梯度裁剪，torch.nn.utils.clip_grad_norm。</li>
<li>参数初始化，lstm的h用orthogonal，其它用he或xavier。</li>
<li>激活函数用relu一般就够了，也可以试试leaky relu。</li>
<li>batchnorm和dropout可以试，放的位置很重要。优先尝试放在最后输出层之前，以及embedding层之后。RNN可以试layer_norm。有些任务上加了这些层可能会有负作用。</li>
<li>metric learning中先试标label的分类方法。然后可以用triplet loss，margin这个参数的设置很重要。</li>
<li>batchsize设置小一点通常会有一些提升，某些任务batchsize设成1有奇效。</li>
<li>embedding层的embedsize可以小一些（64 or 128），之后LSTM或CNN的hiddensize要稍微大一些（256 or 512）。（ALBERT论文里面大概也是这个意思）</li>
<li>模型方面，可以先用2或3层LSTM试一下，通常效果都不错。</li>
<li>weight decay可以试一下，我一般用1e-4。</li>
<li>有CNN的地方就用shortcut。CNN层数加到某一个值之后对结果影响就不大了，这个值作为参数可以调一下。</li>
<li>GRU和LSTM在大部分任务上效果差不多。</li>
<li>看论文时候不要全信，能复现的尽量复现一下，许多论文都会做低baseline，但实际使用时很多baseline效果很不错。</li>
<li>对于大多数任务，数据比模型重要。面对新任务时先分析数据，再根据数据设计模型，并决定各个参数。例如nlp有些任务中的padding长度，通常需要达到数据集的90%以上，可用pandas的describe函数进行分析。</li>
</ol>
<h1 id="PyTorch模型训练提速的技巧"><a href="#PyTorch模型训练提速的技巧" class="headerlink" title="PyTorch模型训练提速的技巧"></a>PyTorch模型训练提速的技巧</h1><h2 id="Pytorch-Lightning"><a href="#Pytorch-Lightning" class="headerlink" title="Pytorch-Lightning"></a>Pytorch-Lightning</h2><p>你可以在Pytorch的库Pytorch- lightning中找到我在这里讨论的每一个优化。Lightning是在Pytorch之上的一个封装，它可以自动训练，同时让研究人员完全控制关键的模型组件。Lightning 使用最新的最佳实践，并将你可能出错的地方最小化。</p>
<p>我们为MNIST定义LightningModel并使用Trainer来训练模型。</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">from</span> pytorch_lightning <span class="token keyword">import</span> Trainer
model <span class="token operator">=</span> LightningModule<span class="token punctuation">(</span>…<span class="token punctuation">)</span>
trainer <span class="token operator">=</span> Trainer<span class="token punctuation">(</span><span class="token punctuation">)</span>
trainer<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>model<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>
<h2 id="1-DataLoaders"><a href="#1-DataLoaders" class="headerlink" title="1. DataLoaders"></a><strong>1. DataLoaders</strong></h2><p>这可能是最容易获得速度增益的地方。保存h5py或numpy文件以加速数据加载的时代已经一去不复返了，使用Pytorch dataloader加载图像数据很简单(对于NLP数据，请查看TorchText)。</p>
<p>在lightning中，你不需要指定训练循环，只需要定义dataLoaders和Trainer就会在需要的时候调用它们。</p>
<pre class="line-numbers language-python"><code class="language-python">dataset <span class="token operator">=</span> MNIST<span class="token punctuation">(</span>root<span class="token operator">=</span>self<span class="token punctuation">.</span>hparams<span class="token punctuation">.</span>data_root<span class="token punctuation">,</span> train<span class="token operator">=</span>train<span class="token punctuation">,</span> download<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
loader <span class="token operator">=</span> DataLoader<span class="token punctuation">(</span>dataset<span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">32</span><span class="token punctuation">,</span> shuffle<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
<span class="token keyword">for</span> batch <span class="token keyword">in</span> loader<span class="token punctuation">:</span>
  x<span class="token punctuation">,</span> y <span class="token operator">=</span> batch
  model<span class="token punctuation">.</span>training_step<span class="token punctuation">(</span>x<span class="token punctuation">,</span> y<span class="token punctuation">)</span>
  <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h2 id="2-DataLoaders-中的-workers-的数量"><a href="#2-DataLoaders-中的-workers-的数量" class="headerlink" title="2. DataLoaders 中的 workers 的数量"></a><strong>2. DataLoaders 中的 workers 的数量</strong></h2><p>另一个加速的神奇之处是允许批量并行加载。因此，您可以一次装载nb_workers个batch，而不是一次装载一个batch。</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># slow</span>
loader <span class="token operator">=</span> DataLoader<span class="token punctuation">(</span>dataset<span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">32</span><span class="token punctuation">,</span> shuffle<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
<span class="token comment" spellcheck="true"># fast (use 10 workers)</span>
loader <span class="token operator">=</span> DataLoader<span class="token punctuation">(</span>dataset<span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">32</span><span class="token punctuation">,</span> shuffle<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> num_workers<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>
<h2 id="3-Batch-size"><a href="#3-Batch-size" class="headerlink" title="3. Batch size"></a><strong>3. Batch size</strong></h2><p>在开始下一个优化步骤之前，将batch size增大到CPU-RAM或GPU-RAM所允许的最大范围。</p>
<p>下一节将重点介绍如何帮助减少内存占用，以便你可以继续增加batch size。</p>
<p>记住，你可能需要再次更新你的学习率。一个好的经验法则是，如果batch size加倍，那么学习率就加倍。</p>
<h2 id="4-梯度累加"><a href="#4-梯度累加" class="headerlink" title="4. 梯度累加"></a><strong>4. 梯度累加</strong></h2><p>在你已经达到计算资源上限的情况下，你的batch size仍然太小(比如8)，然后我们需要模拟一个更大的batch size来进行梯度下降，以提供一个良好的估计。</p>
<p>假设我们想要达到128的batch size大小。我们需要以batch size为8执行16个前向传播和向后传播，然后再执行一次优化步骤。</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># clear last step</span>
optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token comment" spellcheck="true"># 16 accumulated gradient steps</span>
scaled_loss <span class="token operator">=</span> <span class="token number">0</span>
<span class="token keyword">for</span> accumulated_step_i <span class="token keyword">in</span> range<span class="token punctuation">(</span><span class="token number">16</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
     out <span class="token operator">=</span> model<span class="token punctuation">.</span>forward<span class="token punctuation">(</span><span class="token punctuation">)</span>
     loss <span class="token operator">=</span> some_loss<span class="token punctuation">(</span>out<span class="token punctuation">,</span>y<span class="token punctuation">)</span>    
     loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
      scaled_loss <span class="token operator">+=</span> loss<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token comment" spellcheck="true"># update weights after 8 steps. effective batch = 8*16</span>
optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token comment" spellcheck="true"># loss is now scaled up by the number of accumulated batches</span>
actual_loss <span class="token operator">=</span> scaled_loss <span class="token operator">/</span> <span class="token number">16</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>在lightning中，全部都给你做好了，只需要设置<code>accumulate_grad_batches=16</code>：</p>
<pre class="line-numbers language-python"><code class="language-python">trainer <span class="token operator">=</span> Trainer<span class="token punctuation">(</span>accumulate_grad_batches<span class="token operator">=</span><span class="token number">16</span><span class="token punctuation">)</span>
trainer<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>model<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>
<h2 id="5-保留的计算图"><a href="#5-保留的计算图" class="headerlink" title="5. 保留的计算图"></a><strong>5. 保留的计算图</strong></h2><p>一个最简单撑爆你的内存的方法是为了记录日志存储你的loss。</p>
<pre class="line-numbers language-python"><code class="language-python">losses <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>
losses<span class="token punctuation">.</span>append<span class="token punctuation">(</span>loss<span class="token punctuation">)</span>

<span class="token keyword">print</span><span class="token punctuation">(</span>f<span class="token string">'current loss: {torch.mean(losses)'</span><span class="token punctuation">}</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>上面的问题是，<strong>loss</strong>仍然包含有整个图的副本。在这种情况下，调用.item()来释放它。</p>
<pre class="line-numbers language-python"><code class="language-python">!<span class="token punctuation">[</span>1_CER3v8cok2UOBNsmnBrzPQ<span class="token punctuation">]</span><span class="token punctuation">(</span><span class="token number">9</span> Tips For Training Lightning<span class="token operator">-</span>Fast Neural Networks In Pytorch<span class="token punctuation">.</span>assets<span class="token operator">/</span>1_CER3v8cok2UOBNsmnBrzPQ<span class="token punctuation">.</span>gif<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># bad</span>
losses<span class="token punctuation">.</span>append<span class="token punctuation">(</span>loss<span class="token punctuation">)</span>

<span class="token comment" spellcheck="true"># good</span>
losses<span class="token punctuation">.</span>append<span class="token punctuation">(</span>loss<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>Lightning会非常小心，确保不会保留计算图的副本。</p>
<h2 id="6-单个GPU训练"><a href="#6-单个GPU训练" class="headerlink" title="6. 单个GPU训练"></a><strong>6. 单个GPU训练</strong></h2><p>一旦你已经完成了前面的步骤，是时候进入GPU训练了。在GPU上的训练将使多个GPU cores之间的数学计算并行化。你得到的加速取决于你所使用的GPU类型。我推荐个人用2080Ti，公司用V100。</p>
<p>乍一看，这可能会让你不知所措，但你真的只需要做两件事:1)移动你的模型到GPU, 2)每当你运行数据通过它，把数据放到GPU上。</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># put model on GPU</span>
model<span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>

<span class="token comment" spellcheck="true"># put data on gpu (cuda on a variable returns a cuda copy)</span>
x <span class="token operator">=</span> x<span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>

<span class="token comment" spellcheck="true"># runs on GPU now</span>
model<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>如果你使用Lightning，你什么都不用做，只需要设置<code>Trainer(gpus=1)</code>。</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># ask lightning to use gpu 0 for training</span>
trainer <span class="token operator">=</span> Trainer<span class="token punctuation">(</span>gpus<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
trainer<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>model<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>
<p>在GPU上进行训练时，要注意的主要事情是限制CPU和GPU之间的传输次数。</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># expensive</span>
x <span class="token operator">=</span> x<span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># very expensive</span>
x <span class="token operator">=</span> x<span class="token punctuation">.</span>cpu<span class="token punctuation">(</span><span class="token punctuation">)</span>
x <span class="token operator">=</span> x<span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>
<p>如果内存耗尽，不要将数据移回CPU以节省内存。在求助于GPU之前，尝试以其他方式优化你的代码或GPU之间的内存分布。</p>
<p>另一件需要注意的事情是调用强制GPU同步的操作。清除内存缓存就是一个例子。</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># really bad idea. Stops all the GPUs until they all catch up</span>
torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>empty_cache<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>
<p>但是，如果使用Lightning，惟一可能出现问题的地方是在定义Lightning Module时。Lightning会特别注意不去犯这类错误。</p>
<h2 id="7-16-bit-精度"><a href="#7-16-bit-精度" class="headerlink" title="7. 16-bit 精度"></a><strong>7. 16-bit 精度</strong></h2><p>16bit精度是将内存占用减半的惊人技术。大多数模型使用32bit精度数字进行训练。然而，最近的研究发现，16bit模型也可以工作得很好。混合精度意味着对某些内容使用16bit，但将权重等内容保持在32bit。</p>
<p>要在Pytorch中使用16bit精度，请安装NVIDIA的apex库，并对你的模型进行这些更改。</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># enable 16-bit on the model and the optimizer</span>
model<span class="token punctuation">,</span> optimizers <span class="token operator">=</span> amp<span class="token punctuation">.</span>initialize<span class="token punctuation">(</span>model<span class="token punctuation">,</span> optimizers<span class="token punctuation">,</span> opt_level<span class="token operator">=</span><span class="token string">'O2'</span><span class="token punctuation">)</span>

<span class="token comment" spellcheck="true"># when doing .backward, let amp do it so it can scale the loss</span>
<span class="token keyword">with</span> amp<span class="token punctuation">.</span>scale_loss<span class="token punctuation">(</span>loss<span class="token punctuation">,</span> optimizer<span class="token punctuation">)</span> <span class="token keyword">as</span> scaled_loss<span class="token punctuation">:</span>                      
    scaled_loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p><em>amp</em>包会处理好大部分事情。如果梯度爆炸或趋向于0，它甚至会缩放loss。</p>
<p>在lightning中，启用16bit并不需要修改模型中的任何内容，也不需要执行我上面所写的操作。设置<code>Trainer(precision=16)</code>就可以了。</p>
<pre class="line-numbers language-python"><code class="language-python">trainer <span class="token operator">=</span> Trainer<span class="token punctuation">(</span>amp_level<span class="token operator">=</span><span class="token string">'O2'</span><span class="token punctuation">,</span> use_amp<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
trainer<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>model<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>
<h2 id="8-移动到多个GPUs中"><a href="#8-移动到多个GPUs中" class="headerlink" title="8. 移动到多个GPUs中"></a><strong>8. 移动到多个GPUs中</strong></h2><p>现在，事情变得非常有趣了。有3种(也许更多？)方法来进行多GPU训练。</p>
<h3 id="分batch训练"><a href="#分batch训练" class="headerlink" title="分batch训练"></a><strong>分batch训练</strong></h3><p><img src="/images/loading.gif" data-original="../images/basic/image-20211024153912407.png" alt=""></p>
<p>第一种方法被称为“分batch训练”。该策略将模型复制到每个GPU上，每个GPU获得batch的一部分。</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># copy model on each GPU and give a fourth of the batch to each</span>
model <span class="token operator">=</span> DataParallel<span class="token punctuation">(</span>model<span class="token punctuation">,</span> devices<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span> <span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

<span class="token comment" spellcheck="true"># out has 4 outputs (one for each gpu)</span>
out <span class="token operator">=</span> model<span class="token punctuation">(</span>x<span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>在lightning中，你只需要增加GPUs的数量，然后告诉trainer，其他什么都不用做。</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># ask lightning to use 4 GPUs for training</span>
trainer <span class="token operator">=</span> Trainer<span class="token punctuation">(</span>gpus<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
trainer<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>model<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>
<h3 id="模型分布训练"><a href="#模型分布训练" class="headerlink" title="模型分布训练"></a><strong>模型分布训练</strong></h3><p><img src="/images/loading.gif" data-original="../images/basic/image-20211024153929011.png" alt=""></p>
<p>有时你的模型可能太大不能完全放到内存中。例如，带有编码器和解码器的序列到序列模型在生成输出时可能会占用20GB RAM。在本例中，我们希望将编码器和解码器放在独立的GPU上。</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># each model is sooo big we can't fit both in memory</span>
encoder_rnn<span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>
decoder_rnn<span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>

<span class="token comment" spellcheck="true"># run input through encoder on GPU 0</span>
encoder_out <span class="token operator">=</span> encoder_rnn<span class="token punctuation">(</span>x<span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

<span class="token comment" spellcheck="true"># run output through decoder on the next GPU</span>
out <span class="token operator">=</span> decoder_rnn<span class="token punctuation">(</span>encoder_out<span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

<span class="token comment" spellcheck="true"># normally we want to bring all outputs back to GPU 0</span>
out <span class="token operator">=</span> out<span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>对于这种类型的训练，在Lightning中不需要指定任何GPU，你应该把LightningModule中的模块放到正确的GPU上。</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">MyModule</span><span class="token punctuation">(</span>LightningModule<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>encoder <span class="token operator">=</span> RNN<span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>decoder <span class="token operator">=</span> RNN<span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span>
    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment" spellcheck="true"># models won't be moved after the first forward because </span>
        <span class="token comment" spellcheck="true"># they are already on the correct GPUs</span>
        self<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>decoder<span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>
        out <span class="token operator">=</span> self<span class="token punctuation">.</span>encoder<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        out <span class="token operator">=</span> self<span class="token punctuation">.</span>decoder<span class="token punctuation">(</span>out<span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

<span class="token comment" spellcheck="true"># don't pass GPUs to trainer</span>
model <span class="token operator">=</span> MyModule<span class="token punctuation">(</span><span class="token punctuation">)</span>
trainer <span class="token operator">=</span> Trainer<span class="token punctuation">(</span><span class="token punctuation">)</span>
trainer<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>model<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h3 id="两者混合"><a href="#两者混合" class="headerlink" title="两者混合"></a><strong>两者混合</strong></h3><p>在上面的情况下，编码器和解码器仍然可以从并行化操作中获益。</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># change these lines</span>
self<span class="token punctuation">.</span>encoder <span class="token operator">=</span> RNN<span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span>
self<span class="token punctuation">.</span>decoder <span class="token operator">=</span> RNN<span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span>

<span class="token comment" spellcheck="true"># to these</span>
<span class="token comment" spellcheck="true"># now each RNN is based on a different gpu set</span>
self<span class="token punctuation">.</span>encoder <span class="token operator">=</span> DataParallel<span class="token punctuation">(</span>self<span class="token punctuation">.</span>encoder<span class="token punctuation">,</span> devices<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
self<span class="token punctuation">.</span>decoder <span class="token operator">=</span> DataParallel<span class="token punctuation">(</span>self<span class="token punctuation">.</span>encoder<span class="token punctuation">,</span> devices<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

<span class="token comment" spellcheck="true"># in forward...</span>
out <span class="token operator">=</span> self<span class="token punctuation">.</span>encoder<span class="token punctuation">(</span>x<span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

<span class="token comment" spellcheck="true"># notice inputs on first gpu in device</span>
sout <span class="token operator">=</span> self<span class="token punctuation">.</span>decoder<span class="token punctuation">(</span>out<span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># &lt;--- the 4 here</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>使用多个GPU时要考虑的注意事项：</p>
<ul>
<li>如果模型已经在GPU上了，model.cuda()不会做任何事情。</li>
<li>总是把输入放在设备列表中的第一个设备上。</li>
<li>在设备之间传输数据是昂贵的，把它作为最后的手段。</li>
<li>优化器和梯度会被保存在GPU 0上，因此，GPU 0上使用的内存可能会比其他GPU大得多。</li>
</ul>
<h2 id="9-多节点GPU训练"><a href="#9-多节点GPU训练" class="headerlink" title="9. 多节点GPU训练"></a><strong>9. 多节点GPU训练</strong></h2><p><img src="/images/loading.gif" data-original="../images/basic/image-20211024153952915.png" alt=""></p>
<p>每台机器上的每个GPU都有一个模型的副本。每台机器获得数据的一部分，并且只在那部分上训练。每台机器都能同步梯度。</p>
<p>如果你已经做到了这一步，那么你现在可以在几分钟内训练Imagenet了！这并没有你想象的那么难，但是它可能需要你对计算集群的更多知识。这些说明假设你正在集群上使用SLURM。</p>
<p>Pytorch允许多节点训练，通过在每个节点上复制每个GPU上的模型并同步梯度。所以，每个模型都是在每个GPU上独立初始化的，本质上独立地在数据的一个分区上训练，除了它们都从所有模型接收梯度更新。</p>
<p>在高层次上：</p>
<ol>
<li>在每个GPU上初始化一个模型的副本(确保设置种子，让每个模型初始化到相同的权重，否则它会失败)。</li>
<li>将数据集分割成子集(使用DistributedSampler)。每个GPU只在它自己的小子集上训练。</li>
<li>在.backward()上，所有副本都接收到所有模型的梯度副本。这是模型之间唯一一次的通信。</li>
</ol>
<p>Pytorch有一个很好的抽象，叫做DistributedDataParallel，它可以帮你实现这个功能。要使用DDP，你需要做<strong>4</strong>的事情：</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">tng_dataloader</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
     d <span class="token operator">=</span> MNIST<span class="token punctuation">(</span><span class="token punctuation">)</span>

     <span class="token comment" spellcheck="true"># 4: Add distributed sampler</span>
     <span class="token comment" spellcheck="true"># sampler sends a portion of tng data to each machine</span>
     dist_sampler <span class="token operator">=</span> DistributedSampler<span class="token punctuation">(</span>dataset<span class="token punctuation">)</span>
     dataloader <span class="token operator">=</span> DataLoader<span class="token punctuation">(</span>d<span class="token punctuation">,</span> shuffle<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> sampler<span class="token operator">=</span>dist_sampler<span class="token punctuation">)</span>

<span class="token keyword">def</span> <span class="token function">main_process_entrypoint</span><span class="token punctuation">(</span>gpu_nb<span class="token punctuation">)</span><span class="token punctuation">:</span>
     <span class="token comment" spellcheck="true"># 2: set up connections  between all gpus across all machines</span>
     <span class="token comment" spellcheck="true"># all gpus connect to a single GPU "root"</span>
     <span class="token comment" spellcheck="true"># the default uses env://</span>
     world <span class="token operator">=</span> nb_gpus <span class="token operator">*</span> nb_nodes
     dist<span class="token punctuation">.</span>init_process_group<span class="token punctuation">(</span><span class="token string">"nccl"</span><span class="token punctuation">,</span> rank<span class="token operator">=</span>gpu_nb<span class="token punctuation">,</span> world_size<span class="token operator">=</span>world<span class="token punctuation">)</span>

     <span class="token comment" spellcheck="true"># 3: wrap model in DPP</span>
     torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>set_device<span class="token punctuation">(</span>gpu_nb<span class="token punctuation">)</span>
     model<span class="token punctuation">.</span>cuda<span class="token punctuation">(</span>gpu_nb<span class="token punctuation">)</span>
     model <span class="token operator">=</span> DistributedDataParallel<span class="token punctuation">(</span>model<span class="token punctuation">,</span> device_ids<span class="token operator">=</span><span class="token punctuation">[</span>gpu_nb<span class="token punctuation">]</span><span class="token punctuation">)</span>

     <span class="token comment" spellcheck="true"># train your model now...</span>

<span class="token keyword">if</span>  __name__ <span class="token operator">==</span> <span class="token string">'__main__'</span><span class="token punctuation">:</span>
     <span class="token comment" spellcheck="true"># 1: spawn number of processes</span>
     <span class="token comment" spellcheck="true"># your cluster will call main for each machine</span>
     mp<span class="token punctuation">.</span>spawn<span class="token punctuation">(</span>main_process_entrypoint<span class="token punctuation">,</span> nprocs<span class="token operator">=</span><span class="token number">8</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>然而，在Lightning中，只需设置节点数量，它就会为你处理其余的事情。</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># train on 1024 gpus across 128 nodes</span>
trainer <span class="token operator">=</span> Trainer<span class="token punctuation">(</span>nb_gpu_nodes<span class="token operator">=</span><span class="token number">128</span><span class="token punctuation">,</span> gpus<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>
<p>Lightning还附带了一个SlurmCluster管理器，可以方便地帮助你提交SLURM作业的正确详细信息。</p>
<h2 id="10-在单个节点上多GPU更快的训练"><a href="#10-在单个节点上多GPU更快的训练" class="headerlink" title="10. 在单个节点上多GPU更快的训练"></a><strong>10. 在单个节点上多GPU更快的训练</strong></h2><p>事实证明，distributedDataParallel比DataParallel快得多，因为它只执行梯度同步的通信。所以，一个好的hack是使用distributedDataParallel替换DataParallel，即使是在单机上进行训练。</p>
<p>在Lightning中，这很容易通过将distributed_backend设置为<strong>ddp</strong>和设置GPUs的数量来实现。</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># train on 4 gpus on the same machine MUCH faster than DataParallel</span>
trainer <span class="token operator">=</span> Trainer<span class="token punctuation">(</span>distributed_backend<span class="token operator">=</span><span class="token string">'ddp'</span><span class="token punctuation">,</span> gpus<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>
<h2 id="对模型加速的思考"><a href="#对模型加速的思考" class="headerlink" title="对模型加速的思考"></a>对模型加速的思考</h2><p>首先，我要确保在数据加载中没有瓶颈。为此，我使用了我所描述的现有数据加载解决方案，但是如果没有一种解决方案满足你的需要，请考虑离线处理和缓存到高性能数据存储中，比如h5py。</p>
<p>接下来看看你在训练步骤中要做什么。确保你的前向传播速度快，避免过多的计算以及最小化CPU和GPU之间的数据传输。最后，避免做一些会降低GPU速度的事情(本指南中有介绍)。</p>
<p>接下来，我试图最大化我的batch size，这通常是受GPU内存大小的限制。现在，需要关注在使用大的batch size的时候如何在多个GPUs上分布并最小化延迟（比如，我可能会尝试着在多个gpu上使用8000 +的有效batch size）。</p>
<p>然而，你需要小心大的batch size。针对你的具体问题，请查阅相关文献，看看人们都忽略了什么！</p>
<script>
        document.querySelectorAll('.github-emoji')
          .forEach(el => {
            if (!el.dataset.src) { return; }
            const img = document.createElement('img');
            img.style = 'display:none !important;';
            img.src = el.dataset.src;
            img.addEventListener('error', () => {
              img.remove();
              el.style.color = 'inherit';
              el.style.backgroundImage = 'none';
              el.style.background = 'none';
            });
            img.addEventListener('load', () => {
              img.remove();
            });
            document.body.appendChild(img);
          });
      </script>
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://jackhcc.github.io" rel="external nofollow noreferrer">杰克成</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://jackhcc.github.io/posts/dl-series1.html">https://jackhcc.github.io/posts/dl-series1.html</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="https://jackhcc.github.io" target="_blank">杰克成</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/tags/PyTorch/">
                                    <span class="chip bg-color">PyTorch</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/libs/share/css/share.min.css">

<div id="article-share">
    
    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
                <style>
    #reward {
        margin: 40px 0;
        text-align: center;
    }

    #reward .reward-link {
        font-size: 1.4rem;
        line-height: 38px;
    }

    #reward .btn-floating:hover {
        box-shadow: 0 6px 12px rgba(0, 0, 0, 0.2), 0 5px 15px rgba(0, 0, 0, 0.2);
    }

    #rewardModal {
        width: 320px;
        height: 350px;
    }

    #rewardModal .reward-title {
        margin: 15px auto;
        padding-bottom: 5px;
    }

    #rewardModal .modal-content {
        padding: 10px;
    }

    #rewardModal .close {
        position: absolute;
        right: 15px;
        top: 15px;
        color: rgba(0, 0, 0, 0.5);
        font-size: 1.3rem;
        line-height: 20px;
        cursor: pointer;
    }

    #rewardModal .close:hover {
        color: #ef5350;
        transform: scale(1.3);
        -moz-transform:scale(1.3);
        -webkit-transform:scale(1.3);
        -o-transform:scale(1.3);
    }

    #rewardModal .reward-tabs {
        margin: 0 auto;
        width: 210px;
    }

    .reward-tabs .tabs {
        height: 38px;
        margin: 10px auto;
        padding-left: 0;
    }

    .reward-content ul {
        padding-left: 0 !important;
    }

    .reward-tabs .tabs .tab {
        height: 38px;
        line-height: 38px;
    }

    .reward-tabs .tab a {
        color: #fff;
        background-color: #ccc;
    }

    .reward-tabs .tab a:hover {
        background-color: #ccc;
        color: #fff;
    }

    .reward-tabs .wechat-tab .active {
        color: #fff !important;
        background-color: #22AB38 !important;
    }

    .reward-tabs .alipay-tab .active {
        color: #fff !important;
        background-color: #019FE8 !important;
    }

    .reward-tabs .reward-img {
        width: 210px;
        height: 210px;
    }
</style>

<div id="reward">
    <a href="#rewardModal" class="reward-link modal-trigger btn-floating btn-medium waves-effect waves-light red">赏</a>

    <!-- Modal Structure -->
    <div id="rewardModal" class="modal">
        <div class="modal-content">
            <a class="close modal-close"><i class="fas fa-times"></i></a>
            <h4 class="reward-title">你的赏识是我前进的动力</h4>
            <div class="reward-content">
                <div class="reward-tabs">
                    <ul class="tabs row">
                        <li class="tab col s6 alipay-tab waves-effect waves-light"><a href="#alipay">支付宝</a></li>
                        <li class="tab col s6 wechat-tab waves-effect waves-light"><a href="#wechat">微 信</a></li>
                    </ul>
                    <div id="alipay">
                        <img src="/images/loading.gif" data-original="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/medias/reward/aliqr.png" class="reward-img" alt="支付宝打赏二维码">
                    </div>
                    <div id="wechat">
                        <img src="/images/loading.gif" data-original="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/medias/reward/wxqr.png" class="reward-img" alt="微信打赏二维码">
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>

<script>
    $(function () {
        $('.tabs').tabs();
    });
</script>
            
        </div>
    </div>

    
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/libs/gitalk/gitalk.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/css/my-gitalk.css">

<div class="card gitalk-card" data-aos="fade-up">
    <div class="comment_headling" style="font-size: 20px; font-weight: 700; position: relative; left: 20px; top: 15px; padding-bottom: 5px;">
        <i class="fas fa-comments fa-fw" aria-hidden="true"></i>
        <span>评论</span>
    </div>
    <div id="gitalk-container" class="card-content"></div>
</div>

<script src="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/libs/gitalk/gitalk.min.js"></script>
<script>
    let gitalk = new Gitalk({
        clientID: '3821a0bbb773038a51fc',
        clientSecret: '4b30b507d67ec5497ec0e77f43f80cb3e0d7dd3a',
        repo: 'JackHCC.github.io',
        owner: 'JackHCC',
        admin: "JackHCC",
        id: '2021-10-04T20-04-00',
        distractionFreeMode: false  // Facebook-like distraction free mode
    });

    gitalk.render('gitalk-container');
</script>
    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/posts/dl-series2.html">
                    <div class="card-image">
                        
                        
                        <img src="/images/loading.gif" data-original="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/medias/featureimages/18.jpg" class="responsive-img" alt="DL专栏2-Activate Function">
                        
                        <span class="card-title">DL专栏2-Activate Function</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            常见的激活函数
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2021-10-05
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/Deep-Learning/" class="post-category">
                                    Deep Learning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/Activate-Function/">
                        <span class="chip bg-color">Activate Function</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/posts/blog-python24.html">
                    <div class="card-image">
                        
                        
                        <img src="/images/loading.gif" data-original="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/medias/featureimages/0.jpg" class="responsive-img" alt="Python Module">
                        
                        <span class="card-title">Python Module</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Python好用的小众模块
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2021-10-03
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/Python/" class="post-category">
                                    Python
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/Python/">
                        <span class="chip bg-color">Python</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/libs/codeBlock/codeBlockFuction.js"></script>

<!-- 代码语言 -->

<script type="text/javascript" src="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/libs/codeBlock/codeShrink.js"></script>


<!-- 代码块折行 -->

<style type="text/css">
code[class*="language-"], pre[class*="language-"] { white-space: pre !important; }
</style>


    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('4'),
            headingSelector: 'h1, h2, h3, h4'
        });

        // modify the toc link href to support Chinese.
        let i = 0;
        let tocHeading = 'toc-heading-';
        $('#toc-content a').each(function () {
            $(this).attr('href', '#' + tocHeading + (++i));
        });

        // modify the heading title id to support Chinese.
        i = 0;
        $('#articleContent').children('h1, h2, h3, h4').each(function () {
            $(this).attr('id', tocHeading + (++i));
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>

    

</main>



    <footer class="page-footer bg-color">
    <div class="container row center-align" style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            <span id="year">2020</span>
            <a href="https://jackhcc.github.io" target="_blank">杰克成</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            <br>
            
            &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                class="white-color">3591.2k</span>&nbsp;字
            
            
            
            
            
            
            <span id="busuanzi_container_site_pv">
                |&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;<span id="busuanzi_value_site_pv"
                    class="white-color"></span>&nbsp;次
            </span>
            
            
            <span id="busuanzi_container_site_uv">
                |&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;<span id="busuanzi_value_site_uv"
                    class="white-color"></span>&nbsp;人
            </span>
            
            <br>
            
            <span id="sitetime">载入运行时间...</span>
            <script>
                function siteTime() {
                    var seconds = 1000;
                    var minutes = seconds * 60;
                    var hours = minutes * 60;
                    var days = hours * 24;
                    var years = days * 365;
                    var today = new Date();
                    var startYear = "2020";
                    var startMonth = "2";
                    var startDate = "27";
                    var startHour = "6";
                    var startMinute = "30";
                    var startSecond = "0";
                    var todayYear = today.getFullYear();
                    var todayMonth = today.getMonth() + 1;
                    var todayDate = today.getDate();
                    var todayHour = today.getHours();
                    var todayMinute = today.getMinutes();
                    var todaySecond = today.getSeconds();
                    var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                    var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                    var diff = t2 - t1;
                    var diffYears = Math.floor(diff / years);
                    var diffDays = Math.floor((diff / days) - diffYears * 365);
                    var diffHours = Math.floor((diff - (diffYears * 365 + diffDays) * days) / hours);
                    var diffMinutes = Math.floor((diff - (diffYears * 365 + diffDays) * days - diffHours * hours) /
                        minutes);
                    var diffSeconds = Math.floor((diff - (diffYears * 365 + diffDays) * days - diffHours * hours -
                        diffMinutes * minutes) / seconds);
                    if (startYear == todayYear) {
                        document.getElementById("year").innerHTML = todayYear;
                        document.getElementById("sitetime").innerHTML = "本站已安全运行 " + diffDays + " 天 " + diffHours +
                            " 小时 " + diffMinutes + " 分钟 " + diffSeconds + " 秒";
                    } else {
                        document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                        document.getElementById("sitetime").innerHTML = "本站已安全运行 " + diffYears + " 年 " + diffDays +
                            " 天 " + diffHours + " 小时 " + diffMinutes + " 分钟 " + diffSeconds + " 秒";
                    }
                }
                setInterval(siteTime, 1000);
            </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/JackHCC" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:jackcc0701@163.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>



    <a href="https://www.facebook.com/profile.php?id=100046343443643" class="tooltipped" target="_blank" data-tooltip="关注我的Facebook: https://www.facebook.com/profile.php?id=100046343443643" data-position="top" data-delay="50">
        <i class="fab fa-facebook-f"></i>
    </a>



    <a href="https://twitter.com/JackChe66021834" class="tooltipped" target="_blank" data-tooltip="关注我的Twitter: https://twitter.com/JackChe66021834" data-position="top" data-delay="50">
        <i class="fab fa-twitter"></i>
    </a>



    <a href="tencent://AddContact/?fromId=50&fromSubId=1&subcmd=all&uin=2508074836" class="tooltipped" target="_blank" data-tooltip="QQ联系我: 2508074836" data-position="top" data-delay="50">
        <i class="fab fa-qq"></i>
    </a>



    <a href="https://weibo.com/u/6885584679" class="tooltipped" target="_blank" data-tooltip="关注我的微博: https://weibo.com/u/6885584679" data-position="top" data-delay="50">
        <i class="fab fa-weibo"></i>
    </a>



    <a href="https://www.zhihu.com/people/8f8482f01f0d6a04e844efe32e0f0710" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/8f8482f01f0d6a04e844efe32e0f0710" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script src="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/js/search.js"></script>
<script type="text/javascript">
$(function () {
    searchFunc("https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/" + "search.xml", 'searchInput', 'searchResult');
});
</script>
    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/libs/materialize/materialize.min.js"></script>
    <script src="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/libs/aos/aos.js"></script>
    <script src="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/js/matery.js"></script>

    <canvas class="fireworks" style="position: fixed;left: 0;top: 0;z-index: 1; pointer-events: none;" ></canvas>
    <script type="text/javascript" src="//cdn.bootcss.com/animejs/2.2.0/anime.min.js"></script>
    <script type="text/javascript" src="/js/fireworks.js"></script>

    <script type="text/javascript">
        //只在桌面版网页启用特效
        var windowWidth = $(window).width();
        if (windowWidth > 768) {
            document.write('<script type="text/javascript" src="/js/sakura.js"><\/script>'); }
    </script>

    <!-- weather -->
	<script type="text/javascript">
	WIDGET = {FID: 'TToslpmkVO'}
	</script>
	<script type="text/javascript" src="https://apip.weatherdt.com/float/static/js/r.js?v=1111"></script>


    <!-- Global site tag (gtag.js) - Google Analytics -->


    <!-- Baidu Analytics -->

<script>
    var _hmt = _hmt || [];
    (function () {
        var hm = document.createElement("script");
        hm.src = "https://hm.baidu.com/hm.js?";
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(hm, s);
    })();
</script>

    <!-- Baidu Push -->

    
    
    <script async src="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/libs/others/busuanzi.pure.mini.js"></script>
    

    
        <script src="//code.tidio.co/kqhlkxviiccyoa0czpfpu4ijuey9hfre.js"></script>
        <script> 
            $(document).ready(function () {
                setInterval(change_Tidio, 50);  
                function change_Tidio() { 
                    var tidio=$("#tidio-chat iframe");
                    if(tidio.css("display")=="block"&& $(window).width()>977 ){
                        document.getElementById("tidio-chat-iframe").style.bottom= ($("div#backTop.top-scroll").css("display")=="none" &&$(window).width()>977)>0? "-40px" : ($("div.toc-title").length&&$(window).width()>977)>0?"85px":"20px";   
                        document.getElementById("tidio-chat-iframe").style.right="-15px";   
                        document.getElementById("tidio-chat-iframe").style.height=parseInt(tidio.css("height"))>=520?"520px":tidio.css("height");
                        document.getElementById("tidio-chat-iframe").style.zIndex="997";
                    } 
                    else if(tidio.css("display")=="block"&&$(window).width()>601 &&$(window).width()<992 ){
                        document.getElementById("tidio-chat-iframe").style.bottom= ($("div#backTop.top-scroll").css("display")=="none" && 601< $(window).width()<992)>0? "-40px":"20px" ;   
                        document.getElementById("tidio-chat-iframe").style.right="-15px"; 
                        document.getElementById("tidio-chat-iframe").style.zIndex="997";
                    }
                    else if(tidio.css("display")=="block"&&$(window).width()<601 && parseInt(tidio.css("height"))<230){
                        document.getElementById("tidio-chat-iframe").style.bottom= ($("div#backTop.top-scroll").css("display")=="none" && $(window).width()<601)>0? "-10px":"45px" ;   
                        document.getElementById("tidio-chat-iframe").style.zIndex="997";
                    }
                    if( tidio.css("display")=="block"&&$(window).width()<601 && parseInt(tidio.css("height"))>=230){
                        document.getElementById("tidio-chat-iframe").style.zIndex="998";
                    }
                } 
            }); 
        </script>
    

    

    
    <script type="text/javascript" color="0,0,255"
        pointColor="0,0,255" opacity='0.7'
        zIndex="-1" count="99"
        src="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/libs/background/canvas-nest.js"></script>
    

    

    
    <script type="text/javascript" src="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/libs/background/ribbon-dynamic.js" async="async"></script>
    
    
    
    <script src="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/libs/instantpage/instantpage.js" type="module"></script>
    

        <script src="//cdn.jsdelivr.net/npm/js-base64/base64.min.js"></script>
        <script>
        $('a').each(function() {
          const $this = $(this);
          const href = $this.attr('href');
          if (href && href.match('^((http|https|thunder|qqdl|ed2k|Flashget|qbrowser|ftp|rtsp|mms)://)')) {
            const strs = href.split('/');
            if (strs.length >= 3) {
                const host = strs[2];
                if (host !== 'your_domain' || window.location.host) {
                    $this.attr('href', '/go.html?u='+Base64.encode(href)+'').attr('rel', 'external nofollow noopener noreferrer');
                    if (true) {
                        $this.attr('target', '_blank');
                    }
                }
            }
          }
        });
        </script><script>!function(e){var c=Array.prototype.slice.call(document.querySelectorAll("img[data-original]"));function i(){for(var r=0;r<c.length;r++)t=c[r],0<=(n=t.getBoundingClientRect()).bottom&&0<=n.left&&n.top<=(e.innerHeight||document.documentElement.clientHeight)&&function(){var t,n,e,i,o=c[r];t=o,n=function(){c=c.filter(function(t){return o!==t})},e=new Image,i=t.getAttribute("data-original"),e.onload=function(){t.src=i,n&&n()},e.src=i}();var t,n}i(),e.addEventListener("scroll",function(){var t,n;t=i,n=e,clearTimeout(t.tId),t.tId=setTimeout(function(){t.call(n)},500)})}(this);</script><script>window.addEventListener("load",function(){var t=/\.(gif|jpg|jpeg|tiff|png)$/i,r=/^data:image\/[a-z]+;base64,/;Array.prototype.slice.call(document.querySelectorAll("img[data-original]")).forEach(function(a){var e=a.parentNode;"A"===e.tagName&&(e.href.match(t)||e.href.match(r))&&(e.href=a.dataset.original)})});</script></body>

</html>

