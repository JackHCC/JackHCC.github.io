<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Gensim API, JackHCC">
    <meta name="description" content="Gensim Python包详解">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <title>Gensim API | JackHCC</title>
    <link rel="icon" type="image/png" href="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/medias/favicon.png">

    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/libs/awesome/css/all.css">
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/libs/materialize/materialize.min.css">
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/libs/aos/aos.css">
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/libs/animate/animate.min.css">
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/libs/lightGallery/css/lightgallery.min.css">
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/css/matery.css">
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/css/my.css">
    
    <script src="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/libs/jquery/jquery.min.js"></script>
    
<meta name="generator" content="Hexo 4.2.0"><link rel="alternate" href="/atom.xml" title="JackHCC" type="application/atom+xml">
<link rel="stylesheet" href="/css/prism-hopscotch.css" type="text/css">
<link rel="stylesheet" href="/css/prism-line-numbers.css" type="text/css"></head>


<body>
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper head-container">
            <div class="brand-logo">
                <a href="/" class="waves-effect waves-light">
                    
                    <img src="/images/loading.gif" data-original="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/medias/me.jpg" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">JackHCC</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="" class="waves-effect waves-light">

      
      <i class="fas fa-list" style="zoom: 0.6;"></i>
      
      <span>Tools</span>
      <i class="fas fa-chevron-down" aria-hidden="true" style="zoom: 0.6;"></i>
    </a>
    <ul class="sub-nav menus_item_child ">
      
      <li>
        <a href="https://creativecc.cn/" target="_blank" rel="noopener">
          
          <i class="fas fa-book" style="margin-top: -20px; zoom: 0.6;"></i>
          
          <span>Creative工具导航</span>
        </a>
      </li>
      
      <li>
        <a href="https://blog.creativecc.cn/Arxiv-NLP-Reporter/" target="_blank" rel="noopener">
          
          <i class="fas fa-film" style="margin-top: -20px; zoom: 0.6;"></i>
          
          <span>NLP每日论文</span>
        </a>
      </li>
      
      <li>
        <a href="http://chat.creativecc.cn/" target="_blank" rel="noopener">
          
          <i class="fas fa-music" style="margin-top: -20px; zoom: 0.6;"></i>
          
          <span>RocketChat聊天室</span>
        </a>
      </li>
      
      <li>
        <a href="/contact">
          
          <i class="fas fa-comments" style="margin-top: -20px; zoom: 0.6;"></i>
          
          <span>Contact留言板</span>
        </a>
      </li>
      
    </ul>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/friends" class="waves-effect waves-light">
      
      <i class="fas fa-address-book" style="zoom: 0.6;"></i>
      
      <span>友情链接</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/about" class="waves-effect waves-light">
      
      <i class="fas fa-user-circle" style="zoom: 0.6;"></i>
      
      <span>关于</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>

<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/images/loading.gif" data-original="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/medias/me.jpg" class="logo-img circle responsive-img">
        
        <div class="logo-name">JackHCC</div>
        <div class="logo-desc">
            
            Make the world betterrrr!!!
            
        </div>
    </div>

    

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="javascript:;">
			
				<i class="fa-fw fas fa-list"></i>
			
			Tools
			<span class="m-icon"><i class="fas fa-chevron-right"></i></span>
		</a>
            <ul  style="background:  ;" >
              
                <li>   
				
                  <a href="https://creativecc.cn/ " target="_blank" rel="noopener" style="margin-left:75px";>
				  
				   <i class="fa fas fa-book" style="position: absolute;left:50px" ></i>
			      
		          <span>Creative工具导航</span>
                  </a>
                </li>
              
                <li>   
				
                  <a href="https://blog.creativecc.cn/Arxiv-NLP-Reporter/ " target="_blank" rel="noopener" style="margin-left:75px";>
				  
				   <i class="fa fas fa-film" style="position: absolute;left:50px" ></i>
			      
		          <span>NLP每日论文</span>
                  </a>
                </li>
              
                <li>   
				
                  <a href="http://chat.creativecc.cn/ " target="_blank" rel="noopener" style="margin-left:75px";>
				  
				   <i class="fa fas fa-music" style="position: absolute;left:50px" ></i>
			      
		          <span>RocketChat聊天室</span>
                  </a>
                </li>
              
                <li>   
				
                  <a href="/contact " style="margin-left:75px";>
				  
				   <i class="fa fas fa-comments" style="position: absolute;left:50px" ></i>
			      
		          <span>Contact留言板</span>
                  </a>
                </li>
               
            </ul>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/friends" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-address-book"></i>
			
			友情链接
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/about" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-user-circle"></i>
			
			关于
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/JackHCC/JackHCC.github.io" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>

        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/JackHCC/JackHCC.github.io" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    
<script src="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/libs/cryptojs/crypto-js.min.js"></script>
<script>
    (function() {
        let pwd = '';
        if (pwd && pwd.length > 0) {
            if (pwd !== CryptoJS.SHA256(prompt('请输入访问本文章的密码')).toString(CryptoJS.enc.Hex)) {
                alert('密码错误，将返回主页！');
                location.href = '/';
            }
        }
    })();
</script>




<div class="bg-cover pd-header post-cover" style="background-image: url('https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/medias/featureimages/17.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Gensim API</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/libs/tocbot/tocbot.css">
<style>
    #articleContent h1::before,
    #articleContent h2::before,
    #articleContent h3::before,
    #articleContent h4::before,
    #articleContent h5::before,
    #articleContent h6::before {
        display: block;
        content: " ";
        height: 100px;
        margin-top: -100px;
        visibility: hidden;
    }

    #articleContent :focus {
        outline: none;
    }

    .toc-fixed {
        position: fixed;
        top: 64px;
    }

    .toc-widget {
        width: 345px;
        padding-left: 20px;
    }

    .toc-widget .toc-title {
        margin: 35px 0 15px 0;
        padding-left: 17px;
        font-size: 1.5rem;
        font-weight: bold;
        line-height: 1.5rem;
    }

    .toc-widget ol {
        padding: 0;
        list-style: none;
    }

    #toc-content {
        height: calc(100vh - 250px);
        overflow: auto;
    }

    #toc-content ol {
        padding-left: 10px;
    }

    #toc-content ol li {
        padding-left: 10px;
    }

    #toc-content .toc-link:hover {
        color: #42b983;
        font-weight: 700;
        text-decoration: underline;
    }

    #toc-content .toc-link::before {
        background-color: transparent;
        max-height: 25px;
    }

    #toc-content .is-active-link {
        color: #42b983;
    }

    #toc-content .is-active-link::before {
        background-color: #42b983;
    }

    #floating-toc-btn {
        position: fixed;
        right: 30px;
        bottom: 146px;
        padding-top: 15px;
        margin-bottom: 0;
        z-index: 998;
    }

    #floating-toc-btn .btn-floating {
        width: 48px;
        height: 48px;
    }

    #floating-toc-btn .btn-floating i {
        line-height: 48px;
        font-size: 1.4rem;
    }
</style>
<div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/tags/NLP/">
                                <span class="chip bg-color">NLP</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/categories/Deep-Learning/" class="post-category">
                                Deep Learning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2021-08-09
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2021-09-04
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    35.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    169 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>

        </div>
        <hr class="clearfix">
        <div class="card-content article-card-content">
            <div id="articleContent">
                <h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><p><a href="https://radimrehurek.com/gensim/apiref.html" target="_blank" rel="noopener">Gensim API</a></p>
<p>Gensim是一个<a href="https://radimrehurek.com/gensim/intro.html#availability" target="_blank" rel="noopener">免费的</a> Python库，旨在从文档中自动提取语义主题，尽可能高效（计算机方面）和 painlessly（人性化）。</p>
<p>Gensim旨在处理原始的非结构化数字文本（<strong><code>纯文本</code></strong>）。</p>
<p>在Gensim的算法，比如<a href="https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec" target="_blank" rel="noopener"><code>Word2Vec</code></a>，<a href="https://radimrehurek.com/gensim/models/fasttext.html#gensim.models.fasttext.FastText" target="_blank" rel="noopener"><code>FastText</code></a>，潜在语义分析（LSI，LSA，see <a href="https://radimrehurek.com/gensim/models/lsimodel.html#gensim.models.lsimodel.LsiModel" target="_blank" rel="noopener"><code>LsiModel</code></a>），隐含狄利克雷分布（LDA，见<a href="https://radimrehurek.com/gensim/models/ldamodel.html#gensim.models.ldamodel.LdaModel" target="_blank" rel="noopener"><code>LdaModel</code></a>）等，自动训练文档的躯体内检查统计共生模式发现的文件的语义结构。这些算法是<strong>无监督的</strong>，这意味着不需要人工输入 - 您只需要一个纯文本文档。</p>
<p>一旦找到这些统计模式，任何纯文本文档（句子，短语，单词……）都可以在新的语义表示中简洁地表达，并查询与其他文档（单词，短语……）的主题相似性。</p>
<blockquote>
<p>注意 如果前面的段落让您感到困惑，您可以在Wikipedia上阅读有关<a href="https://en.wikipedia.org/wiki/Vector_space_model" target="_blank" rel="noopener">向量空间模型</a>和<a href="https://en.wikipedia.org/wiki/Latent_semantic_indexing" target="_blank" rel="noopener">无监督文档分析的</a>更多信息。</p>
</blockquote>
<h2 id="功能"><a href="#功能" class="headerlink" title="功能"></a>功能</h2><ul>
<li><strong>内存独立性</strong> - 任何时候都不需要整个训练语料库完全驻留在RAM中（可以处理大型的Web级语料库）。</li>
<li><strong>内存共享</strong> - 经过训练的模型可以持久保存到磁盘并通过<a href="https://en.wikipedia.org/wiki/Mmap" target="_blank" rel="noopener">mmap</a>加载回来。多个进程可以共享相同的数据，从而减少RAM占用空间。</li>
<li>一些流行的向量空间算法的高效实现，包括<a href="https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec" target="_blank" rel="noopener"><code>Word2Vec</code></a>，<a href="https://radimrehurek.com/gensim/models/doc2vec.html#gensim.models.doc2vec.Doc2Vec" target="_blank" rel="noopener"><code>Doc2Vec</code></a>，<a href="https://radimrehurek.com/gensim/models/fasttext.html#gensim.models.fasttext.FastText" target="_blank" rel="noopener"><code>FastText</code></a>，TF-IDF，潜在语义分析（LSI，LSA，见<a href="https://radimrehurek.com/gensim/models/lsimodel.html#gensim.models.lsimodel.LsiModel" target="_blank" rel="noopener"><code>LsiModel</code></a>），隐含狄利克雷分布（LDA，见<a href="https://radimrehurek.com/gensim/models/ldamodel.html#gensim.models.ldamodel.LdaModel" target="_blank" rel="noopener"><code>LdaModel</code></a>）或随机投影（见<a href="https://radimrehurek.com/gensim/models/rpmodel.html#gensim.models.rpmodel.RpModel" target="_blank" rel="noopener"><code>RpModel</code></a>）。</li>
<li>来自几种流行数据格式的I / O包装器和读卡器。</li>
<li>在语义表示中对文档进行快速相似性查询。</li>
</ul>
<p>Gensim背后的<strong>主要设计目标</strong>是：</p>
<ol>
<li>为开发人员提供简单的接口和低API学习曲线。适合原型设计。</li>
<li>记忆独立性与输入语料库的大小有关; 所有中间步骤和算法都以流式方式运行，一次访问一个文档。</li>
</ol>
<p>我们还为NLP，文档分析，索引，搜索和集群构建了一个高性能的商业服务器：<a href="https://scaletext.ai/" target="_blank" rel="noopener">https://scaletext.ai</a>。ScaleText既可以在本地使用，也可以作为SaaS使用。</p>
<h2 id="可用性"><a href="#可用性" class="headerlink" title="可用性"></a>可用性</h2><p>Gensim根据OSI批准的<a href="https://www.gnu.org/licenses/old-licenses/lgpl-2.1.en.html" target="_blank" rel="noopener">GNU LGPLv2.1许可证授权</a>，可以从其<a href="https://github.com/piskvorky/gensim/" target="_blank" rel="noopener">Github存储库</a> 或<a href="https://pypi.python.org/pypi/gensim" target="_blank" rel="noopener">Python Package Index下载</a>。</p>
<p>也可以看看</p>
<p>有关Gensim部署的更多信息，请参阅<a href="https://radimrehurek.com/gensim/install.html" target="_blank" rel="noopener">安装</a>页面。</p>
<h2 id="核心概念"><a href="#核心概念" class="headerlink" title="核心概念"></a>核心概念</h2><blockquote>
<p>文集</p>
</blockquote>
<p>数字文档的集合。Corpora在Gensim担任两个角色：</p>
<ol>
<li><p>模型训练的输入语料库用于自动训练机器学习模型，例如 <a href="https://radimrehurek.com/gensim/models/lsimodel.html#gensim.models.lsimodel.LsiModel" target="_blank" rel="noopener"><code>LsiModel</code></a>或<a href="https://radimrehurek.com/gensim/models/ldamodel.html#gensim.models.ldamodel.LdaModel" target="_blank" rel="noopener"><code>LdaModel</code></a>。</p>
<p>模型使用此<em>培训语料库</em>来查找共同的主题和主题，初始化其内部模型参数。</p>
<p>Gensim专注于<em>无监督</em>模型，因此无需人工干预，例如昂贵的注释或手工标记文件。</p>
</li>
<li><p>要组织的文件。训练之后，可以使用主题模型从新文档中提取主题（培训语料库中未见的文档）。</p>
<p>这样的语料库可以通过语义相似性，聚类等进行<a href="https://radimrehurek.com/gensim/tut3.html" target="_blank" rel="noopener">索引</a>，查询。</p>
</li>
</ol>
<blockquote>
<p>向量空间模型</p>
</blockquote>
<p>在向量空间模型（VSM）中，每个文档由一系列要素表示。例如，单个功能可以被视为问答配对：</p>
<ol>
<li>单词<em>splonge</em>在文档中出现了多少次？零。</li>
<li>该文件包含多少段？二。</li>
<li>该文档使用了多少种字体？五。</li>
</ol>
<p>问题通常只能由它的一个整数标识符（如所表示1，2和3在此），因此，该文件的表示变得一系列像<code>(1, 0.0), (2, 2.0), (3, 5.0)</code> 对。</p>
<p>如果我们提前知道所有问题，我们可能会隐瞒并简单地写成 <code>(0.0, 2.0, 5.0)</code>。</p>
<p>该答案序列可以被认为是 <strong>向量(vector)</strong>（在这种情况下是三维密集矢量）。出于实际目的，Gensim中只允许答案（或可以转换为）<em>单个浮点数的问题</em>。</p>
<p>每个文档的问题都是相同的，所以看两个向量（代表两个文档），我们希望能够得出结论，例如“这两个向量中的数字非常相似，因此原始文档必须类似“也是”。当然，这些结论是否与现实相符取决于我们选择问题的程度。</p>
<blockquote>
<p>Gensim 稀疏向量，Bag-of-words 向量</p>
</blockquote>
<p>为了节省空间，在Gensim中我们省略了值为0.0的所有向量元素。例如，我们只写（注意缺失的）而不是三维密集向量。每个向量元素是一对（2元组）。此稀疏表示中所有缺失特征的值可以明确地解析为零。<code>(0.0,&lt;span&gt; 2.0,&lt;span&gt; 5.0)``[(2,&lt;span&gt; 2.0),&lt;span&gt; (3,&lt;span&gt; 5.0)]``(1,&lt;span&gt; 0.0)``(feature_id,feature_value)``0.0</code></p>
<p>Gensim中的文档由稀疏向量（有时称为词袋向量）表示。</p>
<blockquote>
<p>Gensim流式语料库</p>
</blockquote>
<p>Gensim没有规定任何特定的语料库格式。语料库只是一个稀疏向量序列（见上文）。</p>
<p>例如 <code>[ [(2, 2.0), (3, 5.0)], [(3, 1.0)] ]</code> 是两个文档的简单语料库=两个稀疏向量：第一个具有两个非零元素，第二个具有一个非零元素。这个特定的语料库表示为普通的Python 。</p>
<p>然而，Gensim的全部功能来自于语料库不必是a <code>list</code>，或<code>NumPy</code>数组，或<code>Pandas</code>数据帧等等。Gensim <em>接受任何对象，当迭代时，连续产生这些稀疏的袋子向量</em>。</p>
<p>这种灵活性允许您创建自己的语料库类，直接从磁盘，网络，数据库，数据帧…流式传输稀疏向量。实现Gensim中的模型，使得它们不需要所有向量一次驻留在RAM中。你甚至可以动态创建稀疏矢量！</p>
<p><a href="https://rare-technologies.com/data-streaming-in-python-generators-iterators-iterables/" target="_blank" rel="noopener">Python流数据处理教程</a>。</p>
<p>有关直接从磁盘流式传输的高效语料库格式的内置示例，请参阅中的Matrix Market格式<a href="https://radimrehurek.com/gensim/corpora/mmcorpus.html#module-gensim.corpora.mmcorpus" target="_blank" rel="noopener"><code>mmcorpus</code></a>。有关如何创建自己的流式语料库的最小蓝图示例，请查看<a href="https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/corpora/csvcorpus.py" target="_blank" rel="noopener">CSV语料库</a>的<a href="https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/corpora/csvcorpus.py" target="_blank" rel="noopener">源代码</a>。</p>
<blockquote>
<p>模型，转型</p>
</blockquote>
<p>Gensim使用<strong>模型</strong>来引用将一个文档表示转换为另一个文档表示所需的代码和相关数据（模型参数）。</p>
<p>在Gensim中，文档被表示为向量（见上文），因此模型可以被认为是从一个向量空间到另一个向量空间的转换。从训练语料库中学习该变换的参数。</p>
<p>训练有素的模型（数据参数）可以持久保存到磁盘，然后加载回来，以继续培训新的培训文档或转换新文档。</p>
<p>Gensim实现多种模式，如<a href="https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec" target="_blank" rel="noopener"><code>Word2Vec</code></a>， <a href="https://radimrehurek.com/gensim/models/lsimodel.html#gensim.models.lsimodel.LsiModel" target="_blank" rel="noopener"><code>LsiModel</code></a>，<a href="https://radimrehurek.com/gensim/models/ldamodel.html#gensim.models.ldamodel.LdaModel" target="_blank" rel="noopener"><code>LdaModel</code></a>， <a href="https://radimrehurek.com/gensim/models/fasttext.html#gensim.models.fasttext.FastText" target="_blank" rel="noopener"><code>FastText</code></a>等见<a href="https://radimrehurek.com/gensim/apiref.html" target="_blank" rel="noopener">API参考</a>的完整列表。</p>
<h1 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h1><h2 id="快速安装"><a href="#快速安装" class="headerlink" title="快速安装"></a>快速安装</h2><p>在您的终端中运行（推荐）：</p>
<pre><code>pip install --upgrade gensim</code></pre><p>或者，对于conda环境：</p>
<pre><code>conda install -c conda-forge gensim</code></pre><hr>
<h2 id="代码依赖"><a href="#代码依赖" class="headerlink" title="代码依赖"></a>代码依赖</h2><p>Gensim在Linux，Windows和Mac OS X上运行，并且应该在支持Python 2.7+和NumPy的任何其他平台上运行。Gensim取决于以下软件：</p>
<ul>
<li><a href="https://www.python.org/" target="_blank" rel="noopener">Python</a> &gt;= 2.7 (tested with versions 2.7, 3.5 and 3.6)</li>
<li><a href="http://www.numpy.org/" target="_blank" rel="noopener">NumPy</a> &gt;= 1.11.3</li>
<li><a href="https://www.scipy.org/" target="_blank" rel="noopener">SciPy</a> &gt;= 0.18.1</li>
<li><a href="https://pypi.org/project/six/" target="_blank" rel="noopener">Six</a> &gt;= 1.5.0</li>
<li><a href="https://pypi.org/project/smart_open/" target="_blank" rel="noopener">smart_open</a> &gt;= 1.2.1</li>
</ul>
<h1 id="Core-概要"><a href="#Core-概要" class="headerlink" title="Core: 概要"></a>Core: 概要</h1><ul>
<li>语料库和向量空间<ul>
<li>从字符串到向量</li>
<li>语料库流 - 一次一个文档</li>
<li>语料库格式</li>
<li>与NumPy和SciPy的兼容性</li>
</ul>
</li>
<li>主题和转换<ul>
<li>转换界面</li>
<li>可用的转换</li>
</ul>
</li>
<li>相似性查询<ul>
<li>相似界面</li>
<li>下一个在哪里</li>
</ul>
</li>
<li>英语维基百科上的实验<ul>
<li>准备语料库</li>
<li>潜在语义分析</li>
<li>潜在的Dirichlet分配</li>
</ul>
</li>
<li>分布式计算<ul>
<li>为何分布式计算？</li>
<li>先决条件</li>
<li>核心概念</li>
<li>可用分布式算法</li>
</ul>
</li>
</ul>
<h2 id="准备"><a href="#准备" class="headerlink" title="准备"></a>准备</h2><p>所有示例都可以直接复制到Python解释器shell。<a href="http://ipython.scipy.org/" target="_blank" rel="noopener">IPython</a>的 <code>cpaste</code> 命令对于复制代码片段（包括主要 <code>&gt;&gt;&gt;</code> 字符）特别方便。</p>
<p>Gensim使用Python的标准 <code>logging</code> 模块来记录各种优先级的各种东西; 要激活日志记录（这是可选的），请运行</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">import</span> logging
<span class="token operator">>></span><span class="token operator">></span> logging<span class="token punctuation">.</span>basicConfig<span class="token punctuation">(</span>format<span class="token operator">=</span><span class="token string">'%(asctime)s : %(levelname)s : %(message)s'</span><span class="token punctuation">,</span> level<span class="token operator">=</span>logging<span class="token punctuation">.</span>INFO<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>
<h2 id="快速示例"><a href="#快速示例" class="headerlink" title="快速示例"></a>快速示例</h2><p>首先，让我们导入gensim并创建一个包含九个文档和十二个特征的小型语料库：</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">from</span> gensim <span class="token keyword">import</span> corpora<span class="token punctuation">,</span> models<span class="token punctuation">,</span> similarities
<span class="token operator">>></span><span class="token operator">></span>
<span class="token operator">>></span><span class="token operator">></span> corpus <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1.0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1.0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1.0</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
<span class="token operator">>></span><span class="token operator">></span>           <span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1.0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">1.0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">1.0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">1.0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">1.0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">1.0</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
<span class="token operator">>></span><span class="token operator">></span>           <span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1.0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">1.0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">1.0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">1.0</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
<span class="token operator">>></span><span class="token operator">></span>           <span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1.0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">2.0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">1.0</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
<span class="token operator">>></span><span class="token operator">></span>           <span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">1.0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">1.0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">1.0</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
<span class="token operator">>></span><span class="token operator">></span>           <span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token number">9</span><span class="token punctuation">,</span> <span class="token number">1.0</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
<span class="token operator">>></span><span class="token operator">></span>           <span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token number">9</span><span class="token punctuation">,</span> <span class="token number">1.0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">1.0</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
<span class="token operator">>></span><span class="token operator">></span>           <span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token number">9</span><span class="token punctuation">,</span> <span class="token number">1.0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">1.0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">11</span><span class="token punctuation">,</span> <span class="token number">1.0</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
<span class="token operator">>></span><span class="token operator">></span>           <span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">1.0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">1.0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">11</span><span class="token punctuation">,</span> <span class="token number">1.0</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>在Gensim中，<em>语料库</em>只是一个对象，当迭代时，返回其表示为稀疏向量的文档。在这种情况下，我们使用元组列表的列表。如果您不熟悉<a href="https://en.wikipedia.org/wiki/Vector_space_model" target="_blank" rel="noopener">矢量空间模型</a>，我们将在下一个关于<a href="https://radimrehurek.com/gensim/tut1.html" target="_blank" rel="noopener">Corpora和Vector Spaces的</a>教程中弥合<strong>原始字符串</strong>，<strong>语料库</strong>和<strong>稀疏矢量</strong>之间的差距。</p>
<p>如果您熟悉向量空间模型，您可能会知道解析文档并将其转换为向量的方式会对任何后续应用程序的质量产生重大影响。</p>
<blockquote>
<p>注意: 在此示例中，整个语料库作为Python列表存储在内存中。但是，语料库接口只表示语料库必须支持对其组成文档的迭代。对于非常大的语料库，有利的是将语料库保持在磁盘上，并且一次一个地顺序访问其文档。所有操作和转换都以这样的方式实现，使得它们在内存方面独立于语料库的大小。</p>
</blockquote>
<p>接下来，让我们初始化一个<em>转换</em>：</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> tfidf <span class="token operator">=</span> models<span class="token punctuation">.</span>TfidfModel<span class="token punctuation">(</span>corpus<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<p>转换用于将文档从一个向量表示转换为另一个向量表示：</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> vec <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">]</span>
<span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">print</span><span class="token punctuation">(</span>tfidf<span class="token punctuation">[</span>vec<span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0.8075244</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">0.5898342</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>
<p>在这里，我们使用了<a href="https://en.wikipedia.org/wiki/Tf–idf" target="_blank" rel="noopener">Tf-Idf</a>，这是一种简单的转换，它将文档表示为词袋计数，并应用对常用术语进行折扣的权重（或者等同于促销稀有术语）。它还将得到的向量缩放到单位长度（在<a href="https://en.wikipedia.org/wiki/Norm_(mathematics)#Euclidean_norm" target="_blank" rel="noopener">欧几里德范数中</a>）。</p>
<p><a href="https://radimrehurek.com/gensim/tut2.html" target="_blank" rel="noopener">主题和转换</a>教程中详细介绍了<a href="https://radimrehurek.com/gensim/tut2.html" target="_blank" rel="noopener">转换</a>。</p>
<p>要通过TfIdf转换整个语料库并对其进行索引，以准备相似性查询：</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> index <span class="token operator">=</span> similarities<span class="token punctuation">.</span>SparseMatrixSimilarity<span class="token punctuation">(</span>tfidf<span class="token punctuation">[</span>corpus<span class="token punctuation">]</span><span class="token punctuation">,</span> num_features<span class="token operator">=</span><span class="token number">12</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<p>并查询我们的查询向量<code>&lt;span class="pre"&gt;vec&lt;/span&gt;</code>与语料库中每个文档的相似性：</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> sims <span class="token operator">=</span> index<span class="token punctuation">[</span>tfidf<span class="token punctuation">[</span>vec<span class="token punctuation">]</span><span class="token punctuation">]</span>
<span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">print</span><span class="token punctuation">(</span>list<span class="token punctuation">(</span>enumerate<span class="token punctuation">(</span>sims<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0.4662244</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0.19139354</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">0.24600551</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">0.82094586</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">0.0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">0.0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">0.0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">0.0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">0.0</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>
<p>如何阅读此输出？文档编号为零（第一个文档）的相似度得分为0.466 = 46.6％，第二个文档的相似度得分为19.1％等。</p>
<p>因此，根据TfIdf文档表示和余弦相似性度量，最类似于我们的查询文档vec是文档号。3，相似度得分为82.1％。请注意，在TfIdf表示中，任何不具有任何共同特征的 <code>vec</code> 文档（文档编号4-8）的相似性得分均为0.0。有关更多详细信息，请参阅<a href="https://radimrehurek.com/gensim/tut3.html" target="_blank" rel="noopener">Similarity Queries</a>教程。</p>
<h1 id="语料库和向量空间"><a href="#语料库和向量空间" class="headerlink" title="语料库和向量空间"></a>语料库和向量空间</h1><p><a href="https://radimrehurek.com/gensim" target="_blank" rel="noopener">Gensim官网</a></p>
<p>别忘了设置</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">import</span> logging
<span class="token operator">>></span><span class="token operator">></span> logging<span class="token punctuation">.</span>basicConfig<span class="token punctuation">(</span>format<span class="token operator">=</span><span class="token string">'%(asctime)s : %(levelname)s : %(message)s'</span><span class="token punctuation">,</span> level<span class="token operator">=</span>logging<span class="token punctuation">.</span>INFO<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>
<p>如果你想看到记录事件。</p>
<h2 id="从字符串到向量"><a href="#从字符串到向量" class="headerlink" title="从字符串到向量"></a>从字符串到向量</h2><p>这一次，让我们从表示为字符串的文档开始：</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">from</span> gensim <span class="token keyword">import</span> corpora
<span class="token operator">>></span><span class="token operator">></span>
<span class="token operator">>></span><span class="token operator">></span> documents <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">"Human machine interface for lab abc computer applications"</span><span class="token punctuation">,</span>
<span class="token operator">>></span><span class="token operator">></span>              <span class="token string">"A survey of user opinion of computer system response time"</span><span class="token punctuation">,</span>
<span class="token operator">>></span><span class="token operator">></span>              <span class="token string">"The EPS user interface management system"</span><span class="token punctuation">,</span>
<span class="token operator">>></span><span class="token operator">></span>              <span class="token string">"System and human system engineering testing of EPS"</span><span class="token punctuation">,</span>
<span class="token operator">>></span><span class="token operator">></span>              <span class="token string">"Relation of user perceived response time to error measurement"</span><span class="token punctuation">,</span>
<span class="token operator">>></span><span class="token operator">></span>              <span class="token string">"The generation of random binary unordered trees"</span><span class="token punctuation">,</span>
<span class="token operator">>></span><span class="token operator">></span>              <span class="token string">"The intersection graph of paths in trees"</span><span class="token punctuation">,</span>
<span class="token operator">>></span><span class="token operator">></span>              <span class="token string">"Graph minors IV Widths of trees and well quasi ordering"</span><span class="token punctuation">,</span>
<span class="token operator">>></span><span class="token operator">></span>              <span class="token string">"Graph minors A survey"</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>这是一个由九个文档组成的小型语料库，每个文档只包含一个句子。</p>
<p>首先，让我们对文档进行标记，删除常用单词（使用玩具停止列表）以及仅在语料库中出现一次的单词：</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> <span class="token comment" spellcheck="true"># remove common words and tokenize</span>
<span class="token operator">>></span><span class="token operator">></span> stoplist <span class="token operator">=</span> set<span class="token punctuation">(</span><span class="token string">'for a of the and to in'</span><span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token operator">>></span><span class="token operator">></span> texts <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">[</span>word <span class="token keyword">for</span> word <span class="token keyword">in</span> document<span class="token punctuation">.</span>lower<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">if</span> word <span class="token operator">not</span> <span class="token keyword">in</span> stoplist<span class="token punctuation">]</span>
<span class="token operator">>></span><span class="token operator">></span>          <span class="token keyword">for</span> document <span class="token keyword">in</span> documents<span class="token punctuation">]</span>
<span class="token operator">>></span><span class="token operator">></span>
<span class="token operator">>></span><span class="token operator">></span> <span class="token comment" spellcheck="true"># remove words that appear only once</span>
<span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">from</span> collections <span class="token keyword">import</span> defaultdict
<span class="token operator">>></span><span class="token operator">></span> frequency <span class="token operator">=</span> defaultdict<span class="token punctuation">(</span>int<span class="token punctuation">)</span>
<span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">for</span> text <span class="token keyword">in</span> texts<span class="token punctuation">:</span>
<span class="token operator">>></span><span class="token operator">></span>     <span class="token keyword">for</span> token <span class="token keyword">in</span> text<span class="token punctuation">:</span>
<span class="token operator">>></span><span class="token operator">></span>         frequency<span class="token punctuation">[</span>token<span class="token punctuation">]</span> <span class="token operator">+=</span> <span class="token number">1</span>
<span class="token operator">>></span><span class="token operator">></span>
<span class="token operator">>></span><span class="token operator">></span> texts <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">[</span>token <span class="token keyword">for</span> token <span class="token keyword">in</span> text <span class="token keyword">if</span> frequency<span class="token punctuation">[</span>token<span class="token punctuation">]</span> <span class="token operator">></span> <span class="token number">1</span><span class="token punctuation">]</span>
<span class="token operator">>></span><span class="token operator">></span>          <span class="token keyword">for</span> text <span class="token keyword">in</span> texts<span class="token punctuation">]</span>
<span class="token operator">>></span><span class="token operator">></span>
<span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">from</span> pprint <span class="token keyword">import</span> pprint  <span class="token comment" spellcheck="true"># pretty-printer</span>
<span class="token operator">>></span><span class="token operator">></span> pprint<span class="token punctuation">(</span>texts<span class="token punctuation">)</span>
<span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token string">'human'</span><span class="token punctuation">,</span> <span class="token string">'interface'</span><span class="token punctuation">,</span> <span class="token string">'computer'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
 <span class="token punctuation">[</span><span class="token string">'survey'</span><span class="token punctuation">,</span> <span class="token string">'user'</span><span class="token punctuation">,</span> <span class="token string">'computer'</span><span class="token punctuation">,</span> <span class="token string">'system'</span><span class="token punctuation">,</span> <span class="token string">'response'</span><span class="token punctuation">,</span> <span class="token string">'time'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
 <span class="token punctuation">[</span><span class="token string">'eps'</span><span class="token punctuation">,</span> <span class="token string">'user'</span><span class="token punctuation">,</span> <span class="token string">'interface'</span><span class="token punctuation">,</span> <span class="token string">'system'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
 <span class="token punctuation">[</span><span class="token string">'system'</span><span class="token punctuation">,</span> <span class="token string">'human'</span><span class="token punctuation">,</span> <span class="token string">'system'</span><span class="token punctuation">,</span> <span class="token string">'eps'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
 <span class="token punctuation">[</span><span class="token string">'user'</span><span class="token punctuation">,</span> <span class="token string">'response'</span><span class="token punctuation">,</span> <span class="token string">'time'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
 <span class="token punctuation">[</span><span class="token string">'trees'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
 <span class="token punctuation">[</span><span class="token string">'graph'</span><span class="token punctuation">,</span> <span class="token string">'trees'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
 <span class="token punctuation">[</span><span class="token string">'graph'</span><span class="token punctuation">,</span> <span class="token string">'minors'</span><span class="token punctuation">,</span> <span class="token string">'trees'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
 <span class="token punctuation">[</span><span class="token string">'graph'</span><span class="token punctuation">,</span> <span class="token string">'minors'</span><span class="token punctuation">,</span> <span class="token string">'survey'</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>处理文件的方式可能会有所不同; 在这里，只拆分空格来标记，然后小写每个单词。实际上，我使用这种特殊的（简单和低效）设置来模仿Deerwester等人的<a href="https://radimrehurek.com/gensim/tut1.html#id3" target="_blank" rel="noopener">原始LSA文章</a>所做的实验。</p>
<p>处理文档的方式是多种多样的，依赖于应用程序和语言，我决定<em>不</em>通过任何接口约束它们。相反，文档由从中提取的特征表示，而不是由其“表面”字符串形式表示：如何使用这些特征取决于您。下面我描述一种常见的通用方法（称为 <em>词袋</em>），但请记住，不同的应用程序域需要不同的功能，而且，一如既往，它是<a href="https://en.wikipedia.org/wiki/Garbage_In,_Garbage_Out" target="_blank" rel="noopener">垃圾，垃圾输出</a> ……</p>
<p>要将文档转换为向量，我们将使用名为<a href="https://en.wikipedia.org/wiki/Bag_of_words" target="_blank" rel="noopener">bag-of-words</a>的文档表示 。在此表示中，每个文档由一个向量表示，其中每个向量元素表示问题 - 答案对，格式为：</p>
<blockquote>
<p>“单词系统出现在文档中的次数是多少？一旦。”</p>
</blockquote>
<p>仅通过它们的（整数）id来表示问题是有利的。问题和ID之间的映射称为字典：</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> dictionary <span class="token operator">=</span> corpora<span class="token punctuation">.</span>Dictionary<span class="token punctuation">(</span>texts<span class="token punctuation">)</span>
<span class="token operator">>></span><span class="token operator">></span> dictionary<span class="token punctuation">.</span>save<span class="token punctuation">(</span><span class="token string">'/tmp/deerwester.dict'</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># store the dictionary, for future reference</span>
<span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">print</span><span class="token punctuation">(</span>dictionary<span class="token punctuation">)</span>
Dictionary<span class="token punctuation">(</span><span class="token number">12</span> unique tokens<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>
<p>在这里，我们为语料库中出现的所有单词分配了一个唯一的整数id <a href="https://radimrehurek.com/gensim/corpora/dictionary.html#gensim.corpora.dictionary.Dictionary" target="_blank" rel="noopener"><code>gensim.corpora.dictionary.Dictionary</code></a>。这会扫描文本，收集字数和相关统计数据。最后，我们看到在处理过的语料库中有12个不同的单词，这意味着每个文档将由12个数字表示（即，通过12-D向量）。要查看单词及其ID之间的映射：</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">print</span><span class="token punctuation">(</span>dictionary<span class="token punctuation">.</span>token2id<span class="token punctuation">)</span>
<span class="token punctuation">{</span><span class="token string">'minors'</span><span class="token punctuation">:</span> <span class="token number">11</span><span class="token punctuation">,</span> <span class="token string">'graph'</span><span class="token punctuation">:</span> <span class="token number">10</span><span class="token punctuation">,</span> <span class="token string">'system'</span><span class="token punctuation">:</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token string">'trees'</span><span class="token punctuation">:</span> <span class="token number">9</span><span class="token punctuation">,</span> <span class="token string">'eps'</span><span class="token punctuation">:</span> <span class="token number">8</span><span class="token punctuation">,</span> <span class="token string">'computer'</span><span class="token punctuation">:</span> <span class="token number">0</span><span class="token punctuation">,</span>
<span class="token string">'survey'</span><span class="token punctuation">:</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token string">'user'</span><span class="token punctuation">:</span> <span class="token number">7</span><span class="token punctuation">,</span> <span class="token string">'human'</span><span class="token punctuation">:</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token string">'time'</span><span class="token punctuation">:</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token string">'interface'</span><span class="token punctuation">:</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token string">'response'</span><span class="token punctuation">:</span> <span class="token number">3</span><span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>
<p>要将标记化文档实际转换为向量：</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> new_doc <span class="token operator">=</span> <span class="token string">"Human computer interaction"</span>
<span class="token operator">>></span><span class="token operator">></span> new_vec <span class="token operator">=</span> dictionary<span class="token punctuation">.</span>doc2bow<span class="token punctuation">(</span>new_doc<span class="token punctuation">.</span>lower<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">print</span><span class="token punctuation">(</span>new_vec<span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># the word "interaction" does not appear in the dictionary and is ignored</span>
<span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>
<p>函数<code>doc2bow()</code>只计算每个不同单词的出现次数，将单词转换为整数单词id，并将结果作为稀疏向量返回。 因此，稀疏向量 <code>[(0, 1), (1, 1)]</code> 读取：在文档“人机交互”中，单词computer （id 0）和human（id 1）出现一次; 其他十个字典单词（隐含地）出现零次。</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> corpus <span class="token operator">=</span> <span class="token punctuation">[</span>dictionary<span class="token punctuation">.</span>doc2bow<span class="token punctuation">(</span>text<span class="token punctuation">)</span> <span class="token keyword">for</span> text <span class="token keyword">in</span> texts<span class="token punctuation">]</span>
<span class="token operator">>></span><span class="token operator">></span> corpora<span class="token punctuation">.</span>MmCorpus<span class="token punctuation">.</span>serialize<span class="token punctuation">(</span><span class="token string">'/tmp/deerwester.mm'</span><span class="token punctuation">,</span> corpus<span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># store to disk, for later use</span>
<span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">print</span><span class="token punctuation">(</span>corpus<span class="token punctuation">)</span>
<span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">]</span>
<span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">]</span>
<span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">]</span>
<span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">]</span>
<span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">]</span>
<span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token number">9</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">]</span>
<span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token number">9</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">]</span>
<span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token number">9</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">11</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">]</span>
<span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">11</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>到目前为止，应该清楚的是，矢量要素 <code>id=10</code> 代表问题“文字中出现多少次文字？”，前六个文件的答案为“零”，其余三个答案为“一” 。事实上，我们已经得到了与<a href="https://radimrehurek.com/gensim/tutorial.html#first-example" target="_blank" rel="noopener">快速示例</a>中完全相同的向量语料库。</p>
<h2 id="语料库流-一次一个文档"><a href="#语料库流-一次一个文档" class="headerlink" title="语料库流 - 一次一个文档"></a>语料库流 - 一次一个文档</h2><p>请注意，上面的语料库完全驻留在内存中，作为普通的Python列表。在这个简单的例子中，它并不重要，但为了使事情清楚，让我们假设语料库中有数百万个文档。将所有这些存储在RAM中是行不通的。相反，我们假设文档存储在磁盘上的文件中，每行一个文档。gensim只要求语料库必须能够一次返回一个文档向量：</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">class</span> <span class="token class-name">MyCorpus</span><span class="token punctuation">(</span>object<span class="token punctuation">)</span><span class="token punctuation">:</span>
<span class="token operator">>></span><span class="token operator">></span>     <span class="token keyword">def</span> <span class="token function">__iter__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
<span class="token operator">>></span><span class="token operator">></span>         <span class="token keyword">for</span> line <span class="token keyword">in</span> open<span class="token punctuation">(</span><span class="token string">'mycorpus.txt'</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
<span class="token operator">>></span><span class="token operator">></span>             <span class="token comment" spellcheck="true"># assume there's one document per line, tokens separated by whitespace</span>
<span class="token operator">>></span><span class="token operator">></span>             <span class="token keyword">yield</span> dictionary<span class="token punctuation">.</span>doc2bow<span class="token punctuation">(</span>line<span class="token punctuation">.</span>lower<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>在<a href="https://radimrehurek.com/gensim/mycorpus.txt" target="_blank" rel="noopener">此处</a>下载示例mycorpus.txt文件。假设每个文档在单个文件中占据一行并不重要; 您可以模拟<strong>iter</strong>函数以适合您的输入格式，无论它是什么。行走目录，解析XML，访问网络……只需解析输入以在每个文档中检索一个干净的标记列表，然后通过字典将标记转换为它们的ID，并在<strong>iter</strong>中生成生成的稀疏向量。</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> corpus_memory_friendly <span class="token operator">=</span> MyCorpus<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># doesn't load the corpus into memory!</span>
<span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">print</span><span class="token punctuation">(</span>corpus_memory_friendly<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>
<p>语料库现在是一个对象。我们没有定义任何打印方式，因此print只输出内存中对象的地址。不是很有用。要查看构成向量，让我们遍历语料库并打印每个文档向量（一次一个）：</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">for</span> vector <span class="token keyword">in</span> corpus_memory_friendly<span class="token punctuation">:</span>  <span class="token comment" spellcheck="true"># load one vector into memory at a time</span>
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>     <span class="token keyword">print</span><span class="token punctuation">(</span>vector<span class="token punctuation">)</span>
<span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">]</span>
<span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">]</span>
<span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">]</span>
<span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">]</span>
<span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">]</span>
<span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token number">9</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">]</span>
<span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token number">9</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">]</span>
<span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token number">9</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">11</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">]</span>
<span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">11</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>尽管输出与普通Python列表的输出相同，但语料库现在更加内存友好，因为一次最多只有一个向量驻留在RAM中。您的语料库现在可以随意扩展。</p>
<p>类似地，构造字典而不将所有文本加载到内存中：</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">from</span> six <span class="token keyword">import</span> iteritems
<span class="token operator">>></span><span class="token operator">></span> <span class="token comment" spellcheck="true"># collect statistics about all tokens</span>
<span class="token operator">>></span><span class="token operator">></span> dictionary <span class="token operator">=</span> corpora<span class="token punctuation">.</span>Dictionary<span class="token punctuation">(</span>line<span class="token punctuation">.</span>lower<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">for</span> line <span class="token keyword">in</span> open<span class="token punctuation">(</span><span class="token string">'mycorpus.txt'</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token operator">>></span><span class="token operator">></span> <span class="token comment" spellcheck="true"># remove stop words and words that appear only once</span>
<span class="token operator">>></span><span class="token operator">></span> stop_ids <span class="token operator">=</span> <span class="token punctuation">[</span>dictionary<span class="token punctuation">.</span>token2id<span class="token punctuation">[</span>stopword<span class="token punctuation">]</span> <span class="token keyword">for</span> stopword <span class="token keyword">in</span> stoplist
<span class="token operator">>></span><span class="token operator">></span>             <span class="token keyword">if</span> stopword <span class="token keyword">in</span> dictionary<span class="token punctuation">.</span>token2id<span class="token punctuation">]</span>
<span class="token operator">>></span><span class="token operator">></span> once_ids <span class="token operator">=</span> <span class="token punctuation">[</span>tokenid <span class="token keyword">for</span> tokenid<span class="token punctuation">,</span> docfreq <span class="token keyword">in</span> iteritems<span class="token punctuation">(</span>dictionary<span class="token punctuation">.</span>dfs<span class="token punctuation">)</span> <span class="token keyword">if</span> docfreq <span class="token operator">==</span> <span class="token number">1</span><span class="token punctuation">]</span>
<span class="token operator">>></span><span class="token operator">></span> dictionary<span class="token punctuation">.</span>filter_tokens<span class="token punctuation">(</span>stop_ids <span class="token operator">+</span> once_ids<span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># remove stop words and words that appear only once</span>
<span class="token operator">>></span><span class="token operator">></span> dictionary<span class="token punctuation">.</span>compactify<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># remove gaps in id sequence after words that were removed</span>
<span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">print</span><span class="token punctuation">(</span>dictionary<span class="token punctuation">)</span>
Dictionary<span class="token punctuation">(</span><span class="token number">12</span> unique tokens<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>这就是它的全部！至少就字袋表示而言。当然，我们用这种语料库做的是另一个问题; 如何计算不同单词的频率可能是有用的，这一点都不清楚。事实证明，它不是，我们需要首先对这个简单的表示应用转换，然后才能使用它来计算任何有意义的文档与文档的相似性。让我们简单地将注意力转向<em>语料库持久性</em>。</p>
<h2 id="语料库格式"><a href="#语料库格式" class="headerlink" title="语料库格式"></a>语料库格式</h2><p>存在几种用于将Vector Space语料库（〜矢量序列）序列化到磁盘的文件格式。 gensim通过前面提到的<em>流式语料库接口</em>实现它们：文件以懒惰的方式从（分别存储到）磁盘读取，一次一个文档，而不是一次将整个语料库读入主存储器。</p>
<p><a href="http://math.nist.gov/MatrixMarket/formats.html" target="_blank" rel="noopener">市场矩阵格式</a>是一种比较值得注意的文件格式。要以Matrix Market格式保存语料库：</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> <span class="token comment" spellcheck="true"># create a toy corpus of 2 documents, as a plain Python list</span>
<span class="token operator">>></span><span class="token operator">></span> corpus <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0.5</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">]</span>  <span class="token comment" spellcheck="true"># make one document empty, for the heck of it</span>
<span class="token operator">>></span><span class="token operator">></span>
<span class="token operator">>></span><span class="token operator">></span> corpora<span class="token punctuation">.</span>MmCorpus<span class="token punctuation">.</span>serialize<span class="token punctuation">(</span><span class="token string">'/tmp/corpus.mm'</span><span class="token punctuation">,</span> corpus<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>
<p>其他格式包括<a href="http://svmlight.joachims.org/" target="_blank" rel="noopener">Joachim的SVMlight格式</a>， <a href="https://www.cs.princeton.edu/~blei/lda-c/" target="_blank" rel="noopener">Blei的LDA-C格式</a>和 <a href="http://gibbslda.sourceforge.net/" target="_blank" rel="noopener">GibbsLDA ++格式</a>。</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> corpora<span class="token punctuation">.</span>SvmLightCorpus<span class="token punctuation">.</span>serialize<span class="token punctuation">(</span><span class="token string">'/tmp/corpus.svmlight'</span><span class="token punctuation">,</span> corpus<span class="token punctuation">)</span>
<span class="token operator">>></span><span class="token operator">></span> corpora<span class="token punctuation">.</span>BleiCorpus<span class="token punctuation">.</span>serialize<span class="token punctuation">(</span><span class="token string">'/tmp/corpus.lda-c'</span><span class="token punctuation">,</span> corpus<span class="token punctuation">)</span>
<span class="token operator">>></span><span class="token operator">></span> corpora<span class="token punctuation">.</span>LowCorpus<span class="token punctuation">.</span>serialize<span class="token punctuation">(</span><span class="token string">'/tmp/corpus.low'</span><span class="token punctuation">,</span> corpus<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>
<p>相反，要从Matrix Market文件加载语料库迭代器：</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> corpus <span class="token operator">=</span> corpora<span class="token punctuation">.</span>MmCorpus<span class="token punctuation">(</span><span class="token string">'/tmp/corpus.mm'</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<p>语料库对象是流，因此通常您将无法直接打印它们：</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">print</span><span class="token punctuation">(</span>corpus<span class="token punctuation">)</span>
MmCorpus<span class="token punctuation">(</span><span class="token number">2</span> documents<span class="token punctuation">,</span> <span class="token number">2</span> features<span class="token punctuation">,</span> <span class="token number">1</span> non<span class="token operator">-</span>zero entries<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>
<p>相反，要查看语料库的内容：</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> <span class="token comment" spellcheck="true"># one way of printing a corpus: load it entirely into memory</span>
<span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">print</span><span class="token punctuation">(</span>list<span class="token punctuation">(</span>corpus<span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># calling list() will convert any sequence to a plain Python list</span>
<span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0.5</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>
<p>要么</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> <span class="token comment" spellcheck="true"># another way of doing it: print one document at a time, making use of the streaming interface</span>
<span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">for</span> doc <span class="token keyword">in</span> corpus<span class="token punctuation">:</span>
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>     <span class="token keyword">print</span><span class="token punctuation">(</span>doc<span class="token punctuation">)</span>
<span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0.5</span><span class="token punctuation">)</span><span class="token punctuation">]</span>
<span class="token punctuation">[</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>第二种方式显然对内存更友好，但是出于测试和开发目的，没有什么比调用的简单性更好<code>list(corpus)</code>。</p>
<p>要以Blei的LDA-C格式保存相同的Matrix Market文档流</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> corpora<span class="token punctuation">.</span>BleiCorpus<span class="token punctuation">.</span>serialize<span class="token punctuation">(</span><span class="token string">'/tmp/corpus.lda-c'</span><span class="token punctuation">,</span> corpus<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<p>通过这种方式，gensim还可以用作内存高效的<strong>I / O格式转换工具</strong>：只需使用一种格式加载文档流，然后立即以另一种格式保存。添加新格式非常容易，请查看<a href="https://github.com/piskvorky/gensim/blob/develop/gensim/corpora/svmlightcorpus.py" target="_blank" rel="noopener">SVMlight语料库</a>的代码示例。</p>
<h2 id="与NumPy和SciPy的兼容性"><a href="#与NumPy和SciPy的兼容性" class="headerlink" title="与NumPy和SciPy的兼容性"></a>与NumPy和SciPy的兼容性</h2><p>gensim还包含<a href="https://radimrehurek.com/gensim/matutils.html" target="_blank" rel="noopener">有效的实用程序函数</a> 来帮助转换为/ numpy矩阵：</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">import</span> gensim
<span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
<span class="token operator">>></span><span class="token operator">></span> numpy_matrix <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>randint<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span> size<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">5</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># random matrix as an example</span>
<span class="token operator">>></span><span class="token operator">></span> corpus <span class="token operator">=</span> gensim<span class="token punctuation">.</span>matutils<span class="token punctuation">.</span>Dense2Corpus<span class="token punctuation">(</span>numpy_matrix<span class="token punctuation">)</span>
<span class="token operator">>></span><span class="token operator">></span> numpy_matrix <span class="token operator">=</span> gensim<span class="token punctuation">.</span>matutils<span class="token punctuation">.</span>corpus2dense<span class="token punctuation">(</span>corpus<span class="token punctuation">,</span> num_terms<span class="token operator">=</span>number_of_corpus_features<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>从/到scipy.sparse矩阵：</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">import</span> scipy<span class="token punctuation">.</span>sparse
<span class="token operator">>></span><span class="token operator">></span> scipy_sparse_matrix <span class="token operator">=</span> scipy<span class="token punctuation">.</span>sparse<span class="token punctuation">.</span>random<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># random sparse matrix as example</span>
<span class="token operator">>></span><span class="token operator">></span> corpus <span class="token operator">=</span> gensim<span class="token punctuation">.</span>matutils<span class="token punctuation">.</span>Sparse2Corpus<span class="token punctuation">(</span>scipy_sparse_matrix<span class="token punctuation">)</span>
<span class="token operator">>></span><span class="token operator">></span> scipy_csc_matrix <span class="token operator">=</span> gensim<span class="token punctuation">.</span>matutils<span class="token punctuation">.</span>corpus2csc<span class="token punctuation">(</span>corpus<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>
<p>要获得完整的参考（想要将字典修剪为更小的尺寸？优化语料库和NumPy / SciPy数组之间的转换？），请参阅<a href="https://radimrehurek.com/gensim/apiref.html" target="_blank" rel="noopener">API文档</a>。或者继续下一个关于<a href="https://radimrehurek.com/gensim/tut2.html" target="_blank" rel="noopener">主题和转换的</a>教程。</p>
<h1 id="主题和转换"><a href="#主题和转换" class="headerlink" title="主题和转换"></a>主题和转换</h1><h2 id="转换接口"><a href="#转换接口" class="headerlink" title="转换接口"></a>转换接口</h2><p>在上一章，我们创建了一个文档语料库，表示为向量流。要继续，让我们启动gensim并使用该语料库：</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">from</span> gensim <span class="token keyword">import</span> corpora<span class="token punctuation">,</span> models<span class="token punctuation">,</span> similarities
<span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">if</span> <span class="token punctuation">(</span>os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>exists<span class="token punctuation">(</span><span class="token string">"/tmp/deerwester.dict"</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
<span class="token operator">>></span><span class="token operator">></span>    dictionary <span class="token operator">=</span> corpora<span class="token punctuation">.</span>Dictionary<span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token string">'/tmp/deerwester.dict'</span><span class="token punctuation">)</span>
<span class="token operator">>></span><span class="token operator">></span>    corpus <span class="token operator">=</span> corpora<span class="token punctuation">.</span>MmCorpus<span class="token punctuation">(</span><span class="token string">'/tmp/deerwester.mm'</span><span class="token punctuation">)</span>
<span class="token operator">>></span><span class="token operator">></span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Used files generated from first tutorial"</span><span class="token punctuation">)</span>
<span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">else</span><span class="token punctuation">:</span>
<span class="token operator">>></span><span class="token operator">></span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Please run first tutorial to generate data set"</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>MmCorpus（9个文件，12个特征，28个非零项）</p>
<p>我将展示如何将文档从一个矢量表示转换为另一个矢量表示。这个过程有两个目标：</p>
<ol>
<li>为了在语料库中显示隐藏的结构，发现单词之间的关系并使用它们以新的（希望）更加语义的方式描述文档。</li>
<li>使文档表示更紧凑。这既提高了效率（新表示消耗更少的资源）和功效（边际数据趋势被忽略，降噪）。</li>
</ol>
<h3 id="创建转换"><a href="#创建转换" class="headerlink" title="创建转换"></a>创建转换</h3><p>转换是标准的Python对象，通常通过<em>训练语料库进行</em>初始化：</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> tfidf <span class="token operator">=</span> models<span class="token punctuation">.</span>TfidfModel<span class="token punctuation">(</span>corpus<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># step 1 -- initialize a model</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<p>我们使用上一章中的旧语料库初始化（训练）转换模型。不同的转换可能需要不同的初始化参数; 在TfIdf的情况下，“训练”仅包括通过提供的语料库一次并计算其所有特征的文档频率。训练其他模型，例如潜在语义分析或潜在Dirichlet分配，涉及更多，因此需要更多时间。</p>
<p><strong>注意：</strong></p>
<p>转换总是在两个特定的向量空间之间转换。必须使用相同的向量空间（=同一组特征id）进行训练以及后续的向量转换。无法使用相同的输入要素空间，例如应用不同的字符串预处理，使用不同的特征ID，或使用预期为TfIdf向量的词袋输入向量，将导致转换调用期间的特征不匹配，从而导致垃圾中的任何一个输出和/或运行时异常。</p>
<h3 id="变换向量"><a href="#变换向量" class="headerlink" title="变换向量"></a>变换向量</h3><p>从现在开始，<code>tfidf</code> 被视为一个只读对象，可用于将任何向量从旧表示（bag-of-words整数计数）转换为新表示（TfIdf实值权重）：</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> doc_bow <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">]</span>
<span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">print</span><span class="token punctuation">(</span>tfidf<span class="token punctuation">[</span>doc_bow<span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># step 2 -- use the model to transform vectors</span>
<span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0.70710678</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0.70710678</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>
<p>或者将转换应用于整个语料库：</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> corpus_tfidf <span class="token operator">=</span> tfidf<span class="token punctuation">[</span>corpus<span class="token punctuation">]</span>
<span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">for</span> doc <span class="token keyword">in</span> corpus_tfidf<span class="token punctuation">:</span>
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>     <span class="token keyword">print</span><span class="token punctuation">(</span>doc<span class="token punctuation">)</span>
<span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0.57735026918962573</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0.57735026918962573</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">0.57735026918962573</span><span class="token punctuation">)</span><span class="token punctuation">]</span>
<span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0.44424552527467476</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">0.44424552527467476</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">0.44424552527467476</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">0.32448702061385548</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">0.44424552527467476</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">0.32448702061385548</span><span class="token punctuation">)</span><span class="token punctuation">]</span>
<span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">0.5710059809418182</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">0.41707573620227772</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">0.41707573620227772</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">0.5710059809418182</span><span class="token punctuation">)</span><span class="token punctuation">]</span>
<span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0.49182558987264147</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">0.71848116070837686</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">0.49182558987264147</span><span class="token punctuation">)</span><span class="token punctuation">]</span>
<span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">0.62825804686700459</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">0.62825804686700459</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">0.45889394536615247</span><span class="token punctuation">)</span><span class="token punctuation">]</span>
<span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token number">9</span><span class="token punctuation">,</span> <span class="token number">1.0</span><span class="token punctuation">)</span><span class="token punctuation">]</span>
<span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token number">9</span><span class="token punctuation">,</span> <span class="token number">0.70710678118654746</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">0.70710678118654746</span><span class="token punctuation">)</span><span class="token punctuation">]</span>
<span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token number">9</span><span class="token punctuation">,</span> <span class="token number">0.50804290089167492</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">0.50804290089167492</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">11</span><span class="token punctuation">,</span> <span class="token number">0.69554641952003704</span><span class="token punctuation">)</span><span class="token punctuation">]</span>
<span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">0.62825804686700459</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">0.45889394536615247</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">11</span><span class="token punctuation">,</span> <span class="token number">0.62825804686700459</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>在这种特殊情况下，我们正在改变我们用于训练的同一语料库，但这只是偶然的。一旦初始化了转换模型，它就可以用在任何向量上（当然它们来自相同的向量空间），即使它们根本没有用在训练语料库中。这是通过LSA的折叠过程，LDA的主题推断等来实现的。</p>
<blockquote>
<p>注意 调用<code>model[corpus]</code>仅在旧<code>corpus</code> 文档流周围创建一个包装器- 实际转换在文档迭代期间即时完成。我们无法在调用 <code>corpus_transformed = model[corpus]</code> 时转换整个语料库，因为这意味着将结果存储在主存中，这与gensim的内存独立目标相矛盾。如果您将多次迭代转换，并且转换成本很高，请先将生成的语料库序列化为磁盘并继续使用它。</p>
</blockquote>
<p>转换也可以序列化，一个在另一个之上，在一个链中：</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> lsi <span class="token operator">=</span> models<span class="token punctuation">.</span>LsiModel<span class="token punctuation">(</span>corpus_tfidf<span class="token punctuation">,</span> id2word<span class="token operator">=</span>dictionary<span class="token punctuation">,</span> num_topics<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># initialize an LSI transformation</span>
<span class="token operator">>></span><span class="token operator">></span> corpus_lsi <span class="token operator">=</span> lsi<span class="token punctuation">[</span>corpus_tfidf<span class="token punctuation">]</span> <span class="token comment" spellcheck="true"># create a double wrapper over the original corpus: bow->tfidf->fold-in-lsi</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>
<p>在这里，我们通过<a href="https://en.wikipedia.org/wiki/Latent_semantic_indexing" target="_blank" rel="noopener">潜在语义索引</a>将我们的Tf-Idf语料库 转换为潜在的2-D空间（因为我们设置了2-D <code>num_topics=2</code>）。现在你可能想知道：这两个潜在的维度代表什么？让我们检查一下<code>models.LsiModel.print_topics()</code>：</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> lsi<span class="token punctuation">.</span>print_topics<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span>
topic <span class="token comment" spellcheck="true">#0(1.594): -0.703*"trees" + -0.538*"graph" + -0.402*"minors" + -0.187*"survey" + -0.061*"system" + -0.060*"response" + -0.060*"time" + -0.058*"user" + -0.049*"computer" + -0.035*"interface"</span>
topic <span class="token comment" spellcheck="true">#1(1.476): -0.460*"system" + -0.373*"user" + -0.332*"eps" + -0.328*"interface" + -0.320*"response" + -0.320*"time" + -0.293*"computer" + -0.280*"human" + -0.171*"survey" + 0.161*"trees"</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>
<p>根据LSI的说法，“树”，“图”和“未成年人”都是相关词（并且对第一个主题的方向贡献最大），而第二个主题实际上与所有其他词有关。正如所料，前五个文件与第二个主题的关联性更强，而剩下的四个文件与第一个主题相关：</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">for</span> doc <span class="token keyword">in</span> corpus_lsi<span class="token punctuation">:</span> <span class="token comment" spellcheck="true"># both bow->tfidf and tfidf->lsi transformations are actually executed here, on the fly</span>
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>     <span class="token keyword">print</span><span class="token punctuation">(</span>doc<span class="token punctuation">)</span>
<span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.066</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0.520</span><span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token comment" spellcheck="true"># "Human machine interface for lab abc computer applications"</span>
<span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.197</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0.761</span><span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token comment" spellcheck="true"># "A survey of user opinion of computer system response time"</span>
<span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.090</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0.724</span><span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token comment" spellcheck="true"># "The EPS user interface management system"</span>
<span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.076</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0.632</span><span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token comment" spellcheck="true"># "System and human system engineering testing of EPS"</span>
<span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.102</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0.574</span><span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token comment" spellcheck="true"># "Relation of user perceived response time to error measurement"</span>
<span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.703</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.161</span><span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token comment" spellcheck="true"># "The generation of random binary unordered trees"</span>
<span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.877</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.168</span><span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token comment" spellcheck="true"># "The intersection graph of paths in trees"</span>
<span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.910</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.141</span><span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token comment" spellcheck="true"># "Graph minors IV Widths of trees and well quasi ordering"</span>
<span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.617</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0.054</span><span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token comment" spellcheck="true"># "Graph minors A survey"</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>使用<code>save()</code>和<code>load()</code>函数实现模型持久性：</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> lsi<span class="token punctuation">.</span>save<span class="token punctuation">(</span><span class="token string">'/tmp/model.lsi'</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># same for tfidf, lda, ...</span>
<span class="token operator">>></span><span class="token operator">></span> lsi <span class="token operator">=</span> models<span class="token punctuation">.</span>LsiModel<span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token string">'/tmp/model.lsi'</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>
<p>接下来的问题可能是：这些文件之间的相似程度如何？有没有办法形式化相似性，以便对于给定的输入文档，我们可以根据它们的相似性订购一些其他文档？</p>
<h2 id="可用的转换"><a href="#可用的转换" class="headerlink" title="可用的转换"></a>可用的转换</h2><p>gensim实现了几种流行的矢量空间模型算法：</p>
<ul>
<li><p><a href="https://en.wikipedia.org/wiki/Tf–idf" target="_blank" rel="noopener">术语频率*反向文档频率，Tf-Idf</a> 期望初始化期间的词袋（整数值）训练语料库。在变换期间，它将采用向量并返回具有相同维度的另一个向量，除了在训练语料库中罕见的特征将增加其值。因此，它将整数值向量转换为实值向量，同时保持维度的数量不变。它还可以任选地将得到的矢量归一化为（欧几里得）单位长度。</p>
<p><code>&gt;&gt;&gt; model = models.TfidfModel(corpus, normalize=True)</code></p>
</li>
<li><p><a href="https://en.wikipedia.org/wiki/Latent_semantic_indexing" target="_blank" rel="noopener">潜在语义索引，LSI（或有时LSA）</a> 将文档从单词袋或（优选地）TfIdf加权空间转换为较低维度的潜在空间。对于上面的玩具语料库，我们只使用了2个潜在维度，但在实际语料库中，建议将200-500的目标维度作为“黄金标准” 。</p>
<p><code>&gt;&gt;&gt; model = models.LsiModel(tfidf_corpus, id2word=dictionary, num_topics=300)</code></p>
<p>LSI培训的独特之处在于我们可以随时继续“培训”，只需提供更多培训文件即可。这是通过在称为在线培训的过程中对底层模型的增量更新来完成的。由于这个特性，输入文档流甚至可能是无限的 - 只需在LSI新文档到达时继续提供它们，同时使用计算的转换模型作为只读！</p>
</li>
</ul>
<pre class="line-numbers language-python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> model<span class="token punctuation">.</span>add_documents<span class="token punctuation">(</span>another_tfidf_corpus<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># now LSI has been trained on tfidf_corpus + another_tfidf_corpus</span>
<span class="token operator">>></span><span class="token operator">></span> lsi_vec <span class="token operator">=</span> model<span class="token punctuation">[</span>tfidf_vec<span class="token punctuation">]</span> <span class="token comment" spellcheck="true"># convert some new document into the LSI space, without affecting the model</span>
<span class="token operator">>></span><span class="token operator">></span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>
<span class="token operator">>></span><span class="token operator">></span> model<span class="token punctuation">.</span>add_documents<span class="token punctuation">(</span>more_documents<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># tfidf_corpus + another_tfidf_corpus + more_documents</span>
<span class="token operator">>></span><span class="token operator">></span> lsi_vec <span class="token operator">=</span> model<span class="token punctuation">[</span>tfidf_vec<span class="token punctuation">]</span>
<span class="token operator">>></span><span class="token operator">></span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<ul>
<li><p>有关<a href="https://radimrehurek.com/gensim/models/lsimodel.html#module-gensim.models.lsimodel" target="_blank" rel="noopener"><code>gensim.models.lsimodel</code></a>如何使LSI逐渐“忘记”无限流中的旧观察的详细信息，请参阅文档。如果你想变脏，还有一些你可以调整的参数会影响速度与内存占用量和LSI算法的数值精度。</p>
<p>gensim使用了一种新颖的在线增量流分布式训练算法（相当满口！）。gensim还执行Halko等人的随机多遍算法。加速核心部分的计算，另请参阅<a href="https://radimrehurek.com/gensim/wiki.html" target="_blank" rel="noopener">英语维基百科</a>以上的实验，便通过在计算机集群中分配计算来进一步提高速度。</p>
</li>
<li><p><a href="http://www.cis.hut.fi/ella/publications/randproj_kdd.pdf" target="_blank" rel="noopener">随机投影，RP</a>旨在减少向量空间维度。这是一种非常有效的（内存和CPU友好的）方法，通过投入一点随机性来近似文档之间的TfIdf距离。建议的目标维度再次为数百/数千，具体取决于您的数据集。</p>
<p><code>&gt;&gt;&gt; model = models.RpModel(tfidf_corpus, num_topics=500)</code></p>
</li>
<li><p><a href="https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation" target="_blank" rel="noopener">Latent Dirichlet Allocation，LDA</a> 是另一种从词袋计数转变为低维度主题空间的转变。LDA是LSA（也称为多项PCA）的概率扩展，因此LDA的主题可以解释为对单词的概率分布。与LSA一样，这些分布也是从训练语料库中自动推断出来的。文档又被解释为这些主题的（软）混合（再次，就像LSA一样）。</p>
<p><code>&gt;&gt;&gt; model = models.LdaModel(corpus, id2word=dictionary, num_topics=100)</code></p>
<p>gensim使用在线LDA参数估计的快速实现，修改为在计算机集群上以<a href="https://radimrehurek.com/gensim/distributed.html" target="_blank" rel="noopener">分布式模式</a>运行。</p>
</li>
<li><p><a href="http://jmlr.csail.mit.edu/proceedings/papers/v15/wang11a/wang11a.pdf" target="_blank" rel="noopener">分层Dirichlet过程，HDP</a> 是一种非参数贝叶斯方法（请注意缺少的请求主题数）：</p>
<p><code>&gt;&gt;&gt; model = models.HdpModel(corpus, id2word=dictionary)</code></p>
<p>gensim使用快速在线实现。HDP模型是gensim的新成员，并且在学术方面仍然很粗糙 - 谨慎使用。</p>
</li>
</ul>
<p>添加新的VSM转换（例如不同的加权方案）相当简单; 有关更多信息和示例，请参阅<a href="https://radimrehurek.com/gensim/apiref.html" target="_blank" rel="noopener">API参考</a>或直接参阅<a href="https://github.com/piskvorky/gensim/blob/develop/gensim/models/tfidfmodel.py" target="_blank" rel="noopener">Python代码</a>。</p>
<p>值得重申的是，这些都是独特的<strong>增量</strong>实现，不需要整个训练语料库一次性存在于主存储器中。有了内存，我现在正在改进<a href="https://radimrehurek.com/gensim/distributed.html" target="_blank" rel="noopener">分布式计算</a>，以提高CPU效率。</p>
<h1 id="相似性查询"><a href="#相似性查询" class="headerlink" title="相似性查询"></a>相似性查询</h1><h2 id="相似性界面"><a href="#相似性界面" class="headerlink" title="相似性界面"></a>相似性界面</h2><p>为了说明在gensim中如何做到这一点，让我们考虑与之前的例子相同的语料库（它最初来自Deerwester等人的<a href="http://www.cs.bham.ac.uk/~pxt/IDA/lsa_ind.pdf" target="_blank" rel="noopener">“潜在语义分析索引”</a> 1990年开篇 文章）：</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">from</span> gensim <span class="token keyword">import</span> corpora<span class="token punctuation">,</span> models<span class="token punctuation">,</span> similarities
<span class="token operator">>></span><span class="token operator">></span> dictionary <span class="token operator">=</span> corpora<span class="token punctuation">.</span>Dictionary<span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token string">'/tmp/deerwester.dict'</span><span class="token punctuation">)</span>
<span class="token operator">>></span><span class="token operator">></span> corpus <span class="token operator">=</span> corpora<span class="token punctuation">.</span>MmCorpus<span class="token punctuation">(</span><span class="token string">'/tmp/deerwester.mm'</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># comes from the first tutorial, "From strings to vectors"</span>
<span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">print</span><span class="token punctuation">(</span>corpus<span class="token punctuation">)</span>
MmCorpus<span class="token punctuation">(</span><span class="token number">9</span> documents<span class="token punctuation">,</span> <span class="token number">12</span> features<span class="token punctuation">,</span> <span class="token number">28</span> non<span class="token operator">-</span>zero entries<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>按照Deerwester的例子，我们首先使用这个小的语料库来定义一个二维LSI空间：</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> lsi <span class="token operator">=</span> models<span class="token punctuation">.</span>LsiModel<span class="token punctuation">(</span>corpus<span class="token punctuation">,</span> id2word<span class="token operator">=</span>dictionary<span class="token punctuation">,</span> num_topics<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<p>现在假设用户输入查询“人机交互”。我们希望按照与此查询相关的递减顺序对我们的九个语料库文档进行排序。与现代搜索引擎不同，这里我们只关注可能相似性的一个方面 - 关于其文本（单词）的明显语义相关性。没有超链接，没有随机游走静态排名，只是布尔关键字匹配的语义扩展：</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> doc <span class="token operator">=</span> <span class="token string">"Human computer interaction"</span>
<span class="token operator">>></span><span class="token operator">></span> vec_bow <span class="token operator">=</span> dictionary<span class="token punctuation">.</span>doc2bow<span class="token punctuation">(</span>doc<span class="token punctuation">.</span>lower<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token operator">>></span><span class="token operator">></span> vec_lsi <span class="token operator">=</span> lsi<span class="token punctuation">[</span>vec_bow<span class="token punctuation">]</span> <span class="token comment" spellcheck="true"># convert the query to LSI space</span>
<span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">print</span><span class="token punctuation">(</span>vec_lsi<span class="token punctuation">)</span>
<span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.461821</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0.070028</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>此外，我们将考虑<a href="https://en.wikipedia.org/wiki/Cosine_similarity" target="_blank" rel="noopener">余弦相似性</a> 来确定两个向量的相似性。余弦相似度是向量空间建模中的标准度量，但是无论向量表示概率分布， <a href="https://en.wikipedia.org/wiki/Kullback–Leibler_divergence#Symmetrised_divergence" target="_blank" rel="noopener">不同的相似性度量</a>可能更合适。</p>
<h3 id="初始化查询结构"><a href="#初始化查询结构" class="headerlink" title="初始化查询结构"></a>初始化查询结构</h3><p>为了准备相似性查询，我们需要输入我们想要与后续查询进行比较的所有文档。在我们的例子中，它们与用于训练LSI的九个文件相同，转换为二维LSA空间。但这只是偶然的，我们也可能完全索引不同的语料库。</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> index <span class="token operator">=</span> similarities<span class="token punctuation">.</span>MatrixSimilarity<span class="token punctuation">(</span>lsi<span class="token punctuation">[</span>corpus<span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># transform corpus to LSI space and index it</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<blockquote>
<p>警告</p>
</blockquote>
<ul>
<li><code>similarities.MatrixSimilarity</code>只有当整个向量集适合内存时，该类才适用。例如，当与此类一起使用时，一百万个文档的语料库在256维LSI空间中将需要2GB的RAM。</li>
<li>如果没有2GB的可用RAM，则需要使用<code>similarities.Similarity</code>该类。此类通过在磁盘上的多个文件（称为分片）之间拆分索引，在固定内存中运行。它使用<code>similarities.MatrixSimilarity</code>和<code>similarities.SparseMatrixSimilarity</code>内部，所以它仍然很快，虽然稍微复杂一点。</li>
</ul>
<p>索引持久性通过标准<code>save()</code>和<code>load()</code>函数处理：</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> index<span class="token punctuation">.</span>save<span class="token punctuation">(</span><span class="token string">'/tmp/deerwester.index'</span><span class="token punctuation">)</span>
<span class="token operator">>></span><span class="token operator">></span> index <span class="token operator">=</span> similarities<span class="token punctuation">.</span>MatrixSimilarity<span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token string">'/tmp/deerwester.index'</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>
<p>对于所有相似性索引类（<code>similarities.Similarity</code>， <code>similarities.MatrixSimilarity</code>和<code>similarities.SparseMatrixSimilarity</code>）都是如此。同样在下文中，索引可以是任何这些的对象。如果有疑问，请使用<code>similarities.Similarity</code>，因为它是最具扩展性的版本，并且它还支持稍后向索引添加更多文档。</p>
<h3 id="执行查询"><a href="#执行查询" class="headerlink" title="执行查询"></a>执行查询</h3><p>要获得我们的查询文档与九个索引文档的相似性：</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> sims <span class="token operator">=</span> index<span class="token punctuation">[</span>vec_lsi<span class="token punctuation">]</span> <span class="token comment" spellcheck="true"># perform a similarity query against the corpus</span>
<span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">print</span><span class="token punctuation">(</span>list<span class="token punctuation">(</span>enumerate<span class="token punctuation">(</span>sims<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># print (document_number, document_similarity) 2-tuples</span>
<span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0.99809301</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0.93748635</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">0.99844527</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">0.9865886</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">0.90755945</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.12416792</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">6</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.1063926</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">7</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.098794639</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">0.05004178</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>
<p>余弦测量返回范围中的相似度（越大，越相似），因此第一个文档的得分为0.99809301等。</p>
<p>使用一些标准的Python魔术，我们将这些相似性按降序排序，并获得查询 <code>人机交互</code> 的最终答案：</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> sims <span class="token operator">=</span> sorted<span class="token punctuation">(</span>enumerate<span class="token punctuation">(</span>sims<span class="token punctuation">)</span><span class="token punctuation">,</span> key<span class="token operator">=</span><span class="token keyword">lambda</span> item<span class="token punctuation">:</span> <span class="token operator">-</span>item<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">print</span><span class="token punctuation">(</span>sims<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># print sorted (document number, similarity score) 2-tuples</span>
<span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">0.99844527</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token comment" spellcheck="true"># The EPS user interface management system</span>
<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0.99809301</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token comment" spellcheck="true"># Human machine interface for lab abc computer applications</span>
<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">0.9865886</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token comment" spellcheck="true"># System and human system engineering testing of EPS</span>
<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0.93748635</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token comment" spellcheck="true"># A survey of user opinion of computer system response time</span>
<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">0.90755945</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token comment" spellcheck="true"># Relation of user perceived response time to error measurement</span>
<span class="token punctuation">(</span><span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">0.050041795</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token comment" spellcheck="true"># Graph minors A survey</span>
<span class="token punctuation">(</span><span class="token number">7</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.098794639</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token comment" spellcheck="true"># Graph minors IV Widths of trees and well quasi ordering</span>
<span class="token punctuation">(</span><span class="token number">6</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.1063926</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token comment" spellcheck="true"># The intersection graph of paths in trees</span>
<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.12416792</span><span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token comment" spellcheck="true"># The generation of random binary unordered trees</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>（我将原始文档以“字符串形式”添加到输出注释中，以提高清晰度。）</p>
<p>这里要注意的是文件没有。标准布尔全文搜索永远不会返回2（<code>EPS用户界面管理系统</code>）和4（<code>用户感知响应时间与错误测量的关系</code>），因为他们不与 <code>人机交互</code> 分享任何常用词。然而，在应用LSI之后，我们可以观察到它们都获得了相当高的相似性得分（第2个实际上是最相似的！），这更符合我们对它们与查询共享 <code>computer-human</code> 相关主题的直觉。事实上，这种语义概括是我们首先应用转换并进行主题建模的原因。</p>
<h2 id="下一个在哪里"><a href="#下一个在哪里" class="headerlink" title="下一个在哪里"></a>下一个在哪里</h2><p>恭喜你，你已经完成了教程-现在你知道作品:-)深入到更多的细节如何gensim，您可以通过浏览<a href="https://radimrehurek.com/gensim/apiref.html" target="_blank" rel="noopener">API文档</a>，请参阅<a href="https://radimrehurek.com/gensim/wiki.html" target="_blank" rel="noopener">维基百科的实验</a>或者是退房<a href="https://radimrehurek.com/gensim/distributed.html" target="_blank" rel="noopener">分布式计算</a>中gensim。</p>
<p>gensim是一个相当成熟的软件包，已被许多个人和公司成功使用，用于快速原型制作和生产。这并不意味着它是完美的：</p>
<ul>
<li>有些部分可以更有效地实现（例如，在C中），或者更好地利用并行性（多个机器内核）</li>
<li>新算法一直在发布; 帮助gensim通过<a href="https://groups.google.com/group/gensim" target="_blank" rel="noopener">讨论</a>和<a href="https://github.com/piskvorky/gensim/wiki/Developer-page" target="_blank" rel="noopener">贡献代码</a>来跟上</li>
<li>您的<strong>反馈非常受欢迎</strong>和赞赏（而且不仅仅是代码！）： <a href="https://github.com/piskvorky/gensim/wiki/Ideas-&amp;-Features-proposals" target="_blank" rel="noopener">创意贡献</a>， <a href="https://github.com/piskvorky/gensim/issues" target="_blank" rel="noopener">错误报告</a>或只考虑贡献 <a href="https://groups.google.com/group/gensim/topics" target="_blank" rel="noopener">用户故事和一般问题</a>。</li>
</ul>
<p>在所有NLP（甚至机器学习）子域中，gensim没有野心成为一个包罗万象的框架。它的使命是帮助NLP从业者轻松地在大型数据集上尝试流行的主题建模算法，并促进研究人员对新算法的原型设计。</p>
<h1 id="英语维基百科上的实验"><a href="#英语维基百科上的实验" class="headerlink" title="英语维基百科上的实验"></a>英语维基百科上的实验</h1><p>为了测试gensim性能，我们针对英文版的Wikipedia运行它。</p>
<p>此页面描述了获取和处理Wikipedia的过程，以便任何人都可以重现结果。假设您已正确<a href="https://radimrehurek.com/gensim/install.html" target="_blank" rel="noopener">安装</a> gensim。</p>
<h2 id="准备语料库"><a href="#准备语料库" class="headerlink" title="准备语料库"></a>准备语料库</h2><ol>
<li><p>首先，从<a href="https://download.wikimedia.org/enwiki/" target="_blank" rel="noopener">http://download.wikimedia.org/enwiki/</a>下载所有维基百科文章的转储 （您需要文件enwiki-latest-pages-articles.xml.bz2或enwiki-YYYYMMDD-pages-articles.xml。 bz2用于特定于日期的转储）。此文件大小约为8GB，包含英语维基百科的所有文章（压缩版本）。</p>
</li>
<li><p>将文章转换为纯文本（处理Wiki标记）并将结果存储为稀疏TF-IDF向量。在Python中，这很容易在运行中进行，我们甚至不需要将整个存档解压缩到磁盘。gensim中包含一个脚本 可以执行此操作，运行：</p>
<p><code>$ python -m gensim.scripts.make_wiki</code></p>
</li>
</ol>
<blockquote>
<p>注意</p>
</blockquote>
<ul>
<li>这个预处理步骤通过8.2GB压缩wiki转储进行两次传递（一次用于提取字典，一次用于创建和存储稀疏向量），并且在笔记本电脑上花费大约9个小时，因此您可能想要喝咖啡或二。</li>
<li>此外，您将需要大约35GB的可用磁盘空间来存储稀疏输出向量。我建议立即压缩这些文件，例如使用bzip2（低至~13GB）。gensim可以直接使用压缩文件，因此可以节省磁盘空间。</li>
</ul>
<h2 id="潜在语义分析"><a href="#潜在语义分析" class="headerlink" title="潜在语义分析"></a>潜在语义分析</h2><p>首先让我们加载在上面第二步中创建的语料库迭代器和字典：</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">import</span> logging<span class="token punctuation">,</span> gensim
<span class="token operator">>></span><span class="token operator">></span> logging<span class="token punctuation">.</span>basicConfig<span class="token punctuation">(</span>format<span class="token operator">=</span><span class="token string">'%(asctime)s : %(levelname)s : %(message)s'</span><span class="token punctuation">,</span> level<span class="token operator">=</span>logging<span class="token punctuation">.</span>INFO<span class="token punctuation">)</span>

<span class="token operator">>></span><span class="token operator">></span> <span class="token comment" spellcheck="true"># load id->word mapping (the dictionary), one of the results of step 2 above</span>
<span class="token operator">>></span><span class="token operator">></span> id2word <span class="token operator">=</span> gensim<span class="token punctuation">.</span>corpora<span class="token punctuation">.</span>Dictionary<span class="token punctuation">.</span>load_from_text<span class="token punctuation">(</span><span class="token string">'wiki_en_wordids.txt'</span><span class="token punctuation">)</span>
<span class="token operator">>></span><span class="token operator">></span> <span class="token comment" spellcheck="true"># load corpus iterator</span>
<span class="token operator">>></span><span class="token operator">></span> mm <span class="token operator">=</span> gensim<span class="token punctuation">.</span>corpora<span class="token punctuation">.</span>MmCorpus<span class="token punctuation">(</span><span class="token string">'wiki_en_tfidf.mm'</span><span class="token punctuation">)</span>
<span class="token operator">>></span><span class="token operator">></span> <span class="token comment" spellcheck="true"># mm = gensim.corpora.MmCorpus('wiki_en_tfidf.mm.bz2') # use this if you compressed the TFIDF output (recommended)</span>

<span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">print</span><span class="token punctuation">(</span>mm<span class="token punctuation">)</span>
MmCorpus<span class="token punctuation">(</span><span class="token number">3931787</span> documents<span class="token punctuation">,</span> <span class="token number">100000</span> features<span class="token punctuation">,</span> <span class="token number">756379027</span> non<span class="token operator">-</span>zero entries<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>我们看到我们的语料库包含3.9M文档，100K特征（不同的标记）和稀疏TF-IDF矩阵中的0.76G非零条目。维基百科语料库共包含约22.4亿个令牌。</p>
<p>现在我们准备计算英语维基百科的LSA：</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> <span class="token comment" spellcheck="true"># extract 400 LSI topics; use the default one-pass algorithm</span>
<span class="token operator">>></span><span class="token operator">></span> lsi <span class="token operator">=</span> gensim<span class="token punctuation">.</span>models<span class="token punctuation">.</span>lsimodel<span class="token punctuation">.</span>LsiModel<span class="token punctuation">(</span>corpus<span class="token operator">=</span>mm<span class="token punctuation">,</span> id2word<span class="token operator">=</span>id2word<span class="token punctuation">,</span> num_topics<span class="token operator">=</span><span class="token number">400</span><span class="token punctuation">)</span>

<span class="token operator">>></span><span class="token operator">></span> <span class="token comment" spellcheck="true"># print the most contributing words (both positively and negatively) for each of the first ten topics</span>
<span class="token operator">>></span><span class="token operator">></span> lsi<span class="token punctuation">.</span>print_topics<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">)</span>
topic <span class="token comment" spellcheck="true">#0(332.762): 0.425*"utc" + 0.299*"talk" + 0.293*"page" + 0.226*"article" + 0.224*"delete" + 0.216*"discussion" + 0.205*"deletion" + 0.198*"should" + 0.146*"debate" + 0.132*"be"</span>
topic <span class="token comment" spellcheck="true">#1(201.852): 0.282*"link" + 0.209*"he" + 0.145*"com" + 0.139*"his" + -0.137*"page" + -0.118*"delete" + 0.114*"blacklist" + -0.108*"deletion" + -0.105*"discussion" + 0.100*"diff"</span>
topic <span class="token comment" spellcheck="true">#2(191.991): -0.565*"link" + -0.241*"com" + -0.238*"blacklist" + -0.202*"diff" + -0.193*"additions" + -0.182*"users" + -0.158*"coibot" + -0.136*"user" + 0.133*"he" + -0.130*"resolves"</span>
topic <span class="token comment" spellcheck="true">#3(141.284): -0.476*"image" + -0.255*"copyright" + -0.245*"fair" + -0.225*"use" + -0.173*"album" + -0.163*"cover" + -0.155*"resolution" + -0.141*"licensing" + 0.137*"he" + -0.121*"copies"</span>
topic <span class="token comment" spellcheck="true">#4(130.909): 0.264*"population" + 0.246*"age" + 0.243*"median" + 0.213*"income" + 0.195*"census" + -0.189*"he" + 0.184*"households" + 0.175*"were" + 0.167*"females" + 0.166*"males"</span>
topic <span class="token comment" spellcheck="true">#5(120.397): 0.304*"diff" + 0.278*"utc" + 0.213*"you" + -0.171*"additions" + 0.165*"talk" + -0.159*"image" + 0.159*"undo" + 0.155*"www" + -0.152*"page" + 0.148*"contribs"</span>
topic <span class="token comment" spellcheck="true">#6(115.414): -0.362*"diff" + -0.203*"www" + 0.197*"you" + -0.180*"undo" + -0.180*"kategori" + 0.164*"users" + 0.157*"additions" + -0.150*"contribs" + -0.139*"he" + -0.136*"image"</span>
topic <span class="token comment" spellcheck="true">#7(111.440): 0.429*"kategori" + 0.276*"categoria" + 0.251*"category" + 0.207*"kategorija" + 0.198*"kategorie" + -0.188*"diff" + 0.163*"категория" + 0.153*"categoría" + 0.139*"kategoria" + 0.133*"categorie"</span>
topic <span class="token comment" spellcheck="true">#8(109.907): 0.385*"album" + 0.224*"song" + 0.209*"chart" + 0.204*"band" + 0.169*"released" + 0.151*"music" + 0.142*"diff" + 0.141*"vocals" + 0.138*"she" + 0.132*"guitar"</span>
topic <span class="token comment" spellcheck="true">#9(102.599): -0.237*"league" + -0.214*"he" + -0.180*"season" + -0.174*"football" + -0.166*"team" + 0.159*"station" + -0.137*"played" + -0.131*"cup" + 0.131*"she" + -0.128*"utc"</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>在我的笔记本电脑上创建维基百科的LSI模型大约需要4小时9分钟。这是约<strong>每分钟16000的文件，包括所有的I / O</strong>。</p>
<blockquote>
<p>注意 如果您需要更快的结果，请参阅分布式计算教程。请注意，gensim中的BLAS库透明地使用多个内核，因此可以“免费”在多核计算机上更快地处理相同的数据，而无需任何分布式设置。</p>
</blockquote>
<p>我们看到总处理时间主要是从原始维基百科XML转储准备TF-IDF语料库的预处理步骤，花费了9小时。</p>
<p>gensim中使用的算法只需要查看每个输入文档一次，因此它适用于文档作为不可重复的流，或者多次存储/迭代语料库的成本太高的环境。</p>
<h2 id="潜在Dirichlet分配"><a href="#潜在Dirichlet分配" class="headerlink" title="潜在Dirichlet分配"></a>潜在Dirichlet分配</h2><p>与上面的Latent Semantic Analysis一样，首先加载语料库迭代器和字典：</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">import</span> logging<span class="token punctuation">,</span> gensim
<span class="token operator">>></span><span class="token operator">></span> logging<span class="token punctuation">.</span>basicConfig<span class="token punctuation">(</span>format<span class="token operator">=</span><span class="token string">'%(asctime)s : %(levelname)s : %(message)s'</span><span class="token punctuation">,</span> level<span class="token operator">=</span>logging<span class="token punctuation">.</span>INFO<span class="token punctuation">)</span>

<span class="token operator">>></span><span class="token operator">></span> <span class="token comment" spellcheck="true"># load id->word mapping (the dictionary), one of the results of step 2 above</span>
<span class="token operator">>></span><span class="token operator">></span> id2word <span class="token operator">=</span> gensim<span class="token punctuation">.</span>corpora<span class="token punctuation">.</span>Dictionary<span class="token punctuation">.</span>load_from_text<span class="token punctuation">(</span><span class="token string">'wiki_en_wordids.txt'</span><span class="token punctuation">)</span>
<span class="token operator">>></span><span class="token operator">></span> <span class="token comment" spellcheck="true"># load corpus iterator</span>
<span class="token operator">>></span><span class="token operator">></span> mm <span class="token operator">=</span> gensim<span class="token punctuation">.</span>corpora<span class="token punctuation">.</span>MmCorpus<span class="token punctuation">(</span><span class="token string">'wiki_en_tfidf.mm'</span><span class="token punctuation">)</span>
<span class="token operator">>></span><span class="token operator">></span> <span class="token comment" spellcheck="true"># mm = gensim.corpora.MmCorpus('wiki_en_tfidf.mm.bz2') # use this if you compressed the TFIDF output</span>

<span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">print</span><span class="token punctuation">(</span>mm<span class="token punctuation">)</span>
MmCorpus<span class="token punctuation">(</span><span class="token number">3931787</span> documents<span class="token punctuation">,</span> <span class="token number">100000</span> features<span class="token punctuation">,</span> <span class="token number">756379027</span> non<span class="token operator">-</span>zero entries<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>我们将运行在线LDA，这是一个算法，需要一大堆文件，更新LDA模型，取另一个块，更新模型等。在线LDA可以与批处理LDA进行对比，批处理LDA处理整个语料库（一次完整通过），然后更新模型，然后另一个传递，另一个更新…不同的是，给定一个相当固定的文档流（没有太多的主题漂移），较小的块（子扇区）上的在线更新本身相当不错，因此模型估计收敛更快。因此，我们可能只需要对语料库进行一次完整传递：如果语料库有300万篇文章，并且我们在每10,000篇文章后更新一次，这意味着我们将在一次传递中完成300次更新，很可能足以有一个非常准确的主题估计：</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> <span class="token comment" spellcheck="true"># extract 100 LDA topics, using 1 pass and updating once every 1 chunk (10,000 documents)</span>
<span class="token operator">>></span><span class="token operator">></span> lda <span class="token operator">=</span> gensim<span class="token punctuation">.</span>models<span class="token punctuation">.</span>ldamodel<span class="token punctuation">.</span>LdaModel<span class="token punctuation">(</span>corpus<span class="token operator">=</span>mm<span class="token punctuation">,</span> id2word<span class="token operator">=</span>id2word<span class="token punctuation">,</span> num_topics<span class="token operator">=</span><span class="token number">100</span><span class="token punctuation">,</span> update_every<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> chunksize<span class="token operator">=</span><span class="token number">10000</span><span class="token punctuation">,</span> passes<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
using serial LDA version on this node
running online LDA training<span class="token punctuation">,</span> <span class="token number">100</span> topics<span class="token punctuation">,</span> <span class="token number">1</span> passes over the supplied corpus of <span class="token number">3931787</span> documents<span class="token punctuation">,</span> updating model once every <span class="token number">10000</span> documents
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>与LSA不同，来自LDA的主题更容易理解：</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> <span class="token comment" spellcheck="true"># print the most contributing words for 20 randomly selected topics</span>
<span class="token operator">>></span><span class="token operator">></span> lda<span class="token punctuation">.</span>print_topics<span class="token punctuation">(</span><span class="token number">20</span><span class="token punctuation">)</span>
topic <span class="token comment" spellcheck="true">#0: 0.009*river + 0.008*lake + 0.006*island + 0.005*mountain + 0.004*area + 0.004*park + 0.004*antarctic + 0.004*south + 0.004*mountains + 0.004*dam</span>
topic <span class="token comment" spellcheck="true">#1: 0.026*relay + 0.026*athletics + 0.025*metres + 0.023*freestyle + 0.022*hurdles + 0.020*ret + 0.017*divisão + 0.017*athletes + 0.016*bundesliga + 0.014*medals</span>
topic <span class="token comment" spellcheck="true">#2: 0.002*were + 0.002*he + 0.002*court + 0.002*his + 0.002*had + 0.002*law + 0.002*government + 0.002*police + 0.002*patrolling + 0.002*their</span>
topic <span class="token comment" spellcheck="true">#3: 0.040*courcelles + 0.035*centimeters + 0.023*mattythewhite + 0.021*wine + 0.019*stamps + 0.018*oko + 0.017*perennial + 0.014*stubs + 0.012*ovate + 0.011*greyish</span>
topic <span class="token comment" spellcheck="true">#4: 0.039*al + 0.029*sysop + 0.019*iran + 0.015*pakistan + 0.014*ali + 0.013*arab + 0.010*islamic + 0.010*arabic + 0.010*saudi + 0.010*muhammad</span>
topic <span class="token comment" spellcheck="true">#5: 0.020*copyrighted + 0.020*northamerica + 0.014*uncopyrighted + 0.007*rihanna + 0.005*cloudz + 0.005*knowles + 0.004*gaga + 0.004*zombie + 0.004*wigan + 0.003*maccabi</span>
topic <span class="token comment" spellcheck="true">#6: 0.061*israel + 0.056*israeli + 0.030*sockpuppet + 0.025*jerusalem + 0.025*tel + 0.023*aviv + 0.022*palestinian + 0.019*ifk + 0.016*palestine + 0.014*hebrew</span>
topic <span class="token comment" spellcheck="true">#7: 0.015*melbourne + 0.014*rovers + 0.013*vfl + 0.012*australian + 0.012*wanderers + 0.011*afl + 0.008*dinamo + 0.008*queensland + 0.008*tracklist + 0.008*brisbane</span>
topic <span class="token comment" spellcheck="true">#8: 0.011*film + 0.007*her + 0.007*she + 0.004*he + 0.004*series + 0.004*his + 0.004*episode + 0.003*films + 0.003*television + 0.003*best</span>
topic <span class="token comment" spellcheck="true">#9: 0.019*wrestling + 0.013*château + 0.013*ligue + 0.012*discus + 0.012*estonian + 0.009*uci + 0.008*hockeyarchives + 0.008*wwe + 0.008*estonia + 0.007*reign</span>
topic <span class="token comment" spellcheck="true">#10: 0.078*edits + 0.059*notability + 0.035*archived + 0.025*clearer + 0.022*speedy + 0.021*deleted + 0.016*hook + 0.015*checkuser + 0.014*ron + 0.011*nominator</span>
topic <span class="token comment" spellcheck="true">#11: 0.013*admins + 0.009*acid + 0.009*molniya + 0.009*chemical + 0.007*ch + 0.007*chemistry + 0.007*compound + 0.007*anemone + 0.006*mg + 0.006*reaction</span>
topic <span class="token comment" spellcheck="true">#12: 0.018*india + 0.013*indian + 0.010*tamil + 0.009*singh + 0.008*film + 0.008*temple + 0.006*kumar + 0.006*hindi + 0.006*delhi + 0.005*bengal</span>
topic <span class="token comment" spellcheck="true">#13: 0.047*bwebs + 0.024*malta + 0.020*hobart + 0.019*basa + 0.019*columella + 0.019*huon + 0.018*tasmania + 0.016*popups + 0.014*tasmanian + 0.014*modèle</span>
topic <span class="token comment" spellcheck="true">#14: 0.014*jewish + 0.011*rabbi + 0.008*bgwhite + 0.008*lebanese + 0.007*lebanon + 0.006*homs + 0.005*beirut + 0.004*jews + 0.004*hebrew + 0.004*caligari</span>
topic <span class="token comment" spellcheck="true">#15: 0.025*german + 0.020*der + 0.017*von + 0.015*und + 0.014*berlin + 0.012*germany + 0.012*die + 0.010*des + 0.008*kategorie + 0.007*cross</span>
topic <span class="token comment" spellcheck="true">#16: 0.003*can + 0.003*system + 0.003*power + 0.003*are + 0.003*energy + 0.002*data + 0.002*be + 0.002*used + 0.002*or + 0.002*using</span>
topic <span class="token comment" spellcheck="true">#17: 0.049*indonesia + 0.042*indonesian + 0.031*malaysia + 0.024*singapore + 0.022*greek + 0.021*jakarta + 0.016*greece + 0.015*dord + 0.014*athens + 0.011*malaysian</span>
topic <span class="token comment" spellcheck="true">#18: 0.031*stakes + 0.029*webs + 0.018*futsal + 0.014*whitish + 0.013*hyun + 0.012*thoroughbred + 0.012*dnf + 0.012*jockey + 0.011*medalists + 0.011*racehorse</span>
topic <span class="token comment" spellcheck="true">#19: 0.119*oblast + 0.034*uploaded + 0.034*uploads + 0.033*nordland + 0.025*selsoviet + 0.023*raion + 0.022*krai + 0.018*okrug + 0.015*hålogaland + 0.015*russiae + 0.020*manga + 0.017*dragon + 0.012*theme + 0.011*dvd + 0.011*super + 0.011*hunter + 0.009*ash + 0.009*dream + 0.009*angel</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>在我的笔记本电脑上创建维基百科的这个LDA模型需要大约6小时20分钟。如果您需要更快地获得结果，请考虑在计算机群集上运行<a href="https://radimrehurek.com/gensim/dist_lda.html" target="_blank" rel="noopener">Distributed Latent Dirichlet Allocation</a>。</p>
<p>注意LDA和LSA运行之间的两个区别：我们要求LSA提取400个主题，LDA只有100个主题（因此速度差异实际上更大）。其次，gensim中的LSA实现是真正的在线：如果输入流的性质随时间变化，LSA将在相当少量的更新中重新定位自己以反映这些变化。相比之下，LDA并不是真正的在线，因为后来更新对模型的影响逐渐减弱。如果输入文档流中存在主题偏差，LDA将会变得混乱，并且在调整自身以适应新的状态时会越来越慢。</p>
<p>简而言之，如果使用LDA逐步将新文档添加到模型中，请务必小心。<strong>批量使用LDA</strong>，其中整个训练语料库事先已知或未显示主题漂移，<strong>是可以的并且不受影响</strong>。</p>
<p>要运行批量LDA（不在线），请使用以下方法训练LdaModel：</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> <span class="token comment" spellcheck="true"># extract 100 LDA topics, using 20 full passes, no online updates</span>
<span class="token operator">>></span><span class="token operator">></span> lda <span class="token operator">=</span> gensim<span class="token punctuation">.</span>models<span class="token punctuation">.</span>ldamodel<span class="token punctuation">.</span>LdaModel<span class="token punctuation">(</span>corpus<span class="token operator">=</span>mm<span class="token punctuation">,</span> id2word<span class="token operator">=</span>id2word<span class="token punctuation">,</span> num_topics<span class="token operator">=</span><span class="token number">100</span><span class="token punctuation">,</span> update_every<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> passes<span class="token operator">=</span><span class="token number">20</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>
<p>像往常一样，训练有素的模型可以用来将新的，看不见的文档（简单的词袋计数向量）转换为LDA主题分布：</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> doc_lda <span class="token operator">=</span> lda<span class="token punctuation">[</span>doc_bow<span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<h1 id="分布式计算"><a href="#分布式计算" class="headerlink" title="分布式计算"></a>分布式计算</h1><h2 id="为何分布式计算？"><a href="#为何分布式计算？" class="headerlink" title="为何分布式计算？"></a>为何分布式计算？</h2><p>需要构建一个语料库的语义表示，这个语料库是数百万个大型文档并且它将永远存在？您可以使用几台闲置的机器吗？ <a href="https://en.wikipedia.org/wiki/Distributed_computing" target="_blank" rel="noopener">分布式计算</a>试图通过将给定任务拆分为几个较小的子任务来加速计算，并将它们并行传递到多个计算节点。</p>
<p>在gensim的上下文中，计算节点是由其IP地址/端口标识的计算机，并且通过TCP / IP进行通信。整个可用机器集合称为<em>集群</em>。分布非常粗糙（进行的通信不多），因此允许网络具有相对较高的延迟。</p>
<blockquote>
<p>警告</p>
</blockquote>
<ul>
<li>使用分布式计算的主要原因是使事情运行得更快。在gensim中，大多数耗时的东西是在NumPy内部的线性代数的低级例程中完成的，与任何gensim代码无关。 *为NumPy 安装快速 <a href="https://en.wikipedia.org/wiki/Basic_Linear_Algebra_Subprograms" target="_blank" rel="noopener">BLAS（基本线性代数）</a> *库可以将性能提高15倍！因此，在开始购买这些额外的计算机之前，请考虑安装一个针对您的特定计算机优化的快速线程BLAS（而不是通用的二进制分布式库）。选项包括供应商的BLAS库（英特尔的MKL，AMD的ACML，OS X的vecLib，Sun的Sunperf，……）或一些开源替代品（GotoBLAS，ALTAS）。</li>
<li>要查看您正在使用的BLAS和LAPACK，请键入您的shell： <code>python -c 'import scipy; scipy.show_config()'</code></li>
</ul>
<h2 id="先决条件"><a href="#先决条件" class="headerlink" title="先决条件"></a>先决条件</h2><p>对于节点之间的通信，gensim使用<a href="https://pypi.python.org/pypi/Pyro4" target="_blank" rel="noopener">Pyro（PYthon远程对象）</a>，版本&gt; = 4.27。这是一个用于Python中的低级套接字通信和远程过程调用（RPC）的库。Pyro4是一个纯Python库，因此它的安装非常简单，只需将* .py文件复制到Python的导入路径上：</p>
<pre><code>pip install Pyro4</code></pre><p>您不必安装Pyro来运行gensim，但如果不这样做，您将无法访问分布式功能（即，所有内容都将始终以串行模式运行，此页面上的示例不适用）。</p>
<h2 id="核心概念-1"><a href="#核心概念-1" class="headerlink" title="核心概念"></a>核心概念</h2><p>与往常一样，gensim努力寻求一个清晰明了的API（见<a href="https://radimrehurek.com/gensim/intro.html#design" target="_blank" rel="noopener">功能</a>）。为此，<em>您无需在代码中进行任何更改</em>，以便在计算机集群上运行它！</p>
<p>您需要做的是在开始计算之前在每个集群节点上运行一个<a href="https://radimrehurek.com/gensim/distributed.html#term-worker" target="_blank" rel="noopener">工作</a>脚本（见下文）。运行此脚本告诉gensim它可以使用该节点作为从属程序将某些工作委托给它。在初始化期间，gensim中的算法将尝试查找并奴役所有可用的工作节点。</p>
<ul>
<li>Node</li>
</ul>
<p>一个逻辑工作单位。可以对应单个物理计算机，但您也可以在一台计算机上运行多个工作程序，从而生成多个逻辑节点。</p>
<ul>
<li>Cluster</li>
</ul>
<p>通过TCP / IP进行通信的几个节点。目前，网络广播用于发现和连接所有通信节点，因此节点必须位于同一<a href="https://en.wikipedia.org/wiki/Broadcast_domain" target="_blank" rel="noopener">广播域内</a>。</p>
<ul>
<li>Worker</li>
</ul>
<p>在每个节点上创建的进程。要从群集中删除节点，只需终止其工作进程。</p>
<ul>
<li>Dispatcher</li>
</ul>
<p>调度员将负责协商所有计算，排队（“调度”）个人工作给工人。计算永远不会直接与工作节点“交谈”，只能通过此调度程序。与worker不同，集群中一次只能有一个活动调度程序。</p>
<h2 id="可用的分布式算法"><a href="#可用的分布式算法" class="headerlink" title="可用的分布式算法"></a>可用的分布式算法</h2><ul>
<li><a href="https://radimrehurek.com/gensim/dist_lsi.html" target="_blank" rel="noopener">分布式潜在语义分析</a></li>
<li><a href="https://radimrehurek.com/gensim/dist_lda.html" target="_blank" rel="noopener">分布式潜在Dirichlet分配</a></li>
</ul>
<p><strong>Core End</strong></p>
<hr>
<p><strong>Model Start</strong></p>
<h1 id="Word2Vec-Model"><a href="#Word2Vec-Model" class="headerlink" title="Word2Vec Model"></a>Word2Vec Model</h1><p>如果你错过了热门话题，Word2Vec 是一种广泛使用的基于神经网络的算法，通常被称为“深度学习”（尽管 word2vec 本身相当浅）。word2vec 使用大量未注释的纯文本，自动学习单词之间的关系。输出是向量，每个词一个向量，具有显着的线性关系，允许我们做如下事情：</p>
<ul>
<li>vec(“king”) - vec(“man”) + vec(“woman”) =~ vec(“queen”)</li>
<li>vec(“蒙特利尔加拿大人队”) – vec(“蒙特利尔”) + vec(“多伦多”) =~ vec(“多伦多枫叶队”)。</li>
</ul>
<p>Word2vec 在<a href="https://github.com/RaRe-Technologies/movie-plots-by-genre" target="_blank" rel="noopener">自动文本标记</a>、推荐系统和机器翻译中非常有用。</p>
<ol>
<li>引入<code>Word2Vec</code>作为对传统词袋的改进</li>
<li>展示<code>Word2Vec</code>使用预训练模型的演示</li>
<li>演示从您自己的数据训练新模型</li>
<li>演示加载和保存模型</li>
<li>介绍几个训练参数并展示它们的效果</li>
<li>讨论内存要求</li>
<li>通过应用降维来可视化 Word2Vec 嵌入</li>
</ol>
<h2 id="Review-Bag-of-words"><a href="#Review-Bag-of-words" class="headerlink" title="Review: Bag-of-words"></a>Review: Bag-of-words</h2><p>You may be familiar with the <a href="https://en.wikipedia.org/wiki/Bag-of-words_model" target="_blank" rel="noopener">bag-of-words model</a> from the <a href="https://radimrehurek.com/gensim/auto_examples/core/run_core_concepts.html#core-concepts-vector" target="_blank" rel="noopener">Vector</a> section. This model transforms each document to a fixed-length vector of integers. For example, given the sentences:</p>
<ul>
<li><code>John likes to watch movies. Mary likes movies too.</code></li>
<li><code>John also likes to watch football games. Mary hates football.</code></li>
</ul>
<p>The model outputs the vectors:</p>
<ul>
<li><code>[1, 2, 1, 1, 2, 1, 1, 0, 0, 0, 0]</code></li>
<li><code>[1, 1, 1, 1, 0, 1, 0, 1, 2, 1, 1]</code></li>
</ul>
<p>Each vector has 10 elements, where each element counts the number of times a particular word occurred in the document. The order of elements is arbitrary. In the example above, the order of the elements corresponds to the words: <code>["John", "likes", "to", "watch", "movies", "Mary", "too", "also", "football", "games", "hates"]</code>.</p>
<p>Bag-of-words models are surprisingly effective, but have several weaknesses.</p>
<p>First, they lose all information about word order: “John likes Mary” and “Mary likes John” correspond to identical vectors. There is a solution: bag of <a href="https://en.wikipedia.org/wiki/N-gram" target="_blank" rel="noopener">n-grams</a> models consider word phrases of length n to represent documents as fixed-length vectors to capture local word order but suffer from data sparsity and high dimensionality.</p>
<p>Second, the model does not attempt to learn the meaning of the underlying words, and as a consequence, the distance between vectors doesn’t always reflect the difference in meaning. The <code>Word2Vec</code> model addresses this second problem.</p>
<h2 id="介绍：Word2Vec模型"><a href="#介绍：Word2Vec模型" class="headerlink" title="介绍：Word2Vec模型"></a>介绍：<code>Word2Vec</code>模型</h2><p><code>Word2Vec</code>是一个更新的模型，它使用浅层神经网络将单词嵌入到低维向量空间中。结果是一组词向量，其中向量空间中靠近的向量根据上下文具有相似的含义，而彼此远离的词向量具有不同的含义。例如，<code>strong</code>和<code>powerful</code>会靠近在一起，<code>strong</code>并且<code>Paris</code>会相对较远。</p>
<p>这个模型有两个版本，<a href="https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec" target="_blank" rel="noopener"><code>Word2Vec</code></a> 类都实现了它们：</p>
<ol>
<li>Skip-grams (SG)</li>
<li>Continuous-bag-of-words (CBOW)</li>
</ol>
<p>The <a href="http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model" target="_blank" rel="noopener">Word2Vec Skip-gram</a> model, for example, takes in pairs (word1, word2) generated by moving a window across text data, and trains a 1-hidden-layer neural network based on the synthetic task of given an input word, giving us a predicted probability distribution of nearby words to the input. A virtual <a href="https://en.wikipedia.org/wiki/One-hot" target="_blank" rel="noopener">one-hot</a> encoding of words goes through a ‘projection layer’ to the hidden layer; these projection weights are later interpreted as the word embeddings. So if the hidden layer has 300 neurons, this network will give us 300-dimensional word embeddings.</p>
<p>Continuous-bag-of-words Word2vec is very similar to the skip-gram model. It is also a 1-hidden-layer neural network. The synthetic training task now uses the average of multiple input context words, rather than a single word as in skip-gram, to predict the center word. Again, the projection weights that turn one-hot words into averageable vectors, of the same width as the hidden layer, are interpreted as the word embeddings.</p>
<h2 id="Word2Vec-Demo"><a href="#Word2Vec-Demo" class="headerlink" title="Word2Vec Demo"></a>Word2Vec Demo</h2><p>为了看看能做什么<code>Word2Vec</code>，让我们下载一个预先训练的模型并使用它。我们将获取在部分 Google 新闻数据集上训练的 Word2Vec 模型，该模型涵盖大约 300 万个单词和短语。这样的模型可能需要几个小时来训练，但由于它已经可用，用 Gensim 下载和加载它只需要几分钟。</p>
<blockquote>
<p>该模型大约为 2GB，因此您需要良好的网络连接才能继续。否则，请跳到下面的“训练你自己的模型”部分。</p>
</blockquote>
<p>您还可以查看<a href="https://radimrehurek.com/2014/02/word2vec-tutorial/#app" target="_blank" rel="noopener">在线 word2vec 演示</a>，您可以在其中亲自尝试此向量代数。该演示<code>word2vec</code>在<strong>大约 1000 亿字</strong>的<strong>整个</strong>Google 新闻数据集上 运行。</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> gensim<span class="token punctuation">.</span>downloader <span class="token keyword">as</span> api
wv <span class="token operator">=</span> api<span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token string">'word2vec-google-news-300'</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>
<p>一个常见的操作是检索模型的词汇表。这是微不足道的：</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">for</span> index<span class="token punctuation">,</span> word <span class="token keyword">in</span> enumerate<span class="token punctuation">(</span>wv<span class="token punctuation">.</span>index_to_key<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">if</span> index <span class="token operator">==</span> <span class="token number">10</span><span class="token punctuation">:</span>
        <span class="token keyword">break</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>f<span class="token string">"word #{index}/{len(wv.index_to_key)} is {word}"</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>
<pre><code>word #0/3000000 is &lt;/s&gt;
word #1/3000000 is in
word #2/3000000 is for
word #3/3000000 is that
word #4/3000000 is is
word #5/3000000 is on
word #6/3000000 is ##
word #7/3000000 is The
word #8/3000000 is with
word #9/3000000 is said</code></pre><p>我们可以轻松获得模型熟悉的术语的向量：</p>
<pre class="line-numbers language-python"><code class="language-python">vec_king <span class="token operator">=</span> wv<span class="token punctuation">[</span><span class="token string">'king'</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<p>不幸的是，该模型无法为不熟悉的单词推断向量。这是 Word2Vec 的一个限制：如果这个限制对您很重要，请查看 FastText 模型。</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">try</span><span class="token punctuation">:</span>
    vec_cameroon <span class="token operator">=</span> wv<span class="token punctuation">[</span><span class="token string">'cameroon'</span><span class="token punctuation">]</span>
<span class="token keyword">except</span> KeyError<span class="token punctuation">:</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"The word 'cameroon' does not appear in this model"</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>
<pre><code>The word 'cameroon' does not appear in this model</code></pre><p>继续，<code>Word2Vec</code>支持多个开箱即用的单词相似性任务。您可以看到随着单词变得越来越不相似，相似度是如何直观地降低的。</p>
<pre class="line-numbers language-python"><code class="language-python">pairs <span class="token operator">=</span> <span class="token punctuation">[</span>
    <span class="token punctuation">(</span><span class="token string">'car'</span><span class="token punctuation">,</span> <span class="token string">'minivan'</span><span class="token punctuation">)</span><span class="token punctuation">,</span>   <span class="token comment" spellcheck="true"># a minivan is a kind of car</span>
    <span class="token punctuation">(</span><span class="token string">'car'</span><span class="token punctuation">,</span> <span class="token string">'bicycle'</span><span class="token punctuation">)</span><span class="token punctuation">,</span>   <span class="token comment" spellcheck="true"># still a wheeled vehicle</span>
    <span class="token punctuation">(</span><span class="token string">'car'</span><span class="token punctuation">,</span> <span class="token string">'airplane'</span><span class="token punctuation">)</span><span class="token punctuation">,</span>  <span class="token comment" spellcheck="true"># ok, no wheels, but still a vehicle</span>
    <span class="token punctuation">(</span><span class="token string">'car'</span><span class="token punctuation">,</span> <span class="token string">'cereal'</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    <span class="token comment" spellcheck="true"># ... and so on</span>
    <span class="token punctuation">(</span><span class="token string">'car'</span><span class="token punctuation">,</span> <span class="token string">'communism'</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
<span class="token punctuation">]</span>
<span class="token keyword">for</span> w1<span class="token punctuation">,</span> w2 <span class="token keyword">in</span> pairs<span class="token punctuation">:</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'%r\t%r\t%.2f'</span> <span class="token operator">%</span> <span class="token punctuation">(</span>w1<span class="token punctuation">,</span> w2<span class="token punctuation">,</span> wv<span class="token punctuation">.</span>similarity<span class="token punctuation">(</span>w1<span class="token punctuation">,</span> w2<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<pre><code>'car'   'minivan'       0.69
'car'   'bicycle'       0.54
'car'   'airplane'      0.42
'car'   'cereal'        0.14
'car'   'communism'     0.06</code></pre><p>打印与“car”或“minivan”最相似的 5 个词</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">print</span><span class="token punctuation">(</span>wv<span class="token punctuation">.</span>most_similar<span class="token punctuation">(</span>positive<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'car'</span><span class="token punctuation">,</span> <span class="token string">'minivan'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> topn<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<pre><code>[('SUV', 0.8532192707061768), ('vehicle', 0.8175783753395081), ('pickup_truck', 0.7763688564300537), ('Jeep', 0.7567334175109863), ('Ford_Explorer', 0.7565720081329346)]</code></pre><p>下面哪个不属于这个序列？</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">print</span><span class="token punctuation">(</span>wv<span class="token punctuation">.</span>doesnt_match<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token string">'fire'</span><span class="token punctuation">,</span> <span class="token string">'water'</span><span class="token punctuation">,</span> <span class="token string">'land'</span><span class="token punctuation">,</span> <span class="token string">'sea'</span><span class="token punctuation">,</span> <span class="token string">'air'</span><span class="token punctuation">,</span> <span class="token string">'car'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<pre><code>car</code></pre><h2 id="Training-Your-Own-Model"><a href="#Training-Your-Own-Model" class="headerlink" title="Training Your Own Model"></a>Training Your Own Model</h2><p>首先，您需要一些数据来训练模型。对于下面的例子中，我们将使用<a href="https://hekyll.services.adelaide.edu.au/dspace/bitstream/2440/28910/1/hdl_28910.pdf" target="_blank" rel="noopener">利评价语料库</a> （你<a href="https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/test/test_data/lee_background.cor" target="_blank" rel="noopener">已经有了</a> ，如果你已经安装了Gensim）。</p>
<p>这个语料库足够小，可以完全放在内存中，但我们将实现一个内存友好的迭代器，逐行读取它以演示如何处理更大的语料库。</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">from</span> gensim<span class="token punctuation">.</span>test<span class="token punctuation">.</span>utils <span class="token keyword">import</span> datapath
<span class="token keyword">from</span> gensim <span class="token keyword">import</span> utils

<span class="token keyword">class</span> <span class="token class-name">MyCorpus</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""An iterator that yields sentences (lists of str)."""</span>

    <span class="token keyword">def</span> <span class="token function">__iter__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        corpus_path <span class="token operator">=</span> datapath<span class="token punctuation">(</span><span class="token string">'lee_background.cor'</span><span class="token punctuation">)</span>
        <span class="token keyword">for</span> line <span class="token keyword">in</span> open<span class="token punctuation">(</span>corpus_path<span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token comment" spellcheck="true"># assume there's one document per line, tokens separated by whitespace</span>
            <span class="token keyword">yield</span> utils<span class="token punctuation">.</span>simple_preprocess<span class="token punctuation">(</span>line<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>如果我们想做任何自定义的预处理，例如解码非标准编码、小写、删除数字、提取命名实体……所有这些都可以在<code>MyCorpus</code>迭代器内部完成，<code>word2vec</code>不需要知道。所需要的只是输入产生一个又一个的句子（utf8 单词列表）。</p>
<p>让我们继续在我们的语料库上训练一个模型。现在不要太担心训练参数，我们稍后会重新讨论它们。</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> gensim<span class="token punctuation">.</span>models

sentences <span class="token operator">=</span> MyCorpus<span class="token punctuation">(</span><span class="token punctuation">)</span>
model <span class="token operator">=</span> gensim<span class="token punctuation">.</span>models<span class="token punctuation">.</span>Word2Vec<span class="token punctuation">(</span>sentences<span class="token operator">=</span>sentences<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>
<p>一旦我们有了我们的模型，我们就可以像上面的演示一样使用它。</p>
<p>模型的主要部分是<code>model.wv</code>，其中“wv”代表“词向量”。</p>
<pre class="line-numbers language-python"><code class="language-python">vec_king <span class="token operator">=</span> model<span class="token punctuation">.</span>wv<span class="token punctuation">[</span><span class="token string">'king'</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<p>检索词汇表的工作方式相同：</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">for</span> index<span class="token punctuation">,</span> word <span class="token keyword">in</span> enumerate<span class="token punctuation">(</span>wv<span class="token punctuation">.</span>index_to_key<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">if</span> index <span class="token operator">==</span> <span class="token number">10</span><span class="token punctuation">:</span>
        <span class="token keyword">break</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>f<span class="token string">"word #{index}/{len(wv.index_to_key)} is {word}"</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>
<pre><code>word #0/3000000 is &lt;/s&gt;
word #1/3000000 is in
word #2/3000000 is for
word #3/3000000 is that
word #4/3000000 is is
word #5/3000000 is on
word #6/3000000 is ##
word #7/3000000 is The
word #8/3000000 is with
word #9/3000000 is said</code></pre><h2 id="Storing-and-loading-models"><a href="#Storing-and-loading-models" class="headerlink" title="Storing and loading models"></a>Storing and loading models</h2><p>您会注意到训练非平凡模型可能需要时间。训练模型并按预期工作后，您可以将其保存到磁盘。这样，您以后就不必再花时间重新训练它。</p>
<p>您可以使用标准的 gensim 方法存储/加载模型：</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> tempfile

<span class="token keyword">with</span> tempfile<span class="token punctuation">.</span>NamedTemporaryFile<span class="token punctuation">(</span>prefix<span class="token operator">=</span><span class="token string">'gensim-model-'</span><span class="token punctuation">,</span> delete<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span> <span class="token keyword">as</span> tmp<span class="token punctuation">:</span>
    temporary_filepath <span class="token operator">=</span> tmp<span class="token punctuation">.</span>name
    model<span class="token punctuation">.</span>save<span class="token punctuation">(</span>temporary_filepath<span class="token punctuation">)</span>
    <span class="token comment" spellcheck="true">#</span>
    <span class="token comment" spellcheck="true"># The model is now safely stored in the filepath.</span>
    <span class="token comment" spellcheck="true"># You can copy it to other machines, share it with others, etc.</span>
    <span class="token comment" spellcheck="true">#</span>
    <span class="token comment" spellcheck="true"># To load a saved model:</span>
    <span class="token comment" spellcheck="true">#</span>
    new_model <span class="token operator">=</span> gensim<span class="token punctuation">.</span>models<span class="token punctuation">.</span>Word2Vec<span class="token punctuation">.</span>load<span class="token punctuation">(</span>temporary_filepath<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>它在内部使用 pickle，可选择<code>mmap</code>将模型的内部大型 NumPy 矩阵直接从磁盘文件放入虚拟内存中，以实现进程间内存共享。</p>
<p>此外，您可以加载由原始 C 工具创建的模型，使用其文本和二进制格式：</p>
<pre class="line-numbers language-python"><code class="language-python">model <span class="token operator">=</span> gensim<span class="token punctuation">.</span>models<span class="token punctuation">.</span>KeyedVectors<span class="token punctuation">.</span>load_word2vec_format<span class="token punctuation">(</span><span class="token string">'/tmp/vectors.txt'</span><span class="token punctuation">,</span> binary<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
<span class="token comment" spellcheck="true"># using gzipped/bz2 input works too, no need to unzip</span>
model <span class="token operator">=</span> gensim<span class="token punctuation">.</span>models<span class="token punctuation">.</span>KeyedVectors<span class="token punctuation">.</span>load_word2vec_format<span class="token punctuation">(</span><span class="token string">'/tmp/vectors.bin.gz'</span><span class="token punctuation">,</span> binary<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>
<h2 id="Training-Parameters"><a href="#Training-Parameters" class="headerlink" title="Training Parameters"></a>Training Parameters</h2><p><code>Word2Vec</code> 接受几个影响训练速度和质量的参数。</p>
<h3 id="min-count"><a href="#min-count" class="headerlink" title="min_count"></a>min_count</h3><p><code>min_count</code>用于修剪内部字典。在十亿字的语料库中只出现一次或两次的单词可能是无趣的错别字和垃圾。此外，没有足够的数据对这些词进行任何有意义的训练，所以最好忽略它们：</p>
<p>min_count 的默认值=5</p>
<pre class="line-numbers language-python"><code class="language-python">model <span class="token operator">=</span> gensim<span class="token punctuation">.</span>models<span class="token punctuation">.</span>Word2Vec<span class="token punctuation">(</span>sentences<span class="token punctuation">,</span> min_count<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<h3 id="vector-size"><a href="#vector-size" class="headerlink" title="vector_size"></a>vector_size</h3><p><code>vector_size</code> 是 gensim Word2Vec 将单词映射到的 N 维空间的维数 (N)。</p>
<p>更大的尺寸值需要更多的训练数据，但可以产生更好（更准确）的模型。合理的值在数十到数百之间。</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># The default value of vector_size is 100.</span>
model <span class="token operator">=</span> gensim<span class="token punctuation">.</span>models<span class="token punctuation">.</span>Word2Vec<span class="token punctuation">(</span>sentences<span class="token punctuation">,</span> vector_size<span class="token operator">=</span><span class="token number">200</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>
<h3 id="workers"><a href="#workers" class="headerlink" title="workers"></a>workers</h3><p><code>workers</code>，最后一个主要参数（<a href="https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec" target="_blank" rel="noopener">此处</a>为完整列表）用于训练并行化，以加快训练速度：</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># default value of workers=3 (tutorial says 1...)</span>
model <span class="token operator">=</span> gensim<span class="token punctuation">.</span>models<span class="token punctuation">.</span>Word2Vec<span class="token punctuation">(</span>sentences<span class="token punctuation">,</span> workers<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>
<p>该<code>workers</code>参数仅在您安装了<a href="http://cython.org/" target="_blank" rel="noopener">Cython </a>时才有效。如果没有用Cython，你只可以使用的，因为一个核心<a href="https://wiki.python.org/moin/GlobalInterpreterLock" target="_blank" rel="noopener">GIL</a>。</p>
<h2 id="Memory"><a href="#Memory" class="headerlink" title="Memory"></a>Memory</h2><p>在其核心，<code>word2vec</code>模型参数存储为矩阵（NumPy 数组）。每个数组是<strong>#vocabulary</strong>（由<code>min_count</code>参数控制）乘以浮点数（单精度又名 4 字节）的<strong>向量大小</strong>（<code>vector_size</code>参数）。</p>
<p>三个这样的矩阵保存在 RAM 中（正在努力将这个数字减少到两个，甚至一个）。因此，如果您的输入包含 100,000 个唯一单词，并且您要求 layer <code>vector_size=200</code>，则模型将需要大约。 .<code>100,000*200*4*3 bytes = ~229MB</code></p>
<p>存储词汇树需要一点额外的内存（100,000 个单词将占用几兆字节），但除非您的单词是非常长的字符串，否则内存占用将由上述三个矩阵支配。</p>
<h2 id="Evaluating"><a href="#Evaluating" class="headerlink" title="Evaluating"></a>Evaluating</h2><p><code>Word2Vec</code>训练是一项无监督的任务，没有好的方法可以客观地评估结果。评估取决于您的最终应用。</p>
<p>谷歌发布了大约 20,000 个句法和语义测试示例的测试集，遵循“A 对 B 就像 C 对 D”的任务。它在“数据集”文件夹中提供。</p>
<p>例如，比较类型的句法类比是<code>bad:worse;good:?</code>。数据集中共有 9 种句法比较，如复数名词和相反含义的名词。</p>
<p>语义问题包含五种类型的语义类比，例如首都（<code>Paris:France;Tokyo:?</code>）或家庭成员（<code>brother:sister;dad:?</code>）。</p>
<p>Gensim 支持相同的评估集，格式完全相同：</p>
<pre class="line-numbers language-python"><code class="language-python">model<span class="token punctuation">.</span>wv<span class="token punctuation">.</span>evaluate_word_analogies<span class="token punctuation">(</span>datapath<span class="token punctuation">(</span><span class="token string">'questions-words.txt'</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<pre><code>(0.0, [{'section': 'capital-common-countries', 'correct': [], 'incorrect': [('CANBERRA', 'AUSTRALIA', 'KABUL', 'AFGHANISTAN'), ('CANBERRA', 'AUSTRALIA', 'PARIS', 'FRANCE'), ('KABUL', 'AFGHANISTAN', 'PARIS', 'FRANCE'), ('KABUL', 'AFGHANISTAN', 'CANBERRA', 'AUSTRALIA'), ('PARIS', 'FRANCE', 'CANBERRA', 'AUSTRALIA'), ('PARIS', 'FRANCE', 'KABUL', 'AFGHANISTAN')]}, {'section': 'capital-world', 'correct': [], 'incorrect': [('CANBERRA', 'AUSTRALIA', 'KABUL', 'AFGHANISTAN'), ('KABUL', 'AFGHANISTAN', 'PARIS', 'FRANCE')]}, {'section': 'currency', 'correct': [], 'incorrect': []}, {'section': 'city-in-state', 'correct': [], 'incorrect': []}, {'section': 'family', 'correct': [], 'incorrect': [('HE', 'SHE', 'HIS', 'HER'), ('HE', 'SHE', 'MAN', 'WOMAN'), ('HIS', 'HER', 'MAN', 'WOMAN'), ('HIS', 'HER', 'HE', 'SHE'), ('MAN', 'WOMAN', 'HE', 'SHE'), ('MAN', 'WOMAN', 'HIS', 'HER')]}, {'section': 'gram1-adjective-to-adverb', 'correct': [], 'incorrect': []}, {'section': 'gram2-opposite', 'correct': [], 'incorrect': []}, {'section': 'gram3-comparative', 'correct': [], 'incorrect': [('GOOD', 'BETTER', 'GREAT', 'GREATER'), ('GOOD', 'BETTER', 'LONG', 'LONGER'), ('GOOD', 'BETTER', 'LOW', 'LOWER'), ('GOOD', 'BETTER', 'SMALL', 'SMALLER'), ('GREAT', 'GREATER', 'LONG', 'LONGER'), ('GREAT', 'GREATER', 'LOW', 'LOWER'), ('GREAT', 'GREATER', 'SMALL', 'SMALLER'), ('GREAT', 'GREATER', 'GOOD', 'BETTER'), ('LONG', 'LONGER', 'LOW', 'LOWER'), ('LONG', 'LONGER', 'SMALL', 'SMALLER'), ('LONG', 'LONGER', 'GOOD', 'BETTER'), ('LONG', 'LONGER', 'GREAT', 'GREATER'), ('LOW', 'LOWER', 'SMALL', 'SMALLER'), ('LOW', 'LOWER', 'GOOD', 'BETTER'), ('LOW', 'LOWER', 'GREAT', 'GREATER'), ('LOW', 'LOWER', 'LONG', 'LONGER'), ('SMALL', 'SMALLER', 'GOOD', 'BETTER'), ('SMALL', 'SMALLER', 'GREAT', 'GREATER'), ('SMALL', 'SMALLER', 'LONG', 'LONGER'), ('SMALL', 'SMALLER', 'LOW', 'LOWER')]}, {'section': 'gram4-superlative', 'correct': [], 'incorrect': [('BIG', 'BIGGEST', 'GOOD', 'BEST'), ('BIG', 'BIGGEST', 'GREAT', 'GREATEST'), ('BIG', 'BIGGEST', 'LARGE', 'LARGEST'), ('GOOD', 'BEST', 'GREAT', 'GREATEST'), ('GOOD', 'BEST', 'LARGE', 'LARGEST'), ('GOOD', 'BEST', 'BIG', 'BIGGEST'), ('GREAT', 'GREATEST', 'LARGE', 'LARGEST'), ('GREAT', 'GREATEST', 'BIG', 'BIGGEST'), ('GREAT', 'GREATEST', 'GOOD', 'BEST'), ('LARGE', 'LARGEST', 'BIG', 'BIGGEST'), ('LARGE', 'LARGEST', 'GOOD', 'BEST'), ('LARGE', 'LARGEST', 'GREAT', 'GREATEST')]}, {'section': 'gram5-present-participle', 'correct': [], 'incorrect': [('GO', 'GOING', 'LOOK', 'LOOKING'), ('GO', 'GOING', 'PLAY', 'PLAYING'), ('GO', 'GOING', 'RUN', 'RUNNING'), ('GO', 'GOING', 'SAY', 'SAYING'), ('LOOK', 'LOOKING', 'PLAY', 'PLAYING'), ('LOOK', 'LOOKING', 'RUN', 'RUNNING'), ('LOOK', 'LOOKING', 'SAY', 'SAYING'), ('LOOK', 'LOOKING', 'GO', 'GOING'), ('PLAY', 'PLAYING', 'RUN', 'RUNNING'), ('PLAY', 'PLAYING', 'SAY', 'SAYING'), ('PLAY', 'PLAYING', 'GO', 'GOING'), ('PLAY', 'PLAYING', 'LOOK', 'LOOKING'), ('RUN', 'RUNNING', 'SAY', 'SAYING'), ('RUN', 'RUNNING', 'GO', 'GOING'), ('RUN', 'RUNNING', 'LOOK', 'LOOKING'), ('RUN', 'RUNNING', 'PLAY', 'PLAYING'), ('SAY', 'SAYING', 'GO', 'GOING'), ('SAY', 'SAYING', 'LOOK', 'LOOKING'), ('SAY', 'SAYING', 'PLAY', 'PLAYING'), ('SAY', 'SAYING', 'RUN', 'RUNNING')]}, {'section': 'gram6-nationality-adjective', 'correct': [], 'incorrect': [('AUSTRALIA', 'AUSTRALIAN', 'FRANCE', 'FRENCH'), ('AUSTRALIA', 'AUSTRALIAN', 'INDIA', 'INDIAN'), ('AUSTRALIA', 'AUSTRALIAN', 'ISRAEL', 'ISRAELI'), ('AUSTRALIA', 'AUSTRALIAN', 'JAPAN', 'JAPANESE'), ('AUSTRALIA', 'AUSTRALIAN', 'SWITZERLAND', 'SWISS'), ('FRANCE', 'FRENCH', 'INDIA', 'INDIAN'), ('FRANCE', 'FRENCH', 'ISRAEL', 'ISRAELI'), ('FRANCE', 'FRENCH', 'JAPAN', 'JAPANESE'), ('FRANCE', 'FRENCH', 'SWITZERLAND', 'SWISS'), ('FRANCE', 'FRENCH', 'AUSTRALIA', 'AUSTRALIAN'), ('INDIA', 'INDIAN', 'ISRAEL', 'ISRAELI'), ('INDIA', 'INDIAN', 'JAPAN', 'JAPANESE'), ('INDIA', 'INDIAN', 'SWITZERLAND', 'SWISS'), ('INDIA', 'INDIAN', 'AUSTRALIA', 'AUSTRALIAN'), ('INDIA', 'INDIAN', 'FRANCE', 'FRENCH'), ('ISRAEL', 'ISRAELI', 'JAPAN', 'JAPANESE'), ('ISRAEL', 'ISRAELI', 'SWITZERLAND', 'SWISS'), ('ISRAEL', 'ISRAELI', 'AUSTRALIA', 'AUSTRALIAN'), ('ISRAEL', 'ISRAELI', 'FRANCE', 'FRENCH'), ('ISRAEL', 'ISRAELI', 'INDIA', 'INDIAN'), ('JAPAN', 'JAPANESE', 'SWITZERLAND', 'SWISS'), ('JAPAN', 'JAPANESE', 'AUSTRALIA', 'AUSTRALIAN'), ('JAPAN', 'JAPANESE', 'FRANCE', 'FRENCH'), ('JAPAN', 'JAPANESE', 'INDIA', 'INDIAN'), ('JAPAN', 'JAPANESE', 'ISRAEL', 'ISRAELI'), ('SWITZERLAND', 'SWISS', 'AUSTRALIA', 'AUSTRALIAN'), ('SWITZERLAND', 'SWISS', 'FRANCE', 'FRENCH'), ('SWITZERLAND', 'SWISS', 'INDIA', 'INDIAN'), ('SWITZERLAND', 'SWISS', 'ISRAEL', 'ISRAELI'), ('SWITZERLAND', 'SWISS', 'JAPAN', 'JAPANESE')]}, {'section': 'gram7-past-tense', 'correct': [], 'incorrect': [('GOING', 'WENT', 'PAYING', 'PAID'), ('GOING', 'WENT', 'PLAYING', 'PLAYED'), ('GOING', 'WENT', 'SAYING', 'SAID'), ('GOING', 'WENT', 'TAKING', 'TOOK'), ('PAYING', 'PAID', 'PLAYING', 'PLAYED'), ('PAYING', 'PAID', 'SAYING', 'SAID'), ('PAYING', 'PAID', 'TAKING', 'TOOK'), ('PAYING', 'PAID', 'GOING', 'WENT'), ('PLAYING', 'PLAYED', 'SAYING', 'SAID'), ('PLAYING', 'PLAYED', 'TAKING', 'TOOK'), ('PLAYING', 'PLAYED', 'GOING', 'WENT'), ('PLAYING', 'PLAYED', 'PAYING', 'PAID'), ('SAYING', 'SAID', 'TAKING', 'TOOK'), ('SAYING', 'SAID', 'GOING', 'WENT'), ('SAYING', 'SAID', 'PAYING', 'PAID'), ('SAYING', 'SAID', 'PLAYING', 'PLAYED'), ('TAKING', 'TOOK', 'GOING', 'WENT'), ('TAKING', 'TOOK', 'PAYING', 'PAID'), ('TAKING', 'TOOK', 'PLAYING', 'PLAYED'), ('TAKING', 'TOOK', 'SAYING', 'SAID')]}, {'section': 'gram8-plural', 'correct': [], 'incorrect': [('BUILDING', 'BUILDINGS', 'CAR', 'CARS'), ('BUILDING', 'BUILDINGS', 'CHILD', 'CHILDREN'), ('BUILDING', 'BUILDINGS', 'MAN', 'MEN'), ('BUILDING', 'BUILDINGS', 'ROAD', 'ROADS'), ('BUILDING', 'BUILDINGS', 'WOMAN', 'WOMEN'), ('CAR', 'CARS', 'CHILD', 'CHILDREN'), ('CAR', 'CARS', 'MAN', 'MEN'), ('CAR', 'CARS', 'ROAD', 'ROADS'), ('CAR', 'CARS', 'WOMAN', 'WOMEN'), ('CAR', 'CARS', 'BUILDING', 'BUILDINGS'), ('CHILD', 'CHILDREN', 'MAN', 'MEN'), ('CHILD', 'CHILDREN', 'ROAD', 'ROADS'), ('CHILD', 'CHILDREN', 'WOMAN', 'WOMEN'), ('CHILD', 'CHILDREN', 'BUILDING', 'BUILDINGS'), ('CHILD', 'CHILDREN', 'CAR', 'CARS'), ('MAN', 'MEN', 'ROAD', 'ROADS'), ('MAN', 'MEN', 'WOMAN', 'WOMEN'), ('MAN', 'MEN', 'BUILDING', 'BUILDINGS'), ('MAN', 'MEN', 'CAR', 'CARS'), ('MAN', 'MEN', 'CHILD', 'CHILDREN'), ('ROAD', 'ROADS', 'WOMAN', 'WOMEN'), ('ROAD', 'ROADS', 'BUILDING', 'BUILDINGS'), ('ROAD', 'ROADS', 'CAR', 'CARS'), ('ROAD', 'ROADS', 'CHILD', 'CHILDREN'), ('ROAD', 'ROADS', 'MAN', 'MEN'), ('WOMAN', 'WOMEN', 'BUILDING', 'BUILDINGS'), ('WOMAN', 'WOMEN', 'CAR', 'CARS'), ('WOMAN', 'WOMEN', 'CHILD', 'CHILDREN'), ('WOMAN', 'WOMEN', 'MAN', 'MEN'), ('WOMAN', 'WOMEN', 'ROAD', 'ROADS')]}, {'section': 'gram9-plural-verbs', 'correct': [], 'incorrect': []}, {'section': 'Total accuracy', 'correct': [], 'incorrect': [('CANBERRA', 'AUSTRALIA', 'KABUL', 'AFGHANISTAN'), ('CANBERRA', 'AUSTRALIA', 'PARIS', 'FRANCE'), ('KABUL', 'AFGHANISTAN', 'PARIS', 'FRANCE'), ('KABUL', 'AFGHANISTAN', 'CANBERRA', 'AUSTRALIA'), ('PARIS', 'FRANCE', 'CANBERRA', 'AUSTRALIA'), ('PARIS', 'FRANCE', 'KABUL', 'AFGHANISTAN'), ('CANBERRA', 'AUSTRALIA', 'KABUL', 'AFGHANISTAN'), ('KABUL', 'AFGHANISTAN', 'PARIS', 'FRANCE'), ('HE', 'SHE', 'HIS', 'HER'), ('HE', 'SHE', 'MAN', 'WOMAN'), ('HIS', 'HER', 'MAN', 'WOMAN'), ('HIS', 'HER', 'HE', 'SHE'), ('MAN', 'WOMAN', 'HE', 'SHE'), ('MAN', 'WOMAN', 'HIS', 'HER'), ('GOOD', 'BETTER', 'GREAT', 'GREATER'), ('GOOD', 'BETTER', 'LONG', 'LONGER'), ('GOOD', 'BETTER', 'LOW', 'LOWER'), ('GOOD', 'BETTER', 'SMALL', 'SMALLER'), ('GREAT', 'GREATER', 'LONG', 'LONGER'), ('GREAT', 'GREATER', 'LOW', 'LOWER'), ('GREAT', 'GREATER', 'SMALL', 'SMALLER'), ('GREAT', 'GREATER', 'GOOD', 'BETTER'), ('LONG', 'LONGER', 'LOW', 'LOWER'), ('LONG', 'LONGER', 'SMALL', 'SMALLER'), ('LONG', 'LONGER', 'GOOD', 'BETTER'), ('LONG', 'LONGER', 'GREAT', 'GREATER'), ('LOW', 'LOWER', 'SMALL', 'SMALLER'), ('LOW', 'LOWER', 'GOOD', 'BETTER'), ('LOW', 'LOWER', 'GREAT', 'GREATER'), ('LOW', 'LOWER', 'LONG', 'LONGER'), ('SMALL', 'SMALLER', 'GOOD', 'BETTER'), ('SMALL', 'SMALLER', 'GREAT', 'GREATER'), ('SMALL', 'SMALLER', 'LONG', 'LONGER'), ('SMALL', 'SMALLER', 'LOW', 'LOWER'), ('BIG', 'BIGGEST', 'GOOD', 'BEST'), ('BIG', 'BIGGEST', 'GREAT', 'GREATEST'), ('BIG', 'BIGGEST', 'LARGE', 'LARGEST'), ('GOOD', 'BEST', 'GREAT', 'GREATEST'), ('GOOD', 'BEST', 'LARGE', 'LARGEST'), ('GOOD', 'BEST', 'BIG', 'BIGGEST'), ('GREAT', 'GREATEST', 'LARGE', 'LARGEST'), ('GREAT', 'GREATEST', 'BIG', 'BIGGEST'), ('GREAT', 'GREATEST', 'GOOD', 'BEST'), ('LARGE', 'LARGEST', 'BIG', 'BIGGEST'), ('LARGE', 'LARGEST', 'GOOD', 'BEST'), ('LARGE', 'LARGEST', 'GREAT', 'GREATEST'), ('GO', 'GOING', 'LOOK', 'LOOKING'), ('GO', 'GOING', 'PLAY', 'PLAYING'), ('GO', 'GOING', 'RUN', 'RUNNING'), ('GO', 'GOING', 'SAY', 'SAYING'), ('LOOK', 'LOOKING', 'PLAY', 'PLAYING'), ('LOOK', 'LOOKING', 'RUN', 'RUNNING'), ('LOOK', 'LOOKING', 'SAY', 'SAYING'), ('LOOK', 'LOOKING', 'GO', 'GOING'), ('PLAY', 'PLAYING', 'RUN', 'RUNNING'), ('PLAY', 'PLAYING', 'SAY', 'SAYING'), ('PLAY', 'PLAYING', 'GO', 'GOING'), ('PLAY', 'PLAYING', 'LOOK', 'LOOKING'), ('RUN', 'RUNNING', 'SAY', 'SAYING'), ('RUN', 'RUNNING', 'GO', 'GOING'), ('RUN', 'RUNNING', 'LOOK', 'LOOKING'), ('RUN', 'RUNNING', 'PLAY', 'PLAYING'), ('SAY', 'SAYING', 'GO', 'GOING'), ('SAY', 'SAYING', 'LOOK', 'LOOKING'), ('SAY', 'SAYING', 'PLAY', 'PLAYING'), ('SAY', 'SAYING', 'RUN', 'RUNNING'), ('AUSTRALIA', 'AUSTRALIAN', 'FRANCE', 'FRENCH'), ('AUSTRALIA', 'AUSTRALIAN', 'INDIA', 'INDIAN'), ('AUSTRALIA', 'AUSTRALIAN', 'ISRAEL', 'ISRAELI'), ('AUSTRALIA', 'AUSTRALIAN', 'JAPAN', 'JAPANESE'), ('AUSTRALIA', 'AUSTRALIAN', 'SWITZERLAND', 'SWISS'), ('FRANCE', 'FRENCH', 'INDIA', 'INDIAN'), ('FRANCE', 'FRENCH', 'ISRAEL', 'ISRAELI'), ('FRANCE', 'FRENCH', 'JAPAN', 'JAPANESE'), ('FRANCE', 'FRENCH', 'SWITZERLAND', 'SWISS'), ('FRANCE', 'FRENCH', 'AUSTRALIA', 'AUSTRALIAN'), ('INDIA', 'INDIAN', 'ISRAEL', 'ISRAELI'), ('INDIA', 'INDIAN', 'JAPAN', 'JAPANESE'), ('INDIA', 'INDIAN', 'SWITZERLAND', 'SWISS'), ('INDIA', 'INDIAN', 'AUSTRALIA', 'AUSTRALIAN'), ('INDIA', 'INDIAN', 'FRANCE', 'FRENCH'), ('ISRAEL', 'ISRAELI', 'JAPAN', 'JAPANESE'), ('ISRAEL', 'ISRAELI', 'SWITZERLAND', 'SWISS'), ('ISRAEL', 'ISRAELI', 'AUSTRALIA', 'AUSTRALIAN'), ('ISRAEL', 'ISRAELI', 'FRANCE', 'FRENCH'), ('ISRAEL', 'ISRAELI', 'INDIA', 'INDIAN'), ('JAPAN', 'JAPANESE', 'SWITZERLAND', 'SWISS'), ('JAPAN', 'JAPANESE', 'AUSTRALIA', 'AUSTRALIAN'), ('JAPAN', 'JAPANESE', 'FRANCE', 'FRENCH'), ('JAPAN', 'JAPANESE', 'INDIA', 'INDIAN'), ('JAPAN', 'JAPANESE', 'ISRAEL', 'ISRAELI'), ('SWITZERLAND', 'SWISS', 'AUSTRALIA', 'AUSTRALIAN'), ('SWITZERLAND', 'SWISS', 'FRANCE', 'FRENCH'), ('SWITZERLAND', 'SWISS', 'INDIA', 'INDIAN'), ('SWITZERLAND', 'SWISS', 'ISRAEL', 'ISRAELI'), ('SWITZERLAND', 'SWISS', 'JAPAN', 'JAPANESE'), ('GOING', 'WENT', 'PAYING', 'PAID'), ('GOING', 'WENT', 'PLAYING', 'PLAYED'), ('GOING', 'WENT', 'SAYING', 'SAID'), ('GOING', 'WENT', 'TAKING', 'TOOK'), ('PAYING', 'PAID', 'PLAYING', 'PLAYED'), ('PAYING', 'PAID', 'SAYING', 'SAID'), ('PAYING', 'PAID', 'TAKING', 'TOOK'), ('PAYING', 'PAID', 'GOING', 'WENT'), ('PLAYING', 'PLAYED', 'SAYING', 'SAID'), ('PLAYING', 'PLAYED', 'TAKING', 'TOOK'), ('PLAYING', 'PLAYED', 'GOING', 'WENT'), ('PLAYING', 'PLAYED', 'PAYING', 'PAID'), ('SAYING', 'SAID', 'TAKING', 'TOOK'), ('SAYING', 'SAID', 'GOING', 'WENT'), ('SAYING', 'SAID', 'PAYING', 'PAID'), ('SAYING', 'SAID', 'PLAYING', 'PLAYED'), ('TAKING', 'TOOK', 'GOING', 'WENT'), ('TAKING', 'TOOK', 'PAYING', 'PAID'), ('TAKING', 'TOOK', 'PLAYING', 'PLAYED'), ('TAKING', 'TOOK', 'SAYING', 'SAID'), ('BUILDING', 'BUILDINGS', 'CAR', 'CARS'), ('BUILDING', 'BUILDINGS', 'CHILD', 'CHILDREN'), ('BUILDING', 'BUILDINGS', 'MAN', 'MEN'), ('BUILDING', 'BUILDINGS', 'ROAD', 'ROADS'), ('BUILDING', 'BUILDINGS', 'WOMAN', 'WOMEN'), ('CAR', 'CARS', 'CHILD', 'CHILDREN'), ('CAR', 'CARS', 'MAN', 'MEN'), ('CAR', 'CARS', 'ROAD', 'ROADS'), ('CAR', 'CARS', 'WOMAN', 'WOMEN'), ('CAR', 'CARS', 'BUILDING', 'BUILDINGS'), ('CHILD', 'CHILDREN', 'MAN', 'MEN'), ('CHILD', 'CHILDREN', 'ROAD', 'ROADS'), ('CHILD', 'CHILDREN', 'WOMAN', 'WOMEN'), ('CHILD', 'CHILDREN', 'BUILDING', 'BUILDINGS'), ('CHILD', 'CHILDREN', 'CAR', 'CARS'), ('MAN', 'MEN', 'ROAD', 'ROADS'), ('MAN', 'MEN', 'WOMAN', 'WOMEN'), ('MAN', 'MEN', 'BUILDING', 'BUILDINGS'), ('MAN', 'MEN', 'CAR', 'CARS'), ('MAN', 'MEN', 'CHILD', 'CHILDREN'), ('ROAD', 'ROADS', 'WOMAN', 'WOMEN'), ('ROAD', 'ROADS', 'BUILDING', 'BUILDINGS'), ('ROAD', 'ROADS', 'CAR', 'CARS'), ('ROAD', 'ROADS', 'CHILD', 'CHILDREN'), ('ROAD', 'ROADS', 'MAN', 'MEN'), ('WOMAN', 'WOMEN', 'BUILDING', 'BUILDINGS'), ('WOMAN', 'WOMEN', 'CAR', 'CARS'), ('WOMAN', 'WOMEN', 'CHILD', 'CHILDREN'), ('WOMAN', 'WOMEN', 'MAN', 'MEN'), ('WOMAN', 'WOMEN', 'ROAD', 'ROADS')]}])</code></pre><p>此<code>evaluate_word_analogies</code>方法采用一个<a href="https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.KeyedVectors.evaluate_word_analogies" target="_blank" rel="noopener">可选参数</a> <code>restrict_vocab</code>，该<a href="https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.KeyedVectors.evaluate_word_analogies" target="_blank" rel="noopener">参数</a>限制了要考虑的测试示例。</p>
<p>在 2016 年 12 月发布的 Gensim 中，我们添加了一种更好的方法来评估语义相似性。</p>
<p>默认情况下，它使用学术数据集 WS-353，但可以基于它创建特定于您的业务的数据集。它包含单词对以及人工分配的相似性判断。它衡量两个词的相关性或共现。例如，“coast”和“shore”非常相似，因为它们出现在相同的上下文中。同时，“衣服”和“壁橱”不太相似，因为它们相关但不可互换。</p>
<pre class="line-numbers language-python"><code class="language-python">model<span class="token punctuation">.</span>wv<span class="token punctuation">.</span>evaluate_word_pairs<span class="token punctuation">(</span>datapath<span class="token punctuation">(</span><span class="token string">'wordsim353.tsv'</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<pre><code>((0.1014236962315867, 0.44065378924434523), SpearmanrResult(correlation=0.07441989763914543, pvalue=0.571997364</code></pre><blockquote>
<p>在 Google 或 WS-353 测试集上的良好性能并不意味着 word2vec 将在您的应用程序中运行良好，反之亦然。最好直接评估您的预期任务。有关如何在分类器管道中使用 word2vec 的示例，请参阅本<a href="https://github.com/RaRe-Technologies/movie-plots-by-genre" target="_blank" rel="noopener">教程</a>。</p>
</blockquote>
<h2 id="Online-training-Resuming-training"><a href="#Online-training-Resuming-training" class="headerlink" title="Online training / Resuming training"></a>Online training / Resuming training</h2><p>高级用户可以加载模型并使用更多句子和<a href="https://radimrehurek.com/gensim/auto_examples/tutorials/online_w2v_tutorial.ipynb" target="_blank" rel="noopener">新词汇</a>继续训练它：</p>
<pre class="line-numbers language-python"><code class="language-python">model <span class="token operator">=</span> gensim<span class="token punctuation">.</span>models<span class="token punctuation">.</span>Word2Vec<span class="token punctuation">.</span>load<span class="token punctuation">(</span>temporary_filepath<span class="token punctuation">)</span>
more_sentences <span class="token operator">=</span> <span class="token punctuation">[</span>
    <span class="token punctuation">[</span><span class="token string">'Advanced'</span><span class="token punctuation">,</span> <span class="token string">'users'</span><span class="token punctuation">,</span> <span class="token string">'can'</span><span class="token punctuation">,</span> <span class="token string">'load'</span><span class="token punctuation">,</span> <span class="token string">'a'</span><span class="token punctuation">,</span> <span class="token string">'model'</span><span class="token punctuation">,</span>
     <span class="token string">'and'</span><span class="token punctuation">,</span> <span class="token string">'continue'</span><span class="token punctuation">,</span> <span class="token string">'training'</span><span class="token punctuation">,</span> <span class="token string">'it'</span><span class="token punctuation">,</span> <span class="token string">'with'</span><span class="token punctuation">,</span> <span class="token string">'more'</span><span class="token punctuation">,</span> <span class="token string">'sentences'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
<span class="token punctuation">]</span>
model<span class="token punctuation">.</span>build_vocab<span class="token punctuation">(</span>more_sentences<span class="token punctuation">,</span> update<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
model<span class="token punctuation">.</span>train<span class="token punctuation">(</span>more_sentences<span class="token punctuation">,</span> total_examples<span class="token operator">=</span>model<span class="token punctuation">.</span>corpus_count<span class="token punctuation">,</span> epochs<span class="token operator">=</span>model<span class="token punctuation">.</span>epochs<span class="token punctuation">)</span>

<span class="token comment" spellcheck="true"># cleaning up temporary file</span>
<span class="token keyword">import</span> os
os<span class="token punctuation">.</span>remove<span class="token punctuation">(</span>temporary_filepath<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>您可能需要将<code>total_words</code>参数调整为<code>train()</code>，具体取决于您要模拟的学习率衰减。</p>
<p>请注意，无法使用 C 工具生成的模型恢复训练，<code>KeyedVectors.load_word2vec_format()</code>. 您仍然可以将它们用于查询/相似性，但缺少对训练（词汇树）至关重要的信息。</p>
<h2 id="Training-Loss-Computation"><a href="#Training-Loss-Computation" class="headerlink" title="Training Loss Computation"></a>Training Loss Computation</h2><p>该参数<code>compute_loss</code>可用于在训练 Word2Vec 模型时切换损失计算。计算出的损失存储在模型属性中<code>running_training_loss</code>，可以使用<code>get_latest_training_loss</code>如下函数检索 ：</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># instantiating and training the Word2Vec model</span>
model_with_loss <span class="token operator">=</span> gensim<span class="token punctuation">.</span>models<span class="token punctuation">.</span>Word2Vec<span class="token punctuation">(</span>
    sentences<span class="token punctuation">,</span>
    min_count<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span>
    compute_loss<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
    hs<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span>
    sg<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span>
    seed<span class="token operator">=</span><span class="token number">42</span><span class="token punctuation">,</span>
<span class="token punctuation">)</span>

<span class="token comment" spellcheck="true"># getting the training loss value</span>
training_loss <span class="token operator">=</span> model_with_loss<span class="token punctuation">.</span>get_latest_training_loss<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>training_loss<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<pre><code>1369454.25</code></pre><h2 id="Benchmarks"><a href="#Benchmarks" class="headerlink" title="Benchmarks"></a>Benchmarks</h2><p>让我们运行一些基准测试，看看训练损失计算代码对训练时间的影响。</p>
<p>我们将使用以下数据作为基准：</p>
<ol>
<li>Lee背景语料库：包含在gensim的测试数据中</li>
<li>Text8 语料库。为了演示语料库大小的影响，我们将查看语料库的前 1MB、10MB、50MB 以及整个内容。</li>
</ol>
<pre><code>import io
import os

import gensim.models.word2vec
import gensim.downloader as api
import smart_open


def head(path, size):
    with smart_open.open(path) as fin:
        return io.StringIO(fin.read(size))


def generate_input_data():
    lee_path = datapath('lee_background.cor')
    ls = gensim.models.word2vec.LineSentence(lee_path)
    ls.name = '25kB'
    yield ls

    text8_path = api.load('text8').fn
    labels = ('1MB', '10MB', '50MB', '100MB')
    sizes = (1024 ** 2, 10 * 1024 ** 2, 50 * 1024 ** 2, 100 * 1024 ** 2)
    for l, s in zip(labels, sizes):
        ls = gensim.models.word2vec.LineSentence(head(text8_path, s))
        ls.name = l
        yield ls


input_data = list(generate_input_data())</code></pre><p>我们现在比较输入数据和模型训练参数（如<code>hs</code>和 ）的不同组合所花费的训练时间<code>sg</code>。</p>
<p>对于每个组合，我们重复测试几次以获得测试持续时间的平均值和标准偏差。</p>
<pre><code># Temporarily reduce logging verbosity
logging.root.level = logging.ERROR

import time
import numpy as np
import pandas as pd

train_time_values = []
seed_val = 42
sg_values = [0, 1]
hs_values = [0, 1]

fast = True
if fast:
    input_data_subset = input_data[:3]
else:
    input_data_subset = input_data


for data in input_data_subset:
    for sg_val in sg_values:
        for hs_val in hs_values:
            for loss_flag in [True, False]:
                time_taken_list = []
                for i in range(3):
                    start_time = time.time()
                    w2v_model = gensim.models.Word2Vec(
                        data,
                        compute_loss=loss_flag,
                        sg=sg_val,
                        hs=hs_val,
                        seed=seed_val,
                    )
                    time_taken_list.append(time.time() - start_time)

                time_taken_list = np.array(time_taken_list)
                time_mean = np.mean(time_taken_list)
                time_std = np.std(time_taken_list)

                model_result = {
                    'train_data': data.name,
                    'compute_loss': loss_flag,
                    'sg': sg_val,
                    'hs': hs_val,
                    'train_time_mean': time_mean,
                    'train_time_std': time_std,
                }
                print("Word2vec model #%i: %s" % (len(train_time_values), model_result))
                train_time_values.append(model_result)

train_times_table = pd.DataFrame(train_time_values)
train_times_table = train_times_table.sort_values(
    by=['train_data', 'sg', 'hs', 'compute_loss'],
    ascending=[False, False, True, False],
)
print(train_times_table)</code></pre><h2 id="Visualising-Word-Embeddings"><a href="#Visualising-Word-Embeddings" class="headerlink" title="Visualising Word Embeddings"></a>Visualising Word Embeddings</h2><p>通过使用 tSNE 将单词的维数减少到 2 维，可以将模型制作的词嵌入可视化。</p>
<p>可视化可用于注意数据中的语义和句法趋势。</p>
<p>例子：</p>
<ul>
<li>语义：像 cat、dog、cow 等词有靠近的倾向</li>
<li>句法：像 run、running 或 cut、cutting 这样的词靠得很近。</li>
</ul>
<p>也可以注意到像 vKing - vMan = vQueen - vWoman 这样的向量关系。</p>
<blockquote>
<p>用于可视化的模型是在一个小语料库上训练的。因此，有些关系可能不是那么清楚。</p>
</blockquote>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>decomposition <span class="token keyword">import</span> IncrementalPCA    <span class="token comment" spellcheck="true"># inital reduction</span>
<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>manifold <span class="token keyword">import</span> TSNE                   <span class="token comment" spellcheck="true"># final reduction</span>
<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np                                  <span class="token comment" spellcheck="true"># array handling</span>


<span class="token keyword">def</span> <span class="token function">reduce_dimensions</span><span class="token punctuation">(</span>model<span class="token punctuation">)</span><span class="token punctuation">:</span>
    num_dimensions <span class="token operator">=</span> <span class="token number">2</span>  <span class="token comment" spellcheck="true"># final num dimensions (2D, 3D, etc)</span>

    <span class="token comment" spellcheck="true"># extract the words &amp; their vectors, as numpy arrays</span>
    vectors <span class="token operator">=</span> np<span class="token punctuation">.</span>asarray<span class="token punctuation">(</span>model<span class="token punctuation">.</span>wv<span class="token punctuation">.</span>vectors<span class="token punctuation">)</span>
    labels <span class="token operator">=</span> np<span class="token punctuation">.</span>asarray<span class="token punctuation">(</span>model<span class="token punctuation">.</span>wv<span class="token punctuation">.</span>index_to_key<span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># fixed-width numpy strings</span>

    <span class="token comment" spellcheck="true"># reduce using t-SNE</span>
    tsne <span class="token operator">=</span> TSNE<span class="token punctuation">(</span>n_components<span class="token operator">=</span>num_dimensions<span class="token punctuation">,</span> random_state<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>
    vectors <span class="token operator">=</span> tsne<span class="token punctuation">.</span>fit_transform<span class="token punctuation">(</span>vectors<span class="token punctuation">)</span>

    x_vals <span class="token operator">=</span> <span class="token punctuation">[</span>v<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token keyword">for</span> v <span class="token keyword">in</span> vectors<span class="token punctuation">]</span>
    y_vals <span class="token operator">=</span> <span class="token punctuation">[</span>v<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token keyword">for</span> v <span class="token keyword">in</span> vectors<span class="token punctuation">]</span>
    <span class="token keyword">return</span> x_vals<span class="token punctuation">,</span> y_vals<span class="token punctuation">,</span> labels


x_vals<span class="token punctuation">,</span> y_vals<span class="token punctuation">,</span> labels <span class="token operator">=</span> reduce_dimensions<span class="token punctuation">(</span>model<span class="token punctuation">)</span>

<span class="token keyword">def</span> <span class="token function">plot_with_plotly</span><span class="token punctuation">(</span>x_vals<span class="token punctuation">,</span> y_vals<span class="token punctuation">,</span> labels<span class="token punctuation">,</span> plot_in_notebook<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">from</span> plotly<span class="token punctuation">.</span>offline <span class="token keyword">import</span> init_notebook_mode<span class="token punctuation">,</span> iplot<span class="token punctuation">,</span> plot
    <span class="token keyword">import</span> plotly<span class="token punctuation">.</span>graph_objs <span class="token keyword">as</span> go

    trace <span class="token operator">=</span> go<span class="token punctuation">.</span>Scatter<span class="token punctuation">(</span>x<span class="token operator">=</span>x_vals<span class="token punctuation">,</span> y<span class="token operator">=</span>y_vals<span class="token punctuation">,</span> mode<span class="token operator">=</span><span class="token string">'text'</span><span class="token punctuation">,</span> text<span class="token operator">=</span>labels<span class="token punctuation">)</span>
    data <span class="token operator">=</span> <span class="token punctuation">[</span>trace<span class="token punctuation">]</span>

    <span class="token keyword">if</span> plot_in_notebook<span class="token punctuation">:</span>
        init_notebook_mode<span class="token punctuation">(</span>connected<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
        iplot<span class="token punctuation">(</span>data<span class="token punctuation">,</span> filename<span class="token operator">=</span><span class="token string">'word-embedding-plot'</span><span class="token punctuation">)</span>
    <span class="token keyword">else</span><span class="token punctuation">:</span>
        plot<span class="token punctuation">(</span>data<span class="token punctuation">,</span> filename<span class="token operator">=</span><span class="token string">'word-embedding-plot.html'</span><span class="token punctuation">)</span>


<span class="token keyword">def</span> <span class="token function">plot_with_matplotlib</span><span class="token punctuation">(</span>x_vals<span class="token punctuation">,</span> y_vals<span class="token punctuation">,</span> labels<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> plt
    <span class="token keyword">import</span> random

    random<span class="token punctuation">.</span>seed<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>

    plt<span class="token punctuation">.</span>figure<span class="token punctuation">(</span>figsize<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">12</span><span class="token punctuation">,</span> <span class="token number">12</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    plt<span class="token punctuation">.</span>scatter<span class="token punctuation">(</span>x_vals<span class="token punctuation">,</span> y_vals<span class="token punctuation">)</span>

    <span class="token comment" spellcheck="true">#</span>
    <span class="token comment" spellcheck="true"># Label randomly subsampled 25 data points</span>
    <span class="token comment" spellcheck="true">#</span>
    indices <span class="token operator">=</span> list<span class="token punctuation">(</span>range<span class="token punctuation">(</span>len<span class="token punctuation">(</span>labels<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    selected_indices <span class="token operator">=</span> random<span class="token punctuation">.</span>sample<span class="token punctuation">(</span>indices<span class="token punctuation">,</span> <span class="token number">25</span><span class="token punctuation">)</span>
    <span class="token keyword">for</span> i <span class="token keyword">in</span> selected_indices<span class="token punctuation">:</span>
        plt<span class="token punctuation">.</span>annotate<span class="token punctuation">(</span>labels<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>x_vals<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">,</span> y_vals<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

<span class="token keyword">try</span><span class="token punctuation">:</span>
    get_ipython<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token keyword">except</span> Exception<span class="token punctuation">:</span>
    plot_function <span class="token operator">=</span> plot_with_matplotlib
<span class="token keyword">else</span><span class="token punctuation">:</span>
    plot_function <span class="token operator">=</span> plot_with_plotly

plot_function<span class="token punctuation">(</span>x_vals<span class="token punctuation">,</span> y_vals<span class="token punctuation">,</span> labels<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p><img src="/images/loading.gif" data-original="../images/ML/sphx_glr_run_word2vec_001.png" alt=""></p>
<h2 id="Links"><a href="#Links" class="headerlink" title="Links"></a>Links</h2><ul>
<li>API docs: <a href="https://radimrehurek.com/gensim/models/word2vec.html#module-gensim.models.word2vec" target="_blank" rel="noopener"><code>gensim.models.word2vec</code></a></li>
<li><a href="https://code.google.com/archive/p/word2vec/" target="_blank" rel="noopener">Original C toolkit and word2vec papers by Google</a>.</li>
</ul>
<h1 id="Doc2Vec-Model"><a href="#Doc2Vec-Model" class="headerlink" title="Doc2Vec Model"></a>Doc2Vec Model</h1><p>介绍 Gensim 的 Doc2Vec 模型并演示其在<a href="https://hekyll.services.adelaide.edu.au/dspace/bitstream/2440/28910/1/hdl_28910.pdf" target="_blank" rel="noopener">Lee Corpus</a>上的使用 。</p>
<p>Doc2Vec 是一个<a href="https://radimrehurek.com/gensim/auto_examples/core/run_core_concepts.html#core-concepts-model" target="_blank" rel="noopener">模型</a>，将每个<a href="https://radimrehurek.com/gensim/auto_examples/core/run_core_concepts.html#core-concepts-document" target="_blank" rel="noopener">文档</a>表示 为一个<a href="https://radimrehurek.com/gensim/auto_examples/core/run_core_concepts.html#core-concepts-vector" target="_blank" rel="noopener">Vector</a>。下面介绍了模型并演示了如何训练和评估它。</p>
<p>以下是我们将要做的事情的列表：</p>
<ol>
<li>查看相关模型：bag-of-words、Word2Vec、Doc2Vec</li>
<li>加载和预处理训练和测试语料库（参见<a href="https://radimrehurek.com/gensim/auto_examples/core/run_core_concepts.html#core-concepts-corpus" target="_blank" rel="noopener">语料库</a>）</li>
<li>使用训练语料库训练Doc2Vec<a href="https://radimrehurek.com/gensim/auto_examples/core/run_core_concepts.html#core-concepts-model" target="_blank" rel="noopener">模型</a>模型</li>
<li>演示如何使用经过训练的模型来推断<a href="https://radimrehurek.com/gensim/auto_examples/core/run_core_concepts.html#core-concepts-vector" target="_blank" rel="noopener">向量</a></li>
<li>评估模型</li>
<li>在测试语料库上测试模型</li>
</ol>
<h2 id="Introducing-Paragraph-Vector"><a href="#Introducing-Paragraph-Vector" class="headerlink" title="Introducing: Paragraph Vector"></a>Introducing: Paragraph Vector</h2><blockquote>
<p>In Gensim, we refer to the Paragraph Vector model as <code>Doc2Vec</code>.</p>
</blockquote>
<p>Le and Mikolov in 2014 introduced the <a href="https://cs.stanford.edu/~quocle/paragraph_vector.pdf" target="_blank" rel="noopener">Doc2Vec algorithm</a>, which usually outperforms such simple-averaging of <code>Word2Vec</code> vectors.</p>
<p>The basic idea is: act as if a document has another floating word-like vector, which contributes to all training predictions, and is updated like other word-vectors, but we will call it a doc-vector. Gensim’s <a href="https://radimrehurek.com/gensim/models/doc2vec.html#gensim.models.doc2vec.Doc2Vec" target="_blank" rel="noopener"><code>Doc2Vec</code></a> class implements this algorithm.</p>
<p>There are two implementations:</p>
<ol>
<li>Paragraph Vector - Distributed Memory (PV-DM)</li>
<li>Paragraph Vector - Distributed Bag of Words (PV-DBOW)</li>
</ol>
<blockquote>
<p>Don’t let the implementation details below scare you. They’re advanced material: if it’s too much, then move on to the next section.</p>
</blockquote>
<p><strong>PV-DM</strong> is analogous to Word2Vec CBOW. The doc-vectors are obtained by training a neural network on the synthetic task of predicting a center word based an average of both context word-vectors and the full document’s doc-vector.</p>
<p><strong>PV-DBOW</strong> is analogous to Word2Vec SG. The doc-vectors are obtained by training a neural network on the synthetic task of predicting a target word just from the full document’s doc-vector. (It is also common to combine this with skip-gram testing, using both the doc-vector and nearby word-vectors to predict a single target word, but only one at a time.)</p>
<h2 id="Prepare-the-Training-and-Test-Data"><a href="#Prepare-the-Training-and-Test-Data" class="headerlink" title="Prepare the Training and Test Data"></a>Prepare the Training and Test Data</h2><p>我们将使用 gensim 中包含的<a href="https://hekyll.services.adelaide.edu.au/dspace/bitstream/2440/28910/1/hdl_28910.pdf" target="_blank" rel="noopener">Lee 背景语料库</a>来训练我们的模型。该语料库包含 314 份选自澳大利亚广播公司新闻邮件服务的文档，该服务提供标题故事的文本电子邮件，涵盖多个广泛主题。</p>
<p>我们将使用 包含 50 个文档的更短的<a href="https://hekyll.services.adelaide.edu.au/dspace/bitstream/2440/28910/1/hdl_28910.pdf" target="_blank" rel="noopener">Lee 语料库</a>通过眼睛测试我们的模型。</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> os
<span class="token keyword">import</span> gensim
<span class="token comment" spellcheck="true"># Set file names for train and test data</span>
test_data_dir <span class="token operator">=</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>join<span class="token punctuation">(</span>gensim<span class="token punctuation">.</span>__path__<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token string">'test'</span><span class="token punctuation">,</span> <span class="token string">'test_data'</span><span class="token punctuation">)</span>
lee_train_file <span class="token operator">=</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>join<span class="token punctuation">(</span>test_data_dir<span class="token punctuation">,</span> <span class="token string">'lee_background.cor'</span><span class="token punctuation">)</span>
lee_test_file <span class="token operator">=</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>join<span class="token punctuation">(</span>test_data_dir<span class="token punctuation">,</span> <span class="token string">'lee.cor'</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h2 id="Define-a-Function-to-Read-and-Preprocess-Text"><a href="#Define-a-Function-to-Read-and-Preprocess-Text" class="headerlink" title="Define a Function to Read and Preprocess Text"></a>Define a Function to Read and Preprocess Text</h2><p>下面，我们定义一个函数来：</p>
<ul>
<li>打开训练/测试文件（使用拉丁语编码）</li>
<li>逐行读取文件</li>
<li>预处理每一行（将文本标记为单个单词，删除标点符号，设置为小写等）</li>
</ul>
<p>我们正在阅读的文件是<strong>corpus</strong>。文件的每一行都是一个<strong>文档</strong>。</p>
<blockquote>
<p>要训练模型，我们需要将标签/编号与训练语料库的每个文档相关联。在我们的例子中，标签只是从零开始的行号。</p>
</blockquote>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> smart_open

<span class="token keyword">def</span> <span class="token function">read_corpus</span><span class="token punctuation">(</span>fname<span class="token punctuation">,</span> tokens_only<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">with</span> smart_open<span class="token punctuation">.</span>open<span class="token punctuation">(</span>fname<span class="token punctuation">,</span> encoding<span class="token operator">=</span><span class="token string">"iso-8859-1"</span><span class="token punctuation">)</span> <span class="token keyword">as</span> f<span class="token punctuation">:</span>
        <span class="token keyword">for</span> i<span class="token punctuation">,</span> line <span class="token keyword">in</span> enumerate<span class="token punctuation">(</span>f<span class="token punctuation">)</span><span class="token punctuation">:</span>
            tokens <span class="token operator">=</span> gensim<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>simple_preprocess<span class="token punctuation">(</span>line<span class="token punctuation">)</span>
            <span class="token keyword">if</span> tokens_only<span class="token punctuation">:</span>
                <span class="token keyword">yield</span> tokens
            <span class="token keyword">else</span><span class="token punctuation">:</span>
                <span class="token comment" spellcheck="true"># For training data, add tags</span>
                <span class="token keyword">yield</span> gensim<span class="token punctuation">.</span>models<span class="token punctuation">.</span>doc2vec<span class="token punctuation">.</span>TaggedDocument<span class="token punctuation">(</span>tokens<span class="token punctuation">,</span> <span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span>

train_corpus <span class="token operator">=</span> list<span class="token punctuation">(</span>read_corpus<span class="token punctuation">(</span>lee_train_file<span class="token punctuation">)</span><span class="token punctuation">)</span>
test_corpus <span class="token operator">=</span> list<span class="token punctuation">(</span>read_corpus<span class="token punctuation">(</span>lee_test_file<span class="token punctuation">,</span> tokens_only<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>我们来看看训练语料</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">print</span><span class="token punctuation">(</span>train_corpus<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<pre><code>[TaggedDocument(words=['hundreds', 'of', 'people', 'have', 'been', 'forced', 'to', 'vacate', 'their', 'homes', 'in', 'the', 'southern', 'highlands', 'of', 'new', 'south', 'wales', 'as', 'strong', 'winds', 'today', 'pushed', 'huge', 'bushfire', 'towards', 'the', 'town', 'of', 'hill', 'top', 'new', 'blaze', 'near', 'goulburn', 'south', 'west', 'of', 'sydney', 'has', 'forced', 'the', 'closure', 'of', 'the', 'hume', 'highway', 'at', 'about', 'pm', 'aedt', 'marked', 'deterioration', 'in', 'the', 'weather', 'as', 'storm', 'cell', 'moved', 'east', 'across', 'the', 'blue', 'mountains', 'forced', 'authorities', 'to', 'make', 'decision', 'to', 'evacuate', 'people', 'from', 'homes', 'in', 'outlying', 'streets', 'at', 'hill', 'top', 'in', 'the', 'new', 'south', 'wales', 'southern', 'highlands', 'an', 'estimated', 'residents', 'have', 'left', 'their', 'homes', 'for', 'nearby', 'mittagong', 'the', 'new', 'south', 'wales', 'rural', 'fire', 'service', 'says', 'the', 'weather', 'conditions', 'which', 'caused', 'the', 'fire', 'to', 'burn', 'in', 'finger', 'formation', 'have', 'now', 'eased', 'and', 'about', 'fire', 'units', 'in', 'and', 'around', 'hill', 'top', 'are', 'optimistic', 'of', 'defending', 'all', 'properties', 'as', 'more', 'than', 'blazes', 'burn', 'on', 'new', 'year', 'eve', 'in', 'new', 'south', 'wales', 'fire', 'crews', 'have', 'been', 'called', 'to', 'new', 'fire', 'at', 'gunning', 'south', 'of', 'goulburn', 'while', 'few', 'details', 'are', 'available', 'at', 'this', 'stage', 'fire', 'authorities', 'says', 'it', 'has', 'closed', 'the', 'hume', 'highway', 'in', 'both', 'directions', 'meanwhile', 'new', 'fire', 'in', 'sydney', 'west', 'is', 'no', 'longer', 'threatening', 'properties', 'in', 'the', 'cranebrook', 'area', 'rain', 'has', 'fallen', 'in', 'some', 'parts', 'of', 'the', 'illawarra', 'sydney', 'the', 'hunter', 'valley', 'and', 'the', 'north', 'coast', 'but', 'the', 'bureau', 'of', 'meteorology', 'claire', 'richards', 'says', 'the', 'rain', 'has', 'done', 'little', 'to', 'ease', 'any', 'of', 'the', 'hundred', 'fires', 'still', 'burning', 'across', 'the', 'state', 'the', 'falls', 'have', 'been', 'quite', 'isolated', 'in', 'those', 'areas', 'and', 'generally', 'the', 'falls', 'have', 'been', 'less', 'than', 'about', 'five', 'millimetres', 'she', 'said', 'in', 'some', 'places', 'really', 'not', 'significant', 'at', 'all', 'less', 'than', 'millimetre', 'so', 'there', 'hasn', 'been', 'much', 'relief', 'as', 'far', 'as', 'rain', 'is', 'concerned', 'in', 'fact', 'they', 've', 'probably', 'hampered', 'the', 'efforts', 'of', 'the', 'firefighters', 'more', 'because', 'of', 'the', 'wind', 'gusts', 'that', 'are', 'associated', 'with', 'those', 'thunderstorms'], tags=[0]), TaggedDocument(words=['indian', 'security', 'forces', 'have', 'shot', 'dead', 'eight', 'suspected', 'militants', 'in', 'night', 'long', 'encounter', 'in', 'southern', 'kashmir', 'the', 'shootout', 'took', 'place', 'at', 'dora', 'village', 'some', 'kilometers', 'south', 'of', 'the', 'kashmiri', 'summer', 'capital', 'srinagar', 'the', 'deaths', 'came', 'as', 'pakistani', 'police', 'arrested', 'more', 'than', 'two', 'dozen', 'militants', 'from', 'extremist', 'groups', 'accused', 'of', 'staging', 'an', 'attack', 'on', 'india', 'parliament', 'india', 'has', 'accused', 'pakistan', 'based', 'lashkar', 'taiba', 'and', 'jaish', 'mohammad', 'of', 'carrying', 'out', 'the', 'attack', 'on', 'december', 'at', 'the', 'behest', 'of', 'pakistani', 'military', 'intelligence', 'military', 'tensions', 'have', 'soared', 'since', 'the', 'raid', 'with', 'both', 'sides', 'massing', 'troops', 'along', 'their', 'border', 'and', 'trading', 'tit', 'for', 'tat', 'diplomatic', 'sanctions', 'yesterday', 'pakistan', 'announced', 'it', 'had', 'arrested', 'lashkar', 'taiba', 'chief', 'hafiz', 'mohammed', 'saeed', 'police', 'in', 'karachi', 'say', 'it', 'is', 'likely', 'more', 'raids', 'will', 'be', 'launched', 'against', 'the', 'two', 'groups', 'as', 'well', 'as', 'other', 'militant', 'organisations', 'accused', 'of', 'targetting', 'india', 'military', 'tensions', 'between', 'india', 'and', 'pakistan', 'have', 'escalated', 'to', 'level', 'not', 'seen', 'since', 'their', 'war'], tags=[1])]</code></pre><p>测试语料库如下所示：</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">print</span><span class="token punctuation">(</span>test_corpus<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<pre><code>[['the', 'national', 'executive', 'of', 'the', 'strife', 'torn', 'democrats', 'last', 'night', 'appointed', 'little', 'known', 'west', 'australian', 'senator', 'brian', 'greig', 'as', 'interim', 'leader', 'shock', 'move', 'likely', 'to', 'provoke', 'further', 'conflict', 'between', 'the', 'party', 'senators', 'and', 'its', 'organisation', 'in', 'move', 'to', 'reassert', 'control', 'over', 'the', 'party', 'seven', 'senators', 'the', 'national', 'executive', 'last', 'night', 'rejected', 'aden', 'ridgeway', 'bid', 'to', 'become', 'interim', 'leader', 'in', 'favour', 'of', 'senator', 'greig', 'supporter', 'of', 'deposed', 'leader', 'natasha', 'stott', 'despoja', 'and', 'an', 'outspoken', 'gay', 'rights', 'activist'], ['cash', 'strapped', 'financial', 'services', 'group', 'amp', 'has', 'shelved', 'million', 'plan', 'to', 'buy', 'shares', 'back', 'from', 'investors', 'and', 'will', 'raise', 'million', 'in', 'fresh', 'capital', 'after', 'profits', 'crashed', 'in', 'the', 'six', 'months', 'to', 'june', 'chief', 'executive', 'paul', 'batchelor', 'said', 'the', 'result', 'was', 'solid', 'in', 'what', 'he', 'described', 'as', 'the', 'worst', 'conditions', 'for', 'stock', 'markets', 'in', 'years', 'amp', 'half', 'year', 'profit', 'sank', 'per', 'cent', 'to', 'million', 'or', 'share', 'as', 'australia', 'largest', 'investor', 'and', 'fund', 'manager', 'failed', 'to', 'hit', 'projected', 'per', 'cent', 'earnings', 'growth', 'targets', 'and', 'was', 'battered', 'by', 'falling', 'returns', 'on', 'share', 'markets']]</code></pre><p>请注意，测试语料库只是一个列表列表，不包含任何标签。</p>
<h2 id="Training-the-Model"><a href="#Training-the-Model" class="headerlink" title="Training the Model"></a>Training the Model</h2><p>现在，我们将实例化一个 Doc2Vec 模型，其向量大小为 50 维，并在训练语料库上迭代 40 次。我们将最小字数设置为 2，以便丢弃出现次数很少的单词。（没有各种代表性的例子，保留这种不常用的词通常会使模型变得更糟！）已发表的<a href="https://cs.stanford.edu/~quocle/paragraph_vector.pdf" target="_blank" rel="noopener">段落向量论文</a> 结果中的典型迭代次数，使用数万到数百万的文档，为 10-20。更多的迭代需要更多的时间并最终达到收益递减的点。</p>
<p>然而，这是一个非常非常小的数据集（300 个文档），文档很短（几百个单词）。添加训练通道有时可以帮助处理如此小的数据集。</p>
<pre class="line-numbers language-python"><code class="language-python">model <span class="token operator">=</span> gensim<span class="token punctuation">.</span>models<span class="token punctuation">.</span>doc2vec<span class="token punctuation">.</span>Doc2Vec<span class="token punctuation">(</span>vector_size<span class="token operator">=</span><span class="token number">50</span><span class="token punctuation">,</span> min_count<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> epochs<span class="token operator">=</span><span class="token number">40</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<p>建立词汇</p>
<pre class="line-numbers language-python"><code class="language-python">model<span class="token punctuation">.</span>build_vocab<span class="token punctuation">(</span>train_corpus<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<pre><code>2020-09-30 21:08:55,026 : INFO : collecting all words and their counts
2020-09-30 21:08:55,027 : INFO : PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags
2020-09-30 21:08:55,043 : INFO : collected 6981 word types and 300 unique tags from a corpus of 300 examples and 58152 words
2020-09-30 21:08:55,043 : INFO : Loading a fresh vocabulary
2020-09-30 21:08:55,064 : INFO : effective_min_count=2 retains 3955 unique words (56% of original 6981, drops 3026)
2020-09-30 21:08:55,064 : INFO : effective_min_count=2 leaves 55126 word corpus (94% of original 58152, drops 3026)
2020-09-30 21:08:55,098 : INFO : deleting the raw counts dictionary of 6981 items
2020-09-30 21:08:55,100 : INFO : sample=0.001 downsamples 46 most-common words
2020-09-30 21:08:55,100 : INFO : downsampling leaves estimated 42390 word corpus (76.9% of prior 55126)
2020-09-30 21:08:55,149 : INFO : estimated required memory for 3955 words and 50 dimensions: 3679500 bytes
2020-09-30 21:08:55,149 : INFO : resetting layer weights</code></pre><p>本质上，词汇表是<code>model.wv.index_to_key</code>从训练语料库中提取的所有独特单词的列表（可通过 访问 ）。使用该<code>model.wv.get_vecattr()</code>方法可以获得每个单词的附加属性，例如，查看<code>penalty</code>在训练语料库中出现的次数：</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">print</span><span class="token punctuation">(</span>f<span class="token string">"Word 'penalty' appeared {model.wv.get_vecattr('penalty', 'count')} times in the training corpus."</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<pre><code>Word 'penalty' appeared 4 times in the training corpus.</code></pre><p>接下来，在语料库上训练模型。如果正在使用优化的 Gensim（带有 BLAS 库），这应该不会超过 3 秒。如果不使用 BLAS 库，这应该不会超过 2 分钟，因此如果您珍惜时间，请使用优化的 Gensim 和 BLAS。</p>
<pre class="line-numbers language-python"><code class="language-python">model<span class="token punctuation">.</span>train<span class="token punctuation">(</span>train_corpus<span class="token punctuation">,</span> total_examples<span class="token operator">=</span>model<span class="token punctuation">.</span>corpus_count<span class="token punctuation">,</span> epochs<span class="token operator">=</span>model<span class="token punctuation">.</span>epochs<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<p>现在，我们可以使用经过训练的模型通过将单词列表传递给<code>model.infer_vector</code>函数来推断任何文本片段的向量。然后可以通过余弦相似度将该向量与其他向量进行比较。</p>
<pre class="line-numbers language-python"><code class="language-python">vector <span class="token operator">=</span> model<span class="token punctuation">.</span>infer_vector<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token string">'only'</span><span class="token punctuation">,</span> <span class="token string">'you'</span><span class="token punctuation">,</span> <span class="token string">'can'</span><span class="token punctuation">,</span> <span class="token string">'prevent'</span><span class="token punctuation">,</span> <span class="token string">'forest'</span><span class="token punctuation">,</span> <span class="token string">'fires'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>vector<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>
<p>出去：</p>
<pre><code>[-0.08478509  0.05011684  0.0675064  -0.19926868 -0.1235586   0.01768214
 -0.12645927  0.01062329  0.06113973  0.35424358  0.01320948  0.07561274
 -0.01645093  0.0692549   0.08346193 -0.01599065  0.08287009 -0.0139379
 -0.17772709 -0.26271465  0.0442089  -0.04659882 -0.12873884  0.28799203
 -0.13040264  0.12478471 -0.14091878 -0.09698066 -0.07903259 -0.10124907
 -0.28239366  0.13270256  0.04445919 -0.24210942 -0.1907376  -0.07264525
 -0.14167067 -0.22816683 -0.00663796  0.23165748 -0.10436232 -0.01028251
 -0.04064698  0.08813146  0.01072008 -0.149789    0.05923386  0.16301566
  0.05815683  0.1258063 ]</code></pre><p>请注意，<code>infer_vector()</code>它<em>不</em>接受一个字符串，而是一个字符串标记列表，它应该已经以与<code>words</code>原始训练文档对象的属性相同的方式进行标记 。</p>
<p>另请注意，由于底层训练/推理算法是使用内部随机化的迭代逼近问题，因此同一文本的重复推理将返回略有不同的向量。</p>
<h2 id="Assessing-the-Model"><a href="#Assessing-the-Model" class="headerlink" title="Assessing the Model"></a>Assessing the Model</h2><p>为了评估我们的新模型，我们将首先为训练语料库的每个文档推断新向量，将推断的向量与训练语料库进行比较，然后根据自相似性返回文档的排名。基本上，我们假装训练语料库是一些新的未见数据，然后查看它们与训练模型的比较。预期我们可能会过度拟合我们的模型（即，所有等级都将小于 2），因此我们应该能够非常轻松地找到类似的文档。此外，我们将跟踪第二个等级以比较不太相似的文档。</p>
<pre class="line-numbers language-python"><code class="language-python">ranks <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
second_ranks <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
<span class="token keyword">for</span> doc_id <span class="token keyword">in</span> range<span class="token punctuation">(</span>len<span class="token punctuation">(</span>train_corpus<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    inferred_vector <span class="token operator">=</span> model<span class="token punctuation">.</span>infer_vector<span class="token punctuation">(</span>train_corpus<span class="token punctuation">[</span>doc_id<span class="token punctuation">]</span><span class="token punctuation">.</span>words<span class="token punctuation">)</span>
    sims <span class="token operator">=</span> model<span class="token punctuation">.</span>dv<span class="token punctuation">.</span>most_similar<span class="token punctuation">(</span><span class="token punctuation">[</span>inferred_vector<span class="token punctuation">]</span><span class="token punctuation">,</span> topn<span class="token operator">=</span>len<span class="token punctuation">(</span>model<span class="token punctuation">.</span>dv<span class="token punctuation">)</span><span class="token punctuation">)</span>
    rank <span class="token operator">=</span> <span class="token punctuation">[</span>docid <span class="token keyword">for</span> docid<span class="token punctuation">,</span> sim <span class="token keyword">in</span> sims<span class="token punctuation">]</span><span class="token punctuation">.</span>index<span class="token punctuation">(</span>doc_id<span class="token punctuation">)</span>
    ranks<span class="token punctuation">.</span>append<span class="token punctuation">(</span>rank<span class="token punctuation">)</span>

    second_ranks<span class="token punctuation">.</span>append<span class="token punctuation">(</span>sims<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>让我们计算每个文档相对于训练语料库的排名</p>
<p>注意。由于随机播种和非常小的语料库，结果因运行而异</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> collections

counter <span class="token operator">=</span> collections<span class="token punctuation">.</span>Counter<span class="token punctuation">(</span>ranks<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>counter<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>
<pre><code>Counter({0: 292, 1: 8})</code></pre><p>基本上，超过 95% 的推断文档被发现与自身最相似，大约 5% 的时间被错误地认为与另一个文档最相似。根据训练向量检查推断向量是一种关于模型是否以有用一致的方式运行的“健全性检查”，尽管不是真正的“准确度”值。</p>
<p>这很好，并不完全令人惊讶。我们可以看一个例子：</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'Document ({}): «{}»\n'</span><span class="token punctuation">.</span>format<span class="token punctuation">(</span>doc_id<span class="token punctuation">,</span> <span class="token string">' '</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span>train_corpus<span class="token punctuation">[</span>doc_id<span class="token punctuation">]</span><span class="token punctuation">.</span>words<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>u<span class="token string">'SIMILAR/DISSIMILAR DOCS PER MODEL %s:\n'</span> <span class="token operator">%</span> model<span class="token punctuation">)</span>
<span class="token keyword">for</span> label<span class="token punctuation">,</span> index <span class="token keyword">in</span> <span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token string">'MOST'</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token string">'SECOND-MOST'</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token string">'MEDIAN'</span><span class="token punctuation">,</span> len<span class="token punctuation">(</span>sims<span class="token punctuation">)</span><span class="token operator">//</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token string">'LEAST'</span><span class="token punctuation">,</span> len<span class="token punctuation">(</span>sims<span class="token punctuation">)</span> <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">:</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>u<span class="token string">'%s %s: «%s»\n'</span> <span class="token operator">%</span> <span class="token punctuation">(</span>label<span class="token punctuation">,</span> sims<span class="token punctuation">[</span>index<span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token string">' '</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span>train_corpus<span class="token punctuation">[</span>sims<span class="token punctuation">[</span>index<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">.</span>words<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>
<pre><code>Document (299): «australia will take on france in the doubles rubber of the davis cup tennis final today with the tie levelled at wayne arthurs and todd woodbridge are scheduled to lead australia in the doubles against cedric pioline and fabrice santoro however changes can be made to the line up up to an hour before the match and australian team captain john fitzgerald suggested he might do just that we ll make team appraisal of the whole situation go over the pros and cons and make decision french team captain guy forget says he will not make changes but does not know what to expect from australia todd is the best doubles player in the world right now so expect him to play he said would probably use wayne arthurs but don know what to expect really pat rafter salvaged australia davis cup campaign yesterday with win in the second singles match rafter overcame an arm injury to defeat french number one sebastien grosjean in three sets the australian says he is happy with his form it not very pretty tennis there isn too many consistent bounces you are playing like said bit of classic old grass court rafter said rafter levelled the score after lleyton hewitt shock five set loss to nicholas escude in the first singles rubber but rafter says he felt no added pressure after hewitt defeat knew had good team to back me up even if we were down he said knew could win on the last day know the boys can win doubles so even if we were down still feel we are good enough team to win and vice versa they are good enough team to beat us as well»

SIMILAR/DISSIMILAR DOCS PER MODEL Doc2Vec(dm/m,d50,n5,w5,mc2,s0.001,t3):

MOST (299, 0.9482713341712952): «australia will take on france in the doubles rubber of the davis cup tennis final today with the tie levelled at wayne arthurs and todd woodbridge are scheduled to lead australia in the doubles against cedric pioline and fabrice santoro however changes can be made to the line up up to an hour before the match and australian team captain john fitzgerald suggested he might do just that we ll make team appraisal of the whole situation go over the pros and cons and make decision french team captain guy forget says he will not make changes but does not know what to expect from australia todd is the best doubles player in the world right now so expect him to play he said would probably use wayne arthurs but don know what to expect really pat rafter salvaged australia davis cup campaign yesterday with win in the second singles match rafter overcame an arm injury to defeat french number one sebastien grosjean in three sets the australian says he is happy with his form it not very pretty tennis there isn too many consistent bounces you are playing like said bit of classic old grass court rafter said rafter levelled the score after lleyton hewitt shock five set loss to nicholas escude in the first singles rubber but rafter says he felt no added pressure after hewitt defeat knew had good team to back me up even if we were down he said knew could win on the last day know the boys can win doubles so even if we were down still feel we are good enough team to win and vice versa they are good enough team to beat us as well»

SECOND-MOST (104, 0.8029672503471375): «australian cricket captain steve waugh has supported fast bowler brett lee after criticism of his intimidatory bowling to the south african tailenders in the first test in adelaide earlier this month lee was fined for giving new zealand tailender shane bond an unsportsmanlike send off during the third test in perth waugh says tailenders should not be protected from short pitched bowling these days you re earning big money you ve got responsibility to learn how to bat he said mean there no times like years ago when it was not professional and sort of bowlers code these days you re professional our batsmen work very hard at their batting and expect other tailenders to do likewise meanwhile waugh says his side will need to guard against complacency after convincingly winning the first test by runs waugh says despite the dominance of his side in the first test south africa can never be taken lightly it only one test match out of three or six whichever way you want to look at it so there lot of work to go he said but it nice to win the first battle definitely it gives us lot of confidence going into melbourne you know the big crowd there we love playing in front of the boxing day crowd so that will be to our advantage as well south africa begins four day match against new south wales in sydney on thursday in the lead up to the boxing day test veteran fast bowler allan donald will play in the warm up match and is likely to take his place in the team for the second test south african captain shaun pollock expects much better performance from his side in the melbourne test we still believe that we didn play to our full potential so if we can improve on our aspects the output we put out on the field will be lot better and we still believe we have side that is good enough to beat australia on our day he said»

MEDIAN (238, 0.2635717988014221): «centrelink is urging people affected by job cuts at regional pay tv operator austar and travel company traveland to seek information about their income support options traveland has announced it is shedding more than jobs around australia and austar is letting employees go centrelink finance information officer peter murray says those facing uncertain futures should head to centrelink in the next few days centrelink is the shopfront now for commonwealth services for income support and the employment network so that it is important if people haven been to us before they might get pleasant surprise at the range of services that we do offer to try and help them through situations where things might have changed for them mr murray said»

LEAST (243, -0.13247375190258026): «four afghan factions have reached agreement on an interim cabinet during talks in germany the united nations says the administration which will take over from december will be headed by the royalist anti taliban commander hamed karzai it concludes more than week of negotiations outside bonn and is aimed at restoring peace and stability to the war ravaged country the year old former deputy foreign minister who is currently battling the taliban around the southern city of kandahar is an ally of the exiled afghan king mohammed zahir shah he will serve as chairman of an interim authority that will govern afghanistan for six month period before loya jirga or grand traditional assembly of elders in turn appoints an month transitional government meanwhile united states marines are now reported to have been deployed in eastern afghanistan where opposition forces are closing in on al qaeda soldiers reports from the area say there has been gun battle between the opposition and al qaeda close to the tora bora cave complex where osama bin laden is thought to be hiding in the south of the country american marines are taking part in patrols around the air base they have secured near kandahar but are unlikely to take part in any assault on the city however the chairman of the joint chiefs of staff general richard myers says they are prepared for anything they are prepared for engagements they re robust fighting force and they re absolutely ready to engage if that required he said»</code></pre><p>请注意，最相似的文档（通常是相同的文本）的相似度得分接近 1.0。然而，排名第二的文档的相似度得分应该明显较低（假设文档实际上不同），并且当我们检查文本本身时，推理变得明显。</p>
<p>我们可以重复运行下一个单元格以查看其他目标文档比较的采样。</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># Pick a random document from the corpus and infer a vector from the model</span>
<span class="token keyword">import</span> random
doc_id <span class="token operator">=</span> random<span class="token punctuation">.</span>randint<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> len<span class="token punctuation">(</span>train_corpus<span class="token punctuation">)</span> <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">)</span>

<span class="token comment" spellcheck="true"># Compare and print the second-most-similar document</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'Train Document ({}): «{}»\n'</span><span class="token punctuation">.</span>format<span class="token punctuation">(</span>doc_id<span class="token punctuation">,</span> <span class="token string">' '</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span>train_corpus<span class="token punctuation">[</span>doc_id<span class="token punctuation">]</span><span class="token punctuation">.</span>words<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
sim_id <span class="token operator">=</span> second_ranks<span class="token punctuation">[</span>doc_id<span class="token punctuation">]</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'Similar Document {}: «{}»\n'</span><span class="token punctuation">.</span>format<span class="token punctuation">(</span>sim_id<span class="token punctuation">,</span> <span class="token string">' '</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span>train_corpus<span class="token punctuation">[</span>sim_id<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">.</span>words<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<pre><code>Train Document (292): «rival afghan factions are deadlocked over the shape of future government the northern alliance has demanded day adjournment of power sharing talks in germany after its president burhanuddin rabbani objected to the appointment system for an interim administration president rabbani has objected to the plans for an interim government to be drawn up by appointment as discussed in bonn saying the interim leaders should be voted in by afghans themselves he also says there is no real need for sizeable international security force president rabbani says he would prefer local afghan factions drew up their own internal security forces of around personnel but if the world insisted there should be an international security presence there should be no more than or personnel in their security forces he says president rabbani objections are likely to cast doubt on his delegation ability to commit the northern alliance to any course of action decided upon in bonn he now threatens to undermine the very process he claims to support in the quest for stable government in afghanistan»

Similar Document (13, 0.7867921590805054): «talks between afghan and british officials in kabul have ended without final ag</code></pre><h2 id="Testing-the-Model"><a href="#Testing-the-Model" class="headerlink" title="Testing the Model"></a>Testing the Model</h2><p>使用上述相同的方法，我们将推断随机选择的测试文档的向量，并通过眼睛将文档与我们的模型进行比较。</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># Pick a random document from the test corpus and infer a vector from the model</span>
doc_id <span class="token operator">=</span> random<span class="token punctuation">.</span>randint<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> len<span class="token punctuation">(</span>test_corpus<span class="token punctuation">)</span> <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">)</span>
inferred_vector <span class="token operator">=</span> model<span class="token punctuation">.</span>infer_vector<span class="token punctuation">(</span>test_corpus<span class="token punctuation">[</span>doc_id<span class="token punctuation">]</span><span class="token punctuation">)</span>
sims <span class="token operator">=</span> model<span class="token punctuation">.</span>dv<span class="token punctuation">.</span>most_similar<span class="token punctuation">(</span><span class="token punctuation">[</span>inferred_vector<span class="token punctuation">]</span><span class="token punctuation">,</span> topn<span class="token operator">=</span>len<span class="token punctuation">(</span>model<span class="token punctuation">.</span>dv<span class="token punctuation">)</span><span class="token punctuation">)</span>

<span class="token comment" spellcheck="true"># Compare and print the most/median/least similar documents from the train corpus</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'Test Document ({}): «{}»\n'</span><span class="token punctuation">.</span>format<span class="token punctuation">(</span>doc_id<span class="token punctuation">,</span> <span class="token string">' '</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span>test_corpus<span class="token punctuation">[</span>doc_id<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>u<span class="token string">'SIMILAR/DISSIMILAR DOCS PER MODEL %s:\n'</span> <span class="token operator">%</span> model<span class="token punctuation">)</span>
<span class="token keyword">for</span> label<span class="token punctuation">,</span> index <span class="token keyword">in</span> <span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token string">'MOST'</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token string">'MEDIAN'</span><span class="token punctuation">,</span> len<span class="token punctuation">(</span>sims<span class="token punctuation">)</span><span class="token operator">//</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token string">'LEAST'</span><span class="token punctuation">,</span> len<span class="token punctuation">(</span>sims<span class="token punctuation">)</span> <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">:</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>u<span class="token string">'%s %s: «%s»\n'</span> <span class="token operator">%</span> <span class="token punctuation">(</span>label<span class="token punctuation">,</span> sims<span class="token punctuation">[</span>index<span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token string">' '</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span>train_corpus<span class="token punctuation">[</span>sims<span class="token punctuation">[</span>index<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">.</span>words<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<pre><code>Test Document (49): «labor needed to distinguish itself from the government on the issue of asylum seekers greens leader bob brown has said his senate colleague kerry nettle intends to move motion today on the first anniversary of the tampa crisis condemning the government over its refugee policy and calling for an end to mandatory detention we greens want to bring the government to book over its serial breach of international obligations as far as asylum seekers in this country are concerned senator brown said today»

SIMILAR/DISSIMILAR DOCS PER MODEL Doc2Vec(dm/m,d50,n5,w5,mc2,s0.001,t3):

MOST (218, 0.8016394376754761): «refugee support groups are strongly critical of federal government claims that the pacific solution program is working well the immigration minister philip ruddock says he is pleased with the program which uses pacific island nations to process asylum seekers wanting to come to australia president of the hazara ethnic society of australia hassan ghulam says the australian government is bullying smaller nations into accepting asylum seekers if the pacific countries wanted refugees they can clearly raise their voice in the united nations and say yes we are accepting refugees and why australia who gives this authority to the australian government to force the pacific countries to accept refugees in this form or in the other form he asked»

MEDIAN (204, 0.3319269120693207): «an iraqi doctor being held at sydney villawood detention centre claims he was prevented from receiving human rights award dr aamer sultan had been awarded special commendation at yesterday human rights and equal opportunity commission awards in sydney but was not able to receive the honour in person dr sultan says he had been hoping to attend the ceremony but says the management at villawood stopped him from going submitted formal request to the centre manager who promised me that he will present the matter to migration management here who are the main authority here they also came back that unfortunately we can not fulfill this request for you but they didn give any explanation dr sultan says he was disappointed by the decision the immigration minister philip ruddock has written letter of complaint to the medical journal of australia about an article penned by dr sultan on the psychological state of detainees at villawood the journal has published research dr sultan conducted with former visiting psychologist to the centre kevin sullivan their survey of detainees over nine months found all but one displayed symptoms of psychological distress at some time the article says per cent acknowledged chronic depressive symptoms and close to half of the group had reached severe stages of depression»

LEAST (157, -0.10524928569793701): «british man has been found guilty by unanimous verdict of the kidnap and murder of an e</code></pre><h2 id="Additional-Resources"><a href="#Additional-Resources" class="headerlink" title="Additional Resources"></a>Additional Resources</h2><ul>
<li><a href="https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf" target="_blank" rel="noopener">Word2Vec Paper</a></li>
<li><a href="https://cs.stanford.edu/~quocle/paragraph_vector.pdf" target="_blank" rel="noopener">Doc2Vec Paper</a></li>
<li><a href="http://faculty.sites.uci.edu/mdlee" target="_blank" rel="noopener">Dr. Michael D. Lee’s Website</a></li>
<li><a href="http://faculty.sites.uci.edu/mdlee/similarity-data/" target="_blank" rel="noopener">Lee Corpus</a></li>
<li><a href="https://radimrehurek.com/gensim/auto_examples/tutorials/doc2vec-IMDB.ipynb" target="_blank" rel="noopener">IMDB Doc2Vec Tutorial</a></li>
</ul>
<h1 id="FastText-Model"><a href="#FastText-Model" class="headerlink" title="FastText Model"></a>FastText Model</h1><p>介绍 Gensim 的 fastText 模型并演示其在 Lee 语料库上的使用。</p>
<p>在这里，我们将学习使用 fastText 库来训练词嵌入模型、保存和加载它们并执行类似于 Word2Vec 的相似性操作和向量查找。</p>
<h2 id="什么时候使用-fastText？"><a href="#什么时候使用-fastText？" class="headerlink" title="什么时候使用 fastText？"></a>什么时候使用 fastText？</h2><p><a href="https://github.com/facebookresearch/fastText" target="_blank" rel="noopener">fastText</a>背后的主要原则是单词的形态结构携带有关单词含义的重要信息。像 Word2Vec 这样的传统词嵌入没有考虑这种结构，它为每个单独的词训练一个独特的词嵌入。这对于形态丰富的语言（德语、土耳其语）尤其重要，在这些语言中，单个单词可以有大量的形态形式，每种形式都可能很少出现，因此很难训练好的词嵌入。</p>
<p>fastText 试图通过将每个单词视为其子单词的聚合来解决这个问题。为了简单和语言独立，子词被视为词的字符 ngrams。单词的向量被简单地视为其组成部分 char-ngram 的所有向量的总和。</p>
<p>根据<a href="https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/Word2Vec_FastText_Comparison.ipynb" target="_blank" rel="noopener">笔记</a>中 Word2Vec 和 fastText 的详细比较，fastText 在句法任务上的表现明显优于原始 Word2Vec，尤其是在训练语料库规模较小的情况下。不过，Word2Vec 在语义任务上略胜于 fastText。随着训练语料库规模的增加，差异变得越来越小。</p>
<p>只要训练数据中至少存在一个 char-ngram，fastText 甚至可以通过对其组成部分 char-ngram 的向量求和来获得词汇外 (OOV) 词的向量。</p>
<h2 id="训练模型"><a href="#训练模型" class="headerlink" title="训练模型"></a>训练模型</h2><p>对于以下示例，我们将使用 Lee 语料库（如果您安装了 Gensim，您已经拥有）来训练我们的模型。</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">from</span> pprint <span class="token keyword">import</span> pprint <span class="token keyword">as</span> <span class="token keyword">print</span>
<span class="token keyword">from</span> gensim<span class="token punctuation">.</span>models<span class="token punctuation">.</span>fasttext <span class="token keyword">import</span> FastText
<span class="token keyword">from</span> gensim<span class="token punctuation">.</span>test<span class="token punctuation">.</span>utils <span class="token keyword">import</span> datapath

<span class="token comment" spellcheck="true"># Set file names for train and test data</span>
corpus_file <span class="token operator">=</span> datapath<span class="token punctuation">(</span><span class="token string">'lee_background.cor'</span><span class="token punctuation">)</span>

model <span class="token operator">=</span> FastText<span class="token punctuation">(</span>vector_size<span class="token operator">=</span><span class="token number">100</span><span class="token punctuation">)</span>

<span class="token comment" spellcheck="true"># build the vocabulary</span>
model<span class="token punctuation">.</span>build_vocab<span class="token punctuation">(</span>corpus_file<span class="token operator">=</span>corpus_file<span class="token punctuation">)</span>

<span class="token comment" spellcheck="true"># train the model</span>
model<span class="token punctuation">.</span>train<span class="token punctuation">(</span>
    corpus_file<span class="token operator">=</span>corpus_file<span class="token punctuation">,</span> epochs<span class="token operator">=</span>model<span class="token punctuation">.</span>epochs<span class="token punctuation">,</span>
    total_examples<span class="token operator">=</span>model<span class="token punctuation">.</span>corpus_count<span class="token punctuation">,</span> total_words<span class="token operator">=</span>model<span class="token punctuation">.</span>corpus_total_words<span class="token punctuation">,</span>
<span class="token punctuation">)</span>

<span class="token keyword">print</span><span class="token punctuation">(</span>model<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>出去：</p>
<pre><code>&lt;gensim.models.fasttext.FastText object at 0x20ce0d390&gt;</code></pre><h3 id="训练超参数"><a href="#训练超参数" class="headerlink" title="训练超参数"></a>训练超参数</h3><p>用于训练模型的超参数遵循与 Word2Vec 相同的模式。FastText 支持原始 word2vec 中的以下参数：</p>
<ul>
<li><strong>model</strong>: Training architecture. Allowed values: <strong>cbow, skipgram</strong> (Default cbow)</li>
<li><strong>vector_size</strong>: Dimensionality of vector embeddings to be learnt (Default <strong>100</strong>)</li>
<li><strong>alpha</strong>: Initial learning rate (Default <strong>0.025</strong>)</li>
<li><strong>window</strong>: Context window size (Default <strong>5</strong>)</li>
<li><strong>min_count</strong>: Ignore words with number of occurrences below this (Default <strong>5</strong>)</li>
<li><strong>loss</strong>: Training objective. Allowed values: <strong>ns, hs, softmax</strong> (Default ns)</li>
<li><strong>sample</strong>: Threshold for downsampling higher-frequency words (Default <strong>0.001</strong>)</li>
<li><strong>negative</strong>: Number of negative words to sample, for <strong>ns</strong> (Default 5)</li>
<li><strong>epochs</strong>: Number of epochs (Default <strong>5</strong>)</li>
<li><strong>sorted_vocab</strong>: Sort vocab by descending frequency (Default 1)</li>
<li><strong>threads</strong>: Number of threads to use (Default 12)</li>
</ul>
<p>In addition, fastText has three additional parameters:</p>
<ul>
<li><strong>min_n</strong>: min length of char ngrams (Default 3)</li>
<li><strong>max_n</strong>: max length of char ngrams (Default 6)</li>
<li><strong>bucke</strong>t: number of buckets used for hashing ngrams (Default 2000000)</li>
</ul>
<p>参数<code>min_n</code>并<code>max_n</code>控制在训练和查找嵌入时每个单词分解成的字符 ngram 的长度。如果<code>max_n</code>设置为 0 或小于<code>min_n</code>，则不使用字符 ngram，模型有效地简化为 Word2Vec。</p>
<p>为了限制正在训练的模型的内存要求，使用了将 ngram 映射到 1 到 K 中的整数的散列函数。为了散列这些字符序列，使用了<a href="http://www.isthe.com/chongo/tech/comp/fnv" target="_blank" rel="noopener">Fowler-Noll-Vo 散列函数</a>（FNV-1a 变体）。</p>
<p><strong>注意：</strong>您可以在使用 Gensim 的 fastText 本地实现的同时继续训练您的模型。</p>
<h2 id="保存-加载模型"><a href="#保存-加载模型" class="headerlink" title="保存/加载模型"></a>保存/加载模型</h2><p>模型可以通过<code>load</code>和<code>save</code>方法保存和加载，就像 Gensim 中的任何其他模型一样。</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># Save a model trained via Gensim's fastText implementation to temp.</span>
<span class="token keyword">import</span> tempfile
<span class="token keyword">import</span> os
<span class="token keyword">with</span> tempfile<span class="token punctuation">.</span>NamedTemporaryFile<span class="token punctuation">(</span>prefix<span class="token operator">=</span><span class="token string">'saved_model_gensim-'</span><span class="token punctuation">,</span> delete<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span> <span class="token keyword">as</span> tmp<span class="token punctuation">:</span>
    model<span class="token punctuation">.</span>save<span class="token punctuation">(</span>tmp<span class="token punctuation">.</span>name<span class="token punctuation">,</span> separately<span class="token operator">=</span><span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

<span class="token comment" spellcheck="true"># Load back the same model.</span>
loaded_model <span class="token operator">=</span> FastText<span class="token punctuation">.</span>load<span class="token punctuation">(</span>tmp<span class="token punctuation">.</span>name<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>loaded_model<span class="token punctuation">)</span>

os<span class="token punctuation">.</span>unlink<span class="token punctuation">(</span>tmp<span class="token punctuation">.</span>name<span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># demonstration complete, don't need the temp file anymore</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>出去：</p>
<pre><code>&lt;gensim.models.fasttext.FastText object at 0x20cc99d30&gt;</code></pre><p>该<code>save_word2vec_format</code>也可用于fastText车型，但将导致所有向量丢失的n-gram。因此，以这种方式加载的模型将表现为常规 word2vec 模型。</p>
<h2 id="词向量查找"><a href="#词向量查找" class="headerlink" title="词向量查找"></a>词向量查找</h2><p>查找 fastText 词（包括 OOV 词）所需的所有信息都包含在其<code>model.wv</code>属性中。</p>
<p>如果您不需要继续训练您的模型，您可以导出并保存此.wv 属性并丢弃模型，以节省空间和 RAM。</p>
<pre class="line-numbers language-python"><code class="language-python">wv <span class="token operator">=</span> model<span class="token punctuation">.</span>wv
<span class="token keyword">print</span><span class="token punctuation">(</span>wv<span class="token punctuation">)</span>

<span class="token comment" spellcheck="true">#</span>
<span class="token comment" spellcheck="true"># FastText models support vector lookups for out-of-vocabulary words by summing up character ngrams belonging to the word.</span>
<span class="token comment" spellcheck="true">#</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'night'</span> <span class="token keyword">in</span> wv<span class="token punctuation">.</span>key_to_index<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<pre class="line-numbers language-python"><code class="language-python"><span class="token operator">&lt;</span>gensim<span class="token punctuation">.</span>models<span class="token punctuation">.</span>fasttext<span class="token punctuation">.</span>FastTextKeyedVectors object at <span class="token number">0x20ce0d828</span><span class="token operator">></span>
<span class="token boolean">True</span>

<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'nights'</span> <span class="token keyword">in</span> wv<span class="token punctuation">.</span>key_to_index<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>
<pre class="line-numbers language-python"><code class="language-python"><span class="token boolean">False</span>

<span class="token keyword">print</span><span class="token punctuation">(</span>wv<span class="token punctuation">[</span><span class="token string">'night'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>
<pre class="line-numbers language-python"><code class="language-python">array<span class="token punctuation">(</span><span class="token punctuation">[</span> <span class="token number">0.12453239</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.26018462</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.04087191</span><span class="token punctuation">,</span>  <span class="token number">0.2563215</span> <span class="token punctuation">,</span>  <span class="token number">0.31401935</span><span class="token punctuation">,</span>
        <span class="token number">0.16155584</span><span class="token punctuation">,</span>  <span class="token number">0.39527607</span><span class="token punctuation">,</span>  <span class="token number">0.27404118</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.45236284</span><span class="token punctuation">,</span>  <span class="token number">0.06942682</span><span class="token punctuation">,</span>
        <span class="token number">0.36584955</span><span class="token punctuation">,</span>  <span class="token number">0.51162827</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.51161295</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.192019</span>  <span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.5068029</span> <span class="token punctuation">,</span>
       <span class="token operator">-</span><span class="token number">0.07426998</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.6276584</span> <span class="token punctuation">,</span>  <span class="token number">0.22271585</span><span class="token punctuation">,</span>  <span class="token number">0.19990133</span><span class="token punctuation">,</span>  <span class="token number">0.2582401</span> <span class="token punctuation">,</span>
        <span class="token number">0.14329399</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.01959469</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.45576197</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.06447829</span><span class="token punctuation">,</span>  <span class="token number">0.1493489</span> <span class="token punctuation">,</span>
        <span class="token number">0.17261286</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.13472046</span><span class="token punctuation">,</span>  <span class="token number">0.26546794</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.34596932</span><span class="token punctuation">,</span>  <span class="token number">0.5626187</span> <span class="token punctuation">,</span>
       <span class="token operator">-</span><span class="token number">0.7038802</span> <span class="token punctuation">,</span>  <span class="token number">0.15603925</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.03104019</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.06228801</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.13480644</span><span class="token punctuation">,</span>
       <span class="token operator">-</span><span class="token number">0.0684596</span> <span class="token punctuation">,</span>  <span class="token number">0.24728075</span><span class="token punctuation">,</span>  <span class="token number">0.55081636</span><span class="token punctuation">,</span>  <span class="token number">0.07330963</span><span class="token punctuation">,</span>  <span class="token number">0.32814154</span><span class="token punctuation">,</span>
        <span class="token number">0.1574982</span> <span class="token punctuation">,</span>  <span class="token number">0.56742406</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.31233737</span><span class="token punctuation">,</span>  <span class="token number">0.14195296</span><span class="token punctuation">,</span>  <span class="token number">0.0540203</span> <span class="token punctuation">,</span>
        <span class="token number">0.01718009</span><span class="token punctuation">,</span>  <span class="token number">0.05519052</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.04002226</span><span class="token punctuation">,</span>  <span class="token number">0.16157456</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.5134223</span> <span class="token punctuation">,</span>
       <span class="token operator">-</span><span class="token number">0.01033936</span><span class="token punctuation">,</span>  <span class="token number">0.05745083</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.39208183</span><span class="token punctuation">,</span>  <span class="token number">0.52553374</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1.0542839</span> <span class="token punctuation">,</span>
        <span class="token number">0.2145304</span> <span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.15234643</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.35197273</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.6215585</span> <span class="token punctuation">,</span>  <span class="token number">0.01796502</span><span class="token punctuation">,</span>
        <span class="token number">0.21242104</span><span class="token punctuation">,</span>  <span class="token number">0.30762967</span><span class="token punctuation">,</span>  <span class="token number">0.2787644</span> <span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.19908747</span><span class="token punctuation">,</span>  <span class="token number">0.7144409</span> <span class="token punctuation">,</span>
        <span class="token number">0.45586124</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.21344525</span><span class="token punctuation">,</span>  <span class="token number">0.26920903</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.651759</span>  <span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.37096855</span><span class="token punctuation">,</span>
       <span class="token operator">-</span><span class="token number">0.16243419</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.3085725</span> <span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.70485127</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.04926324</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.80278563</span><span class="token punctuation">,</span>
       <span class="token operator">-</span><span class="token number">0.24352737</span><span class="token punctuation">,</span>  <span class="token number">0.6427129</span> <span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.3530421</span> <span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.29960123</span><span class="token punctuation">,</span>  <span class="token number">0.01466726</span><span class="token punctuation">,</span>
       <span class="token operator">-</span><span class="token number">0.18253349</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.2489397</span> <span class="token punctuation">,</span>  <span class="token number">0.00648343</span><span class="token punctuation">,</span>  <span class="token number">0.18057272</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.11812428</span><span class="token punctuation">,</span>
       <span class="token operator">-</span><span class="token number">0.49044088</span><span class="token punctuation">,</span>  <span class="token number">0.1847386</span> <span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.27946883</span><span class="token punctuation">,</span>  <span class="token number">0.3941279</span> <span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.39211616</span><span class="token punctuation">,</span>
        <span class="token number">0.26847798</span><span class="token punctuation">,</span>  <span class="token number">0.41468227</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.3953728</span> <span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.25371104</span><span class="token punctuation">,</span>  <span class="token number">0.3390468</span> <span class="token punctuation">,</span>
       <span class="token operator">-</span><span class="token number">0.16447693</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.18722224</span><span class="token punctuation">,</span>  <span class="token number">0.2782088</span> <span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.0696249</span> <span class="token punctuation">,</span>  <span class="token number">0.4313547</span> <span class="token punctuation">]</span><span class="token punctuation">,</span>
      dtype<span class="token operator">=</span>float32<span class="token punctuation">)</span>

<span class="token keyword">print</span><span class="token punctuation">(</span>wv<span class="token punctuation">[</span><span class="token string">'nights'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<pre><code>array([ 0.10586783, -0.22489995, -0.03636307,  0.22263278,  0.27037606,
        0.1394871 ,  0.3411114 ,  0.2369042 , -0.38989475,  0.05935   ,
        0.31713557,  0.44301754, -0.44249156, -0.16652377, -0.4388366 ,
       -0.06266895, -0.5436303 ,  0.19294666,  0.17363031,  0.22459263,
        0.12532061, -0.01866964, -0.3936521 , -0.05507145,  0.12905194,
        0.14942174, -0.11657442,  0.22935589, -0.29934618,  0.4859668 ,
       -0.6073519 ,  0.13433163, -0.02491274, -0.05468523, -0.11884545,
       -0.06117092,  0.21444008,  0.4775469 ,  0.06227469,  0.28350767,
        0.13580805,  0.48993143, -0.27067345,  0.1252003 ,  0.04606731,
        0.01598426,  0.04640368, -0.03456376,  0.14138013, -0.44429192,
       -0.00865329,  0.05027836, -0.341311  ,  0.45402458, -0.91097856,
        0.1868968 , -0.13116683, -0.30361563, -0.5364188 ,  0.01603454,
        0.18146741,  0.26708448,  0.24074472, -0.17163375,  0.61906886,
        0.39530373, -0.18259627,  0.23319626, -0.5634787 , -0.31959867,
       -0.13945322, -0.269441  , -0.60941464, -0.0403638 , -0.69563633,
       -0.2098089 ,  0.5569868 , -0.30320194, -0.25840232,  0.01436759,
       -0.15632603, -0.21624804,  0.00434287,  0.15566474, -0.10228094,
       -0.4249678 ,  0.16197811, -0.24147548,  0.34205705, -0.3391568 ,
        0.23235887,  0.35860622, -0.34247142, -0.21777524,  0.29318404,
       -0.1407287 , -0.16115218,  0.24247572, -0.06217333,  0.37221798],
      dtype=float32)</code></pre><h2 id="相似操作"><a href="#相似操作" class="headerlink" title="相似操作"></a>相似操作</h2><p>相似性操作的工作方式与 word2vec 相同。<strong>也可以使用词汇表外的单词，前提是它们在训练数据中至少有一个字符 ngram。</strong></p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"nights"</span> <span class="token keyword">in</span> wv<span class="token punctuation">.</span>key_to_index<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<pre class="line-numbers language-python"><code class="language-python"><span class="token boolean">False</span>

<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"night"</span> <span class="token keyword">in</span> wv<span class="token punctuation">.</span>key_to_index<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>
<pre class="line-numbers language-python"><code class="language-python"><span class="token boolean">True</span>

<span class="token keyword">print</span><span class="token punctuation">(</span>wv<span class="token punctuation">.</span>similarity<span class="token punctuation">(</span><span class="token string">"night"</span><span class="token punctuation">,</span> <span class="token string">"nights"</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>
<pre><code>0.9999929</code></pre><p>句法相似的词在 fastText 模型中通常具有很高的相似性，因为大量的组成字符-ngram 将是相同的。因此，fastText 在语法任务上通常比 Word2Vec 做得更好。<a href="https://radimrehurek.com/gensim/auto_examples/tutorials/Word2Vec_FastText_Comparison.ipynb" target="_blank" rel="noopener">此处</a>提供了详细的比较。</p>
<h3 id="其他相似操作"><a href="#其他相似操作" class="headerlink" title="其他相似操作"></a>其他相似操作</h3><p>示例训练语料库是一个玩具语料库，预计结果不会很好，仅用于概念验证</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">print</span><span class="token punctuation">(</span>wv<span class="token punctuation">.</span>most_similar<span class="token punctuation">(</span><span class="token string">"nights"</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<pre class="line-numbers language-python"><code class="language-python"><span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token string">'night'</span><span class="token punctuation">,</span> <span class="token number">0.9999929070472717</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
 <span class="token punctuation">(</span><span class="token string">'night.'</span><span class="token punctuation">,</span> <span class="token number">0.9999895095825195</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
 <span class="token punctuation">(</span><span class="token string">'flights'</span><span class="token punctuation">,</span> <span class="token number">0.999988853931427</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
 <span class="token punctuation">(</span><span class="token string">'rights'</span><span class="token punctuation">,</span> <span class="token number">0.9999886751174927</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
 <span class="token punctuation">(</span><span class="token string">'residents'</span><span class="token punctuation">,</span> <span class="token number">0.9999884366989136</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
 <span class="token punctuation">(</span><span class="token string">'overnight'</span><span class="token punctuation">,</span> <span class="token number">0.9999883770942688</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
 <span class="token punctuation">(</span><span class="token string">'commanders'</span><span class="token punctuation">,</span> <span class="token number">0.999988317489624</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
 <span class="token punctuation">(</span><span class="token string">'reached'</span><span class="token punctuation">,</span> <span class="token number">0.9999881386756897</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
 <span class="token punctuation">(</span><span class="token string">'commander'</span><span class="token punctuation">,</span> <span class="token number">0.9999880790710449</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
 <span class="token punctuation">(</span><span class="token string">'leading'</span><span class="token punctuation">,</span> <span class="token number">0.999987781047821</span><span class="token punctuation">)</span><span class="token punctuation">]</span>

<span class="token keyword">print</span><span class="token punctuation">(</span>wv<span class="token punctuation">.</span>n_similarity<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token string">'sushi'</span><span class="token punctuation">,</span> <span class="token string">'shop'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token string">'japanese'</span><span class="token punctuation">,</span> <span class="token string">'restaurant'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<pre class="line-numbers language-python"><code class="language-python"><span class="token number">0.9999402</span>

<span class="token keyword">print</span><span class="token punctuation">(</span>wv<span class="token punctuation">.</span>doesnt_match<span class="token punctuation">(</span><span class="token string">"breakfast cereal dinner lunch"</span><span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>
<pre class="line-numbers language-python"><code class="language-python"><span class="token string">'lunch'</span>

<span class="token keyword">print</span><span class="token punctuation">(</span>wv<span class="token punctuation">.</span>most_similar<span class="token punctuation">(</span>positive<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'baghdad'</span><span class="token punctuation">,</span> <span class="token string">'england'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> negative<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'london'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>
<pre class="line-numbers language-python"><code class="language-python"><span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token string">'attempt'</span><span class="token punctuation">,</span> <span class="token number">0.999660074710846</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
 <span class="token punctuation">(</span><span class="token string">'biggest'</span><span class="token punctuation">,</span> <span class="token number">0.9996545314788818</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
 <span class="token punctuation">(</span><span class="token string">'again'</span><span class="token punctuation">,</span> <span class="token number">0.9996527433395386</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
 <span class="token punctuation">(</span><span class="token string">'against'</span><span class="token punctuation">,</span> <span class="token number">0.9996523857116699</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
 <span class="token punctuation">(</span><span class="token string">'doubles'</span><span class="token punctuation">,</span> <span class="token number">0.9996522068977356</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
 <span class="token punctuation">(</span><span class="token string">'Royal'</span><span class="token punctuation">,</span> <span class="token number">0.9996512532234192</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
 <span class="token punctuation">(</span><span class="token string">'Airlines'</span><span class="token punctuation">,</span> <span class="token number">0.9996494054794312</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
 <span class="token punctuation">(</span><span class="token string">'forced'</span><span class="token punctuation">,</span> <span class="token number">0.9996494054794312</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
 <span class="token punctuation">(</span><span class="token string">'arrest'</span><span class="token punctuation">,</span> <span class="token number">0.9996492266654968</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
 <span class="token punctuation">(</span><span class="token string">'follows'</span><span class="token punctuation">,</span> <span class="token number">0.999649167060852</span><span class="token punctuation">)</span><span class="token punctuation">]</span>

<span class="token keyword">print</span><span class="token punctuation">(</span>wv<span class="token punctuation">.</span>evaluate_word_analogies<span class="token punctuation">(</span>datapath<span class="token punctuation">(</span><span class="token string">'questions-words.txt'</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<pre><code>(0.24489795918367346,
 [{'correct': [], 'incorrect': [], 'section': 'capital-common-countries'},
  {'correct': [], 'incorrect': [], 'section': 'capital-world'},
  {'correct': [], 'incorrect': [], 'section': 'currency'},
  {'correct': [], 'incorrect': [], 'section': 'city-in-state'},
  {'correct': [],
   'incorrect': [('HE', 'SHE', 'HIS', 'HER'), ('HIS', 'HER', 'HE', 'SHE')],
   'section': 'family'},
  {'correct': [], 'incorrect': [], 'section': 'gram1-adjective-to-adverb'},
  {'correct': [], 'incorrect': [], 'section': 'gram2-opposite'},
  {'correct': [('GOOD', 'BETTER', 'LOW', 'LOWER'),
               ('GREAT', 'GREATER', 'LOW', 'LOWER'),
               ('LONG', 'LONGER', 'LOW', 'LOWER')],
   'incorrect': [('GOOD', 'BETTER', 'GREAT', 'GREATER'),
                 ('GOOD', 'BETTER', 'LONG', 'LONGER'),
                 ('GREAT', 'GREATER', 'LONG', 'LONGER'),
                 ('GREAT', 'GREATER', 'GOOD', 'BETTER'),
                 ('LONG', 'LONGER', 'GOOD', 'BETTER'),
                 ('LONG', 'LONGER', 'GREAT', 'GREATER'),
                 ('LOW', 'LOWER', 'GOOD', 'BETTER'),
                 ('LOW', 'LOWER', 'GREAT', 'GREATER'),
                 ('LOW', 'LOWER', 'LONG', 'LONGER')],
   'section': 'gram3-comparative'},
  {'correct': [('BIG', 'BIGGEST', 'LARGE', 'LARGEST'),
               ('GOOD', 'BEST', 'LARGE', 'LARGEST'),
               ('GREAT', 'GREATEST', 'LARGE', 'LARGEST')],
   'incorrect': [('BIG', 'BIGGEST', 'GOOD', 'BEST'),
                 ('BIG', 'BIGGEST', 'GREAT', 'GREATEST'),
                 ('GOOD', 'BEST', 'GREAT', 'GREATEST'),
                 ('GOOD', 'BEST', 'BIG', 'BIGGEST'),
                 ('GREAT', 'GREATEST', 'BIG', 'BIGGEST'),
                 ('GREAT', 'GREATEST', 'GOOD', 'BEST'),
                 ('LARGE', 'LARGEST', 'BIG', 'BIGGEST'),
                 ('LARGE', 'LARGEST', 'GOOD', 'BEST'),
                 ('LARGE', 'LARGEST', 'GREAT', 'GREATEST')],
   'section': 'gram4-superlative'},
  {'correct': [('GO', 'GOING', 'SAY', 'SAYING'),
               ('LOOK', 'LOOKING', 'PLAY', 'PLAYING'),
               ('LOOK', 'LOOKING', 'SAY', 'SAYING'),
               ('LOOK', 'LOOKING', 'GO', 'GOING'),
               ('PLAY', 'PLAYING', 'SAY', 'SAYING'),
               ('PLAY', 'PLAYING', 'GO', 'GOING'),
               ('SAY', 'SAYING', 'GO', 'GOING')],
   'incorrect': [('GO', 'GOING', 'LOOK', 'LOOKING'),
                 ('GO', 'GOING', 'PLAY', 'PLAYING'),
                 ('GO', 'GOING', 'RUN', 'RUNNING'),
                 ('LOOK', 'LOOKING', 'RUN', 'RUNNING'),
                 ('PLAY', 'PLAYING', 'RUN', 'RUNNING'),
                 ('PLAY', 'PLAYING', 'LOOK', 'LOOKING'),
                 ('RUN', 'RUNNING', 'SAY', 'SAYING'),
                 ('RUN', 'RUNNING', 'GO', 'GOING'),
                 ('RUN', 'RUNNING', 'LOOK', 'LOOKING'),
                 ('RUN', 'RUNNING', 'PLAY', 'PLAYING'),
                 ('SAY', 'SAYING', 'LOOK', 'LOOKING'),
                 ('SAY', 'SAYING', 'PLAY', 'PLAYING'),
                 ('SAY', 'SAYING', 'RUN', 'RUNNING')],
   'section': 'gram5-present-participle'},
  {'correct': [('AUSTRALIA', 'AUSTRALIAN', 'INDIA', 'INDIAN'),
               ('AUSTRALIA', 'AUSTRALIAN', 'ISRAEL', 'ISRAELI'),
               ('FRANCE', 'FRENCH', 'INDIA', 'INDIAN'),
               ('INDIA', 'INDIAN', 'ISRAEL', 'ISRAELI'),
               ('ISRAEL', 'ISRAELI', 'INDIA', 'INDIAN'),
               ('SWITZERLAND', 'SWISS', 'INDIA', 'INDIAN')],
   'incorrect': [('AUSTRALIA', 'AUSTRALIAN', 'FRANCE', 'FRENCH'),
                 ('AUSTRALIA', 'AUSTRALIAN', 'SWITZERLAND', 'SWISS'),
                 ('FRANCE', 'FRENCH', 'ISRAEL', 'ISRAELI'),
                 ('FRANCE', 'FRENCH', 'SWITZERLAND', 'SWISS'),
                 ('FRANCE', 'FRENCH', 'AUSTRALIA', 'AUSTRALIAN'),
                 ('INDIA', 'INDIAN', 'SWITZERLAND', 'SWISS'),
                 ('INDIA', 'INDIAN', 'AUSTRALIA', 'AUSTRALIAN'),
                 ('INDIA', 'INDIAN', 'FRANCE', 'FRENCH'),
                 ('ISRAEL', 'ISRAELI', 'SWITZERLAND', 'SWISS'),
                 ('ISRAEL', 'ISRAELI', 'AUSTRALIA', 'AUSTRALIAN'),
                 ('ISRAEL', 'ISRAELI', 'FRANCE', 'FRENCH'),
                 ('SWITZERLAND', 'SWISS', 'AUSTRALIA', 'AUSTRALIAN'),
                 ('SWITZERLAND', 'SWISS', 'FRANCE', 'FRENCH'),
                 ('SWITZERLAND', 'SWISS', 'ISRAEL', 'ISRAELI')],
   'section': 'gram6-nationality-adjective'},
  {'correct': [],
   'incorrect': [('GOING', 'WENT', 'PAYING', 'PAID'),
                 ('GOING', 'WENT', 'PLAYING', 'PLAYED'),
                 ('GOING', 'WENT', 'SAYING', 'SAID'),
                 ('GOING', 'WENT', 'TAKING', 'TOOK'),
                 ('PAYING', 'PAID', 'PLAYING', 'PLAYED'),
                 ('PAYING', 'PAID', 'SAYING', 'SAID'),
                 ('PAYING', 'PAID', 'TAKING', 'TOOK'),
                 ('PAYING', 'PAID', 'GOING', 'WENT'),
                 ('PLAYING', 'PLAYED', 'SAYING', 'SAID'),
                 ('PLAYING', 'PLAYED', 'TAKING', 'TOOK'),
                 ('PLAYING', 'PLAYED', 'GOING', 'WENT'),
                 ('PLAYING', 'PLAYED', 'PAYING', 'PAID'),
                 ('SAYING', 'SAID', 'TAKING', 'TOOK'),
                 ('SAYING', 'SAID', 'GOING', 'WENT'),
                 ('SAYING', 'SAID', 'PAYING', 'PAID'),
                 ('SAYING', 'SAID', 'PLAYING', 'PLAYED'),
                 ('TAKING', 'TOOK', 'GOING', 'WENT'),
                 ('TAKING', 'TOOK', 'PAYING', 'PAID'),
                 ('TAKING', 'TOOK', 'PLAYING', 'PLAYED'),
                 ('TAKING', 'TOOK', 'SAYING', 'SAID')],
   'section': 'gram7-past-tense'},
  {'correct': [('BUILDING', 'BUILDINGS', 'CAR', 'CARS'),
               ('BUILDING', 'BUILDINGS', 'CHILD', 'CHILDREN'),
               ('CAR', 'CARS', 'BUILDING', 'BUILDINGS'),
               ('CHILD', 'CHILDREN', 'CAR', 'CARS'),
               ('MAN', 'MEN', 'CAR', 'CARS')],
   'incorrect': [('BUILDING', 'BUILDINGS', 'MAN', 'MEN'),
                 ('CAR', 'CARS', 'CHILD', 'CHILDREN'),
                 ('CAR', 'CARS', 'MAN', 'MEN'),
                 ('CHILD', 'CHILDREN', 'MAN', 'MEN'),
                 ('CHILD', 'CHILDREN', 'BUILDING', 'BUILDINGS'),
                 ('MAN', 'MEN', 'BUILDING', 'BUILDINGS'),
                 ('MAN', 'MEN', 'CHILD', 'CHILDREN')],
   'section': 'gram8-plural'},
  {'correct': [], 'incorrect': [], 'section': 'gram9-plural-verbs'},
  {'correct': [('GOOD', 'BETTER', 'LOW', 'LOWER'),
               ('GREAT', 'GREATER', 'LOW', 'LOWER'),
               ('LONG', 'LONGER', 'LOW', 'LOWER'),
               ('BIG', 'BIGGEST', 'LARGE', 'LARGEST'),
               ('GOOD', 'BEST', 'LARGE', 'LARGEST'),
               ('GREAT', 'GREATEST', 'LARGE', 'LARGEST'),
               ('GO', 'GOING', 'SAY', 'SAYING'),
               ('LOOK', 'LOOKING', 'PLAY', 'PLAYING'),
               ('LOOK', 'LOOKING', 'SAY', 'SAYING'),
               ('LOOK', 'LOOKING', 'GO', 'GOING'),
               ('PLAY', 'PLAYING', 'SAY', 'SAYING'),
               ('PLAY', 'PLAYING', 'GO', 'GOING'),
               ('SAY', 'SAYING', 'GO', 'GOING'),
               ('AUSTRALIA', 'AUSTRALIAN', 'INDIA', 'INDIAN'),
               ('AUSTRALIA', 'AUSTRALIAN', 'ISRAEL', 'ISRAELI'),
               ('FRANCE', 'FRENCH', 'INDIA', 'INDIAN'),
               ('INDIA', 'INDIAN', 'ISRAEL', 'ISRAELI'),
               ('ISRAEL', 'ISRAELI', 'INDIA', 'INDIAN'),
               ('SWITZERLAND', 'SWISS', 'INDIA', 'INDIAN'),
               ('BUILDING', 'BUILDINGS', 'CAR', 'CARS'),
               ('BUILDING', 'BUILDINGS', 'CHILD', 'CHILDREN'),
               ('CAR', 'CARS', 'BUILDING', 'BUILDINGS'),
               ('CHILD', 'CHILDREN', 'CAR', 'CARS'),
               ('MAN', 'MEN', 'CAR', 'CARS')],
   'incorrect': [('HE', 'SHE', 'HIS', 'HER'),
                 ('HIS', 'HER', 'HE', 'SHE'),
                 ('GOOD', 'BETTER', 'GREAT', 'GREATER'),
                 ('GOOD', 'BETTER', 'LONG', 'LONGER'),
                 ('GREAT', 'GREATER', 'LONG', 'LONGER'),
                 ('GREAT', 'GREATER', 'GOOD', 'BETTER'),
                 ('LONG', 'LONGER', 'GOOD', 'BETTER'),
                 ('LONG', 'LONGER', 'GREAT', 'GREATER'),
                 ('LOW', 'LOWER', 'GOOD', 'BETTER'),
                 ('LOW', 'LOWER', 'GREAT', 'GREATER'),
                 ('LOW', 'LOWER', 'LONG', 'LONGER'),
                 ('BIG', 'BIGGEST', 'GOOD', 'BEST'),
                 ('BIG', 'BIGGEST', 'GREAT', 'GREATEST'),
                 ('GOOD', 'BEST', 'GREAT', 'GREATEST'),
                 ('GOOD', 'BEST', 'BIG', 'BIGGEST'),
                 ('GREAT', 'GREATEST', 'BIG', 'BIGGEST'),
                 ('GREAT', 'GREATEST', 'GOOD', 'BEST'),
                 ('LARGE', 'LARGEST', 'BIG', 'BIGGEST'),
                 ('LARGE', 'LARGEST', 'GOOD', 'BEST'),
                 ('LARGE', 'LARGEST', 'GREAT', 'GREATEST'),
                 ('GO', 'GOING', 'LOOK', 'LOOKING'),
                 ('GO', 'GOING', 'PLAY', 'PLAYING'),
                 ('GO', 'GOING', 'RUN', 'RUNNING'),
                 ('LOOK', 'LOOKING', 'RUN', 'RUNNING'),
                 ('PLAY', 'PLAYING', 'RUN', 'RUNNING'),
                 ('PLAY', 'PLAYING', 'LOOK', 'LOOKING'),
                 ('RUN', 'RUNNING', 'SAY', 'SAYING'),
                 ('RUN', 'RUNNING', 'GO', 'GOING'),
                 ('RUN', 'RUNNING', 'LOOK', 'LOOKING'),
                 ('RUN', 'RUNNING', 'PLAY', 'PLAYING'),
                 ('SAY', 'SAYING', 'LOOK', 'LOOKING'),
                 ('SAY', 'SAYING', 'PLAY', 'PLAYING'),
                 ('SAY', 'SAYING', 'RUN', 'RUNNING'),
                 ('AUSTRALIA', 'AUSTRALIAN', 'FRANCE', 'FRENCH'),
                 ('AUSTRALIA', 'AUSTRALIAN', 'SWITZERLAND', 'SWISS'),
                 ('FRANCE', 'FRENCH', 'ISRAEL', 'ISRAELI'),
                 ('FRANCE', 'FRENCH', 'SWITZERLAND', 'SWISS'),
                 ('FRANCE', 'FRENCH', 'AUSTRALIA', 'AUSTRALIAN'),
                 ('INDIA', 'INDIAN', 'SWITZERLAND', 'SWISS'),
                 ('INDIA', 'INDIAN', 'AUSTRALIA', 'AUSTRALIAN'),
                 ('INDIA', 'INDIAN', 'FRANCE', 'FRENCH'),
                 ('ISRAEL', 'ISRAELI', 'SWITZERLAND', 'SWISS'),
                 ('ISRAEL', 'ISRAELI', 'AUSTRALIA', 'AUSTRALIAN'),
                 ('ISRAEL', 'ISRAELI', 'FRANCE', 'FRENCH'),
                 ('SWITZERLAND', 'SWISS', 'AUSTRALIA', 'AUSTRALIAN'),
                 ('SWITZERLAND', 'SWISS', 'FRANCE', 'FRENCH'),
                 ('SWITZERLAND', 'SWISS', 'ISRAEL', 'ISRAELI'),
                 ('GOING', 'WENT', 'PAYING', 'PAID'),
                 ('GOING', 'WENT', 'PLAYING', 'PLAYED'),
                 ('GOING', 'WENT', 'SAYING', 'SAID'),
                 ('GOING', 'WENT', 'TAKING', 'TOOK'),
                 ('PAYING', 'PAID', 'PLAYING', 'PLAYED'),
                 ('PAYING', 'PAID', 'SAYING', 'SAID'),
                 ('PAYING', 'PAID', 'TAKING', 'TOOK'),
                 ('PAYING', 'PAID', 'GOING', 'WENT'),
                 ('PLAYING', 'PLAYED', 'SAYING', 'SAID'),
                 ('PLAYING', 'PLAYED', 'TAKING', 'TOOK'),
                 ('PLAYING', 'PLAYED', 'GOING', 'WENT'),
                 ('PLAYING', 'PLAYED', 'PAYING', 'PAID'),
                 ('SAYING', 'SAID', 'TAKING', 'TOOK'),
                 ('SAYING', 'SAID', 'GOING', 'WENT'),
                 ('SAYING', 'SAID', 'PAYING', 'PAID'),
                 ('SAYING', 'SAID', 'PLAYING', 'PLAYED'),
                 ('TAKING', 'TOOK', 'GOING', 'WENT'),
                 ('TAKING', 'TOOK', 'PAYING', 'PAID'),
                 ('TAKING', 'TOOK', 'PLAYING', 'PLAYED'),
                 ('TAKING', 'TOOK', 'SAYING', 'SAID'),
                 ('BUILDING', 'BUILDINGS', 'MAN', 'MEN'),
                 ('CAR', 'CARS', 'CHILD', 'CHILDREN'),
                 ('CAR', 'CARS', 'MAN', 'MEN'),
                 ('CHILD', 'CHILDREN', 'MAN', 'MEN'),
                 ('CHILD', 'CHILDREN', 'BUILDING', 'BUILDINGS'),
                 ('MAN', 'MEN', 'BUILDING', 'BUILDINGS'),
                 ('MAN', 'MEN', 'CHILD', 'CHILDREN')],
   'section': 'Total accuracy'}])</code></pre><h3 id="文字移动距离"><a href="#文字移动距离" class="headerlink" title="文字移动距离"></a>文字移动距离</h3><p>您将需要<code>pyemd</code>本节的可选库.<code>pip install pyemd</code></p>
<p>让我们从两句话开始：</p>
<pre class="line-numbers language-python"><code class="language-python">sentence_obama <span class="token operator">=</span> <span class="token string">'Obama speaks to the media in Illinois'</span><span class="token punctuation">.</span>lower<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token punctuation">)</span>
sentence_president <span class="token operator">=</span> <span class="token string">'The president greets the press in Chicago'</span><span class="token punctuation">.</span>lower<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>
<p>删除他们的停用词。</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">from</span> gensim<span class="token punctuation">.</span>parsing<span class="token punctuation">.</span>preprocessing <span class="token keyword">import</span> STOPWORDS
sentence_obama <span class="token operator">=</span> <span class="token punctuation">[</span>w <span class="token keyword">for</span> w <span class="token keyword">in</span> sentence_obama <span class="token keyword">if</span> w <span class="token operator">not</span> <span class="token keyword">in</span> STOPWORDS<span class="token punctuation">]</span>
sentence_president <span class="token operator">=</span> <span class="token punctuation">[</span>w <span class="token keyword">for</span> w <span class="token keyword">in</span> sentence_president <span class="token keyword">if</span> w <span class="token operator">not</span> <span class="token keyword">in</span> STOPWORDS<span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>
<p>计算两个句子之间的词移动距离。</p>
<pre class="line-numbers language-python"><code class="language-python">distance <span class="token operator">=</span> wv<span class="token punctuation">.</span>wmdistance<span class="token punctuation">(</span>sentence_obama<span class="token punctuation">,</span> sentence_president<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>f<span class="token string">"Word Movers Distance is {distance} (lower means closer)"</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>
<pre><code>'Word Movers Distance is 0.015923231075180694 (lower means closer)'</code></pre><pre><code>import matplotlib.pyplot as plt
import matplotlib.image as mpimg
img = mpimg.imread('fasttext-logo-color-web.png')
imgplot = plt.imshow(img)
_ = plt.axis('off')</code></pre><p><img src="/images/loading.gif" data-original="../images/ML/sphx_glr_run_fasttext_001.png" alt=""></p>
<h1 id="Fast-Similarity-Queries-with-Annoy-and-Word2Vec"><a href="#Fast-Similarity-Queries-with-Annoy-and-Word2Vec" class="headerlink" title="Fast Similarity Queries with Annoy and Word2Vec"></a>Fast Similarity Queries with Annoy and Word2Vec</h1><p>介绍 Annoy 库，用于基于 Word2Vec 学习的向量进行相似性查询。</p>
<pre class="line-numbers language-python"><code class="language-python">LOGS <span class="token operator">=</span> <span class="token boolean">False</span>  <span class="token comment" spellcheck="true"># Set to True if you want to see progress in logs.</span>
<span class="token keyword">if</span> LOGS<span class="token punctuation">:</span>
    <span class="token keyword">import</span> logging
    logging<span class="token punctuation">.</span>basicConfig<span class="token punctuation">(</span>format<span class="token operator">=</span><span class="token string">'%(asctime)s : %(levelname)s : %(message)s'</span><span class="token punctuation">,</span> level<span class="token operator">=</span>logging<span class="token punctuation">.</span>INFO<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>
<p>该 <a href="https://github.com/spotify/annoy" target="_blank" rel="noopener">Annoy “Approximate Nearest Neighbors Oh Yeah”</a>库使相似的查询与Word2Vec模型。当前在 Gensim 的向量空间中寻找 k 个最近邻的实现在索引文档的数量上通过蛮力具有线性复杂性，尽管具有极低的常数因子。检索到的结果是准确的，这在许多应用程序中是一种矫枉过正：在亚线性时间内检索到的近似结果可能就足够了。Annoy 可以更快地找到近似最近邻。</p>
<ol>
<li>Download Text8 Corpus</li>
<li>Train the Word2Vec model</li>
<li>Construct AnnoyIndex with model &amp; make a similarity query</li>
<li>Compare to the traditional indexer</li>
<li>Persist indices to disk</li>
<li>Save memory by via memory-mapping indices saved to disk</li>
<li>Evaluate relationship of <code>num_trees</code> to initialization time and accuracy</li>
<li>Work with Google’s word2vec C formats</li>
</ol>
<h2 id="1-下载Text8语料库"><a href="#1-下载Text8语料库" class="headerlink" title="1.下载Text8语料库"></a>1.下载Text8语料库</h2><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> gensim<span class="token punctuation">.</span>downloader <span class="token keyword">as</span> api
text8_path <span class="token operator">=</span> api<span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token string">'text8'</span><span class="token punctuation">,</span> return_path<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Using corpus from"</span><span class="token punctuation">,</span> text8_path<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>
<pre><code>Using corpus from /Users/kofola3/gensim-data/text8/text8.gz</code></pre><h2 id="2-训练-Word2Vec-模型"><a href="#2-训练-Word2Vec-模型" class="headerlink" title="2. 训练 Word2Vec 模型"></a>2. 训练 Word2Vec 模型</h2><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">from</span> gensim<span class="token punctuation">.</span>models <span class="token keyword">import</span> Word2Vec<span class="token punctuation">,</span> KeyedVectors
<span class="token keyword">from</span> gensim<span class="token punctuation">.</span>models<span class="token punctuation">.</span>word2vec <span class="token keyword">import</span> Text8Corpus

<span class="token comment" spellcheck="true"># Using params from Word2Vec_FastText_Comparison</span>
params <span class="token operator">=</span> <span class="token punctuation">{</span>
    <span class="token string">'alpha'</span><span class="token punctuation">:</span> <span class="token number">0.05</span><span class="token punctuation">,</span>
    <span class="token string">'vector_size'</span><span class="token punctuation">:</span> <span class="token number">100</span><span class="token punctuation">,</span>
    <span class="token string">'window'</span><span class="token punctuation">:</span> <span class="token number">5</span><span class="token punctuation">,</span>
    <span class="token string">'epochs'</span><span class="token punctuation">:</span> <span class="token number">5</span><span class="token punctuation">,</span>
    <span class="token string">'min_count'</span><span class="token punctuation">:</span> <span class="token number">5</span><span class="token punctuation">,</span>
    <span class="token string">'sample'</span><span class="token punctuation">:</span> <span class="token number">1e</span><span class="token operator">-</span><span class="token number">4</span><span class="token punctuation">,</span>
    <span class="token string">'sg'</span><span class="token punctuation">:</span> <span class="token number">1</span><span class="token punctuation">,</span>
    <span class="token string">'hs'</span><span class="token punctuation">:</span> <span class="token number">0</span><span class="token punctuation">,</span>
    <span class="token string">'negative'</span><span class="token punctuation">:</span> <span class="token number">5</span><span class="token punctuation">,</span>
<span class="token punctuation">}</span>
model <span class="token operator">=</span> Word2Vec<span class="token punctuation">(</span>Text8Corpus<span class="token punctuation">(</span>text8_path<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token operator">**</span>params<span class="token punctuation">)</span>
wv <span class="token operator">=</span> model<span class="token punctuation">.</span>wv
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Using trained model"</span><span class="token punctuation">,</span> wv<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<pre><code>Using trained model &lt;gensim.models.keyedvectors.KeyedVectors object at 0x2095fb0f0&gt;</code></pre><h2 id="3-用模型构建-AnnoyIndex-并进行相似度查询"><a href="#3-用模型构建-AnnoyIndex-并进行相似度查询" class="headerlink" title="3. 用模型构建 AnnoyIndex 并进行相似度查询"></a>3. 用模型构建 AnnoyIndex 并进行相似度查询</h2><p><code>AnnoyIndexer</code>为了在 Gensim 中使用 Annoy ，需要创建一个实例。该<code>AnnoyIndexer</code>班位于<code>gensim.similarities.annoy</code>。</p>
<p><code>AnnoyIndexer()</code> 需要两个参数：</p>
<ul>
<li><strong>型号</strong>: A<code>Word2Vec</code>或<code>Doc2Vec</code>型号。</li>
<li><strong>num_trees</strong> : 一个正整数。<code>num_trees</code>影响构建时间和索引大小。<strong>值越大，结果越准确，但索引越大</strong>。可以在<a href="https://github.com/spotify/annoy#how-does-it-work" target="_blank" rel="noopener">此处</a>找到有关 Annoy 中树木功能的更多信息 。<code>num_trees</code>、构建时间和准确性之间的关系将在本教程的后面进行研究。</li>
</ul>
<p>现在我们已准备好进行查询，让我们在 Text8 语料库中找出与“science”最相似的前 5 个词。为了进行相似性查询，我们<code>Word2Vec.most_similar</code>像传统上一样调用 ，但增加了一个参数，<code>indexer</code>。</p>
<p>除了 Annoy，Gensim 还支持 NMSLIB 索引器。NMSLIB 是一个类似于 Annoy 的库——两者都支持对相似向量的快速近似搜索。</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">from</span> gensim<span class="token punctuation">.</span>similarities<span class="token punctuation">.</span>annoy <span class="token keyword">import</span> AnnoyIndexer

<span class="token comment" spellcheck="true"># 100 trees are being used in this example</span>
annoy_index <span class="token operator">=</span> AnnoyIndexer<span class="token punctuation">(</span>model<span class="token punctuation">,</span> <span class="token number">100</span><span class="token punctuation">)</span>
<span class="token comment" spellcheck="true"># Derive the vector for the word "science" in our model</span>
vector <span class="token operator">=</span> wv<span class="token punctuation">[</span><span class="token string">"science"</span><span class="token punctuation">]</span>
<span class="token comment" spellcheck="true"># The instance of AnnoyIndexer we just created is passed</span>
approximate_neighbors <span class="token operator">=</span> wv<span class="token punctuation">.</span>most_similar<span class="token punctuation">(</span><span class="token punctuation">[</span>vector<span class="token punctuation">]</span><span class="token punctuation">,</span> topn<span class="token operator">=</span><span class="token number">11</span><span class="token punctuation">,</span> indexer<span class="token operator">=</span>annoy_index<span class="token punctuation">)</span>
<span class="token comment" spellcheck="true"># Neatly print the approximate_neighbors and their corresponding cosine similarity values</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Approximate Neighbors"</span><span class="token punctuation">)</span>
<span class="token keyword">for</span> neighbor <span class="token keyword">in</span> approximate_neighbors<span class="token punctuation">:</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>neighbor<span class="token punctuation">)</span>

normal_neighbors <span class="token operator">=</span> wv<span class="token punctuation">.</span>most_similar<span class="token punctuation">(</span><span class="token punctuation">[</span>vector<span class="token punctuation">]</span><span class="token punctuation">,</span> topn<span class="token operator">=</span><span class="token number">11</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"\nExact Neighbors"</span><span class="token punctuation">)</span>
<span class="token keyword">for</span> neighbor <span class="token keyword">in</span> normal_neighbors<span class="token punctuation">:</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>neighbor<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<pre><code>Approximate Neighbors
('science', 1.0)
('fiction', 0.6577868759632111)
('crichton', 0.5896251797676086)
('interdisciplinary', 0.5887056291103363)
('astrobiology', 0.5863820314407349)
('multidisciplinary', 0.5813699960708618)
('protoscience', 0.5805026590824127)
('vinge', 0.5781905055046082)
('astronautics', 0.5768974423408508)
('aaas', 0.574912428855896)
('brookings', 0.5739299058914185)

Exact Neighbors
('science', 1.0)
('fiction', 0.7657802700996399)
('crichton', 0.6631850600242615)
('interdisciplinary', 0.661673903465271)
('astrobiology', 0.6578403115272522)
('bimonthly', 0.6501255631446838)
('actuarial', 0.6495736837387085)
('multidisciplinary', 0.6494976878166199)
('protoscience', 0.6480439305305481)
('vinge', 0.6441534757614136)
('xenobiology', 0.6438207030296326)</code></pre><p>向量的余弦相似度越接近 1，该词与我们的查询越相似，即“科学”的向量。相似词的排名和包含在 10 个最相似词中的词组存在一些差异。</p>
<h2 id="4-与传统索引器的比较"><a href="#4-与传统索引器的比较" class="headerlink" title="4. 与传统索引器的比较"></a>4. 与传统索引器的比较</h2><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># Set up the model and vector that we are using in the comparison</span>
annoy_index <span class="token operator">=</span> AnnoyIndexer<span class="token punctuation">(</span>model<span class="token punctuation">,</span> <span class="token number">100</span><span class="token punctuation">)</span>

<span class="token comment" spellcheck="true"># Dry run to make sure both indexes are fully in RAM</span>
normed_vectors <span class="token operator">=</span> wv<span class="token punctuation">.</span>get_normed_vectors<span class="token punctuation">(</span><span class="token punctuation">)</span>
vector <span class="token operator">=</span> normed_vectors<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
wv<span class="token punctuation">.</span>most_similar<span class="token punctuation">(</span><span class="token punctuation">[</span>vector<span class="token punctuation">]</span><span class="token punctuation">,</span> topn<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">,</span> indexer<span class="token operator">=</span>annoy_index<span class="token punctuation">)</span>
wv<span class="token punctuation">.</span>most_similar<span class="token punctuation">(</span><span class="token punctuation">[</span>vector<span class="token punctuation">]</span><span class="token punctuation">,</span> topn<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">)</span>

<span class="token keyword">import</span> time
<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np

<span class="token keyword">def</span> <span class="token function">avg_query_time</span><span class="token punctuation">(</span>annoy_index<span class="token operator">=</span>None<span class="token punctuation">,</span> queries<span class="token operator">=</span><span class="token number">1000</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""Average query time of a most_similar method over 1000 random queries."""</span>
    total_time <span class="token operator">=</span> <span class="token number">0</span>
    <span class="token keyword">for</span> _ <span class="token keyword">in</span> range<span class="token punctuation">(</span>queries<span class="token punctuation">)</span><span class="token punctuation">:</span>
        rand_vec <span class="token operator">=</span> normed_vectors<span class="token punctuation">[</span>np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>randint<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> len<span class="token punctuation">(</span>wv<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">]</span>
        start_time <span class="token operator">=</span> time<span class="token punctuation">.</span>process_time<span class="token punctuation">(</span><span class="token punctuation">)</span>
        wv<span class="token punctuation">.</span>most_similar<span class="token punctuation">(</span><span class="token punctuation">[</span>rand_vec<span class="token punctuation">]</span><span class="token punctuation">,</span> topn<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">,</span> indexer<span class="token operator">=</span>annoy_index<span class="token punctuation">)</span>
        total_time <span class="token operator">+=</span> time<span class="token punctuation">.</span>process_time<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">-</span> start_time
    <span class="token keyword">return</span> total_time <span class="token operator">/</span> queries

queries <span class="token operator">=</span> <span class="token number">1000</span>

gensim_time <span class="token operator">=</span> avg_query_time<span class="token punctuation">(</span>queries<span class="token operator">=</span>queries<span class="token punctuation">)</span>
annoy_time <span class="token operator">=</span> avg_query_time<span class="token punctuation">(</span>annoy_index<span class="token punctuation">,</span> queries<span class="token operator">=</span>queries<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Gensim (s/query):\t{0:.5f}"</span><span class="token punctuation">.</span>format<span class="token punctuation">(</span>gensim_time<span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Annoy (s/query):\t{0:.5f}"</span><span class="token punctuation">.</span>format<span class="token punctuation">(</span>annoy_time<span class="token punctuation">)</span><span class="token punctuation">)</span>
speed_improvement <span class="token operator">=</span> gensim_time <span class="token operator">/</span> annoy_time
<span class="token keyword">print</span> <span class="token punctuation">(</span><span class="token string">"\nAnnoy is {0:.2f} times faster on average on this particular run"</span><span class="token punctuation">.</span>format<span class="token punctuation">(</span>speed_improvement<span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>出去：</p>
<pre><code>Gensim (s/query):       0.00585
Annoy (s/query):        0.00052

Annoy is 11.25 times faster on average on this particular run</code></pre><p><strong>此加速因子绝不是恒定的，</strong>并且会因运行而异，并且特别针对此数据集、BLAS 设置、Annoy 参数（随着树大小的增加加速因子减小）、机器规格以及其他因素。</p>
<blockquote>
<p>annoy indexer 的初始化时间不包括在时间中。您使用的最佳 knn 算法将取决于您需要进行多少查询以及语料库的大小。如果您只进行很少的相似性查询，则初始化 annoy 索引器所花费的时间将比使用蛮力方法检索结果所花费的时间更长。但是，如果您要进行许多查询，则初始化烦人的索引器所需的时间将被索引器初始化后令人难以置信的快速检索时间所弥补</p>
<p>Gensim 的 ‘most_similar’ 方法使用点积形式的 numpy 操作，而 Annoy 的方法不是。如果您机器上的“numpy”正在使用 ATLAS 或 LAPACK 等 BLAS 库之一，它将在多核上运行（仅当您的机器具有多核支持时）。查看<a href="https://scipy-cookbook.readthedocs.io/items/ParallelProgramming.html" target="_blank" rel="noopener">SciPy Cookbook</a> 了解更多详细信息。</p>
</blockquote>
<h2 id="5-将索引持久化到磁盘"><a href="#5-将索引持久化到磁盘" class="headerlink" title="5. 将索引持久化到磁盘"></a>5. 将索引持久化到磁盘</h2><p>您可以从/向磁盘保存和加载索引，以防止每次都必须构建它们。这将在磁盘上创建两个文件，<em>fname</em>和 <em>fname.d</em>。需要这两个文件才能正确恢复所有属性。在加载索引之前，您必须创建一个空的 AnnoyIndexer 对象。</p>
<pre class="line-numbers language-python"><code class="language-python">fname <span class="token operator">=</span> <span class="token string">'/tmp/mymodel.index'</span>

<span class="token comment" spellcheck="true"># Persist index to disk</span>
annoy_index<span class="token punctuation">.</span>save<span class="token punctuation">(</span>fname<span class="token punctuation">)</span>

<span class="token comment" spellcheck="true"># Load index back</span>
<span class="token keyword">import</span> os<span class="token punctuation">.</span>path
<span class="token keyword">if</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>exists<span class="token punctuation">(</span>fname<span class="token punctuation">)</span><span class="token punctuation">:</span>
    annoy_index2 <span class="token operator">=</span> AnnoyIndexer<span class="token punctuation">(</span><span class="token punctuation">)</span>
    annoy_index2<span class="token punctuation">.</span>load<span class="token punctuation">(</span>fname<span class="token punctuation">)</span>
    annoy_index2<span class="token punctuation">.</span>model <span class="token operator">=</span> model

<span class="token comment" spellcheck="true"># Results should be identical to above</span>
vector <span class="token operator">=</span> wv<span class="token punctuation">[</span><span class="token string">"science"</span><span class="token punctuation">]</span>
approximate_neighbors2 <span class="token operator">=</span> wv<span class="token punctuation">.</span>most_similar<span class="token punctuation">(</span><span class="token punctuation">[</span>vector<span class="token punctuation">]</span><span class="token punctuation">,</span> topn<span class="token operator">=</span><span class="token number">11</span><span class="token punctuation">,</span> indexer<span class="token operator">=</span>annoy_index2<span class="token punctuation">)</span>
<span class="token keyword">for</span> neighbor <span class="token keyword">in</span> approximate_neighbors2<span class="token punctuation">:</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>neighbor<span class="token punctuation">)</span>

<span class="token keyword">assert</span> approximate_neighbors <span class="token operator">==</span> approximate_neighbors2<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<pre><code>('science', 1.0)
('fiction', 0.6577868759632111)
('crichton', 0.5896251797676086)
('interdisciplinary', 0.5887056291103363)
('astrobiology', 0.5863820314407349)
('multidisciplinary', 0.5813699960708618)
('protoscience', 0.5805026590824127)
('vinge', 0.5781905055046082)
('astronautics', 0.5768974423408508)
('aaas', 0.574912428855896)
('brookings', 0.5739299058914185)</code></pre><p>请务必在负载时使用与最初使用的模型相同的模型，否则会出现意外行为。</p>
<h2 id="6-通过保存到磁盘的内存映射索引来节省内存"><a href="#6-通过保存到磁盘的内存映射索引来节省内存" class="headerlink" title="6. 通过保存到磁盘的内存映射索引来节省内存"></a>6. 通过保存到磁盘的内存映射索引来节省内存</h2><p>Annoy 库有一个有用的功能，即索引可以从磁盘进行内存映射。当多个进程使用相同的索引时，它可以节省内存。</p>
<p>下面是两段代码。第一个对每个进程都有一个单独的索引。第二个片段通过内存映射在两个进程之间共享索引。第二个示例使用较少的总 RAM，因为它是共享的。</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># Remove verbosity from code below (if logging active)</span>
<span class="token keyword">if</span> LOGS<span class="token punctuation">:</span>
    logging<span class="token punctuation">.</span>disable<span class="token punctuation">(</span>logging<span class="token punctuation">.</span>CRITICAL<span class="token punctuation">)</span>

<span class="token keyword">from</span> multiprocessing <span class="token keyword">import</span> Process
<span class="token keyword">import</span> os
<span class="token keyword">import</span> psutil<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>坏例子：两个进程从磁盘加载 Word2vec 模型并从该模型创建自己的 Annoy 索引。</p>
<pre class="line-numbers language-python"><code class="language-python">model<span class="token punctuation">.</span>save<span class="token punctuation">(</span><span class="token string">'/tmp/mymodel.pkl'</span><span class="token punctuation">)</span>

<span class="token keyword">def</span> <span class="token function">f</span><span class="token punctuation">(</span>process_id<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'Process Id: {}'</span><span class="token punctuation">.</span>format<span class="token punctuation">(</span>os<span class="token punctuation">.</span>getpid<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    process <span class="token operator">=</span> psutil<span class="token punctuation">.</span>Process<span class="token punctuation">(</span>os<span class="token punctuation">.</span>getpid<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    new_model <span class="token operator">=</span> Word2Vec<span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token string">'/tmp/mymodel.pkl'</span><span class="token punctuation">)</span>
    vector <span class="token operator">=</span> new_model<span class="token punctuation">.</span>wv<span class="token punctuation">[</span><span class="token string">"science"</span><span class="token punctuation">]</span>
    annoy_index <span class="token operator">=</span> AnnoyIndexer<span class="token punctuation">(</span>new_model<span class="token punctuation">,</span> <span class="token number">100</span><span class="token punctuation">)</span>
    approximate_neighbors <span class="token operator">=</span> new_model<span class="token punctuation">.</span>wv<span class="token punctuation">.</span>most_similar<span class="token punctuation">(</span><span class="token punctuation">[</span>vector<span class="token punctuation">]</span><span class="token punctuation">,</span> topn<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">,</span> indexer<span class="token operator">=</span>annoy_index<span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'\nMemory used by process {}: {}\n---'</span><span class="token punctuation">.</span>format<span class="token punctuation">(</span>os<span class="token punctuation">.</span>getpid<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> process<span class="token punctuation">.</span>memory_info<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

<span class="token comment" spellcheck="true"># Create and run two parallel processes to share the same index file.</span>
p1 <span class="token operator">=</span> Process<span class="token punctuation">(</span>target<span class="token operator">=</span>f<span class="token punctuation">,</span> args<span class="token operator">=</span><span class="token punctuation">(</span><span class="token string">'1'</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
p1<span class="token punctuation">.</span>start<span class="token punctuation">(</span><span class="token punctuation">)</span>
p1<span class="token punctuation">.</span>join<span class="token punctuation">(</span><span class="token punctuation">)</span>
p2 <span class="token operator">=</span> Process<span class="token punctuation">(</span>target<span class="token operator">=</span>f<span class="token punctuation">,</span> args<span class="token operator">=</span><span class="token punctuation">(</span><span class="token string">'2'</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
p2<span class="token punctuation">.</span>start<span class="token punctuation">(</span><span class="token punctuation">)</span>
p2<span class="token punctuation">.</span>join<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>很好的例子：两个进程从磁盘加载 Word2vec 模型和索引，并对索引进行内存映射。</p>
<pre class="line-numbers language-python"><code class="language-python">model<span class="token punctuation">.</span>save<span class="token punctuation">(</span><span class="token string">'/tmp/mymodel.pkl'</span><span class="token punctuation">)</span>

<span class="token keyword">def</span> <span class="token function">f</span><span class="token punctuation">(</span>process_id<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'Process Id: {}'</span><span class="token punctuation">.</span>format<span class="token punctuation">(</span>os<span class="token punctuation">.</span>getpid<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    process <span class="token operator">=</span> psutil<span class="token punctuation">.</span>Process<span class="token punctuation">(</span>os<span class="token punctuation">.</span>getpid<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    new_model <span class="token operator">=</span> Word2Vec<span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token string">'/tmp/mymodel.pkl'</span><span class="token punctuation">)</span>
    vector <span class="token operator">=</span> new_model<span class="token punctuation">.</span>wv<span class="token punctuation">[</span><span class="token string">"science"</span><span class="token punctuation">]</span>
    annoy_index <span class="token operator">=</span> AnnoyIndexer<span class="token punctuation">(</span><span class="token punctuation">)</span>
    annoy_index<span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token string">'/tmp/mymodel.index'</span><span class="token punctuation">)</span>
    annoy_index<span class="token punctuation">.</span>model <span class="token operator">=</span> new_model
    approximate_neighbors <span class="token operator">=</span> new_model<span class="token punctuation">.</span>wv<span class="token punctuation">.</span>most_similar<span class="token punctuation">(</span><span class="token punctuation">[</span>vector<span class="token punctuation">]</span><span class="token punctuation">,</span> topn<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">,</span> indexer<span class="token operator">=</span>annoy_index<span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'\nMemory used by process {}: {}\n---'</span><span class="token punctuation">.</span>format<span class="token punctuation">(</span>os<span class="token punctuation">.</span>getpid<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> process<span class="token punctuation">.</span>memory_info<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

<span class="token comment" spellcheck="true"># Creating and running two parallel process to share the same index file.</span>
p1 <span class="token operator">=</span> Process<span class="token punctuation">(</span>target<span class="token operator">=</span>f<span class="token punctuation">,</span> args<span class="token operator">=</span><span class="token punctuation">(</span><span class="token string">'1'</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
p1<span class="token punctuation">.</span>start<span class="token punctuation">(</span><span class="token punctuation">)</span>
p1<span class="token punctuation">.</span>join<span class="token punctuation">(</span><span class="token punctuation">)</span>
p2 <span class="token operator">=</span> Process<span class="token punctuation">(</span>target<span class="token operator">=</span>f<span class="token punctuation">,</span> args<span class="token operator">=</span><span class="token punctuation">(</span><span class="token string">'2'</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
p2<span class="token punctuation">.</span>start<span class="token punctuation">(</span><span class="token punctuation">)</span>
p2<span class="token punctuation">.</span>join<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h2 id="7-评估与num-trees初始化时间和准确性的关系"><a href="#7-评估与num-trees初始化时间和准确性的关系" class="headerlink" title="7. 评估与num_trees初始化时间和准确性的关系"></a>7. 评估与<code>num_trees</code>初始化时间和准确性的关系</h2><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> plt<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<p>构建初始化时间和准确度度量的数据集：</p>
<pre class="line-numbers language-python"><code class="language-python">exact_results <span class="token operator">=</span> <span class="token punctuation">[</span>element<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token keyword">for</span> element <span class="token keyword">in</span> wv<span class="token punctuation">.</span>most_similar<span class="token punctuation">(</span><span class="token punctuation">[</span>normed_vectors<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span> topn<span class="token operator">=</span><span class="token number">100</span><span class="token punctuation">)</span><span class="token punctuation">]</span>

x_values <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
y_values_init <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
y_values_accuracy <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>

<span class="token keyword">for</span> x <span class="token keyword">in</span> range<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">300</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    x_values<span class="token punctuation">.</span>append<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
    start_time <span class="token operator">=</span> time<span class="token punctuation">.</span>time<span class="token punctuation">(</span><span class="token punctuation">)</span>
    annoy_index <span class="token operator">=</span> AnnoyIndexer<span class="token punctuation">(</span>model<span class="token punctuation">,</span> x<span class="token punctuation">)</span>
    y_values_init<span class="token punctuation">.</span>append<span class="token punctuation">(</span>time<span class="token punctuation">.</span>time<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">-</span> start_time<span class="token punctuation">)</span>
    approximate_results <span class="token operator">=</span> wv<span class="token punctuation">.</span>most_similar<span class="token punctuation">(</span><span class="token punctuation">[</span>normed_vectors<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span> topn<span class="token operator">=</span><span class="token number">100</span><span class="token punctuation">,</span> indexer<span class="token operator">=</span>annoy_index<span class="token punctuation">)</span>
    top_words <span class="token operator">=</span> <span class="token punctuation">[</span>result<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token keyword">for</span> result <span class="token keyword">in</span> approximate_results<span class="token punctuation">]</span>
    y_values_accuracy<span class="token punctuation">.</span>append<span class="token punctuation">(</span>len<span class="token punctuation">(</span>set<span class="token punctuation">(</span>top_words<span class="token punctuation">)</span><span class="token punctuation">.</span>intersection<span class="token punctuation">(</span>exact_results<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>绘图结果：</p>
<pre class="line-numbers language-python"><code class="language-python">plt<span class="token punctuation">.</span>figure<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> figsize<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">12</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>subplot<span class="token punctuation">(</span><span class="token number">121</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>x_values<span class="token punctuation">,</span> y_values_init<span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>title<span class="token punctuation">(</span><span class="token string">"num_trees vs initalization time"</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>ylabel<span class="token punctuation">(</span><span class="token string">"Initialization time (s)"</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>xlabel<span class="token punctuation">(</span><span class="token string">"num_trees"</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>subplot<span class="token punctuation">(</span><span class="token number">122</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>x_values<span class="token punctuation">,</span> y_values_accuracy<span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>title<span class="token punctuation">(</span><span class="token string">"num_trees vs accuracy"</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>ylabel<span class="token punctuation">(</span><span class="token string">"%% accuracy"</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>xlabel<span class="token punctuation">(</span><span class="token string">"num_trees"</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>tight_layout<span class="token punctuation">(</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p><img src="/images/loading.gif" data-original="https://radimrehurek.com/gensim/_images/sphx_glr_run_annoy_001.png" alt="num_trees 与初始化时间，num_trees 与准确性"></p>
<p>出去：</p>
<pre><code>/Volumes/work/workspace/vew/gensim3.6/lib/python3.6/site-packages/matplotlib/figure.py:445: UserWarning:

Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.</code></pre><p>从上面可以看出， annoy indexer 的初始化时间随着 num_trees 的增加呈线性增长。初始化时间会因语料库而异。在上图中，我们使用了（微小的）Lee 语料库。</p>
<p>此外，在这个数据集中，准确性似乎与树的数量呈对数相关。我们看到更多树的准确性有所提高，但这种关系是非线性的。</p>
<h2 id="8-使用-Google-的-word2vec-文件"><a href="#8-使用-Google-的-word2vec-文件" class="headerlink" title="8. 使用 Google 的 word2vec 文件"></a>8. 使用 Google 的 word2vec 文件</h2><p>我们的模型可以导出为 word2vec C 格式。有二进制和纯文本 word2vec 格式。两者都可以用各种其他软件读取，或作为<code>KeyedVectors</code>对象导入回 Gensim 。</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># To export our model as text</span>
wv<span class="token punctuation">.</span>save_word2vec_format<span class="token punctuation">(</span><span class="token string">'/tmp/vectors.txt'</span><span class="token punctuation">,</span> binary<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>

<span class="token keyword">from</span> smart_open <span class="token keyword">import</span> open
<span class="token comment" spellcheck="true"># View the first 3 lines of the exported file</span>
<span class="token comment" spellcheck="true"># The first line has the total number of entries and the vector dimension count.</span>
<span class="token comment" spellcheck="true"># The next lines have a key (a string) followed by its vector.</span>
<span class="token keyword">with</span> open<span class="token punctuation">(</span><span class="token string">'/tmp/vectors.txt'</span><span class="token punctuation">,</span> encoding<span class="token operator">=</span><span class="token string">'utf8'</span><span class="token punctuation">)</span> <span class="token keyword">as</span> myfile<span class="token punctuation">:</span>
    <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span>myfile<span class="token punctuation">.</span>readline<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>strip<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

<span class="token comment" spellcheck="true"># To import a word2vec text model</span>
wv <span class="token operator">=</span> KeyedVectors<span class="token punctuation">.</span>load_word2vec_format<span class="token punctuation">(</span><span class="token string">'/tmp/vectors.txt'</span><span class="token punctuation">,</span> binary<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>

<span class="token comment" spellcheck="true"># To export a model as binary</span>
wv<span class="token punctuation">.</span>save_word2vec_format<span class="token punctuation">(</span><span class="token string">'/tmp/vectors.bin'</span><span class="token punctuation">,</span> binary<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>

<span class="token comment" spellcheck="true"># To import a word2vec binary model</span>
wv <span class="token operator">=</span> KeyedVectors<span class="token punctuation">.</span>load_word2vec_format<span class="token punctuation">(</span><span class="token string">'/tmp/vectors.bin'</span><span class="token punctuation">,</span> binary<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>

<span class="token comment" spellcheck="true"># To create and save Annoy Index from a loaded `KeyedVectors` object (with 100 trees)</span>
annoy_index <span class="token operator">=</span> AnnoyIndexer<span class="token punctuation">(</span>wv<span class="token punctuation">,</span> <span class="token number">100</span><span class="token punctuation">)</span>
annoy_index<span class="token punctuation">.</span>save<span class="token punctuation">(</span><span class="token string">'/tmp/mymodel.index'</span><span class="token punctuation">)</span>

<span class="token comment" spellcheck="true"># Load and test the saved word vectors and saved Annoy index</span>
wv <span class="token operator">=</span> KeyedVectors<span class="token punctuation">.</span>load_word2vec_format<span class="token punctuation">(</span><span class="token string">'/tmp/vectors.bin'</span><span class="token punctuation">,</span> binary<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
annoy_index <span class="token operator">=</span> AnnoyIndexer<span class="token punctuation">(</span><span class="token punctuation">)</span>
annoy_index<span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token string">'/tmp/mymodel.index'</span><span class="token punctuation">)</span>
annoy_index<span class="token punctuation">.</span>model <span class="token operator">=</span> wv

vector <span class="token operator">=</span> wv<span class="token punctuation">[</span><span class="token string">"cat"</span><span class="token punctuation">]</span>
approximate_neighbors <span class="token operator">=</span> wv<span class="token punctuation">.</span>most_similar<span class="token punctuation">(</span><span class="token punctuation">[</span>vector<span class="token punctuation">]</span><span class="token punctuation">,</span> topn<span class="token operator">=</span><span class="token number">11</span><span class="token punctuation">,</span> indexer<span class="token operator">=</span>annoy_index<span class="token punctuation">)</span>
<span class="token comment" spellcheck="true"># Neatly print the approximate_neighbors and their corresponding cosine similarity values</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Approximate Neighbors"</span><span class="token punctuation">)</span>
<span class="token keyword">for</span> neighbor <span class="token keyword">in</span> approximate_neighbors<span class="token punctuation">:</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>neighbor<span class="token punctuation">)</span>

normal_neighbors <span class="token operator">=</span> wv<span class="token punctuation">.</span>most_similar<span class="token punctuation">(</span><span class="token punctuation">[</span>vector<span class="token punctuation">]</span><span class="token punctuation">,</span> topn<span class="token operator">=</span><span class="token number">11</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"\nExact Neighbors"</span><span class="token punctuation">)</span>
<span class="token keyword">for</span> neighbor <span class="token keyword">in</span> normal_neighbors<span class="token punctuation">:</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>neighbor<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<pre><code>71290 100
the 0.1645237 0.049031682 -0.11330697 0.097082675 -0.099474825 -0.08294691 0.007256336 -0.113704175 0.24664731 -0.062123552 -0.024763709 0.25688595 0.059356388 0.28822595 0.18409002 0.17533085 0.12412363 0.05312752 -0.10347493 0.07136696 0.050333817 0.03533254 0.07569087 -0.41796425 -0.13256022 0.30041444 0.26416314 -0.022389138 -0.20686609 -0.21565206 -0.25032488 -0.12548248 0.077188216 0.2432488 -0.1458781 -0.23084323 -0.13360116 -0.01887776 0.21207437 -0.0022163654 0.047225904 0.18978342 0.19625767 -0.02934954 0.005101277 0.11670754 0.11398655 0.33111402 -0.037173223 0.21018152 -0.07217948 -0.0045775156 -0.18228853 -0.065637104 0.16755614 0.20857134 0.1822439 -0.17496146 0.034775164 0.09327986 -0.011131699 -0.009912837 -0.18504283 -0.0043261787 0.03363841 -0.054994233 0.18313456 -0.22603175 0.15427239 0.22330661 0.026417818 0.09543534 0.09841086 -0.41345838 0.14082615 0.13712159 0.070771925 0.06285282 5.9063022e-05 -0.15651035 -0.016906142 0.14885448 0.07121329 -0.23360902 -0.09033932 -0.11270273 -0.0059097605 -0.04875052 -0.04409246 0.103411175 0.00074150774 -0.08402691 -0.07324047 -0.20355953 -0.091564305 -0.11138651 -0.18119322 0.21025972 -0.06939676 0.0016936468
of 0.19971648 0.15359728 -0.1338489 0.12083505 -0.005847811 -0.085402876 -0.075938866 -0.13501053 0.18837708 -0.1259633 0.110350266 0.108376145 0.015276252 0.33608598 0.22733492 0.11238891 -0.053862635 0.073887356 -0.20558539 -0.099394076 -0.0069137346 -0.114128046 0.027444497 -0.35551408 0.007910002 0.23189865 0.2650087 0.03700684 -0.17699398 -0.35950723 -0.32789174 -0.30379272 0.02704152 0.21078588 -0.023837725 -0.21654394 -0.166978 -0.08431874 0.2691367 -0.0023258273 0.06707064 0.09761329 0.24171327 -0.093486875 0.12232643 0.096265465 0.12889618 0.17138048 0.015292533 0.013243989 -0.09338309 0.0905355 -0.26343557 -0.2523928 0.07358186 0.17042407 0.266381 -0.218722 0.059136674 -0.00048657134 -0.0690399 -0.03615013 -0.059233107 -0.066501416 0.04838442 -0.11165278 0.09096755 -0.18076046 0.20482069 0.34460145 0.03740757 0.019260708 0.03930956 -0.37160733 -0.10296658 0.075969525 0.09362528 0.04970148 -0.07688446 -0.12854671 -0.10089095 0.01764436 0.1420408 -0.17590913 -0.20053966 0.14636976 -0.18029185 -0.081263 -0.048385028 0.26456535 -0.055859976 -0.08821882 -0.15724823 -0.17458497 0.010780472 -0.13346615 -0.12641737 0.16775236 -0.20294443 -0.115340725
Approximate Neighbors
('cat', 1.0)
('cats', 0.5968745350837708)
('meow', 0.5941576957702637)
('leopardus', 0.5938971042633057)
('prionailurus', 0.5928952395915985)
('felis', 0.5831491053104401)
('saimiri', 0.5817937552928925)
('rabbits', 0.5794903337955475)
('caracal', 0.5760406851768494)
('sighthound', 0.5754748582839966)
('oncifelis', 0.5718523561954498)

Exact Neighbors
('cat', 1.0000001192092896)
('cats', 0.6749798059463501)
('meow', 0.6705840826034546)
('leopardus', 0.6701608896255493)
('prionailurus', 0.6685314774513245)
('felis', 0.6524706482887268)
('saimiri', 0.6502071619033813)
('rabbits', 0.6463432312011719)
('purr', 0.6449686288833618)
('caracal', 0.640516996383667)
('sighthound', 0.639556884765625)</code></pre><h2 id=""><a href="#" class="headerlink" title=""></a></h2><h1 id="LDA-Model"><a href="#LDA-Model" class="headerlink" title="LDA Model"></a>LDA Model</h1><p>介绍 Gensim 的 LDA 模型并演示其在 NIPS 语料库中的使用。</p>
<ul>
<li>加载输入数据。</li>
<li>预处理该数据。</li>
<li>将文档转换为BOW向量。</li>
<li>训练 LDA 模型。</li>
</ul>
<p>如果您不熟悉 LDA 模型或如何在 Gensim 中使用它，建议您在继续本教程之前先阅读该模型。对 LDA 模型有基本的了解就足够了。例子：</p>
<ul>
<li><a href="http://blog.echen.me/2011/08/22/introduction-to-latent-dirichlet-allocation" target="_blank" rel="noopener">潜在狄利克雷分配简介</a></li>
<li>Gensim 教程：<a href="https://radimrehurek.com/gensim/auto_examples/core/run_topics_and_transformations.html#sphx-glr-auto-examples-core-run-topics-and-transformations-py" target="_blank" rel="noopener">主题和转换</a></li>
<li>Gensim 的 LDA 模型 API 文档： <code>gensim.models.LdaModel</code></li>
</ul>
<p>LDA(Latent Dirichlet Allocation)是一种文档生成模型。它认为一篇文章是有多个主题的，而每个主题又对应着不同的词。一篇文章的构造过程，首先是以一定的概率选择某个主题，然后再在这个主题下以一定的概率选出某一个词，这样就生成了这篇文章的第一个词。不断重复这个过程，就生成了整片文章。当然这里假定词与词之间是没顺序的。</p>
<p>LDA的使用是上述文档生成的逆过程，它将根据一篇得到的文章，去寻找出这篇文章的主题，以及这些主题对应的词。</p>
<p>现在来看怎么用LDA，LDA会给我们返回什么结果。</p>
<p>LDA是非监督的机器学习模型，并且使用了词袋模型。一篇文章将会用词袋模型构造成词向量。LDA需要我们手动确定要划分的主题的个数，超参数将会在后面讲述，一般超参数对结果无很大影响。</p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210809184231011.png" alt=""></p>
<h2 id="数据"><a href="#数据" class="headerlink" title="数据"></a>数据</h2><p>NIPS（神经信息处理系统）是一个机器学习会议，因此主题应该非常适合本教程的大多数目标受众。您可以从 Sam Roweis 的<a href="http://www.cs.nyu.edu/~roweis/data.html" target="_blank" rel="noopener">网站</a>下载原始数据 。下面的代码也将为您做到这一点。</p>
<blockquote>
<p>语料库包含 1740 个文档，而且不是特别长的文档。所以请记住，本教程不是为了提高效率，在将代码应用于大型数据集之前要小心。</p>
</blockquote>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> io
<span class="token keyword">import</span> os<span class="token punctuation">.</span>path
<span class="token keyword">import</span> re
<span class="token keyword">import</span> tarfile

<span class="token keyword">import</span> smart_open

<span class="token keyword">def</span> <span class="token function">extract_documents</span><span class="token punctuation">(</span>url<span class="token operator">=</span><span class="token string">'https://cs.nyu.edu/~roweis/data/nips12raw_str602.tgz'</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">with</span> smart_open<span class="token punctuation">.</span>open<span class="token punctuation">(</span>url<span class="token punctuation">,</span> <span class="token string">"rb"</span><span class="token punctuation">)</span> <span class="token keyword">as</span> file<span class="token punctuation">:</span>
        <span class="token keyword">with</span> tarfile<span class="token punctuation">.</span>open<span class="token punctuation">(</span>fileobj<span class="token operator">=</span>file<span class="token punctuation">)</span> <span class="token keyword">as</span> tar<span class="token punctuation">:</span>
            <span class="token keyword">for</span> member <span class="token keyword">in</span> tar<span class="token punctuation">.</span>getmembers<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
                <span class="token keyword">if</span> member<span class="token punctuation">.</span>isfile<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">and</span> re<span class="token punctuation">.</span>search<span class="token punctuation">(</span>r<span class="token string">'nipstxt/nips\d+/\d+\.txt'</span><span class="token punctuation">,</span> member<span class="token punctuation">.</span>name<span class="token punctuation">)</span><span class="token punctuation">:</span>
                    member_bytes <span class="token operator">=</span> tar<span class="token punctuation">.</span>extractfile<span class="token punctuation">(</span>member<span class="token punctuation">)</span><span class="token punctuation">.</span>read<span class="token punctuation">(</span><span class="token punctuation">)</span>
                    <span class="token keyword">yield</span> member_bytes<span class="token punctuation">.</span>decode<span class="token punctuation">(</span><span class="token string">'utf-8'</span><span class="token punctuation">,</span> errors<span class="token operator">=</span><span class="token string">'replace'</span><span class="token punctuation">)</span>

docs <span class="token operator">=</span> list<span class="token punctuation">(</span>extract_documents<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>所以我们有一个包含 1740 个文档的列表，其中每个文档都是一个 Unicode 字符串。如果您正在考虑使用自己的语料库，那么在继续本教程的其余部分之前，您需要确保它的格式相同（Unicode 字符串列表）。</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">print</span><span class="token punctuation">(</span>len<span class="token punctuation">(</span>docs<span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>docs<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">500</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>
<pre><code>1740
387
Neural Net and Traditional Classifiers 
William Y. Huang and Richard P. Lippmann
MIT Lincoln Laboratory
Lexington, MA 02173, USA
Abstract
Previous work on nets with continuous-valued inputs led to generative
procedures to construct convex decision regions with two-layer percepttons (one hidden
layer) and arbitrary decision regions with three-layer percepttons (two hidden layers).
Here we demonstrate that two-layer perceptton classifiers trained with back propagation
can form both c</code></pre><h2 id="对文档进行预处理和矢量化"><a href="#对文档进行预处理和矢量化" class="headerlink" title="对文档进行预处理和矢量化"></a>对文档进行预处理和矢量化</h2><p>作为预处理的一部分，我们将：</p>
<ul>
<li>标记化（将文档拆分为标记）。</li>
<li>对令牌进行词形还原。</li>
<li>计算二元组。</li>
<li>计算数据的词袋表示。</li>
</ul>
<p>首先，我们使用来自 NLTK 的正则表达式标记器对文本进行标记。我们删除了数字标记和仅单个字符的标记，因为它们往往没有用处，并且数据集包含很多它们。</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># Tokenize the documents.</span>
<span class="token keyword">from</span> nltk<span class="token punctuation">.</span>tokenize <span class="token keyword">import</span> RegexpTokenizer

<span class="token comment" spellcheck="true"># Split the documents into tokens.</span>
tokenizer <span class="token operator">=</span> RegexpTokenizer<span class="token punctuation">(</span>r<span class="token string">'\w+'</span><span class="token punctuation">)</span>
<span class="token keyword">for</span> idx <span class="token keyword">in</span> range<span class="token punctuation">(</span>len<span class="token punctuation">(</span>docs<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    docs<span class="token punctuation">[</span>idx<span class="token punctuation">]</span> <span class="token operator">=</span> docs<span class="token punctuation">[</span>idx<span class="token punctuation">]</span><span class="token punctuation">.</span>lower<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># Convert to lowercase.</span>
    docs<span class="token punctuation">[</span>idx<span class="token punctuation">]</span> <span class="token operator">=</span> tokenizer<span class="token punctuation">.</span>tokenize<span class="token punctuation">(</span>docs<span class="token punctuation">[</span>idx<span class="token punctuation">]</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># Split into words.</span>

<span class="token comment" spellcheck="true"># Remove numbers, but not words that contain numbers.</span>
docs <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">[</span>token <span class="token keyword">for</span> token <span class="token keyword">in</span> doc <span class="token keyword">if</span> <span class="token operator">not</span> token<span class="token punctuation">.</span>isnumeric<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token keyword">for</span> doc <span class="token keyword">in</span> docs<span class="token punctuation">]</span>

<span class="token comment" spellcheck="true"># Remove words that are only one character.</span>
docs <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">[</span>token <span class="token keyword">for</span> token <span class="token keyword">in</span> doc <span class="token keyword">if</span> len<span class="token punctuation">(</span>token<span class="token punctuation">)</span> <span class="token operator">></span> <span class="token number">1</span><span class="token punctuation">]</span> <span class="token keyword">for</span> doc <span class="token keyword">in</span> docs<span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<pre><code>/home/jonaschn/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/sklearn/feature_extraction/image.py:167: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  dtype=np.int):
/home/jonaschn/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:35: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  eps=np.finfo(np.float).eps,
/home/jonaschn/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:597: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,
/home/jonaschn/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:836: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,
/home/jonaschn/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:862: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  eps=np.finfo(np.float).eps, positive=False):
/home/jonaschn/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:1074: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  max_n_alphas=1000, n_jobs=1, eps=np.finfo(np.float).eps,
/home/jonaschn/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:1306: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  max_n_alphas=1000, n_jobs=1, eps=np.finfo(np.float).eps,
/home/jonaschn/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:1442: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  eps=np.finfo(np.float).eps, copy_X=True, positive=False):
/home/jonaschn/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/sklearn/linear_model/randomized_l1.py:152: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  precompute=False, eps=np.finfo(np.float).eps,
/home/jonaschn/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/sklearn/linear_model/randomized_l1.py:318: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  eps=np.finfo(np.float).eps, random_state=None,
/home/jonaschn/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/sklearn/linear_model/randomized_l1.py:575: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  eps=4 * np.finfo(np.float).eps, n_jobs=1,</code></pre><p>我们使用 NLTK 的 WordNet lemmatizer。在这种情况下，词形还原器比词干分析器更受欢迎，因为它会产生更易读的单词。易于阅读的输出在主题建模中是非常可取的。</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># Lemmatize the documents.</span>
<span class="token keyword">from</span> nltk<span class="token punctuation">.</span>stem<span class="token punctuation">.</span>wordnet <span class="token keyword">import</span> WordNetLemmatizer

lemmatizer <span class="token operator">=</span> WordNetLemmatizer<span class="token punctuation">(</span><span class="token punctuation">)</span>
docs <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">[</span>lemmatizer<span class="token punctuation">.</span>lemmatize<span class="token punctuation">(</span>token<span class="token punctuation">)</span> <span class="token keyword">for</span> token <span class="token keyword">in</span> doc<span class="token punctuation">]</span> <span class="token keyword">for</span> doc <span class="token keyword">in</span> docs<span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>我们在文档中找到了二元组。Bigrams 是两个相邻单词的集合。使用二元组，我们可以在输出中得到像“machine_learning”这样的短语（空格被下划线替换）；没有二元组，我们只会得到“机器”和“学习”。</p>
<p>请注意，在下面的代码中，我们找到了 bigram，然后将它们添加到原始数据中，因为我们希望保留单词“machine”和“learning”以及 bigram“machine_learning”。</p>
<blockquote>
<p>计算大型数据集的 n-gram 可能非常需要计算和内存密集型。</p>
</blockquote>
<pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># Compute bigrams.</span>
<span class="token keyword">from</span> gensim<span class="token punctuation">.</span>models <span class="token keyword">import</span> Phrases

<span class="token comment" spellcheck="true"># Add bigrams and trigrams to docs (only ones that appear 20 times or more).</span>
bigram <span class="token operator">=</span> Phrases<span class="token punctuation">(</span>docs<span class="token punctuation">,</span> min_count<span class="token operator">=</span><span class="token number">20</span><span class="token punctuation">)</span>
<span class="token keyword">for</span> idx <span class="token keyword">in</span> range<span class="token punctuation">(</span>len<span class="token punctuation">(</span>docs<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">for</span> token <span class="token keyword">in</span> bigram<span class="token punctuation">[</span>docs<span class="token punctuation">[</span>idx<span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">:</span>
        <span class="token keyword">if</span> <span class="token string">'_'</span> <span class="token keyword">in</span> token<span class="token punctuation">:</span>
            <span class="token comment" spellcheck="true"># Token is a bigram, add to document.</span>
            docs<span class="token punctuation">[</span>idx<span class="token punctuation">]</span><span class="token punctuation">.</span>append<span class="token punctuation">(</span>token<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<pre><code>/home/jonaschn/Projects/gensim/gensim/similarities/__init__.py:11: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package &lt;https://pypi.org/project/python-Levenshtein/&gt; is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.
  "The gensim.similarities.levenshtein submodule is disabled, because the optional "
2021-03-19 14:09:53,817 : INFO : collecting all words and their counts
2021-03-19 14:09:53,817 : INFO : PROGRESS: at sentence #0, processed 0 words and 0 word types
2021-03-19 14:09:59,172 : INFO : collected 1120198 token types (unigram + bigrams) from a corpus of 4629808 words and 1740 sentences
2021-03-19 14:09:59,172 : INFO : merged Phrases&lt;1120198 vocab, min_count=20, threshold=10.0, max_vocab_size=40000000&gt;
2021-03-19 14:09:59,190 : INFO : Phrases lifecycle event {'msg': 'built Phrases&lt;1120198 vocab, min_count=20, threshold=10.0, max_vocab_size=40000000&gt; in 5.36s', 'datetime': '2021-03-19T14:09:59.189253', 'gensim': '4.0.0.rc1', 'python': '3.7.0 (default, Jun 28 2018, 13:15:42) \n[GCC 7.2.0]', 'platform': 'Linux-4.15.0-136-generic-x86_64-with-debian-buster-sid', 'event': 'created'}</code></pre><p>我们根据<em>文档频率</em>删除稀有词和常用词。下面我们删除出现在少于 20 个文档或超过 50% 的文档中的词。考虑尝试仅根据单词的频率来删除单词，或者将其与这种方法相结合。</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># Remove rare and common tokens.</span>
<span class="token keyword">from</span> gensim<span class="token punctuation">.</span>corpora <span class="token keyword">import</span> Dictionary

<span class="token comment" spellcheck="true"># Create a dictionary representation of the documents.</span>
dictionary <span class="token operator">=</span> Dictionary<span class="token punctuation">(</span>docs<span class="token punctuation">)</span>

<span class="token comment" spellcheck="true"># Filter out words that occur less than 20 documents, or more than 50% of the documents.</span>
dictionary<span class="token punctuation">.</span>filter_extremes<span class="token punctuation">(</span>no_below<span class="token operator">=</span><span class="token number">20</span><span class="token punctuation">,</span> no_above<span class="token operator">=</span><span class="token number">0.5</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<pre><code>2021-03-19 14:10:07,280 : INFO : adding document #0 to Dictionary(0 unique tokens: [])
2021-03-19 14:10:09,906 : INFO : built Dictionary(79429 unique tokens: ['1ooooo', '1st', '25oo', '2o00', '4ooo']...) from 1740 documents (total 4953968 corpus positions)
2021-03-19 14:10:09,906 : INFO : Dictionary lifecycle event {'msg': "built Dictionary(79429 unique tokens: ['1ooooo', '1st', '25oo', '2o00', '4ooo']...) from 1740 documents (total 4953968 corpus positions)", 'datetime': '2021-03-19T14:10:09.906597', 'gensim': '4.0.0.rc1', 'python': '3.7.0 (default, Jun 28 2018, 13:15:42) \n[GCC 7.2.0]', 'platform': 'Linux-4.15.0-136-generic-x86_64-with-debian-buster-sid', 'event': 'created'}
2021-03-19 14:10:10,101 : INFO : discarding 70785 tokens: [('1ooooo', 1), ('25oo', 2), ('2o00', 6), ('4ooo', 2), ('64k', 6), ('a', 1740), ('aaditional', 1), ('above', 1114), ('abstract', 1740), ('acase', 1)]...
2021-03-19 14:10:10,102 : INFO : keeping 8644 tokens which were in no less than 20 and no more than 870 (=50.0%) documents
2021-03-19 14:10:10,128 : INFO : resulting dictionary: Dictionary(8644 unique tokens: ['1st', '5oo', '7th', 'a2', 'a_well']...)</code></pre><p>最后，我们将文档转换为矢量化形式。我们简单地计算每个单词的频率，包括二元组。</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># Bag-of-words representation of the documents.</span>
corpus <span class="token operator">=</span> <span class="token punctuation">[</span>dictionary<span class="token punctuation">.</span>doc2bow<span class="token punctuation">(</span>doc<span class="token punctuation">)</span> <span class="token keyword">for</span> doc <span class="token keyword">in</span> docs<span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>
<p>让我们看看我们必须训练多少令牌和文档。</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'Number of unique tokens: %d'</span> <span class="token operator">%</span> len<span class="token punctuation">(</span>dictionary<span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'Number of documents: %d'</span> <span class="token operator">%</span> len<span class="token punctuation">(</span>corpus<span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>
<pre><code>Number of unique tokens: 8644
Number of documents: 1740</code></pre><h2 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h2><p>我们已准备好训练 LDA 模型。我们将首先讨论如何设置一些训练参数。</p>
<p>首先，房间里的大象：我需要多少主题？对此确实没有简单的答案，这取决于您的数据和应用程序。我在这里使用了 10 个主题，因为我想要一些我可以解释和“标记”的主题，而且结果证明这给了我相当不错的结果。您可能不需要解释所有主题，因此您可以使用大量主题，例如 100。</p>
<p><code>chunksize</code>控制在训练算法中一次处理多少文档。增加块大小将加快训练速度，至少只要文档块可以轻松放入内存。我已经设置了，超过了文档数量，所以我一次性处理所有数据。然而，块大小会影响模型的质量，但在这种情况下差异不大。<code>chunksize = 2000</code></p>
<p><code>passes</code>控制我们在整个语料库上训练模型的频率。通行证的另一个词可能是“纪元”。<code>iterations</code>有点技术性，但本质上它控制了我们对每个文档重复特定循环的频率。将“passes”和“iterations”的数量设置得足够高很重要。</p>
<p>我建议使用以下方法来选择迭代和传递。首先，启用日志记录（如许多 Gensim 教程中所述），并 在. 训练模型时，会在日志中查找如下所示的行：<code>eval_every = 1</code> <code>LdaModel</code></p>
<pre><code>2016-06-21 15:40:06,753 - gensim.models.ldamodel - DEBUG - 68/1566 documents converged within 400 iterations</code></pre><p>如果您设置，您将看到此行 20 次。确保在最后通过时，大多数文档已经收敛。因此，您希望选择足够高的传递和迭代次数，以便发生这种情况。<code>passes = 20</code></p>
<p>我们设置和。同样，这有点技术性，但本质上我们正在自动学习模型中通常必须明确指定的两个参数。<code>alpha = 'auto'</code> <code>eta = 'auto'</code></p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># Train LDA model.</span>
<span class="token keyword">from</span> gensim<span class="token punctuation">.</span>models <span class="token keyword">import</span> LdaModel

<span class="token comment" spellcheck="true"># Set training parameters.</span>
num_topics <span class="token operator">=</span> <span class="token number">10</span>
chunksize <span class="token operator">=</span> <span class="token number">2000</span>
passes <span class="token operator">=</span> <span class="token number">20</span>
iterations <span class="token operator">=</span> <span class="token number">400</span>
eval_every <span class="token operator">=</span> None  <span class="token comment" spellcheck="true"># Don't evaluate model perplexity, takes too much time.</span>

<span class="token comment" spellcheck="true"># Make a index to word dictionary.</span>
temp <span class="token operator">=</span> dictionary<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>  <span class="token comment" spellcheck="true"># This is only to "load" the dictionary.</span>
id2word <span class="token operator">=</span> dictionary<span class="token punctuation">.</span>id2token

model <span class="token operator">=</span> LdaModel<span class="token punctuation">(</span>
    corpus<span class="token operator">=</span>corpus<span class="token punctuation">,</span>
    id2word<span class="token operator">=</span>id2word<span class="token punctuation">,</span>
    chunksize<span class="token operator">=</span>chunksize<span class="token punctuation">,</span>
    alpha<span class="token operator">=</span><span class="token string">'auto'</span><span class="token punctuation">,</span>
    eta<span class="token operator">=</span><span class="token string">'auto'</span><span class="token punctuation">,</span>
    iterations<span class="token operator">=</span>iterations<span class="token punctuation">,</span>
    num_topics<span class="token operator">=</span>num_topics<span class="token punctuation">,</span>
    passes<span class="token operator">=</span>passes<span class="token punctuation">,</span>
    eval_every<span class="token operator">=</span>eval_every
<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>我们可以计算每个主题的主题一致性。下面我们显示平均主题连贯性并按主题连贯性顺序打印主题。</p>
<p>请注意，我们在这里使用了“Umass”主题连贯性度量（参见 <a href="https://radimrehurek.com/gensim/models/ldamodel.html#gensim.models.ldamodel.LdaModel.top_topics" target="_blank" rel="noopener"><code>gensim.models.ldamodel.LdaModel.top_topics()</code></a>参考资料），Gensim 最近获得了“AKSW”主题连贯性度量的实现。</p>
<p>如果你熟悉这个数据集中文章的主题，你会发现下面的主题很有意义。然而，它们并非没有缺陷。我们可以看到一些主题之间存在大量重叠，其他主题很难解释，并且其中大多数至少有一些看起来不合适的术语。如果您能够做得更好，请随时在<a href="http://rare-technologies.com/lda-training-tips/的博客上分享您的方法！" target="_blank" rel="noopener">http://rare-technologies.com/lda-training-tips/的博客上分享您的方法！</a></p>
<pre class="line-numbers language-python"><code class="language-python">top_topics <span class="token operator">=</span> model<span class="token punctuation">.</span>top_topics<span class="token punctuation">(</span>corpus<span class="token punctuation">)</span> <span class="token comment" spellcheck="true">#, num_words=20)</span>

<span class="token comment" spellcheck="true"># Average topic coherence is the sum of topic coherences of all topics, divided by the number of topics.</span>
avg_topic_coherence <span class="token operator">=</span> sum<span class="token punctuation">(</span><span class="token punctuation">[</span>t<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token keyword">for</span> t <span class="token keyword">in</span> top_topics<span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">/</span> num_topics
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'Average topic coherence: %.4f.'</span> <span class="token operator">%</span> avg_topic_coherence<span class="token punctuation">)</span>

<span class="token keyword">from</span> pprint <span class="token keyword">import</span> pprint
pprint<span class="token punctuation">(</span>top_topics<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h2 id="Things-to-experiment-with"><a href="#Things-to-experiment-with" class="headerlink" title="Things to experiment with"></a>Things to experiment with</h2><ul>
<li><code>no_above</code>和方法中的<code>no_below</code>参数<code>filter_extremes</code>。</li>
<li>添加三元组甚至更高阶的 n-gram。</li>
<li>考虑使用保留集或交叉验证是否适合您。</li>
<li>尝试其他数据集。</li>
</ul>
<h2 id="Where-to-go-from-here"><a href="#Where-to-go-from-here" class="headerlink" title="Where to go from here"></a>Where to go from here</h2><ul>
<li>Check out a RaRe blog post on the AKSW topic coherence measure (<a href="http://rare-technologies.com/what-is-topic-coherence/" target="_blank" rel="noopener">http://rare-technologies.com/what-is-topic-coherence/</a>).</li>
<li>pyLDAvis (<a href="https://pyldavis.readthedocs.io/en/latest/index.html" target="_blank" rel="noopener">https://pyldavis.readthedocs.io/en/latest/index.html</a>).</li>
<li>Read some more Gensim tutorials (<a href="https://github.com/RaRe-Technologies/gensim/blob/develop/tutorials.md#tutorials" target="_blank" rel="noopener">https://github.com/RaRe-Technologies/gensim/blob/develop/tutorials.md#tutorials</a>).</li>
</ul>
<h1 id="Soft-Cosine-Measure"><a href="#Soft-Cosine-Measure" class="headerlink" title="Soft Cosine Measure"></a>Soft Cosine Measure</h1><p>演示使用 Gensim 的 SCM 实现。</p>
<p>Soft Cosine Measure (SCM) 是机器学习中一种很有前途的新工具，它允许我们提交查询并返回最相关的文档。本教程介绍了 SCM 并展示了如何使用该<code>inner_product</code>方法计算两个文档之间的 SCM 相似度。</p>
<h2 id="软余弦测量基础"><a href="#软余弦测量基础" class="headerlink" title="软余弦测量基础"></a>软余弦测量基础</h2><p>Soft Cosine Measure (SCM) 是一种允许我们以有意义的方式评估两个文档之间的相似性的方法，即使它们没有共同的单词。它使用单词之间的相似性度量，可以使用单词的 [word2vec]  向量嵌入。</p>
<p>下面针对两个非常相似的句子说明了 SCM。这些句子没有共同的词，但是通过对同义词建模，SCM 能够准确地衡量两个句子之间的相似度。该方法还使用了文档的词袋向量表示（简单地说，就是单词在文档中的频率）。该方法背后的直觉是我们计算标准余弦相似度，假设文档向量以非正交基表示，其中两个基向量之间的角度来自相应单词的 word2vec 嵌入之间的角度。</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> plt
<span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>image <span class="token keyword">as</span> mpimg
img <span class="token operator">=</span> mpimg<span class="token punctuation">.</span>imread<span class="token punctuation">(</span><span class="token string">'scm-hello.png'</span><span class="token punctuation">)</span>
imgplot <span class="token operator">=</span> plt<span class="token punctuation">.</span>imshow<span class="token punctuation">(</span>img<span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>axis<span class="token punctuation">(</span><span class="token string">'off'</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p><img src="/images/loading.gif" data-original="../images/ML/sphx_glr_run_scm_001.png" alt=""></p>
<p>这种方法可能是在 Grigori Sidorov、Alexander Gelbukh、Helena Gomez-Adorno 和 David Pinto 的文章“软度量和软余弦度量：向量空间模型中的特征度量”中首次引入的。</p>
<p>在本教程中，我们将学习如何使用 Gensim 的 SCM 功能，该功能由<code>inner_product</code>一次性计算方法和 <code>SoftCosineSimilarity</code>基于语料库的相似性查询类组成。</p>
<h2 id="计算软余弦测度"><a href="#计算软余弦测度" class="headerlink" title="计算软余弦测度"></a>计算软余弦测度</h2><p>要使用 SCM，您需要一些现有的词嵌入。您可以训练自己的 Word2Vec 模型。</p>
<p>让我们用一些句子来计算之间的距离。</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># Initialize logging.</span>
<span class="token keyword">import</span> logging
logging<span class="token punctuation">.</span>basicConfig<span class="token punctuation">(</span>format<span class="token operator">=</span><span class="token string">'%(asctime)s : %(levelname)s : %(message)s'</span><span class="token punctuation">,</span> level<span class="token operator">=</span>logging<span class="token punctuation">.</span>INFO<span class="token punctuation">)</span>

sentence_obama <span class="token operator">=</span> <span class="token string">'Obama speaks to the media in Illinois'</span>
sentence_president <span class="token operator">=</span> <span class="token string">'The president greets the press in Chicago'</span>
sentence_orange <span class="token operator">=</span> <span class="token string">'Oranges are my favorite fruit'</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>前两句句子的内容非常相似，因此SCM应该很高。相比之下，第三句与前两句无关，SCM应该低。</p>
<p>在计算 SCM 之前，我们要删除停用词（“the”、“to”等），因为这些对句子中的信息贡献不大。</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># Import and download stopwords from NLTK.</span>
<span class="token keyword">from</span> nltk<span class="token punctuation">.</span>corpus <span class="token keyword">import</span> stopwords
<span class="token keyword">from</span> nltk <span class="token keyword">import</span> download
download<span class="token punctuation">(</span><span class="token string">'stopwords'</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># Download stopwords list.</span>
stop_words <span class="token operator">=</span> stopwords<span class="token punctuation">.</span>words<span class="token punctuation">(</span><span class="token string">'english'</span><span class="token punctuation">)</span>

<span class="token keyword">def</span> <span class="token function">preprocess</span><span class="token punctuation">(</span>sentence<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> <span class="token punctuation">[</span>w <span class="token keyword">for</span> w <span class="token keyword">in</span> sentence<span class="token punctuation">.</span>lower<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">if</span> w <span class="token operator">not</span> <span class="token keyword">in</span> stop_words<span class="token punctuation">]</span>

sentence_obama <span class="token operator">=</span> preprocess<span class="token punctuation">(</span>sentence_obama<span class="token punctuation">)</span>
sentence_president <span class="token operator">=</span> preprocess<span class="token punctuation">(</span>sentence_president<span class="token punctuation">)</span>
sentence_orange <span class="token operator">=</span> preprocess<span class="token punctuation">(</span>sentence_orange<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<pre><code>/home/witiko/.virtualenvs/gensim4/lib/python3.7/site-packages/sklearn/feature_extraction/image.py:167: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  dtype=np.int):
/home/witiko/.virtualenvs/gensim4/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:30: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  method='lar', copy_X=True, eps=np.finfo(np.float).eps,
/home/witiko/.virtualenvs/gensim4/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:167: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  method='lar', copy_X=True, eps=np.finfo(np.float).eps,
/home/witiko/.virtualenvs/gensim4/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:284: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  eps=np.finfo(np.float).eps, copy_Gram=True, verbose=0,
/home/witiko/.virtualenvs/gensim4/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:862: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,
/home/witiko/.virtualenvs/gensim4/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:1101: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,
/home/witiko/.virtualenvs/gensim4/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:1127: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  eps=np.finfo(np.float).eps, positive=False):
/home/witiko/.virtualenvs/gensim4/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:1362: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,
/home/witiko/.virtualenvs/gensim4/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:1602: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,
/home/witiko/.virtualenvs/gensim4/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:1738: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  eps=np.finfo(np.float).eps, copy_X=True, positive=False):
[nltk_data] Downloading package stopwords to /home/witiko/nltk_data...
[nltk_data]   Package stopwords is already up-to-date!</code></pre><p>接下来，我们将构建一个字典和一个 TF-IDF 模型，并将句子转换为词袋格式。</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">from</span> gensim<span class="token punctuation">.</span>corpora <span class="token keyword">import</span> Dictionary
documents <span class="token operator">=</span> <span class="token punctuation">[</span>sentence_obama<span class="token punctuation">,</span> sentence_president<span class="token punctuation">,</span> sentence_orange<span class="token punctuation">]</span>
dictionary <span class="token operator">=</span> Dictionary<span class="token punctuation">(</span>documents<span class="token punctuation">)</span>

sentence_obama <span class="token operator">=</span> dictionary<span class="token punctuation">.</span>doc2bow<span class="token punctuation">(</span>sentence_obama<span class="token punctuation">)</span>
sentence_president <span class="token operator">=</span> dictionary<span class="token punctuation">.</span>doc2bow<span class="token punctuation">(</span>sentence_president<span class="token punctuation">)</span>
sentence_orange <span class="token operator">=</span> dictionary<span class="token punctuation">.</span>doc2bow<span class="token punctuation">(</span>sentence_orange<span class="token punctuation">)</span>

<span class="token keyword">from</span> gensim<span class="token punctuation">.</span>models <span class="token keyword">import</span> TfidfModel
documents <span class="token operator">=</span> <span class="token punctuation">[</span>sentence_obama<span class="token punctuation">,</span> sentence_president<span class="token punctuation">,</span> sentence_orange<span class="token punctuation">]</span>
tfidf <span class="token operator">=</span> TfidfModel<span class="token punctuation">(</span>documents<span class="token punctuation">)</span>

sentence_obama <span class="token operator">=</span> tfidf<span class="token punctuation">[</span>sentence_obama<span class="token punctuation">]</span>
sentence_president <span class="token operator">=</span> tfidf<span class="token punctuation">[</span>sentence_president<span class="token punctuation">]</span>
sentence_orange <span class="token operator">=</span> tfidf<span class="token punctuation">[</span>sentence_orange<span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>现在，如前所述，我们将使用一些下载的预训练嵌入。我们将这些加载到一个 Gensim Word2Vec 模型类中，并使用嵌入构建一个术语相似度矩阵。</p>
<blockquote>
<p>我们在这里选择的嵌入需要大量内存。</p>
</blockquote>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> gensim<span class="token punctuation">.</span>downloader <span class="token keyword">as</span> api
model <span class="token operator">=</span> api<span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token string">'word2vec-google-news-300'</span><span class="token punctuation">)</span>

<span class="token keyword">from</span> gensim<span class="token punctuation">.</span>similarities <span class="token keyword">import</span> SparseTermSimilarityMatrix<span class="token punctuation">,</span> WordEmbeddingSimilarityIndex
termsim_index <span class="token operator">=</span> WordEmbeddingSimilarityIndex<span class="token punctuation">(</span>model<span class="token punctuation">)</span>
termsim_matrix <span class="token operator">=</span> SparseTermSimilarityMatrix<span class="token punctuation">(</span>termsim_index<span class="token punctuation">,</span> dictionary<span class="token punctuation">,</span> tfidf<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>因此，让我们使用该<code>inner_product</code>方法计算 SCM 。</p>
<pre class="line-numbers language-python"><code class="language-python">similarity <span class="token operator">=</span> termsim_matrix<span class="token punctuation">.</span>inner_product<span class="token punctuation">(</span>sentence_obama<span class="token punctuation">,</span> sentence_president<span class="token punctuation">,</span> normalized<span class="token operator">=</span><span class="token punctuation">(</span><span class="token boolean">True</span><span class="token punctuation">,</span> <span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'similarity = %.4f'</span> <span class="token operator">%</span> similarity<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>
<pre><code>similarity = 0.2575</code></pre><p>让我们用两个完全不相关的句子尝试同样的事情。请注意，相似度较小。</p>
<pre class="line-numbers language-python"><code class="language-python">similarity <span class="token operator">=</span> termsim_matrix<span class="token punctuation">.</span>inner_product<span class="token punctuation">(</span>sentence_obama<span class="token punctuation">,</span> sentence_orange<span class="token punctuation">,</span> normalized<span class="token operator">=</span><span class="token punctuation">(</span><span class="token boolean">True</span><span class="token punctuation">,</span> <span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'similarity = %.4f'</span> <span class="token operator">%</span> similarity<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>
<pre><code>similarity = 0.0000</code></pre><h1 id="Word-Mover’s-Distance"><a href="#Word-Mover’s-Distance" class="headerlink" title="Word Mover’s Distance"></a>Word Mover’s Distance</h1><p>演示使用 Gensim 的 WMD 实现。</p>
<p>Word Mover 的距离 (WMD) 是机器学习中一种很有前途的新工具，它允许我们提交查询并返回最相关的文档。本教程介绍了 WMD 并展示了如何使用<code>wmdistance</code>.</p>
<h2 id="WMD-Basic"><a href="#WMD-Basic" class="headerlink" title="WMD Basic"></a>WMD Basic</h2><p>WMD 使我们能够以有意义的方式评估两个文档之间的“距离”，即使它们没有共同的词。它使用word2vec向量嵌入。它已被证明在 k-近邻分类中优于许多最先进的方法。</p>
<p>下面用两个非常相似的句子说明了 WMD。句子没有共同的词，但通过匹配相关词，WMD 能够准确衡量两个句子之间的（异）相似度。该方法还使用了文档的词袋表示（简单地说，是单词在文档中的频率），在下图中用 $d$ 表示。该方法背后的直觉是我们找到了文档之间的最小“行进距离”，换句话说，是将文档 1 的分布“移动”到文档 2 的分布的最有效方法。</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># Image from https://vene.ro/images/wmd-obama.png</span>
<span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> plt
<span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>image <span class="token keyword">as</span> mpimg
img <span class="token operator">=</span> mpimg<span class="token punctuation">.</span>imread<span class="token punctuation">(</span><span class="token string">'wmd-obama.png'</span><span class="token punctuation">)</span>
imgplot <span class="token operator">=</span> plt<span class="token punctuation">.</span>imshow<span class="token punctuation">(</span>img<span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>axis<span class="token punctuation">(</span><span class="token string">'off'</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p><img src="/images/loading.gif" data-original="../images/ML/sphx_glr_run_wmd_001.png" alt=""></p>
<p>这种方法是在 Matt Kusner 等人的文章“From Word Embeddings To Document Distances”中介绍的。（<a href="http://jmlr.org/proceedings/papers/v37/kusnerb15.pdf" target="_blank" rel="noopener">链接到 PDF</a>）。它的灵感来自“地球移动者的距离”，并采用了“运输问题”的求解器。</p>
<p>在本教程中，我们将学习如何使用 Gensim 的 WMD 功能，该功能包括<code>wmdistance</code>距离计算方法和 <code>WmdSimilarity</code>基于语料库的相似性查询类。</p>
<h2 id="计算-Word-Mover-的距离"><a href="#计算-Word-Mover-的距离" class="headerlink" title="计算 Word Mover 的距离"></a>计算 Word Mover 的距离</h2><p>要使用 WMD，您需要一些现有的词嵌入。您可以训练自己的 Word2Vec 模型。</p>
<p>让我们用一些句子来计算之间的距离。</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># Initialize logging.</span>
<span class="token keyword">import</span> logging
logging<span class="token punctuation">.</span>basicConfig<span class="token punctuation">(</span>format<span class="token operator">=</span><span class="token string">'%(asctime)s : %(levelname)s : %(message)s'</span><span class="token punctuation">,</span> level<span class="token operator">=</span>logging<span class="token punctuation">.</span>INFO<span class="token punctuation">)</span>

sentence_obama <span class="token operator">=</span> <span class="token string">'Obama speaks to the media in Illinois'</span>
sentence_president <span class="token operator">=</span> <span class="token string">'The president greets the press in Chicago'</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>这些句子的内容非常相似，因此 WMD 应该很低。在计算 WMD 之前，我们要删除停用词（“the”、“to”等），因为这些对句子中的信息贡献不大。</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># Import and download stopwords from NLTK.</span>
<span class="token keyword">from</span> nltk<span class="token punctuation">.</span>corpus <span class="token keyword">import</span> stopwords
<span class="token keyword">from</span> nltk <span class="token keyword">import</span> download
download<span class="token punctuation">(</span><span class="token string">'stopwords'</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># Download stopwords list.</span>
stop_words <span class="token operator">=</span> stopwords<span class="token punctuation">.</span>words<span class="token punctuation">(</span><span class="token string">'english'</span><span class="token punctuation">)</span>

<span class="token keyword">def</span> <span class="token function">preprocess</span><span class="token punctuation">(</span>sentence<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> <span class="token punctuation">[</span>w <span class="token keyword">for</span> w <span class="token keyword">in</span> sentence<span class="token punctuation">.</span>lower<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">if</span> w <span class="token operator">not</span> <span class="token keyword">in</span> stop_words<span class="token punctuation">]</span>

sentence_obama <span class="token operator">=</span> preprocess<span class="token punctuation">(</span>sentence_obama<span class="token punctuation">)</span>
sentence_president <span class="token operator">=</span> preprocess<span class="token punctuation">(</span>sentence_president<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<pre><code>/home/witiko/.virtualenvs/gensim4/lib/python3.7/site-packages/sklearn/feature_extraction/image.py:167: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  dtype=np.int):
/home/witiko/.virtualenvs/gensim4/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:30: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  method='lar', copy_X=True, eps=np.finfo(np.float).eps,
/home/witiko/.virtualenvs/gensim4/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:167: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  method='lar', copy_X=True, eps=np.finfo(np.float).eps,
/home/witiko/.virtualenvs/gensim4/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:284: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  eps=np.finfo(np.float).eps, copy_Gram=True, verbose=0,
/home/witiko/.virtualenvs/gensim4/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:862: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,
/home/witiko/.virtualenvs/gensim4/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:1101: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,
/home/witiko/.virtualenvs/gensim4/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:1127: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  eps=np.finfo(np.float).eps, positive=False):
/home/witiko/.virtualenvs/gensim4/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:1362: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,
/home/witiko/.virtualenvs/gensim4/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:1602: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,
/home/witiko/.virtualenvs/gensim4/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:1738: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  eps=np.finfo(np.float).eps, copy_X=True, positive=False):
[nltk_data] Downloading package stopwords to /home/witiko/nltk_data...
[nltk_data]   Package stopwords is already up-to-date!</code></pre><p>现在，如前所述，我们将使用一些下载的预训练嵌入。我们将这些加载到 Gensim Word2Vec 模型类中。</p>
<blockquote>
<p>我们在这里选择的嵌入需要大量内存。</p>
</blockquote>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> gensim<span class="token punctuation">.</span>downloader <span class="token keyword">as</span> api
model <span class="token operator">=</span> api<span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token string">'word2vec-google-news-300'</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>
<p>因此，让我们使用该<code>wmdistance</code>方法计算 WMD 。</p>
<pre class="line-numbers language-python"><code class="language-python">distance <span class="token operator">=</span> model<span class="token punctuation">.</span>wmdistance<span class="token punctuation">(</span>sentence_obama<span class="token punctuation">,</span> sentence_president<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'distance = %.4f'</span> <span class="token operator">%</span> distance<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>
<pre><code>distance = 1.0175</code></pre><p>让我们用两个完全不相关的句子尝试同样的事情。注意距离更大。</p>
<pre class="line-numbers language-python"><code class="language-python">sentence_orange <span class="token operator">=</span> preprocess<span class="token punctuation">(</span><span class="token string">'Oranges are my favorite fruit'</span><span class="token punctuation">)</span>
distance <span class="token operator">=</span> model<span class="token punctuation">.</span>wmdistance<span class="token punctuation">(</span>sentence_obama<span class="token punctuation">,</span> sentence_orange<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'distance = %.4f'</span> <span class="token operator">%</span> distance<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>
<pre><code>distance = 1.3663</code></pre><h1 id="Other"><a href="#Other" class="headerlink" title="Other"></a>Other</h1><p><a href="https://radimrehurek.com/gensim/auto_examples/howtos/run_doc2vec_imdb.html#sphx-glr-auto-examples-howtos-run-doc2vec-imdb-py" target="_blank" rel="noopener">How to reproduce the doc2vec ‘Paragraph Vector’ paper</a></p>
<p><a href="https://radimrehurek.com/gensim/auto_examples/howtos/run_compare_lda.html" target="_blank" rel="noopener">How to Compare LDA Models</a></p>
<script>
        document.querySelectorAll('.github-emoji')
          .forEach(el => {
            if (!el.dataset.src) { return; }
            const img = document.createElement('img');
            img.style = 'display:none !important;';
            img.src = el.dataset.src;
            img.addEventListener('error', () => {
              img.remove();
              el.style.color = 'inherit';
              el.style.backgroundImage = 'none';
              el.style.background = 'none';
            });
            img.addEventListener('load', () => {
              img.remove();
            });
            document.body.appendChild(img);
          });
      </script>
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://jackhcc.github.io" rel="external nofollow noreferrer">杰克成</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://jackhcc.github.io/posts/gensim.html">https://jackhcc.github.io/posts/gensim.html</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="https://jackhcc.github.io" target="_blank">杰克成</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/tags/NLP/">
                                    <span class="chip bg-color">NLP</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/libs/share/css/share.min.css">

<div id="article-share">
    
    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
                <style>
    #reward {
        margin: 40px 0;
        text-align: center;
    }

    #reward .reward-link {
        font-size: 1.4rem;
        line-height: 38px;
    }

    #reward .btn-floating:hover {
        box-shadow: 0 6px 12px rgba(0, 0, 0, 0.2), 0 5px 15px rgba(0, 0, 0, 0.2);
    }

    #rewardModal {
        width: 320px;
        height: 350px;
    }

    #rewardModal .reward-title {
        margin: 15px auto;
        padding-bottom: 5px;
    }

    #rewardModal .modal-content {
        padding: 10px;
    }

    #rewardModal .close {
        position: absolute;
        right: 15px;
        top: 15px;
        color: rgba(0, 0, 0, 0.5);
        font-size: 1.3rem;
        line-height: 20px;
        cursor: pointer;
    }

    #rewardModal .close:hover {
        color: #ef5350;
        transform: scale(1.3);
        -moz-transform:scale(1.3);
        -webkit-transform:scale(1.3);
        -o-transform:scale(1.3);
    }

    #rewardModal .reward-tabs {
        margin: 0 auto;
        width: 210px;
    }

    .reward-tabs .tabs {
        height: 38px;
        margin: 10px auto;
        padding-left: 0;
    }

    .reward-content ul {
        padding-left: 0 !important;
    }

    .reward-tabs .tabs .tab {
        height: 38px;
        line-height: 38px;
    }

    .reward-tabs .tab a {
        color: #fff;
        background-color: #ccc;
    }

    .reward-tabs .tab a:hover {
        background-color: #ccc;
        color: #fff;
    }

    .reward-tabs .wechat-tab .active {
        color: #fff !important;
        background-color: #22AB38 !important;
    }

    .reward-tabs .alipay-tab .active {
        color: #fff !important;
        background-color: #019FE8 !important;
    }

    .reward-tabs .reward-img {
        width: 210px;
        height: 210px;
    }
</style>

<div id="reward">
    <a href="#rewardModal" class="reward-link modal-trigger btn-floating btn-medium waves-effect waves-light red">赏</a>

    <!-- Modal Structure -->
    <div id="rewardModal" class="modal">
        <div class="modal-content">
            <a class="close modal-close"><i class="fas fa-times"></i></a>
            <h4 class="reward-title">你的赏识是我前进的动力</h4>
            <div class="reward-content">
                <div class="reward-tabs">
                    <ul class="tabs row">
                        <li class="tab col s6 alipay-tab waves-effect waves-light"><a href="#alipay">支付宝</a></li>
                        <li class="tab col s6 wechat-tab waves-effect waves-light"><a href="#wechat">微 信</a></li>
                    </ul>
                    <div id="alipay">
                        <img src="/images/loading.gif" data-original="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/medias/reward/aliqr.png" class="reward-img" alt="支付宝打赏二维码">
                    </div>
                    <div id="wechat">
                        <img src="/images/loading.gif" data-original="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/medias/reward/wxqr.png" class="reward-img" alt="微信打赏二维码">
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>

<script>
    $(function () {
        $('.tabs').tabs();
    });
</script>
            
        </div>
    </div>

    
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/libs/gitalk/gitalk.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/css/my-gitalk.css">

<div class="card gitalk-card" data-aos="fade-up">
    <div class="comment_headling" style="font-size: 20px; font-weight: 700; position: relative; left: 20px; top: 15px; padding-bottom: 5px;">
        <i class="fas fa-comments fa-fw" aria-hidden="true"></i>
        <span>评论</span>
    </div>
    <div id="gitalk-container" class="card-content"></div>
</div>

<script src="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/libs/gitalk/gitalk.min.js"></script>
<script>
    let gitalk = new Gitalk({
        clientID: '3821a0bbb773038a51fc',
        clientSecret: '4b30b507d67ec5497ec0e77f43f80cb3e0d7dd3a',
        repo: 'JackHCC.github.io',
        owner: 'JackHCC',
        admin: "JackHCC",
        id: '2021-08-09T14-45-03',
        distractionFreeMode: false  // Facebook-like distraction free mode
    });

    gitalk.render('gitalk-container');
</script>
    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/posts/ctf-web.html">
                    <div class="card-image">
                        
                        
                        <img src="/images/loading.gif" data-original="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/medias/featureimages/20.jpg" class="responsive-img" alt="CTF-Web要点">
                        
                        <span class="card-title">CTF-Web要点</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            CTF-Web学习要点
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2021-08-12
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/CTF/" class="post-category">
                                    CTF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/CTF-Web/">
                        <span class="chip bg-color">CTF-Web</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/posts/ctf-misc.html">
                    <div class="card-image">
                        
                        
                        <img src="/images/loading.gif" data-original="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/medias/featureimages/20.jpg" class="responsive-img" alt="CTF-Misc要点">
                        
                        <span class="card-title">CTF-Misc要点</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            CTF杂项学习要点
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2021-08-08
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/CTF/" class="post-category">
                                    CTF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/MISC/">
                        <span class="chip bg-color">MISC</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/libs/codeBlock/codeBlockFuction.js"></script>

<!-- 代码语言 -->

<script type="text/javascript" src="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/libs/codeBlock/codeShrink.js"></script>


<!-- 代码块折行 -->

<style type="text/css">
code[class*="language-"], pre[class*="language-"] { white-space: pre !important; }
</style>


    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('4'),
            headingSelector: 'h1, h2, h3, h4'
        });

        // modify the toc link href to support Chinese.
        let i = 0;
        let tocHeading = 'toc-heading-';
        $('#toc-content a').each(function () {
            $(this).attr('href', '#' + tocHeading + (++i));
        });

        // modify the heading title id to support Chinese.
        i = 0;
        $('#articleContent').children('h1, h2, h3, h4').each(function () {
            $(this).attr('id', tocHeading + (++i));
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>

    

</main>


<script src="https://cdn.bootcss.com/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script>
    MathJax.Hub.Config({
        tex2jax: {inlineMath: [['$', '$'], ['\(', '\)']]}
    });
</script>


    <footer class="page-footer bg-color">
    <div class="container row center-align" style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            <span id="year">2020</span>
            <a href="https://jackhcc.github.io" target="_blank">杰克成</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            <br>
            
            &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                class="white-color">3591.2k</span>&nbsp;字
            
            
            
            
            
            
            <span id="busuanzi_container_site_pv">
                |&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;<span id="busuanzi_value_site_pv"
                    class="white-color"></span>&nbsp;次
            </span>
            
            
            <span id="busuanzi_container_site_uv">
                |&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;<span id="busuanzi_value_site_uv"
                    class="white-color"></span>&nbsp;人
            </span>
            
            <br>
            
            <span id="sitetime">载入运行时间...</span>
            <script>
                function siteTime() {
                    var seconds = 1000;
                    var minutes = seconds * 60;
                    var hours = minutes * 60;
                    var days = hours * 24;
                    var years = days * 365;
                    var today = new Date();
                    var startYear = "2020";
                    var startMonth = "2";
                    var startDate = "27";
                    var startHour = "6";
                    var startMinute = "30";
                    var startSecond = "0";
                    var todayYear = today.getFullYear();
                    var todayMonth = today.getMonth() + 1;
                    var todayDate = today.getDate();
                    var todayHour = today.getHours();
                    var todayMinute = today.getMinutes();
                    var todaySecond = today.getSeconds();
                    var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                    var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                    var diff = t2 - t1;
                    var diffYears = Math.floor(diff / years);
                    var diffDays = Math.floor((diff / days) - diffYears * 365);
                    var diffHours = Math.floor((diff - (diffYears * 365 + diffDays) * days) / hours);
                    var diffMinutes = Math.floor((diff - (diffYears * 365 + diffDays) * days - diffHours * hours) /
                        minutes);
                    var diffSeconds = Math.floor((diff - (diffYears * 365 + diffDays) * days - diffHours * hours -
                        diffMinutes * minutes) / seconds);
                    if (startYear == todayYear) {
                        document.getElementById("year").innerHTML = todayYear;
                        document.getElementById("sitetime").innerHTML = "本站已安全运行 " + diffDays + " 天 " + diffHours +
                            " 小时 " + diffMinutes + " 分钟 " + diffSeconds + " 秒";
                    } else {
                        document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                        document.getElementById("sitetime").innerHTML = "本站已安全运行 " + diffYears + " 年 " + diffDays +
                            " 天 " + diffHours + " 小时 " + diffMinutes + " 分钟 " + diffSeconds + " 秒";
                    }
                }
                setInterval(siteTime, 1000);
            </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/JackHCC" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:jackcc0701@163.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>



    <a href="https://www.facebook.com/profile.php?id=100046343443643" class="tooltipped" target="_blank" data-tooltip="关注我的Facebook: https://www.facebook.com/profile.php?id=100046343443643" data-position="top" data-delay="50">
        <i class="fab fa-facebook-f"></i>
    </a>



    <a href="https://twitter.com/JackChe66021834" class="tooltipped" target="_blank" data-tooltip="关注我的Twitter: https://twitter.com/JackChe66021834" data-position="top" data-delay="50">
        <i class="fab fa-twitter"></i>
    </a>



    <a href="tencent://AddContact/?fromId=50&fromSubId=1&subcmd=all&uin=2508074836" class="tooltipped" target="_blank" data-tooltip="QQ联系我: 2508074836" data-position="top" data-delay="50">
        <i class="fab fa-qq"></i>
    </a>



    <a href="https://weibo.com/u/6885584679" class="tooltipped" target="_blank" data-tooltip="关注我的微博: https://weibo.com/u/6885584679" data-position="top" data-delay="50">
        <i class="fab fa-weibo"></i>
    </a>



    <a href="https://www.zhihu.com/people/8f8482f01f0d6a04e844efe32e0f0710" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/8f8482f01f0d6a04e844efe32e0f0710" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script src="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/js/search.js"></script>
<script type="text/javascript">
$(function () {
    searchFunc("https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/" + "search.xml", 'searchInput', 'searchResult');
});
</script>
    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/libs/materialize/materialize.min.js"></script>
    <script src="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/libs/aos/aos.js"></script>
    <script src="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/js/matery.js"></script>

    <canvas class="fireworks" style="position: fixed;left: 0;top: 0;z-index: 1; pointer-events: none;" ></canvas>
    <script type="text/javascript" src="//cdn.bootcss.com/animejs/2.2.0/anime.min.js"></script>
    <script type="text/javascript" src="/js/fireworks.js"></script>

    <script type="text/javascript">
        //只在桌面版网页启用特效
        var windowWidth = $(window).width();
        if (windowWidth > 768) {
            document.write('<script type="text/javascript" src="/js/sakura.js"><\/script>'); }
    </script>

    <!-- weather -->
	<script type="text/javascript">
	WIDGET = {FID: 'TToslpmkVO'}
	</script>
	<script type="text/javascript" src="https://apip.weatherdt.com/float/static/js/r.js?v=1111"></script>


    <!-- Global site tag (gtag.js) - Google Analytics -->


    <!-- Baidu Analytics -->

<script>
    var _hmt = _hmt || [];
    (function () {
        var hm = document.createElement("script");
        hm.src = "https://hm.baidu.com/hm.js?";
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(hm, s);
    })();
</script>

    <!-- Baidu Push -->

    
    
    <script async src="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/libs/others/busuanzi.pure.mini.js"></script>
    

    
        <script src="//code.tidio.co/kqhlkxviiccyoa0czpfpu4ijuey9hfre.js"></script>
        <script> 
            $(document).ready(function () {
                setInterval(change_Tidio, 50);  
                function change_Tidio() { 
                    var tidio=$("#tidio-chat iframe");
                    if(tidio.css("display")=="block"&& $(window).width()>977 ){
                        document.getElementById("tidio-chat-iframe").style.bottom= ($("div#backTop.top-scroll").css("display")=="none" &&$(window).width()>977)>0? "-40px" : ($("div.toc-title").length&&$(window).width()>977)>0?"85px":"20px";   
                        document.getElementById("tidio-chat-iframe").style.right="-15px";   
                        document.getElementById("tidio-chat-iframe").style.height=parseInt(tidio.css("height"))>=520?"520px":tidio.css("height");
                        document.getElementById("tidio-chat-iframe").style.zIndex="997";
                    } 
                    else if(tidio.css("display")=="block"&&$(window).width()>601 &&$(window).width()<992 ){
                        document.getElementById("tidio-chat-iframe").style.bottom= ($("div#backTop.top-scroll").css("display")=="none" && 601< $(window).width()<992)>0? "-40px":"20px" ;   
                        document.getElementById("tidio-chat-iframe").style.right="-15px"; 
                        document.getElementById("tidio-chat-iframe").style.zIndex="997";
                    }
                    else if(tidio.css("display")=="block"&&$(window).width()<601 && parseInt(tidio.css("height"))<230){
                        document.getElementById("tidio-chat-iframe").style.bottom= ($("div#backTop.top-scroll").css("display")=="none" && $(window).width()<601)>0? "-10px":"45px" ;   
                        document.getElementById("tidio-chat-iframe").style.zIndex="997";
                    }
                    if( tidio.css("display")=="block"&&$(window).width()<601 && parseInt(tidio.css("height"))>=230){
                        document.getElementById("tidio-chat-iframe").style.zIndex="998";
                    }
                } 
            }); 
        </script>
    

    

    
    <script type="text/javascript" color="0,0,255"
        pointColor="0,0,255" opacity='0.7'
        zIndex="-1" count="99"
        src="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/libs/background/canvas-nest.js"></script>
    

    

    
    <script type="text/javascript" src="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/libs/background/ribbon-dynamic.js" async="async"></script>
    
    
    
    <script src="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/libs/instantpage/instantpage.js" type="module"></script>
    

        <script src="//cdn.jsdelivr.net/npm/js-base64/base64.min.js"></script>
        <script>
        $('a').each(function() {
          const $this = $(this);
          const href = $this.attr('href');
          if (href && href.match('^((http|https|thunder|qqdl|ed2k|Flashget|qbrowser|ftp|rtsp|mms)://)')) {
            const strs = href.split('/');
            if (strs.length >= 3) {
                const host = strs[2];
                if (host !== 'your_domain' || window.location.host) {
                    $this.attr('href', '/go.html?u='+Base64.encode(href)+'').attr('rel', 'external nofollow noopener noreferrer');
                    if (true) {
                        $this.attr('target', '_blank');
                    }
                }
            }
          }
        });
        </script><script>!function(e){var c=Array.prototype.slice.call(document.querySelectorAll("img[data-original]"));function i(){for(var r=0;r<c.length;r++)t=c[r],0<=(n=t.getBoundingClientRect()).bottom&&0<=n.left&&n.top<=(e.innerHeight||document.documentElement.clientHeight)&&function(){var t,n,e,i,o=c[r];t=o,n=function(){c=c.filter(function(t){return o!==t})},e=new Image,i=t.getAttribute("data-original"),e.onload=function(){t.src=i,n&&n()},e.src=i}();var t,n}i(),e.addEventListener("scroll",function(){var t,n;t=i,n=e,clearTimeout(t.tId),t.tId=setTimeout(function(){t.call(n)},500)})}(this);</script><script>window.addEventListener("load",function(){var t=/\.(gif|jpg|jpeg|tiff|png)$/i,r=/^data:image\/[a-z]+;base64,/;Array.prototype.slice.call(document.querySelectorAll("img[data-original]")).forEach(function(a){var e=a.parentNode;"A"===e.tagName&&(e.href.match(t)||e.href.match(r))&&(e.href=a.dataset.original)})});</script></body>

</html>

