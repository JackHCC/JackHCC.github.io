<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="MindSporeè¯¦è§£, JackHCC">
    <meta name="description" content="MindSporeå­¦ä¹ è®°å½•">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <title>MindSporeè¯¦è§£ | JackHCC</title>
    <link rel="icon" type="image/png" href="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/medias/favicon.png">

    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/libs/awesome/css/all.css">
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/libs/materialize/materialize.min.css">
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/libs/aos/aos.css">
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/libs/animate/animate.min.css">
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/libs/lightGallery/css/lightgallery.min.css">
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/css/matery.css">
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/css/my.css">
    
    <script src="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/libs/jquery/jquery.min.js"></script>
    
<meta name="generator" content="Hexo 4.2.0"><link rel="alternate" href="/atom.xml" title="JackHCC" type="application/atom+xml">
<link rel="stylesheet" href="/css/prism-hopscotch.css" type="text/css">
<link rel="stylesheet" href="/css/prism-line-numbers.css" type="text/css"></head>


<body>
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper head-container">
            <div class="brand-logo">
                <a href="/" class="waves-effect waves-light">
                    
                    <img src="/images/loading.gif" data-original="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/medias/me.jpg" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">JackHCC</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="" class="waves-effect waves-light">

      
      <i class="fas fa-list" style="zoom: 0.6;"></i>
      
      <span>Tools</span>
      <i class="fas fa-chevron-down" aria-hidden="true" style="zoom: 0.6;"></i>
    </a>
    <ul class="sub-nav menus_item_child ">
      
      <li>
        <a href="https://creativecc.cn/" target="_blank" rel="noopener">
          
          <i class="fas fa-book" style="margin-top: -20px; zoom: 0.6;"></i>
          
          <span>Creativeå·¥å…·å¯¼èˆª</span>
        </a>
      </li>
      
      <li>
        <a href="https://blog.creativecc.cn/Arxiv-NLP-Reporter/" target="_blank" rel="noopener">
          
          <i class="fas fa-film" style="margin-top: -20px; zoom: 0.6;"></i>
          
          <span>NLPæ¯æ—¥è®ºæ–‡</span>
        </a>
      </li>
      
      <li>
        <a href="http://chat.creativecc.cn/" target="_blank" rel="noopener">
          
          <i class="fas fa-music" style="margin-top: -20px; zoom: 0.6;"></i>
          
          <span>RocketChatèŠå¤©å®¤</span>
        </a>
      </li>
      
      <li>
        <a href="/contact">
          
          <i class="fas fa-comments" style="margin-top: -20px; zoom: 0.6;"></i>
          
          <span>Contactç•™è¨€æ¿</span>
        </a>
      </li>
      
    </ul>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/friends" class="waves-effect waves-light">
      
      <i class="fas fa-address-book" style="zoom: 0.6;"></i>
      
      <span>å‹æƒ…é“¾æ¥</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/about" class="waves-effect waves-light">
      
      <i class="fas fa-user-circle" style="zoom: 0.6;"></i>
      
      <span>å…³äº</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>

<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/images/loading.gif" data-original="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/medias/me.jpg" class="logo-img circle responsive-img">
        
        <div class="logo-name">JackHCC</div>
        <div class="logo-desc">
            
            Make the world betterrrr!!!
            
        </div>
    </div>

    

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="javascript:;">
			
				<i class="fa-fw fas fa-list"></i>
			
			Tools
			<span class="m-icon"><i class="fas fa-chevron-right"></i></span>
		</a>
            <ul  style="background:  ;" >
              
                <li>   
				
                  <a href="https://creativecc.cn/ " target="_blank" rel="noopener" style="margin-left:75px";>
				  
				   <i class="fa fas fa-book" style="position: absolute;left:50px" ></i>
			      
		          <span>Creativeå·¥å…·å¯¼èˆª</span>
                  </a>
                </li>
              
                <li>   
				
                  <a href="https://blog.creativecc.cn/Arxiv-NLP-Reporter/ " target="_blank" rel="noopener" style="margin-left:75px";>
				  
				   <i class="fa fas fa-film" style="position: absolute;left:50px" ></i>
			      
		          <span>NLPæ¯æ—¥è®ºæ–‡</span>
                  </a>
                </li>
              
                <li>   
				
                  <a href="http://chat.creativecc.cn/ " target="_blank" rel="noopener" style="margin-left:75px";>
				  
				   <i class="fa fas fa-music" style="position: absolute;left:50px" ></i>
			      
		          <span>RocketChatèŠå¤©å®¤</span>
                  </a>
                </li>
              
                <li>   
				
                  <a href="/contact " style="margin-left:75px";>
				  
				   <i class="fa fas fa-comments" style="position: absolute;left:50px" ></i>
			      
		          <span>Contactç•™è¨€æ¿</span>
                  </a>
                </li>
               
            </ul>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/friends" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-address-book"></i>
			
			å‹æƒ…é“¾æ¥
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/about" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-user-circle"></i>
			
			å…³äº
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/JackHCC/JackHCC.github.io" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>

        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/JackHCC/JackHCC.github.io" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    
<script src="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/libs/cryptojs/crypto-js.min.js"></script>
<script>
    (function() {
        let pwd = '';
        if (pwd && pwd.length > 0) {
            if (pwd !== CryptoJS.SHA256(prompt('è¯·è¾“å…¥è®¿é—®æœ¬æ–‡ç« çš„å¯†ç ')).toString(CryptoJS.enc.Hex)) {
                alert('å¯†ç é”™è¯¯ï¼Œå°†è¿”å›ä¸»é¡µï¼');
                location.href = '/';
            }
        }
    })();
</script>




<div class="bg-cover pd-header post-cover" style="background-image: url('https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/medias/featureimages/0.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">MindSporeè¯¦è§£</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/libs/tocbot/tocbot.css">
<style>
    #articleContent h1::before,
    #articleContent h2::before,
    #articleContent h3::before,
    #articleContent h4::before,
    #articleContent h5::before,
    #articleContent h6::before {
        display: block;
        content: " ";
        height: 100px;
        margin-top: -100px;
        visibility: hidden;
    }

    #articleContent :focus {
        outline: none;
    }

    .toc-fixed {
        position: fixed;
        top: 64px;
    }

    .toc-widget {
        width: 345px;
        padding-left: 20px;
    }

    .toc-widget .toc-title {
        margin: 35px 0 15px 0;
        padding-left: 17px;
        font-size: 1.5rem;
        font-weight: bold;
        line-height: 1.5rem;
    }

    .toc-widget ol {
        padding: 0;
        list-style: none;
    }

    #toc-content {
        height: calc(100vh - 250px);
        overflow: auto;
    }

    #toc-content ol {
        padding-left: 10px;
    }

    #toc-content ol li {
        padding-left: 10px;
    }

    #toc-content .toc-link:hover {
        color: #42b983;
        font-weight: 700;
        text-decoration: underline;
    }

    #toc-content .toc-link::before {
        background-color: transparent;
        max-height: 25px;
    }

    #toc-content .is-active-link {
        color: #42b983;
    }

    #toc-content .is-active-link::before {
        background-color: #42b983;
    }

    #floating-toc-btn {
        position: fixed;
        right: 30px;
        bottom: 146px;
        padding-top: 15px;
        margin-bottom: 0;
        z-index: 998;
    }

    #floating-toc-btn .btn-floating {
        width: 48px;
        height: 48px;
    }

    #floating-toc-btn .btn-floating i {
        line-height: 48px;
        font-size: 1.4rem;
    }
</style>
<div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/tags/MindSpore/">
                                <span class="chip bg-color">MindSpore</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/categories/Deep-Learning/" class="post-category">
                                Deep Learning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2021-09-18
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2022-03-19
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    15.3k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    95 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>

        </div>
        <hr class="clearfix">
        <div class="card-content article-card-content">
            <div id="articleContent">
                <h1 id="ğŸMindSpore-API"><a href="#ğŸMindSpore-API" class="headerlink" title="ğŸMindSpore API"></a>ğŸMindSpore API</h1><p>APIå…¥å£ï¼š<a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/index.html" target="_blank" rel="noopener">MindSpore API</a></p>
<p><a href="https://gitee.com/mindspore/models" target="_blank" rel="noopener">MinfSporeå®ç°ç»å…¸æ¨¡å‹</a></p>
<h1 id="mindspore"><a href="#mindspore" class="headerlink" title="mindspore"></a>mindspore</h1><h5 id="classmindspore-dtype"><a href="#classmindspore-dtype" class="headerlink" title="classmindspore.dtype"></a><strong><em>class</em><code>mindspore.dtype</code></strong></h5><p>Create a data type object of MindSpore.</p>
<p>The actual path of <code>dtype</code> is <code>/mindspore/common/dtype.py</code>. Run the following command to import the package:</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">from</span> mindspore <span class="token keyword">import</span> dtype <span class="token keyword">as</span> mstype<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<p><strong>Numeric Type</strong></p>
<p>Currently, MindSpore supports <code>Int</code> type, <code>Uint</code> type and <code>Float</code> type. The following table lists the details.</p>
<table>
<thead>
<tr>
<th>Definition</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr>
<td><code>mindspore.int8</code> , <code>mindspore.byte</code></td>
<td>8-bit integer</td>
</tr>
<tr>
<td><code>mindspore.int16</code> , <code>mindspore.short</code></td>
<td>16-bit integer</td>
</tr>
<tr>
<td><code>mindspore.int32</code> , <code>mindspore.intc</code></td>
<td>32-bit integer</td>
</tr>
<tr>
<td><code>mindspore.int64</code> , <code>mindspore.intp</code></td>
<td>64-bit integer</td>
</tr>
<tr>
<td><code>mindspore.uint8</code> , <code>mindspore.ubyte</code></td>
<td>unsigned 8-bit integer</td>
</tr>
<tr>
<td><code>mindspore.uint16</code> , <code>mindspore.ushort</code></td>
<td>unsigned 16-bit integer</td>
</tr>
<tr>
<td><code>mindspore.uint32</code> , <code>mindspore.uintc</code></td>
<td>unsigned 32-bit integer</td>
</tr>
<tr>
<td><code>mindspore.uint64</code> , <code>mindspore.uintp</code></td>
<td>unsigned 64-bit integer</td>
</tr>
<tr>
<td><code>mindspore.float16</code> , <code>mindspore.half</code></td>
<td>16-bit floating-point number</td>
</tr>
<tr>
<td><code>mindspore.float32</code> , <code>mindspore.single</code></td>
<td>32-bit floating-point number</td>
</tr>
<tr>
<td><code>mindspore.float64</code> , <code>mindspore.double</code></td>
<td>64-bit floating-point number</td>
</tr>
</tbody></table>
<p><strong>Other Type</strong></p>
<p>For other defined types, see the following table.</p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr>
<td><code>tensor</code></td>
<td>MindSporeâ€™s <code>tensor</code> type. Data format uses NCHW. For details, see <a href="https://www.gitee.com/mindspore/mindspore/blob/r1.3/mindspore/common/tensor.py" target="_blank" rel="noopener">tensor</a>.</td>
</tr>
<tr>
<td><code>bool_</code></td>
<td>Boolean <code>True</code> or <code>False</code>.</td>
</tr>
<tr>
<td><code>int_</code></td>
<td>Integer scalar.</td>
</tr>
<tr>
<td><code>uint</code></td>
<td>Unsigned integer scalar.</td>
</tr>
<tr>
<td><code>float_</code></td>
<td>Floating-point scalar.</td>
</tr>
<tr>
<td><code>number</code></td>
<td>Number, including <code>int_</code> , <code>uint</code> , <code>float_</code> and <code>bool_</code> .</td>
</tr>
<tr>
<td><code>list_</code></td>
<td>List constructed by <code>tensor</code> , such as <code>List[T0,T1,...,Tn]</code> , where the element <code>Ti</code> can be of different types.</td>
</tr>
<tr>
<td><code>tuple_</code></td>
<td>Tuple constructed by <code>tensor</code> , such as <code>Tuple[T0,T1,...,Tn]</code> , where the element <code>Ti</code> can be of different types.</td>
</tr>
<tr>
<td><code>function</code></td>
<td>Function. Return in two ways, when function is not None, returns Func directly, the other returns Func(args: List[T0,T1,â€¦,Tn], retval: T) when function is None.</td>
</tr>
<tr>
<td><code>type_type</code></td>
<td>Type definition of type.</td>
</tr>
<tr>
<td><code>type_none</code></td>
<td>No matching return type, corresponding to the <code>type(None)</code> in Python.</td>
</tr>
<tr>
<td><code>symbolic_key</code></td>
<td>The value of a variable is used as a key of the variable in <code>env_type</code> .</td>
</tr>
<tr>
<td><code>env_type</code></td>
<td>Used to store the gradient of the free variable of a function, where the key is the <code>symbolic_key</code> of the free variableâ€™s node and the value is the gradient.</td>
</tr>
</tbody></table>
<ul>
<li><p><strong>Tree Topology</strong></p>
<p>The relationships of the above types are as follows:</p>
<pre><code>â””â”€â”€â”€â”€â”€â”€â”€ number
    â”‚   â”œâ”€â”€â”€ bool_
    â”‚   â”œâ”€â”€â”€ int_
    â”‚   â”‚   â”œâ”€â”€â”€ int8, byte
    â”‚   â”‚   â”œâ”€â”€â”€ int16, short
    â”‚   â”‚   â”œâ”€â”€â”€ int32, intc
    â”‚   â”‚   â””â”€â”€â”€ int64, intp
    â”‚   â”œâ”€â”€â”€ uint
    â”‚   â”‚   â”œâ”€â”€â”€ uint8, ubyte
    â”‚   â”‚   â”œâ”€â”€â”€ uint16, ushort
    â”‚   â”‚   â”œâ”€â”€â”€ uint32, uintc
    â”‚   â”‚   â””â”€â”€â”€ uint64, uintp
    â”‚   â””â”€â”€â”€ float_
    â”‚       â”œâ”€â”€â”€ float16
    â”‚       â”œâ”€â”€â”€ float32
    â”‚       â””â”€â”€â”€ float64
    â”œâ”€â”€â”€ tensor
    â”‚   â”œâ”€â”€â”€ Array[Float32]
    â”‚   â””â”€â”€â”€ ...
    â”œâ”€â”€â”€ list_
    â”‚   â”œâ”€â”€â”€ List[Int32,Float32]
    â”‚   â””â”€â”€â”€ ...
    â”œâ”€â”€â”€ tuple_
    â”‚   â”œâ”€â”€â”€ Tuple[Int32,Float32]
    â”‚   â””â”€â”€â”€ ...
    â”œâ”€â”€â”€ function
    â”‚   â”œâ”€â”€â”€ Func
    â”‚   â”œâ”€â”€â”€ Func[(Int32, Float32), Int32]
    â”‚   â””â”€â”€â”€ ...
    â”œâ”€â”€â”€ type_type
    â”œâ”€â”€â”€ type_none
    â”œâ”€â”€â”€ symbolic_key
    â””â”€â”€â”€ env_type</code></pre></li>
</ul>
<blockquote>
<p><code>mindspore.run_check</code>()<a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/_modules/mindspore/run_check/run_check.html#run_check" target="_blank" rel="noopener">[source]</a></p>
</blockquote>
<p>Provide a convenient API to check if the installation is successful or failed.</p>
<p><strong>Examples</strong></p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> mindspore
mindspore<span class="token punctuation">.</span>run_check<span class="token punctuation">(</span><span class="token punctuation">)</span>

Mindspore version<span class="token punctuation">:</span> xxx
The result of multiplication calculation <span class="token keyword">is</span> correct<span class="token punctuation">,</span> Mindspore has been installed successfully<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<blockquote>
<p><code>mindspore.dtype_to_nptype</code>(<em>type_</em>)<a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/_modules/mindspore/common/dtype.html#dtype_to_nptype" target="_blank" rel="noopener">[source]</a></p>
</blockquote>
<p>Convert MindSpore dtype to numpy data type.</p>
<ul>
<li><p>Parameters</p>
<p><strong>type_</strong> (<code>mindspore.dtype</code>) â€“ MindSporeâ€™s dtype.</p>
</li>
<li><p>Returns</p>
<p>The data type of numpy.</p>
</li>
</ul>
<blockquote>
<p><code>mindspore.issubclass_</code>(<em>type_</em>, <em>dtype</em>)<a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/_modules/mindspore/common/dtype.html#issubclass_" target="_blank" rel="noopener">[source]</a></p>
</blockquote>
<p>Determine whether type_ is a subclass of dtype.</p>
<ul>
<li><p>Parameters</p>
<p><strong>type_</strong> (<code>mindspore.dtype</code>) â€“ Target MindSpore dtype.</p>
<p><strong>dtype</strong> (<code>mindspore.dtype</code>) â€“ Compare MindSpore dtype.</p>
</li>
<li><p>Returns</p>
<p>bool, True or False.</p>
</li>
</ul>
<blockquote>
<p><code>mindspore.dtype_to_pytype</code>(<em>type_</em>)<a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/_modules/mindspore/common/dtype.html#dtype_to_pytype" target="_blank" rel="noopener">[source]</a></p>
</blockquote>
<p>Convert MindSpore dtype to python data type.</p>
<ul>
<li><p>Parameters</p>
<p><strong>type_</strong> (<code>mindspore.dtype</code>) â€“ MindSporeâ€™s dtype.</p>
</li>
<li><p>Returns</p>
<p>Type of python.</p>
</li>
</ul>
<blockquote>
<p><code>mindspore.pytype_to_dtype</code>(<em>obj</em>)<a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/_modules/mindspore/common/dtype.html#pytype_to_dtype" target="_blank" rel="noopener">[source]</a></p>
</blockquote>
<p>Convert python type to MindSpore type.</p>
<ul>
<li><p>Parameters</p>
<p><strong>obj</strong> (<a href="https://docs.python.org/library/functions.html#type" target="_blank" rel="noopener"><em>type</em></a>) â€“ A python type object.</p>
</li>
<li><p>Returns</p>
<p>Type of MindSpore type.</p>
</li>
</ul>
<blockquote>
<p><code>mindspore.get_py_obj_dtype</code>(<em>obj</em>)<a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/_modules/mindspore/common/dtype.html#get_py_obj_dtype" target="_blank" rel="noopener">[source]</a></p>
</blockquote>
<p>Get the MindSpore data type, which corresponds to python type or variable.</p>
<ul>
<li><p>Parameters</p>
<p><strong>obj</strong> (<a href="https://docs.python.org/library/functions.html#type" target="_blank" rel="noopener"><em>type</em></a>) â€“ An object of python type, or a variable of python type.</p>
</li>
<li><p>Returns</p>
<p>Type of MindSpore type.</p>
</li>
</ul>
<h5 id="class-mindspore-Tensor-input-data-None-dtype-None-shape-None-init-None-source"><a href="#class-mindspore-Tensor-input-data-None-dtype-None-shape-None-init-None-source" class="headerlink" title="class mindspore.Tensor(input_data=None, dtype=None, shape=None, init=None)[source]"></a><strong><em>class</em> <code>mindspore.Tensor</code>(<em>input_data=None</em>, <em>dtype=None</em>, <em>shape=None</em>, <em>init=None</em>)<a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/_modules/mindspore/common/tensor.html#Tensor" target="_blank" rel="noopener">[source]</a></strong></h5><p>Tensor is used for data storage.</p>
<p>Tensor inherits tensor object in C++. Some functions are implemented in C++ and some functions are implemented in Python.</p>
<ul>
<li><p>Parameters</p>
<p><strong>input_data</strong> (<em>Union**[</em><a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/api_python/mindspore/mindspore.Tensor.html#mindspore.Tensor" target="_blank" rel="noopener"><em>Tensor</em></a><em>,</em> <a href="https://docs.python.org/library/functions.html#float" target="_blank" rel="noopener"><em>float</em></a><em>,</em> <a href="https://docs.python.org/library/functions.html#int" target="_blank" rel="noopener"><em>int</em></a><em>,</em> <a href="https://docs.python.org/library/functions.html#bool" target="_blank" rel="noopener"><em>bool</em></a><em>,</em> <a href="https://docs.python.org/library/stdtypes.html#tuple" target="_blank" rel="noopener"><em>tuple</em></a><em>,</em> <a href="https://docs.python.org/library/stdtypes.html#list" target="_blank" rel="noopener"><em>list</em></a><em>,</em> <a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" target="_blank" rel="noopener"><em>numpy.ndarray</em></a><em>]</em>) â€“ Input data of the tensor.</p>
<p><strong>dtype</strong> (<a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/api_python/mindspore.html#mindspore.dtype" target="_blank" rel="noopener"><code>mindspore.dtype</code></a>) â€“ Input data should be None, bool or numeric type defined in mindspore.dtype. The argument is used to define the data type of the output tensor. If it is None, the data type of the output tensor will be the same as the input_data. Default: None.</p>
<p><strong>shape</strong> (<em>Union**[</em><a href="https://docs.python.org/library/stdtypes.html#tuple" target="_blank" rel="noopener"><em>tuple</em></a><em>,</em> <a href="https://docs.python.org/library/stdtypes.html#list" target="_blank" rel="noopener"><em>list</em></a><em>,</em> <a href="https://docs.python.org/library/functions.html#int" target="_blank" rel="noopener"><em>int</em></a><em>]</em>) â€“ A list of integers, a tuple of integers or an integer as the shape of output. If input_data is available, shape doesnâ€™t need to be set. Default: None.</p>
<p><strong>init</strong> (<a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/api_python/mindspore.common.initializer.html#mindspore.common.initializer.Initializer" target="_blank" rel="noopener"><em>Initializer</em></a>) â€“ the information of init data. â€˜initâ€™ is used for delayed initialization in parallel mode. Usually, it is not recommended to use â€˜initâ€™ interface to initialize parameters in other conditions. If â€˜initâ€™ interface is used to initialize parameters, the Tensor.init_data API needs to be called to convert Tensor to the actual data.</p>
</li>
<li><p>Outputs:</p>
<p>Tensor. If dtype and shape are not set, return a tensor with the same dtype and shape as input_data. If dtype or shape is set, the dtype or shape of the output Tensor is consistent with the setting.</p>
</li>
</ul>
<p><strong>Examples</strong></p>
<pre><code>&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; import mindspore as ms
&gt;&gt;&gt; from mindspore import Tensor
&gt;&gt;&gt; from mindspore.common.initializer import One
&gt;&gt;&gt; # initialize a tensor with input data
&gt;&gt;&gt; t1 = Tensor(np.zeros([1, 2, 3]), ms.float32)
&gt;&gt;&gt; assert isinstance(t1, Tensor)
&gt;&gt;&gt; assert t1.shape == (1, 2, 3)
&gt;&gt;&gt; assert t1.dtype == ms.float32
&gt;&gt;&gt;
&gt;&gt;&gt; # initialize a tensor with a float scalar
&gt;&gt;&gt; t2 = Tensor(0.1)
&gt;&gt;&gt; assert isinstance(t2, Tensor)
&gt;&gt;&gt; assert t2.dtype == ms.float64
...
&gt;&gt;&gt; # initialize a tensor with init
&gt;&gt;&gt; t3 = Tensor(shape = (1, 3), dtype=ms.float32, init=One())
&gt;&gt;&gt; assert isinstance(t3, Tensor)
&gt;&gt;&gt; assert t3.shape == (1, 3)
&gt;&gt;&gt; assert t3.dtype == ms.float32</code></pre><blockquote>
<p><em>property</em> <code>T</code></p>
</blockquote>
<p>Return the transposed tensor.</p>
<blockquote>
<p><code>abs</code>()<a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/_modules/mindspore/common/tensor.html#Tensor.abs" target="_blank" rel="noopener">[source]</a></p>
</blockquote>
<p>Return absolute value element-wisely.</p>
<ul>
<li><p>Returns</p>
<p>Tensor, with absolute value element-wisely.</p>
</li>
<li><p>Supported Platforms:</p>
<p><code>Ascend</code> <code>GPU</code> <code>CPU</code></p>
</li>
</ul>
<p><strong>Examples</strong></p>
<pre><code>&gt;&gt;&gt; from mindspore import Tensor
&gt;&gt;&gt; a = Tensor([1.1, -2.1]).astype("float32")
&gt;&gt;&gt; output = a.abs()
&gt;&gt;&gt; print(output)</code></pre><blockquote>
<p><code>all</code>(<em>axis=()</em>, <em>keep_dims=False</em>)<a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/_modules/mindspore/common/tensor.html#Tensor.all" target="_blank" rel="noopener">[source]</a></p>
</blockquote>
<p>Check all array elements along a given axis evaluate to True.</p>
<ul>
<li><p>Parameters</p>
<p><strong>axis</strong> (<em>Union**[</em><a href="https://docs.python.org/library/constants.html#None" target="_blank" rel="noopener"><em>None</em></a><em>,</em> <a href="https://docs.python.org/library/functions.html#int" target="_blank" rel="noopener"><em>int</em></a><em>,</em> <a href="https://docs.python.org/library/stdtypes.html#tuple" target="_blank" rel="noopener"><em>tuple</em></a><em>(</em><a href="https://docs.python.org/library/functions.html#int" target="_blank" rel="noopener"><em>int</em></a><em>)</em>) â€“ Dimensions of reduction, when the axis is None or empty tuple, reduce all dimensions. Default: ().</p>
<p><strong>keep_dims</strong> (<a href="https://docs.python.org/library/functions.html#bool" target="_blank" rel="noopener"><em>bool</em></a>) â€“ Whether to keep the reduced dimensions. Default: False.</p>
</li>
<li><p>Returns</p>
<p>Tensor, if all array elements along the given axis evaluate to True, its value is True, otherwise its value is False. If the axis is None or empty tuple, reduce all dimensions.</p>
</li>
<li><p>Supported Platforms:</p>
<p><code>Ascend</code> <code>GPU</code> <code>CPU</code></p>
</li>
</ul>
<p><strong>Examples</strong></p>
<pre><code>&gt;&gt;&gt; from mindspore import Tensor
&gt;&gt;&gt; a = Tensor([True, True, False])
&gt;&gt;&gt; output = a.all()
&gt;&gt;&gt; print(output)</code></pre><blockquote>
<p><code>any</code>(<em>axis=()</em>, <em>keep_dims=False</em>)<a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/_modules/mindspore/common/tensor.html#Tensor.any" target="_blank" rel="noopener">[source]</a></p>
</blockquote>
<p>Check any array element along a given axis evaluate to True.</p>
<ul>
<li><p>Parameters</p>
<p><strong>axis</strong> (<em>Union**[</em><a href="https://docs.python.org/library/constants.html#None" target="_blank" rel="noopener"><em>None</em></a><em>,</em> <a href="https://docs.python.org/library/functions.html#int" target="_blank" rel="noopener"><em>int</em></a><em>,</em> <a href="https://docs.python.org/library/stdtypes.html#tuple" target="_blank" rel="noopener"><em>tuple</em></a><em>(</em><a href="https://docs.python.org/library/functions.html#int" target="_blank" rel="noopener"><em>int</em></a><em>)</em>) â€“ Dimensions of reduction, when the axis is None or empty tuple, reduce all dimensions. Default: ().</p>
<p><strong>keep_dims</strong> (<a href="https://docs.python.org/library/functions.html#bool" target="_blank" rel="noopener"><em>bool</em></a>) â€“ Whether to keep the reduced dimensions. Default: False.</p>
</li>
<li><p>Returns</p>
<p>Tensor, if any array element along the given axis evaluates to True, its value is True, otherwise its value is False. If the axis is None or empty tuple, reduce all dimensions.</p>
</li>
<li><p>Supported Platforms:</p>
<p><code>Ascend</code> <code>GPU</code> <code>CPU</code></p>
</li>
</ul>
<p><strong>Examples</strong></p>
<pre><code>&gt;&gt;&gt; from mindspore import Tensor
&gt;&gt;&gt; a = Tensor([True, True, False])
&gt;&gt;&gt; output = a.any()
&gt;&gt;&gt; print(output)</code></pre><blockquote>
<p><code>argmax</code>(<em>axis=None</em>)<a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/_modules/mindspore/common/tensor.html#Tensor.argmax" target="_blank" rel="noopener">[source]</a></p>
</blockquote>
<p>Return the indices of the maximum values along an axis.</p>
<ul>
<li><p>Parameters</p>
<p><strong>axis</strong> (<a href="https://docs.python.org/library/functions.html#int" target="_blank" rel="noopener"><em>int</em></a><em>,</em> <em>optional</em>) â€“ By default, the index is into the flattened tensor, otherwise along the specified axis.</p>
</li>
<li><p>Returns</p>
<p>Tensor, indices into the input tensor. It has the same shape as self.shape with the dimension along axis removed.</p>
</li>
<li><p>Raises</p>
<p><a href="https://docs.python.org/library/exceptions.html#ValueError" target="_blank" rel="noopener"><strong>ValueError</strong></a> â€“ if the axis is out of range.</p>
</li>
<li><p>Supported Platforms:</p>
<p><code>Ascend</code> <code>GPU</code> <code>CPU</code></p>
</li>
</ul>
<p><strong>Examples</strong></p>
<pre><code>&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; from mindspore import Tensor
&gt;&gt;&gt; a = Tensor(np.arange(10, 16).reshape(2, 3).astype("float32"))
&gt;&gt;&gt; print(a.argmax())</code></pre><blockquote>
<p><code>argmin</code>(<em>axis=None</em>)<a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/_modules/mindspore/common/tensor.html#Tensor.argmin" target="_blank" rel="noopener">[source]</a></p>
</blockquote>
<p>Return the indices of the minimum values along an axis.</p>
<ul>
<li><p>Parameters</p>
<p><strong>axis</strong> (<a href="https://docs.python.org/library/functions.html#int" target="_blank" rel="noopener"><em>int</em></a><em>,</em> <em>optional</em>) â€“ By default, the index is into the flattened tensor, otherwise along the specified axis.</p>
</li>
<li><p>Returns</p>
<p>Tensor, indices into the input tensor. It has the same shape as self.shape with the dimension along axis removed.</p>
</li>
<li><p>Raises</p>
<p><a href="https://docs.python.org/library/exceptions.html#ValueError" target="_blank" rel="noopener"><strong>ValueError</strong></a> â€“ if the axis is out of range.</p>
</li>
<li><p>Supported Platforms:</p>
<p><code>Ascend</code> <code>GPU</code> <code>CPU</code></p>
</li>
</ul>
<p><strong>Examples</strong></p>
<pre><code>&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; from mindspore import Tensor
&gt;&gt;&gt; a = Tensor(np.arange(10, 16).reshape(2, 3).astype("float32"))
&gt;&gt;&gt; print(a.argmin())</code></pre><blockquote>
<p><code>asnumpy</code>()<a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/_modules/mindspore/common/tensor.html#Tensor.asnumpy" target="_blank" rel="noopener">[source]</a></p>
</blockquote>
<p>Convert tensor to numpy array.</p>
<blockquote>
<p><code>astype</code>(<em>dtype</em>, <em>copy=True</em>)<a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/_modules/mindspore/common/tensor.html#Tensor.astype" target="_blank" rel="noopener">[source]</a></p>
</blockquote>
<p>Return a copy of the tensor, cast to a specified type.</p>
<ul>
<li><p>Parameters</p>
<p><strong>dtype</strong> (Union[<code>mindspore.dtype</code>, str]) â€“ Designated tensor dtype, can be in format of <code>mindspore.dtype.float32</code> or float32. Default: <code>mindspore.dtype.float32</code>.</p>
<p><strong>copy</strong> (<a href="https://docs.python.org/library/functions.html#bool" target="_blank" rel="noopener"><em>bool</em></a><em>,</em> <em>optional</em>) â€“ By default, astype always returns a newly allocated tensor. If this is set to false, the input tensor is returned instead of a copy if possible. Default: True.</p>
</li>
<li><p>Returns</p>
<p>Tensor, with the designated dtype.</p>
</li>
<li><p>Raises</p>
<p><a href="https://docs.python.org/library/exceptions.html#TypeError" target="_blank" rel="noopener"><strong>TypeError</strong></a> â€“ If dtype has types not specified above, or values cannot be understood.</p>
</li>
<li><p>Supported Platforms:</p>
<p><code>Ascend</code> <code>GPU</code> <code>CPU</code></p>
</li>
</ul>
<p><strong>Examples</strong></p>
<pre><code>&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; from mindspore import Tensor
&gt;&gt;&gt; x = Tensor(np.ones((1,2,2,1), dtype=np.float32))
&gt;&gt;&gt; x = x.astype("int32")
&gt;&gt;&gt; print(x.dtype)</code></pre><blockquote>
<p><code>choose</code>(<em>choices</em>, <em>mode=â€clipâ€</em>)<a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/_modules/mindspore/common/tensor.html#Tensor.choose" target="_blank" rel="noopener">[source]</a></p>
</blockquote>
<p>Construct an array from an index array and a list of arrays to choose from.</p>
<ul>
<li><p>Parameters</p>
<p><strong>choices</strong> (<em>Union**[</em><a href="https://docs.python.org/library/stdtypes.html#tuple" target="_blank" rel="noopener"><em>tuple</em></a><em>,</em> <a href="https://docs.python.org/library/stdtypes.html#list" target="_blank" rel="noopener"><em>list</em></a><em>,</em> <a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/api_python/mindspore/mindspore.Tensor.html#mindspore.Tensor" target="_blank" rel="noopener"><em>Tensor</em></a><em>]</em>) â€“ Choice arrays. a and all of the choices must be broadcasted to the same shape. If choices is itself an array, then its outermost dimension (i.e., the one corresponding to <code>choices.shape[0]</code>) is taken as defining the â€œsequenceâ€.</p>
<p><strong>mode</strong> (<em>â€˜raiseâ€™**,</em> <em>â€˜wrapâ€™**,</em> <em>â€˜clipâ€™**,</em> <em>optional</em>) â€“Specifies how indices outside <code>[0, n-1]</code> will be treated:â€˜raiseâ€™ â€“ raise an error (default);â€˜wrapâ€™ â€“ wrap around;â€˜clipâ€™ â€“ clip to the range. â€˜clipâ€™ mode means that all indices that are too large are replaced by the index that addresses the last element along that axis. Note that this disables indexing with negative numbers.</p>
</li>
<li><p>Returns</p>
<p>Tensor, the merged result.</p>
</li>
<li><p>Supported Platforms:</p>
<p><code>Ascend</code> <code>GPU</code> <code>CPU</code></p>
</li>
<li><p>Raises</p>
<p><a href="https://docs.python.org/library/exceptions.html#ValueError" target="_blank" rel="noopener"><strong>ValueError</strong></a> â€“ if the input tensor and any of the choices cannot be broadcast.</p>
</li>
</ul>
<p><strong>Examples</strong></p>
<pre><code>&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; from mindspore import Tensor
&gt;&gt;&gt; choices = [[0, 1, 2, 3], [10, 11, 12, 13], [20, 21, 22, 23], [30, 31, 32, 33]]
&gt;&gt;&gt; x = Tensor(np.array([2, 3, 1, 0]))
&gt;&gt;&gt; print(x.choose(choices))
[20 31 12  3]</code></pre><blockquote>
<p><code>clip</code>(<em>xmin</em>, <em>xmax</em>, <em>dtype=None</em>)<a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/_modules/mindspore/common/tensor.html#Tensor.clip" target="_blank" rel="noopener">[source]</a></p>
</blockquote>
<p>Clips (limits) the values in a Tensor.</p>
<p>Given an interval, values outside the interval are clipped to the interval edges. For example, if an interval of 0,1 is specified, values smaller than 0 become 0, and values larger than 1 become 1.</p>
<p>Note</p>
<p>Currently, clip with xmin=nan or xmax=nan is not supported.</p>
<ul>
<li><p>Parameters</p>
<p><strong>xmin</strong> (<em>Tensor**,</em> <em>scalar**,</em> <a href="https://docs.python.org/library/constants.html#None" target="_blank" rel="noopener"><em>None</em></a>) â€“ Minimum value. If None, clipping is not performed on lower interval edge. Not more than one of xmin and xmax may be None.</p>
<p><strong>xmax</strong> (<em>Tensor**,</em> <em>scalar**,</em> <a href="https://docs.python.org/library/constants.html#None" target="_blank" rel="noopener"><em>None</em></a>) â€“ Maximum value. If None, clipping is not performed on upper interval edge. Not more than one of xmin and xmax may be None. If xmin or xmax are tensors, then the three tensors will be broadcasted to match their shapes.</p>
<p><strong>dtype</strong> (<code>mindspore.dtype</code>, optional) â€“ Overrides the dtype of the output Tensor. Default is None.</p>
</li>
<li><p>Returns</p>
<p>Tensor, a tensor with the elements of input tensor, but where values &lt; xmin are replaced with xmin, and those &gt; xmax with xmax.</p>
</li>
<li><p>Raises</p>
<p><a href="https://docs.python.org/library/exceptions.html#TypeError" target="_blank" rel="noopener"><strong>TypeError</strong></a> â€“ If inputs have types not specified above.</p>
<p><a href="https://docs.python.org/library/exceptions.html#ValueError" target="_blank" rel="noopener"><strong>ValueError</strong></a> â€“ If the shapes of x1 and x2 cannot broadcast, or both xmin and xmax are None.</p>
</li>
<li><p>Supported Platforms:</p>
<p><code>Ascend</code> <code>GPU</code> <code>CPU</code></p>
</li>
</ul>
<p><strong>Examples</strong></p>
<pre><code>&gt;&gt;&gt; from mindspore import Tensor
&gt;&gt;&gt; x = Tensor([1, 2, 3, -4, 0, 3, 2, 0]).astype("float32")
&gt;&gt;&gt; output = x.clip(0, 2)
&gt;&gt;&gt; print(output)
[1. 2. 2. 0. 0. 2. 2. 0.]</code></pre><blockquote>
<p><code>copy</code>()<a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/_modules/mindspore/common/tensor.html#Tensor.copy" target="_blank" rel="noopener">[source]</a></p>
</blockquote>
<p>Return a copy of the tensor.</p>
<blockquote>
<p>Note</p>
<p>The current implementation does not support order argument.</p>
</blockquote>
<ul>
<li><p>Returns</p>
<p>Copied tensor.</p>
</li>
<li><p>Supported Platforms:</p>
<p><code>Ascend</code> <code>GPU</code> <code>CPU</code></p>
</li>
</ul>
<p><strong>Examples</strong></p>
<pre><code>&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; from mindspore import Tensor
&gt;&gt;&gt; a = Tensor(np.ones((3,3)).astype("float32"))
&gt;&gt;&gt; output = a.copy()
&gt;&gt;&gt; print(output)</code></pre><blockquote>
<p><code>cumsum</code>(<em>axis=None</em>, <em>dtype=None</em>)<a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/_modules/mindspore/common/tensor.html#Tensor.cumsum" target="_blank" rel="noopener">[source]</a></p>
</blockquote>
<p>Return the cumulative sum of the elements along a given axis.</p>
<blockquote>
<p>Note</p>
<p>If <code>self.dtype</code> is <code>int8</code>, <code>int16</code> or <a href="https://docs.python.org/library/functions.html#bool" target="_blank" rel="noopener"><code>bool</code></a>, the result dtype will be elevated to <code>int32</code>, <code>int64</code> is not supported.</p>
</blockquote>
<ul>
<li><p>Parameters</p>
<p><strong>axis</strong> (<a href="https://docs.python.org/library/functions.html#int" target="_blank" rel="noopener"><em>int</em></a><em>,</em> <em>optional</em>) â€“ Axis along which the cumulative sum is computed. The default (None) is to compute the cumsum over the flattened array.</p>
<p><strong>dtype</strong> (<a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/api_python/mindspore.html#mindspore.dtype" target="_blank" rel="noopener"><code>mindspore.dtype</code></a>, optional) â€“ If not specified, stay the same as original, tensor, unless it has an integer dtype with a precision less than <code>float32</code>. In that case, <code>float32</code> is used. Default: None.</p>
</li>
<li><p>Raises</p>
<p><a href="https://docs.python.org/library/exceptions.html#ValueError" target="_blank" rel="noopener"><strong>ValueError</strong></a> â€“ if the axis is out of range.</p>
</li>
<li><p>Returns</p>
<p>Tensor.</p>
</li>
<li><p>Supported Platforms:</p>
<p><code>Ascend</code> <code>GPU</code> <code>CPU</code></p>
</li>
</ul>
<p>Examples</p>
<pre><code>&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; from mindspore import Tensor
&gt;&gt;&gt; a = Tensor(np.ones((3,3)).astype("float32"))
&gt;&gt;&gt; output = a.cumsum(axis=0)
&gt;&gt;&gt; print(output)</code></pre><blockquote>
<p><code>diagonal</code>(<em>offset=0</em>, <em>axis1=0</em>, <em>axis2=1</em>)<a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/_modules/mindspore/common/tensor.html#Tensor.diagonal" target="_blank" rel="noopener">[source]</a></p>
</blockquote>
<p>Return specified diagonals.</p>
<ul>
<li><p>Parameters</p>
<p><strong>offset</strong> (<a href="https://docs.python.org/library/functions.html#int" target="_blank" rel="noopener"><em>int</em></a><em>,</em> <em>optional</em>) â€“ Offset of the diagonal from the main diagonal. Can be positive or negative. Defaults to main diagonal.</p>
<p><strong>axis1</strong> (<a href="https://docs.python.org/library/functions.html#int" target="_blank" rel="noopener"><em>int</em></a><em>,</em> <em>optional</em>) â€“ Axis to be used as the first axis of the 2-D sub-arrays from which the diagonals should be taken. Defaults to first axis (0).</p>
<p><strong>axis2</strong> (<a href="https://docs.python.org/library/functions.html#int" target="_blank" rel="noopener"><em>int</em></a><em>,</em> <em>optional</em>) â€“ Axis to be used as the second axis of the 2-D sub-arrays from which the diagonals should be taken. Defaults to second axis.</p>
</li>
<li><p>Returns</p>
<p>Tensor, if a is 2-D, then a 1-D array containing the diagonal.</p>
</li>
<li><p>Raises</p>
<p><a href="https://docs.python.org/library/exceptions.html#ValueError" target="_blank" rel="noopener"><strong>ValueError</strong></a> â€“ if the input tensor has less than two dimensions.</p>
</li>
<li><p>Supported Platforms:</p>
<p><code>Ascend</code> <code>GPU</code> <code>CPU</code></p>
</li>
</ul>
<p><strong>Examples</strong></p>
<pre><code>&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; from mindspore import Tensor
&gt;&gt;&gt; a = Tensor(np.arange(4).reshape(2, 2))
&gt;&gt;&gt; print(a)
[[0 1]
[2 3]]
&gt;&gt;&gt; output = a.diagonal()
&gt;&gt;&gt; print(output)</code></pre><blockquote>
<p><em>property</em><code>dtype</code></p>
</blockquote>
<p>Return the dtype of the tensor (<code>mindspore.dtype</code>).</p>
<blockquote>
<p><code>expand_as</code>(<em>x</em>)<a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/_modules/mindspore/common/tensor.html#Tensor.expand_as" target="_blank" rel="noopener">[source]</a></p>
</blockquote>
<p>Expand the dimension of target tensor to the dimension of input tensor.</p>
<ul>
<li><p>Parameters</p>
<p><strong>x</strong> (<em>Tensor</em>) â€“ The input tensor. The shape of input tensor must obey the broadcasting rule.</p>
</li>
<li><p>Returns</p>
<p>Tensor, has the same dimension as input tensor.</p>
</li>
</ul>
<blockquote>
<p><code>fill</code>(<em>value</em>)<a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/_modules/mindspore/common/tensor.html#Tensor.fill" target="_blank" rel="noopener">[source]</a></p>
</blockquote>
<p>Fill the array with a scalar value.</p>
<blockquote>
<p>Note</p>
<p>Unlike Numpy, tensor.fill() will always returns a new tensor, instead of filling the original tensor.</p>
</blockquote>
<ul>
<li><p>Parameters</p>
<p><strong>value</strong> (<em>Union**[</em><a href="https://docs.python.org/library/constants.html#None" target="_blank" rel="noopener"><em>None</em></a><em>,</em> <a href="https://docs.python.org/library/functions.html#int" target="_blank" rel="noopener"><em>int</em></a><em>,</em> <a href="https://docs.python.org/library/functions.html#float" target="_blank" rel="noopener"><em>float</em></a><em>,</em> <a href="https://docs.python.org/library/functions.html#bool" target="_blank" rel="noopener"><em>bool</em></a><em>]</em>) â€“ All elements of a will be assigned this value.</p>
</li>
<li><p>Returns</p>
<p>Tensor, with the original dtype and shape as input tensor.</p>
</li>
<li><p>Raises</p>
<p><a href="https://docs.python.org/library/exceptions.html#TypeError" target="_blank" rel="noopener"><strong>TypeError</strong></a> â€“ If input arguments have types not specified above.</p>
</li>
<li><p>Supported Platforms:</p>
<p><code>Ascend</code> <code>GPU</code> <code>CPU</code></p>
</li>
</ul>
<p><strong>Examples</strong></p>
<pre><code>&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; from mindspore import Tensor
&gt;&gt;&gt; a = Tensor(np.arange(4).reshape((2,2)).astype('float32'))
&gt;&gt;&gt; print(a.fill(1.0))</code></pre><blockquote>
<p><code>flatten</code>(<em>order=â€Câ€</em>)<a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/_modules/mindspore/common/tensor.html#Tensor.flatten" target="_blank" rel="noopener">[source]</a></p>
</blockquote>
<p>Return a copy of the tensor collapsed into one dimension.</p>
<ul>
<li><p>Parameters</p>
<p><strong>order</strong> (<a href="https://docs.python.org/library/stdtypes.html#str" target="_blank" rel="noopener"><em>str</em></a><em>,</em> <em>optional</em>) â€“ Can choose between â€˜Câ€™ and â€˜Fâ€™. â€˜Câ€™ means to flatten in row-major (C-style) order. â€˜Fâ€™ means to flatten in column-major (Fortran-style) order. Only â€˜Câ€™ and â€˜Fâ€™ are supported. Default: â€˜Câ€™.</p>
</li>
<li><p>Returns</p>
<p>Tensor, has the same data type as input.</p>
</li>
<li><p>Supported Platforms:</p>
<p><code>Ascend</code> <code>GPU</code> <code>CPU</code></p>
</li>
<li><p>Raises</p>
<p><a href="https://docs.python.org/library/exceptions.html#TypeError" target="_blank" rel="noopener"><strong>TypeError</strong></a> â€“ If order is not string type.</p>
<p><a href="https://docs.python.org/library/exceptions.html#ValueError" target="_blank" rel="noopener"><strong>ValueError</strong></a> â€“ If order is string type, but not â€˜Câ€™ or â€˜Fâ€™.</p>
</li>
</ul>
<p><strong>Examples</strong></p>
<pre><code>&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; from mindspore import Tensor
&gt;&gt;&gt; x = Tensor(np.ones((2,3,4), dtype=np.float32))
&gt;&gt;&gt; output = x.flatten()
&gt;&gt;&gt; print(output.shape)</code></pre><blockquote>
<p><code>flush_from_cache</code>()<a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/_modules/mindspore/common/tensor.html#Tensor.flush_from_cache" target="_blank" rel="noopener">[source]</a></p>
</blockquote>
<p>Flush cache data to host if tensor is cache enable.</p>
<blockquote>
<p><em>static<em><code>from_numpy</code>(</em>array</em>)<a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/_modules/mindspore/common/tensor.html#Tensor.from_numpy" target="_blank" rel="noopener">[source]</a></p>
</blockquote>
<p>Convert numpy array to Tensor without copy data.</p>
<ul>
<li><p>Parameters</p>
<p><strong>array</strong> (<em>numpy.array</em>) â€“ The input array.</p>
</li>
<li><p>Returns</p>
<p>Tensor, has the same data type as input array.</p>
</li>
</ul>
<blockquote>
<p><em>property</em><code>has_init</code></p>
</blockquote>
<p>tensor is inited.</p>
<blockquote>
<p><code>flush_from_cache</code>()<a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/_modules/mindspore/common/tensor.html#Tensor.flush_from_cache" target="_blank" rel="noopener">[source]</a></p>
</blockquote>
<p>Flush cache data to host if tensor is cache enable.</p>
<blockquote>
<p><em>static<em><code>from_numpy</code>(</em>array</em>)<a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/_modules/mindspore/common/tensor.html#Tensor.from_numpy" target="_blank" rel="noopener">[source]</a></p>
</blockquote>
<p>Convert numpy array to Tensor without copy data.</p>
<ul>
<li><p>Parameters</p>
<p><strong>array</strong> (<em>numpy.array</em>) â€“ The input array.</p>
</li>
<li><p>Returns</p>
<p>Tensor, has the same data type as input array.</p>
</li>
</ul>
<blockquote>
<p><em>property</em><code>has_init</code></p>
</blockquote>
<p>tensor is inited.</p>
<blockquote>
<p><code>init_data</code>(<em>slice_index=None</em>, <em>shape=None</em>, <em>opt_shard_group=None</em>)<a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/_modules/mindspore/common/tensor.html#Tensor.init_data" target="_blank" rel="noopener">[source]</a></p>
</blockquote>
<p>Get the tensor format data of this Tensor. The init_data function can be called once for the same tensor.</p>
<ul>
<li><p>Parameters</p>
<p><strong>slice_index</strong> (<a href="https://docs.python.org/library/functions.html#int" target="_blank" rel="noopener"><em>int</em></a>) â€“ Slice index of a parameterâ€™s slices. It is used when initialize a slice of a parameter, it guarantees that devices using the same slice can generate the same tensor. Default: None.</p>
<p><strong>shape</strong> (<a href="https://docs.python.org/library/stdtypes.html#list" target="_blank" rel="noopener"><em>list</em></a><em>[*<a href="https://docs.python.org/library/functions.html#int" target="_blank" rel="noopener"><em>int</em></a>*]</em>) â€“ Shape of the slice, it is used when initialize a slice of the parameter. Default: None.</p>
<p><strong>opt_shard_group</strong> (<a href="https://docs.python.org/library/stdtypes.html#str" target="_blank" rel="noopener"><em>str</em></a>) â€“ Optimizer shard group which is used in auto or semi auto parallel mode to get one shard of a parameterâ€™s slice. Default: None.</p>
</li>
<li><p>Returns</p>
<p>Initialized Tensor.</p>
</li>
<li><p>Supported Platforms:</p>
<p><code>Ascend</code> <code>GPU</code> <code>CPU</code></p>
</li>
</ul>
<p><strong>Examples</strong></p>
<pre><code>&gt;&gt;&gt; import mindspore as ms
&gt;&gt;&gt; import mindspore.common.initializer as init
&gt;&gt;&gt; x = init.initializer(init.Constant(1), [2, 2], ms.float32)
&gt;&gt;&gt; out = x.init_data()
&gt;&gt;&gt; print(out)</code></pre><blockquote>
<p><code>item</code>(<em>index=None</em>)<a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/_modules/mindspore/common/tensor.html#Tensor.item" target="_blank" rel="noopener">[source]</a></p>
</blockquote>
<p>Getitem from the Tensor with the index.</p>
<p>Note</p>
<p>Tensor.item returns a Tensor scalar instead of a Python scalar.</p>
<ul>
<li><p>Parameters</p>
<p><strong>index</strong> (<em>Union**[</em><a href="https://docs.python.org/library/constants.html#None" target="_blank" rel="noopener"><em>None</em></a><em>,</em> <a href="https://docs.python.org/library/functions.html#int" target="_blank" rel="noopener"><em>int</em></a><em>,</em> <a href="https://docs.python.org/library/stdtypes.html#tuple" target="_blank" rel="noopener"><em>tuple</em></a><em>(</em><a href="https://docs.python.org/library/functions.html#int" target="_blank" rel="noopener"><em>int</em></a><em>)**]</em>) â€“ The index in Tensor. Default: None.</p>
</li>
<li><p>Returns</p>
<p>A Tensor scalar, dtype is the same with the original Tensor.</p>
</li>
<li><p>Raises</p>
<p><a href="https://docs.python.org/library/exceptions.html#ValueError" target="_blank" rel="noopener"><strong>ValueError</strong></a> â€“ If the length of the index is not euqal to self.ndim.</p>
</li>
<li><p>Supported Platforms:</p>
<p><code>Ascend</code> <code>GPU</code></p>
</li>
</ul>
<p><strong>Examples</strong></p>
<pre><code>&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; from mindspore import Tensor
&gt;&gt;&gt; x = Tensor(np.array([[1,2,3],[4,5,6]], dtype=np.float32))
&gt;&gt;&gt; x = x.item((0,1))
&gt;&gt;&gt; print(x)</code></pre><blockquote>
<p><code>itemset</code>(<em>*args</em>)<a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/_modules/mindspore/common/tensor.html#Tensor.itemset" target="_blank" rel="noopener">[source]</a></p>
</blockquote>
<p>Insert scalar into a tensor (scalar is cast to tensorâ€™s dtype, if possible).</p>
<p>There must be at least 1 argument, and define the last argument as item. Then, tensor.itemset(*args) is equivalent to tensor[args]=item.</p>
<ul>
<li><p>Parameters</p>
<p><strong>args</strong> (<em>Union<strong>[</strong>(</em><a href="https://docs.python.org/library/numbers.html#numbers.Number" target="_blank" rel="noopener"><em>numbers.Number</em></a><em>)**,</em> <em>(<strong>int/tuple</strong>(</em><a href="https://docs.python.org/library/functions.html#int" target="_blank" rel="noopener"><em>int</em></a><em>)**,</em> <a href="https://docs.python.org/library/numbers.html#numbers.Number" target="_blank" rel="noopener"><em>numbers.Number</em></a><em>)**]</em>) â€“ The arguments that specify the index and value. If args contain one argument (a scalar), it is only used in case tensor is of size 1. If args contain two arguments, the last argument is the value to be set and must be a scalar, the first argument specifies a single tensor element location. It is either an int or a tuple.</p>
</li>
<li><p>Returns</p>
<p>A new Tensor, with value set by tensor[args]=item.</p>
</li>
<li><p>Raises</p>
<p><a href="https://docs.python.org/library/exceptions.html#ValueError" target="_blank" rel="noopener"><strong>ValueError</strong></a> â€“ If the length of the first argument is not euqal to self.ndim.</p>
<p><a href="https://docs.python.org/library/exceptions.html#IndexError" target="_blank" rel="noopener"><strong>IndexError</strong></a> â€“ If only one argument is provided, and the original Tensor is not scalar.</p>
</li>
<li><p>Supported Platforms:</p>
<p><code>Ascend</code> <code>GPU</code></p>
</li>
</ul>
<p>Examples</p>
<pre><code>&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; from mindspore import Tensor
&gt;&gt;&gt; x = Tensor(np.array([[1,2,3],[4,5,6]], dtype=np.float32))
&gt;&gt;&gt; x = x.itemset((0,1), 4)
&gt;&gt;&gt; print(x)</code></pre><blockquote>
<p><em>property</em><code>itemsize</code></p>
</blockquote>
<p>Return the length of one tensor element in bytes.</p>
<blockquote>
<p><code>max</code>(<em>axis=None</em>, <em>keepdims=False</em>, <em>initial=None</em>, <em>where=True</em>)<a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/_modules/mindspore/common/tensor.html#Tensor.max" target="_blank" rel="noopener">[source]</a></p>
</blockquote>
<p>Return the maximum of a tensor or maximum along an axis.</p>
<ul>
<li><p>Parameters</p>
<p><strong>axis</strong> (<em>Union**[</em><a href="https://docs.python.org/library/constants.html#None" target="_blank" rel="noopener"><em>None</em></a><em>,</em> <a href="https://docs.python.org/library/functions.html#int" target="_blank" rel="noopener"><em>int</em></a><em>,</em> <em>tuple of ints<strong>]</strong>,</em> <em>optional</em>) â€“ Axis or axes along which to operate. By default, flattened input is used. If this is a tuple of ints, the maximum is selected over multiple axes, instead of a single axis or all the axes as before. Default: None.</p>
<p><strong>keepdims</strong> (<a href="https://docs.python.org/library/functions.html#bool" target="_blank" rel="noopener"><em>bool</em></a><em>,</em> <em>optional</em>) â€“ If this is set to True, the axes which are reduced are left in the result as dimensions with size one. With this option, the result will broadcast correctly against the input array. Default: False.</p>
<p><strong>initial</strong> (<em>scalar**,</em> <em>optional</em>) â€“ The minimum value of an output element. Must be present to allow computation on empty slice. Default: None.</p>
<p><strong>where</strong> (<em>bool Tensor**,</em> <em>optional</em>) â€“ A boolean array which is broadcasted to match the dimensions of array, and selects elements to include in the reduction. If non-default value is passed, initial must also be provided. Default: True.</p>
</li>
<li><p>Returns</p>
<p>Tensor or scalar, maximum of input tensor. If axis is None, the result is a scalar value. If axis is given, the result is an array of dimension <code>self.ndim - 1</code>.</p>
</li>
<li><p>Raises</p>
<p><a href="https://docs.python.org/library/exceptions.html#TypeError" target="_blank" rel="noopener"><strong>TypeError</strong></a> â€“ if arguments have types not specified above.</p>
</li>
<li><p>Supported Platforms:</p>
<p><code>Ascend</code> <code>GPU</code> <code>CPU</code></p>
</li>
</ul>
<p><strong>Examples</strong></p>
<pre><code>&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; from mindspore import Tensor
&gt;&gt;&gt; a = Tensor(np.arange(4).reshape((2, 2)).astype('float32'))
&gt;&gt;&gt; output = a.max()
&gt;&gt;&gt; print(output)</code></pre><blockquote>
<p><code>mean</code>(<em>axis=()</em>, <em>keep_dims=False</em>)<a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/_modules/mindspore/common/tensor.html#Tensor.mean" target="_blank" rel="noopener">[source]</a></p>
</blockquote>
<p>Reduce a dimension of a tensor by averaging all elements in the dimension.</p>
<ul>
<li><p>Parameters</p>
<p><strong>axis</strong> (<em>Union**[</em><a href="https://docs.python.org/library/constants.html#None" target="_blank" rel="noopener"><em>None</em></a><em>,</em> <a href="https://docs.python.org/library/functions.html#int" target="_blank" rel="noopener"><em>int</em></a><em>,</em> <a href="https://docs.python.org/library/stdtypes.html#tuple" target="_blank" rel="noopener"><em>tuple</em></a><em>(</em><a href="https://docs.python.org/library/functions.html#int" target="_blank" rel="noopener"><em>int</em></a><em>)**,</em> <a href="https://docs.python.org/library/stdtypes.html#list" target="_blank" rel="noopener"><em>list</em></a><em>(</em><a href="https://docs.python.org/library/functions.html#int" target="_blank" rel="noopener"><em>int</em></a><em>)**]</em>) â€“ Dimensions of reduction, when the axis is None or empty tuple, reduce all dimensions. Default: ().</p>
<p><strong>keep_dims</strong> (<a href="https://docs.python.org/library/functions.html#bool" target="_blank" rel="noopener"><em>bool</em></a>) â€“ Whether to keep the reduced dimensions. Default: False.</p>
</li>
<li><p>Returns</p>
<p>Tensor, has the same data type as input tensor.</p>
</li>
<li><p>Supported Platforms:</p>
<p><code>Ascend</code> <code>GPU</code> <code>CPU</code></p>
</li>
</ul>
<p><strong>Examples</strong></p>
<pre><code>&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; from mindspore import Tensor
&gt;&gt;&gt; input_x = Tensor(np.array([1, 2, 3], dtype=np.float32))
&gt;&gt;&gt; output = input_x.mean()
&gt;&gt;&gt; print(output)</code></pre><blockquote>
<p><code>min</code>(<em>axis=None</em>, <em>keepdims=False</em>, <em>initial=None</em>, <em>where=True</em>)<a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/_modules/mindspore/common/tensor.html#Tensor.min" target="_blank" rel="noopener">[source]</a></p>
</blockquote>
<p>Return the minimum of a tensor or minimum along an axis.</p>
<ul>
<li><p>Parameters</p>
<p><strong>axis</strong> (<em>Union**[</em><a href="https://docs.python.org/library/constants.html#None" target="_blank" rel="noopener"><em>None</em></a><em>,</em> <a href="https://docs.python.org/library/functions.html#int" target="_blank" rel="noopener"><em>int</em></a><em>,</em> <em>tuple of ints<strong>]</strong>,</em> <em>optional</em>) â€“ Axis or axes along which to operate. By default, flattened input is used. If this is a tuple of ints, the minimum is selected over multiple axes, instead of a single axis or all the axes as before. Default: None.</p>
<p><strong>keepdims</strong> (<a href="https://docs.python.org/library/functions.html#bool" target="_blank" rel="noopener"><em>bool</em></a><em>,</em> <em>optional</em>) â€“ If this is set to True, the axes which are reduced are left in the result as dimensions with size one. With this option, the result will broadcast correctly against the input array. Default: False.</p>
<p><strong>initial</strong> (<em>scalar**,</em> <em>optional</em>) â€“ The maximum value of an output element. Must be present to allow computation on empty slice. Default: None.</p>
<p><strong>where</strong> (<em>bool Tensor**,</em> <em>optional</em>) â€“ A boolean array which is broadcasted to match the dimensions of array, and selects elements to include in the reduction. If non-default value is passed, initial must also be provided. Default: True.</p>
</li>
<li><p>Returns</p>
<p>Tensor or scalar, minimum of input tensor. If the axis is None, the result is a scalar value. If axis is given, the result is an array of dimension <code>self.ndim - 1</code>.</p>
</li>
<li><p>Raises</p>
<p><a href="https://docs.python.org/library/exceptions.html#TypeError" target="_blank" rel="noopener"><strong>TypeError</strong></a> â€“ if arguments have types not specified above.</p>
</li>
<li><p>Supported Platforms:</p>
<p><code>Ascend</code> <code>GPU</code> <code>CPU</code></p>
</li>
</ul>
<p>Examples</p>
<pre><code>&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; from mindspore import Tensor
&gt;&gt;&gt; import mindspore.numpy as np
&gt;&gt;&gt; a = Tensor(np.arange(4).reshape((2,2)).astype('float32'))
&gt;&gt;&gt; output = a.min()
&gt;&gt;&gt; print(output)</code></pre><blockquote>
<p><em>property</em><code>nbytes</code></p>
</blockquote>
<p>Return the total number of bytes taken by the tensor.</p>
<blockquote>
<p><em>property</em><code>ndim</code></p>
</blockquote>
<p>Return the number of tensor dimensions.</p>
<blockquote>
<p><code>ptp</code>(<em>axis=None</em>, <em>keepdims=False</em>)<a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/_modules/mindspore/common/tensor.html#Tensor.ptp" target="_blank" rel="noopener">[source]</a></p>
</blockquote>
<p>The name of the function comes from the acronym for â€˜peak to peakâ€™.</p>
<blockquote>
<p>Note</p>
<p>Numpy arguments dtype and out are not supported.</p>
</blockquote>
<ul>
<li><p>Parameters</p>
<p><strong>axis</strong> (<em>Union**[</em><a href="https://docs.python.org/library/constants.html#None" target="_blank" rel="noopener"><em>None</em></a><em>,</em> <a href="https://docs.python.org/library/functions.html#int" target="_blank" rel="noopener"><em>int</em></a><em>,</em> <a href="https://docs.python.org/library/stdtypes.html#tuple" target="_blank" rel="noopener"><em>tuple</em></a><em>(</em><a href="https://docs.python.org/library/functions.html#int" target="_blank" rel="noopener"><em>int</em></a><em>)**]</em>) â€“ Axis or axes along which the range is computed. The default is to compute the variance of the flattened array. Default: None.</p>
<p><strong>keepdims</strong> (<a href="https://docs.python.org/library/functions.html#bool" target="_blank" rel="noopener"><em>bool</em></a>) â€“ If this is set to True, the axes which are reduced are left in the result as dimensions with size one. With this option, the result will broadcast correctly against the array. Default is False.</p>
</li>
<li><p>Returns</p>
<p>Tensor.</p>
</li>
<li><p>Raises</p>
<p><a href="https://docs.python.org/library/exceptions.html#TypeError" target="_blank" rel="noopener"><strong>TypeError</strong></a> â€“ if self is not a tensor, or axis and keepdims have types not specified above.</p>
</li>
<li><p>Supported Platforms:</p>
<p><code>Ascend</code> <code>GPU</code> <code>CPU</code></p>
</li>
</ul>
<p>Examples</p>
<pre><code>&gt;&gt;&gt; from mindspore import Tensor
&gt;&gt;&gt; x = Tensor([[4.0, 9.0, 2.0, 10.0], [6.0, 9.0, 7.0, 12.0]]).astype("float32")
&gt;&gt;&gt; print(x.ptp(axis=1))
[8. 6.]
&gt;&gt;&gt; print(x.ptp(axis=0))</code></pre><blockquote>
<p><code>ravel</code>()<a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/_modules/mindspore/common/tensor.html#Tensor.ravel" target="_blank" rel="noopener">[source]</a></p>
</blockquote>
<p>Return a contiguous flattened tensor.</p>
<ul>
<li><p>Returns</p>
<p>Tensor, a 1-D tensor, containing the same elements of the input.</p>
</li>
<li><p>Supported Platforms:</p>
<p><code>Ascend</code> <code>GPU</code> <code>CPU</code></p>
</li>
</ul>
<p><strong>Examples</strong></p>
<pre><code>&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; from mindspore import Tensor
&gt;&gt;&gt; x = Tensor(np.ones((2,3,4), dtype=np.float32))
&gt;&gt;&gt; output = x.ravel()
&gt;&gt;&gt; print(output.shape)</code></pre><blockquote>
<p><code>repeat</code>(<em>repeats</em>, <em>axis=None</em>)<a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/_modules/mindspore/common/tensor.html#Tensor.repeat" target="_blank" rel="noopener">[source]</a></p>
</blockquote>
<p>Repeat elements of an array.</p>
<ul>
<li><p>Parameters</p>
<p><strong>repeats</strong> (<em>Union**[</em><a href="https://docs.python.org/library/functions.html#int" target="_blank" rel="noopener"><em>int</em></a><em>,</em> <a href="https://docs.python.org/library/stdtypes.html#tuple" target="_blank" rel="noopener"><em>tuple</em></a><em>,</em> <a href="https://docs.python.org/library/stdtypes.html#list" target="_blank" rel="noopener"><em>list</em></a><em>]</em>) â€“ The number of repetitions for each element. repeats is broadcasted to fit the shape of the given axis.</p>
<p><strong>axis</strong> (<a href="https://docs.python.org/library/functions.html#int" target="_blank" rel="noopener"><em>int</em></a><em>,</em> <em>optional</em>) â€“ The axis along which to repeat values. By default, use the flattened input tensor, and return a flat output tensor.</p>
</li>
<li><p>Returns</p>
<p>Tensor, has the same shape as input tensor except along the given axis.</p>
</li>
<li><p>Raises</p>
<p><a href="https://docs.python.org/library/exceptions.html#ValueError" target="_blank" rel="noopener"><strong>ValueError</strong></a> â€“ if the axis is out of range.</p>
<p><a href="https://docs.python.org/library/exceptions.html#TypeError" target="_blank" rel="noopener"><strong>TypeError</strong></a> â€“ if arguments have types not specified above.</p>
</li>
<li><p>Supported Platforms:</p>
<p><code>Ascend</code> <code>GPU</code> <code>CPU</code></p>
</li>
</ul>
<p><strong>Examples</strong></p>
<pre><code>&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; from mindspore import Tensor
&gt;&gt;&gt; x = Tensor(np.array(3))
&gt;&gt;&gt; print(x.repeat(4))
[3 3 3 3]
&gt;&gt;&gt; x = Tensor(np.array([[1, 2],[3, 4]]))
&gt;&gt;&gt; print(x.repeat(2))
[1 1 2 2 3 3 4 4]
&gt;&gt;&gt; print(x.repeat(3, axis=1))
[[1 1 1 2 2 2]
[3 3 3 4 4 4]]
&gt;&gt;&gt; print(x.repeat([1,2], axis=0))</code></pre><blockquote>
<p><code>reshape</code>(<em>*shape</em>)<a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/_modules/mindspore/common/tensor.html#Tensor.reshape" target="_blank" rel="noopener">[source]</a></p>
</blockquote>
<p>Give a new shape to a tensor without changing its data.</p>
<ul>
<li><p>Parameters</p>
<p><strong>shape</strong> (<em>Union**[</em><a href="https://docs.python.org/library/functions.html#int" target="_blank" rel="noopener"><em>int</em></a><em>,</em> <a href="https://docs.python.org/library/stdtypes.html#tuple" target="_blank" rel="noopener"><em>tuple</em></a><em>(</em><a href="https://docs.python.org/library/functions.html#int" target="_blank" rel="noopener"><em>int</em></a><em>)**,</em> <a href="https://docs.python.org/library/stdtypes.html#list" target="_blank" rel="noopener"><em>list</em></a><em>(</em><a href="https://docs.python.org/library/functions.html#int" target="_blank" rel="noopener"><em>int</em></a><em>)**]</em>) â€“ The new shape should be compatible with the original shape. If an integer, then the result will be a 1-D array of that length. One shape dimension can be -1. In this case, the value is inferred from the length of the array and remaining dimensions.</p>
</li>
<li><p>Returns</p>
<p>Tensor, with new specified shape.</p>
</li>
<li><p>Raises</p>
<p><a href="https://docs.python.org/library/exceptions.html#TypeError" target="_blank" rel="noopener"><strong>TypeError</strong></a> â€“ If new_shape is not integer, list or tuple, or x is not tensor.</p>
<p><a href="https://docs.python.org/library/exceptions.html#ValueError" target="_blank" rel="noopener"><strong>ValueError</strong></a> â€“ If new_shape is not compatible with the original shape.</p>
</li>
<li><p>Supported Platforms:</p>
<p><code>Ascend</code> <code>GPU</code> <code>CPU</code></p>
</li>
</ul>
<p><strong>Examples</strong></p>
<pre><code>&gt;&gt;&gt; from mindspore import Tensor
&gt;&gt;&gt; from mindspore import dtype as mstype
&gt;&gt;&gt; x = Tensor([[-0.1, 0.3, 3.6], [0.4, 0.5, -3.2]], dtype=mstype.float32)
&gt;&gt;&gt; output = x.reshape((3, 2))
&gt;&gt;&gt; print(output)</code></pre><blockquote>
<p><code>resize</code>(<em>*new_shape</em>)<a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/_modules/mindspore/common/tensor.html#Tensor.resize" target="_blank" rel="noopener">[source]</a></p>
</blockquote>
<p>Changes shape and size of array in-place.</p>
<p>Note</p>
<p>Instead of changing the size of the input array and returns nothing as in numpy, this method returns a new Tensor with the input size. Numpy argument refcheck is not supported.</p>
<ul>
<li><p>Parameters</p>
<p><strong>new_shape</strong> (<em>Union<strong>[</strong>ints**,</em> <em>tuple of ints**]</em>) â€“ Shape of resized array.</p>
</li>
<li><p>Returns</p>
<p>Tensor.</p>
</li>
<li><p>Supported Platforms:</p>
<p><code>Ascend</code> <code>GPU</code> <code>CPU</code></p>
</li>
</ul>
<p><strong>Examples</strong></p>
<pre><code>&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; from mindspore import Tensor
&gt;&gt;&gt; x = Tensor(np.array([[0, 1], [2, 3]]))
&gt;&gt;&gt; x = x.resize(2, 3)
&gt;&gt;&gt; print(x)</code></pre><blockquote>
<p><code>searchsorted</code>(<em>v</em>, <em>side=â€leftâ€</em>, <em>sorter=None</em>)<a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/_modules/mindspore/common/tensor.html#Tensor.searchsorted" target="_blank" rel="noopener">[source]</a></p>
</blockquote>
<p>Finds indices where elements should be inserted to maintain order.</p>
<ul>
<li><p>Parameters</p>
<p><strong>v</strong> (<em>Union**[</em><a href="https://docs.python.org/library/functions.html#int" target="_blank" rel="noopener"><em>int</em></a><em>,</em> <a href="https://docs.python.org/library/functions.html#float" target="_blank" rel="noopener"><em>float</em></a><em>,</em> <a href="https://docs.python.org/library/functions.html#bool" target="_blank" rel="noopener"><em>bool</em></a><em>,</em> <a href="https://docs.python.org/library/stdtypes.html#list" target="_blank" rel="noopener"><em>list</em></a><em>,</em> <a href="https://docs.python.org/library/stdtypes.html#tuple" target="_blank" rel="noopener"><em>tuple</em></a><em>,</em> <a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/api_python/mindspore/mindspore.Tensor.html#mindspore.Tensor" target="_blank" rel="noopener"><em>Tensor</em></a><em>]</em>) â€“ Values to insert into a.<strong>side</strong> (<em>â€˜leftâ€™**,</em> <em>â€˜rightâ€™**,</em> <em>optional</em>) â€“ If â€˜leftâ€™, the index of the first suitable location found is given. If â€˜rightâ€™, return the last such index. If there is no suitable index, return either 0 or N (where N is the length of a). Default: left.<strong>sorter</strong> (<em>Union**[</em><a href="https://docs.python.org/library/functions.html#int" target="_blank" rel="noopener"><em>int</em></a><em>,</em> <a href="https://docs.python.org/library/functions.html#float" target="_blank" rel="noopener"><em>float</em></a><em>,</em> <a href="https://docs.python.org/library/functions.html#bool" target="_blank" rel="noopener"><em>bool</em></a><em>,</em> <a href="https://docs.python.org/library/stdtypes.html#list" target="_blank" rel="noopener"><em>list</em></a><em>,</em> <a href="https://docs.python.org/library/stdtypes.html#tuple" target="_blank" rel="noopener"><em>tuple</em></a><em>,</em> <a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/api_python/mindspore/mindspore.Tensor.html#mindspore.Tensor" target="_blank" rel="noopener"><em>Tensor</em></a><em>]</em>) â€“ 1-D optional array of integer indices that sort array a into ascending order. They are typically the result of argsort.</p>
</li>
<li><p>Returns</p>
<p>Tensor, array of insertion points with the same shape as v.</p>
</li>
<li><p>Raises</p>
<p><a href="https://docs.python.org/library/exceptions.html#ValueError" target="_blank" rel="noopener"><strong>ValueError</strong></a> â€“ if argument for side or sorter is invalid.</p>
</li>
<li><p>Supported Platforms:</p>
<p><code>Ascend</code> <code>GPU</code> <code>CPU</code></p>
</li>
</ul>
<p><strong>Examples</strong></p>
<pre><code>&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; from mindspore import Tensor
&gt;&gt;&gt; x = Tensor(np.array([1, 2, 3, 4, 5]))
&gt;&gt;&gt; print(x.searchsorted(3))</code></pre><blockquote>
<p><em>property</em><code>shape</code></p>
</blockquote>
<p>Returns the shape of the tensor as a tuple.</p>
<blockquote>
<p><em>property</em><code>size</code></p>
</blockquote>
<p>Returns the total number of elements in tensor.</p>
<blockquote>
<p><code>squeeze</code>(<em>axis=None</em>)<a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/_modules/mindspore/common/tensor.html#Tensor.squeeze" target="_blank" rel="noopener">[source]</a></p>
</blockquote>
<p>Remove single-dimensional entries from the shape of a tensor.</p>
<ul>
<li><p>Parameters</p>
<p><strong>axis</strong> (<em>Union**[</em><a href="https://docs.python.org/library/constants.html#None" target="_blank" rel="noopener"><em>None</em></a><em>,</em> <a href="https://docs.python.org/library/functions.html#int" target="_blank" rel="noopener"><em>int</em></a><em>,</em> <a href="https://docs.python.org/library/stdtypes.html#list" target="_blank" rel="noopener"><em>list</em></a><em>(</em><a href="https://docs.python.org/library/functions.html#int" target="_blank" rel="noopener"><em>int</em></a><em>)**,</em> <a href="https://docs.python.org/library/stdtypes.html#tuple" target="_blank" rel="noopener"><em>tuple</em></a><em>(</em><a href="https://docs.python.org/library/functions.html#int" target="_blank" rel="noopener"><em>int</em></a><em>)<strong>]</strong>,</em> <em>optional</em>) â€“ Selects a subset of the entries of length one in the shape. If an axis is selected with shape entry greater than one, an error is raised. Default is None.</p>
</li>
<li><p>Returns</p>
<p>Tensor, with all or a subset of the dimensions of length 1 removed.</p>
</li>
<li><p>Raises</p>
<p><a href="https://docs.python.org/library/exceptions.html#TypeError" target="_blank" rel="noopener"><strong>TypeError</strong></a> â€“ If input arguments have types not specified above.</p>
<p><a href="https://docs.python.org/library/exceptions.html#ValueError" target="_blank" rel="noopener"><strong>ValueError</strong></a> â€“ If specified axis has shape entry &gt;1&gt;1.</p>
</li>
<li><p>Supported Platforms:</p>
<p><code>Ascend</code> <code>GPU</code> <code>CPU</code></p>
</li>
</ul>
<p>Examples</p>
<pre><code>&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; from mindspore import Tensor
&gt;&gt;&gt; x = Tensor(np.ones((1,2,2,1), dtype=np.float32))
&gt;&gt;&gt; x = x.squeeze()
&gt;&gt;&gt; print(x.shape)</code></pre><blockquote>
<p><code>std</code>(<em>axis=None</em>, <em>ddof=0</em>, <em>keepdims=False</em>)<a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/_modules/mindspore/common/tensor.html#Tensor.std" target="_blank" rel="noopener">[source]</a></p>
</blockquote>
<p>Compute the standard deviation along the specified axis. The standard deviation is the square root of the average of the squared deviations from the mean, i.e., std=sqrt(mean(abs(xâˆ’x.mean())âˆ—âˆ—2))std=sqrt(mean(abs(xâˆ’x.mean())âˆ—âˆ—2)).</p>
<p>Return the standard deviation, which is computed for the flattened array by default, otherwise over the specified axis.</p>
<blockquote>
<p>Note</p>
<p>Numpy arguments dtype, out and where are not supported.</p>
</blockquote>
<ul>
<li><p>Parameters</p>
<p><strong>axis</strong> (<em>Union**[</em><a href="https://docs.python.org/library/constants.html#None" target="_blank" rel="noopener"><em>None</em></a><em>,</em> <a href="https://docs.python.org/library/functions.html#int" target="_blank" rel="noopener"><em>int</em></a><em>,</em> <a href="https://docs.python.org/library/stdtypes.html#tuple" target="_blank" rel="noopener"><em>tuple</em></a><em>(</em><a href="https://docs.python.org/library/functions.html#int" target="_blank" rel="noopener"><em>int</em></a><em>)**]</em>) â€“Axis or axes along which the standard deviation is computed. Default: None.If None, compute the standard deviation of the flattened array.</p>
<p><strong>ddof</strong> (<a href="https://docs.python.org/library/functions.html#int" target="_blank" rel="noopener"><em>int</em></a>) â€“ Means Delta Degrees of Freedom. The divisor used in calculations is Nâˆ’ddofNâˆ’ddof, where NN represents the number of elements. Default: 0.</p>
<p><strong>keepdims</strong> â€“ Default: False.</p>
</li>
<li><p>Returns</p>
<p>Standard deviation tensor.</p>
</li>
<li><p>Supported Platforms:</p>
<p><code>Ascend</code> <code>GPU</code> <code>CPU</code></p>
</li>
</ul>
<p>Examples</p>
<pre><code>&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; from mindspore import Tensor
&gt;&gt;&gt; input_x = Tensor(np.array([1, 2, 3, 4], dtype=np.float32))
&gt;&gt;&gt; output = input_x.std()
&gt;&gt;&gt; print(output)</code></pre><blockquote>
<p><em>property</em><code>strides</code></p>
</blockquote>
<p>Return the tuple of bytes to step in each dimension when traversing a tensor.</p>
<blockquote>
<p><code>sum</code>(<em>axis=None</em>, <em>dtype=None</em>, <em>keepdims=False</em>, <em>initial=None</em>)<a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/_modules/mindspore/common/tensor.html#Tensor.sum" target="_blank" rel="noopener">[source]</a></p>
</blockquote>
<p>Return sum of array elements over a given axis.</p>
<blockquote>
<p>Note</p>
<p>Numpy arguments out, where, casting, order, subok, signature, and extobj are not supported.</p>
</blockquote>
<ul>
<li><p>Parameters</p>
<p><strong>axis</strong> (<em>Union**[</em><a href="https://docs.python.org/library/constants.html#None" target="_blank" rel="noopener"><em>None</em></a><em>,</em> <a href="https://docs.python.org/library/functions.html#int" target="_blank" rel="noopener"><em>int</em></a><em>,</em> <a href="https://docs.python.org/library/stdtypes.html#tuple" target="_blank" rel="noopener"><em>tuple</em></a><em>(</em><a href="https://docs.python.org/library/functions.html#int" target="_blank" rel="noopener"><em>int</em></a><em>)**]</em>) â€“ Axis or axes along which a sum is performed. Default: None. If None, sum all of the elements of the input array. If the axis is negative, it counts from the last to the first axis. If the axis is a tuple of ints, a sum is performed on all of the axes specified in the tuple instead of a single axis or all the axes as before.</p>
<p><strong>dtype</strong> (<a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/api_python/mindspore.html#mindspore.dtype" target="_blank" rel="noopener"><code>mindspore.dtype</code></a>, optional) â€“ defaults to None. Overrides the dtype of the output Tensor.</p>
<p><strong>keepdims</strong> (<a href="https://docs.python.org/library/functions.html#bool" target="_blank" rel="noopener"><em>bool</em></a>) â€“ If this is set to True, the axes which are reduced are left in the result as dimensions with size one. With this option, the result will broadcast correctly against the input array. If the default value is passed, then keepdims will not be passed through to the sum method of sub-classes of ndarray, however any non-default value will be. If the sub-classâ€™ method does not implement keepdims any exceptions will be raised. Default: False.</p>
<p><strong>initial</strong> (<em>scalar</em>) â€“ Starting value for the sum. Default: None.</p>
</li>
<li><p>Returns</p>
<p>Tensor. A tensor with the same shape as input, with the specified axis removed. If input tensor is a 0-d array, or if the axis is None, a scalar is returned.</p>
</li>
<li><p>Raises</p>
<p><a href="https://docs.python.org/library/exceptions.html#TypeError" target="_blank" rel="noopener"><strong>TypeError</strong></a> â€“ If input is not array_like, or axis is not int or tuple of ints, or keepdims is not integer, or initial is not scalar.</p>
<p><a href="https://docs.python.org/library/exceptions.html#ValueError" target="_blank" rel="noopener"><strong>ValueError</strong></a> â€“ If any axis is out of range or duplicate axes exist.</p>
</li>
<li><p>Supported Platforms:</p>
<p><code>Ascend</code> <code>GPU</code> <code>CPU</code></p>
</li>
</ul>
<p><strong>Examples</strong></p>
<pre><code>&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; from mindspore import Tensor
&gt;&gt;&gt; input_x = Tensor(np.array([-1, 0, 1]).astype(np.float32))
&gt;&gt;&gt; print(input_x.sum())
0.0
&gt;&gt;&gt; input_x = Tensor(np.arange(10).reshape(2, 5).astype(np.float32))
&gt;&gt;&gt; print(input_x.sum(axis=1))</code></pre><blockquote>
<p><code>swapaxes</code>(<em>axis1</em>, <em>axis2</em>)<a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/_modules/mindspore/common/tensor.html#Tensor.swapaxes" target="_blank" rel="noopener">[source]</a></p>
</blockquote>
<p>Interchange two axes of a tensor.</p>
<ul>
<li><p>Parameters</p>
<p><strong>axis1</strong> (<a href="https://docs.python.org/library/functions.html#int" target="_blank" rel="noopener"><em>int</em></a>) â€“ First axis.</p>
<p><strong>axis2</strong> (<a href="https://docs.python.org/library/functions.html#int" target="_blank" rel="noopener"><em>int</em></a>) â€“ Second axis.</p>
</li>
<li><p>Returns</p>
<p>Transposed tensor, has the same data type as the input.</p>
</li>
<li><p>Raises</p>
<p><a href="https://docs.python.org/library/exceptions.html#TypeError" target="_blank" rel="noopener"><strong>TypeError</strong></a> â€“ If axis1 or axis2 is not integer.</p>
<p><a href="https://docs.python.org/library/exceptions.html#ValueError" target="_blank" rel="noopener"><strong>ValueError</strong></a> â€“ If axis1 or axis2 is not in the range of âˆ’ndim,ndimâˆ’1.</p>
</li>
<li><p>Supported Platforms:</p>
<p><code>Ascend</code> <code>GPU</code> <code>CPU</code></p>
</li>
</ul>
<p><strong>Examples</strong></p>
<pre><code>&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; from mindspore import Tensor
&gt;&gt;&gt; x = Tensor(np.ones((2,3,4), dtype=np.float32))
&gt;&gt;&gt; output = x.swapaxes(0, 2)
&gt;&gt;&gt; print(output.shape)</code></pre><blockquote>
<p><code>take</code>(<em>indices</em>, <em>axis=None</em>, <em>mode=â€clipâ€</em>)<a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/_modules/mindspore/common/tensor.html#Tensor.take" target="_blank" rel="noopener">[source]</a></p>
</blockquote>
<p>Takes elements from an array along an axis.</p>
<ul>
<li><p>Parameters</p>
<p><strong>indices</strong> (<a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/api_python/mindspore/mindspore.Tensor.html#mindspore.Tensor" target="_blank" rel="noopener"><em>Tensor</em></a>) â€“ The indices with shape (Njâ€¦) of the values to extract.</p>
<p><strong>axis</strong> (<a href="https://docs.python.org/library/functions.html#int" target="_blank" rel="noopener"><em>int</em></a><em>,</em> <em>optional</em>) â€“ The axis over which to select values. By default, the flattened input array is used. Default: None.</p>
<p><strong>mode</strong> (<em>â€˜raiseâ€™**,</em> <em>â€˜wrapâ€™**,</em> <em>â€˜clipâ€™**,</em> <em>optional</em>) â€“edge: Pads with the edge values of arr.raise: Raises an error;wrap: Wraps around;clip: Clips to the range. clip mode means that all indices that are too large are replaced by the index that addresses the last element along that axis. Note that this disables indexing with negative numbers.Default: clip.</p>
</li>
<li><p>Returns</p>
<p>Tensor, the indexed result.</p>
</li>
<li><p>Raises</p>
<p><a href="https://docs.python.org/library/exceptions.html#ValueError" target="_blank" rel="noopener"><strong>ValueError</strong></a> â€“ if axis is out of range, or mode has values other than (â€˜raiseâ€™, â€˜wrapâ€™, â€˜clipâ€™)</p>
</li>
<li><p>Supported Platforms:</p>
<p><code>Ascend</code> <code>GPU</code> <code>CPU</code></p>
</li>
</ul>
<p><strong>Examples</strong></p>
<pre><code>&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; from mindspore import Tensor
&gt;&gt;&gt; a = Tensor(np.array([4, 3, 5, 7, 6, 8]))
&gt;&gt;&gt; indices = Tensor(np.array([0, 1, 4]))
&gt;&gt;&gt; output = a.take(indices)
&gt;&gt;&gt; print(output)</code></pre><blockquote>
<p><code>to_tensor</code>(<em>slice_index=None</em>, <em>shape=None</em>, <em>opt_shard_group=None</em>)<a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/_modules/mindspore/common/tensor.html#Tensor.to_tensor" target="_blank" rel="noopener">[source]</a></p>
</blockquote>
<p>Return init_data() and get the tensor format data of this Tensor.</p>
<blockquote>
<p>Note</p>
<p>The usage of to_tensor is deprecated. Please use init_data.</p>
</blockquote>
<ul>
<li><p>Parameters</p>
<p><strong>slice_index</strong> (<a href="https://docs.python.org/library/functions.html#int" target="_blank" rel="noopener"><em>int</em></a>) â€“ Slice index of a parameterâ€™s slices. It is used when initialize a slice of a parameter, it guarantees that devices using the same slice can generate the same tensor. Default: None.</p>
<p><strong>shape</strong> (<a href="https://docs.python.org/library/stdtypes.html#list" target="_blank" rel="noopener"><em>list</em></a><em>[*<a href="https://docs.python.org/library/functions.html#int" target="_blank" rel="noopener"><em>int</em></a>*]</em>) â€“ Shape of the slice, it is used when initialize a slice of the parameter. Default: None.</p>
<p><strong>opt_shard_group</strong> (<a href="https://docs.python.org/library/stdtypes.html#str" target="_blank" rel="noopener"><em>str</em></a>) â€“ Optimizer shard group which is used in auto or semi auto parallel mode to get one shard of a parameterâ€™s slice. Default: None.</p>
</li>
<li><p>Returns</p>
<p>Initialized Tensor.</p>
</li>
<li><p>Supported Platforms:</p>
<p><code>Ascend</code> <code>GPU</code> <code>CPU</code></p>
</li>
</ul>
<p><strong>Examples</strong></p>
<pre><code>&gt;&gt;&gt; import mindspore as ms
&gt;&gt;&gt; import mindspore.common.initializer as init
&gt;&gt;&gt; x = init.initializer(init.Constant(1), [2, 2], ms.float32)
&gt;&gt;&gt; out = x.to_tensor()
&gt;&gt;&gt; print(out)</code></pre><blockquote>
<p><code>trace</code>(<em>offset=0</em>, <em>axis1=0</em>, <em>axis2=1</em>, <em>dtype=None</em>)<a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/_modules/mindspore/common/tensor.html#Tensor.trace" target="_blank" rel="noopener">[source]</a></p>
</blockquote>
<p>Return the sum along diagonals of the array.</p>
<ul>
<li><p>Parameters</p>
<p><strong>offset</strong> (<a href="https://docs.python.org/library/functions.html#int" target="_blank" rel="noopener"><em>int</em></a><em>,</em> <em>optional</em>) â€“ Offset of the diagonal from the main diagonal. Can be positive or negative. Defaults to main diagonal.</p>
<p><strong>axis1</strong> (<a href="https://docs.python.org/library/functions.html#int" target="_blank" rel="noopener"><em>int</em></a><em>,</em> <em>optional</em>) â€“ Axis to be used as the first axis of the 2-D sub-arrays from which the diagonals should be taken. Defaults to first axis (0).</p>
<p><strong>axis2</strong> (<a href="https://docs.python.org/library/functions.html#int" target="_blank" rel="noopener"><em>int</em></a><em>,</em> <em>optional</em>) â€“ Axis to be used as the second axis of the 2-D sub-arrays from which the diagonals should be taken. Defaults to second axis.</p>
<p><strong>dtype</strong> (<a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/api_python/mindspore.html#mindspore.dtype" target="_blank" rel="noopener"><code>mindspore.dtype</code></a>, optional) â€“ defaults to None. Overrides the dtype of the output Tensor.</p>
</li>
<li><p>Returns</p>
<p>Tensor, sum_along_diagonals.</p>
</li>
<li><p>Raises</p>
<p><a href="https://docs.python.org/library/exceptions.html#ValueError" target="_blank" rel="noopener"><strong>ValueError</strong></a> â€“ if the input tensor has less than two dimensions.</p>
</li>
<li><p>Supported Platforms:</p>
<p><code>Ascend</code> <code>GPU</code> <code>CPU</code></p>
</li>
</ul>
<p><strong>Examples</strong></p>
<pre><code>&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; from mindspore import Tensor
&gt;&gt;&gt; x = Tensor(np.eye(3, dtype=np.float32))
&gt;&gt;&gt; print(x.trace())</code></pre><blockquote>
<p><code>transpose</code>(<em>*axes</em>)<a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/_modules/mindspore/common/tensor.html#Tensor.transpose" target="_blank" rel="noopener">[source]</a></p>
</blockquote>
<p>Return a view of the tensor with axes transposed.</p>
<ul>
<li>For a 1-D tensor this has no effect, as a transposed vector is simply the same vector.</li>
<li>For a 2-D tensor, this is a standard matrix transpose.</li>
<li>For an n-D tensor, if axes are given, their order indicates how the axes are permuted.</li>
</ul>
<p>If axes are not provided and <code>tensor.shape = (i[0], i[1],...i[n-2], i[n-1])</code>, then <code>tensor.transpose().shape = (i[n-1], i[n-2], ... i[1], i[0])</code>.</p>
<ul>
<li><p>Parameters</p>
<p><strong>axes</strong> (<em>Union**[</em><a href="https://docs.python.org/library/constants.html#None" target="_blank" rel="noopener"><em>None</em></a><em>,</em> <a href="https://docs.python.org/library/stdtypes.html#tuple" target="_blank" rel="noopener"><em>tuple</em></a><em>(</em><a href="https://docs.python.org/library/functions.html#int" target="_blank" rel="noopener"><em>int</em></a><em>)**,</em> <a href="https://docs.python.org/library/stdtypes.html#list" target="_blank" rel="noopener"><em>list</em></a><em>(</em><a href="https://docs.python.org/library/functions.html#int" target="_blank" rel="noopener"><em>int</em></a><em>)**,</em> <a href="https://docs.python.org/library/functions.html#int" target="_blank" rel="noopener"><em>int</em></a><em>]**,</em> <em>optional</em>) â€“ If axes is None or blank, the method will reverse the order of the axes. If axes is tuple(int) or list(int), tensor.transpose() will transpose the tensor to the new axes order. If axes is int, this form is simply intended as a convenience alternative to the tuple/list form.</p>
</li>
<li><p>Returns</p>
<p>Tensor, has the same dimension as input tensor, with axes suitably permuted.</p>
</li>
<li><p>Raises</p>
<p><a href="https://docs.python.org/library/exceptions.html#TypeError" target="_blank" rel="noopener"><strong>TypeError</strong></a> â€“ If input arguments have types not specified above.</p>
<p><a href="https://docs.python.org/library/exceptions.html#ValueError" target="_blank" rel="noopener"><strong>ValueError</strong></a> â€“ If the number of axes is not euqal to a.ndim.</p>
</li>
<li><p>Supported Platforms:</p>
<p><code>Ascend</code> <code>GPU</code> <code>CPU</code></p>
</li>
</ul>
<p><strong>Examples</strong></p>
<pre><code>&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; from mindspore import Tensor
&gt;&gt;&gt; x = Tensor(np.ones((1,2,3), dtype=np.float32))
&gt;&gt;&gt; x = x.transpose()
&gt;&gt;&gt; print(x.shape)</code></pre><blockquote>
<p><code>var</code>(<em>axis=None</em>, <em>ddof=0</em>, <em>keepdims=False</em>)<a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/_modules/mindspore/common/tensor.html#Tensor.var" target="_blank" rel="noopener">[source]</a></p>
</blockquote>
<p>Compute the variance along the specified axis.</p>
<p>The variance is the average of the squared deviations from the mean, i.e., var=mean(abs(xâˆ’x.mean())âˆ—âˆ—2)var=mean(abs(xâˆ’x.mean())âˆ—âˆ—2).</p>
<p>Return the variance, which is computed for the flattened array by default, otherwise over the specified axis.</p>
<blockquote>
<p>Note</p>
<p>Numpy arguments dtype, out and where are not supported.</p>
</blockquote>
<ul>
<li><p>Parameters</p>
<p><strong>axis</strong> (<em>Union**[</em><a href="https://docs.python.org/library/constants.html#None" target="_blank" rel="noopener"><em>None</em></a><em>,</em> <a href="https://docs.python.org/library/functions.html#int" target="_blank" rel="noopener"><em>int</em></a><em>,</em> <a href="https://docs.python.org/library/stdtypes.html#tuple" target="_blank" rel="noopener"><em>tuple</em></a><em>(</em><a href="https://docs.python.org/library/functions.html#int" target="_blank" rel="noopener"><em>int</em></a><em>)**]</em>) â€“ Axis or axes along which the variance is computed. The default is to compute the variance of the flattened array. Default: None.</p>
<p><strong>ddof</strong> (<a href="https://docs.python.org/library/functions.html#int" target="_blank" rel="noopener"><em>int</em></a>) â€“ Means Delta Degrees of Freedom. Default: 0. The divisor used in calculations is Nâˆ’ddofNâˆ’ddof, where NN represents the number of elements.</p>
<p><strong>keepdims</strong> (<a href="https://docs.python.org/library/functions.html#bool" target="_blank" rel="noopener"><em>bool</em></a>) â€“ Default: False.</p>
</li>
<li><p>Supported Platforms:</p>
<p><code>Ascend</code> <code>GPU</code> <code>CPU</code></p>
</li>
<li><p>Returns</p>
<p>Standard deviation tensor.</p>
</li>
</ul>
<p><strong>Examples</strong></p>
<pre><code>&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; from mindspore import Tensor
&gt;&gt;&gt; input_x = Tensor(np.array([1., 2., 3., 4.], np.float32))
&gt;&gt;&gt; output = input_x.var()
&gt;&gt;&gt; print(output)</code></pre><blockquote>
<p><code>view</code>(<em>*shape</em>)<a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/_modules/mindspore/common/tensor.html#Tensor.view" target="_blank" rel="noopener">[source]</a></p>
</blockquote>
<p>Reshape the tensor according to the input shape.</p>
<ul>
<li><p>Parameters</p>
<p><strong>shape</strong> (<em>Union**[</em><a href="https://docs.python.org/library/stdtypes.html#tuple" target="_blank" rel="noopener"><em>tuple</em></a><em>(</em><a href="https://docs.python.org/library/functions.html#int" target="_blank" rel="noopener"><em>int</em></a><em>)**,</em> <a href="https://docs.python.org/library/functions.html#int" target="_blank" rel="noopener"><em>int</em></a><em>]</em>) â€“ Dimension of the output tensor.</p>
</li>
<li><p>Returns</p>
<p>Tensor, has the same dimension as the input shape.</p>
</li>
</ul>
<blockquote>
<p><em>property</em><code>virtual_flag</code></p>
</blockquote>
<p>Used to mark whether the tensor is virtual. If the tensor is virtual, return True.</p>
<h5 id="classmindspore-RowTensor-indices-values-dense-shape-source"><a href="#classmindspore-RowTensor-indices-values-dense-shape-source" class="headerlink" title="classmindspore.RowTensor(indices, values, dense_shape)[source]"></a><em>class<em><code>mindspore.RowTensor</code>(</em>indices</em>, <em>values</em>, <em>dense_shape</em>)<a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/_modules/mindspore/common/tensor.html#RowTensor" target="_blank" rel="noopener">[source]</a></h5><p>A sparse representation of a set of tensor slices at given indices.</p>
<p>An RowTensor is typically used to represent a subset of a larger tensor dense of shape [L0, D1, .. , DN] where L0 &gt;&gt; D0.</p>
<p>The values in indices are the indices in the first dimension of the slices that have been extracted from the larger tensor.</p>
<p>The dense tensor dense represented by an RowTensor slices has dense[slices.indices[i], :, :, :, â€¦] = slices.values[i, :, :, :, â€¦].</p>
<p>RowTensor can only be used in the Cellâ€™s construct method.</p>
<p>It is not supported in pynative mode at the moment.</p>
<ul>
<li><p>Parameters</p>
<p><strong>indices</strong> (<a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/api_python/mindspore/mindspore.Tensor.html#mindspore.Tensor" target="_blank" rel="noopener"><em>Tensor</em></a>) â€“ A 1-D integer Tensor of shape [D0].</p>
<p><strong>values</strong> (<a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/api_python/mindspore/mindspore.Tensor.html#mindspore.Tensor" target="_blank" rel="noopener"><em>Tensor</em></a>) â€“ A Tensor of any dtype of shape [D0, D1, â€¦, Dn].</p>
<p><strong>dense_shape</strong> (<a href="https://docs.python.org/library/stdtypes.html#tuple" target="_blank" rel="noopener"><em>tuple</em></a><em>(</em><a href="https://docs.python.org/library/functions.html#int" target="_blank" rel="noopener"><em>int</em></a><em>)</em>) â€“ An integer tuple which contains the shape of the corresponding dense tensor.</p>
</li>
<li><p>Returns</p>
<p>RowTensor, composed of indices, values, and dense_shape.</p>
</li>
</ul>
<p><strong>Examples</strong></p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">import</span> mindspore <span class="token keyword">as</span> ms
<span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">import</span> mindspore<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn
<span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">from</span> mindspore <span class="token keyword">import</span> RowTensor
<span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">class</span> <span class="token class-name">Net</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Cell<span class="token punctuation">)</span><span class="token punctuation">:</span>
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>     <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> dense_shape<span class="token punctuation">)</span><span class="token punctuation">:</span>
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>         super<span class="token punctuation">(</span>Net<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>         self<span class="token punctuation">.</span>dense_shape <span class="token operator">=</span> dense_shape
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>     <span class="token keyword">def</span> <span class="token function">construct</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> indices<span class="token punctuation">,</span> values<span class="token punctuation">)</span><span class="token punctuation">:</span>
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>         x <span class="token operator">=</span> RowTensor<span class="token punctuation">(</span>indices<span class="token punctuation">,</span> values<span class="token punctuation">,</span> self<span class="token punctuation">.</span>dense_shape<span class="token punctuation">)</span>
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>         <span class="token keyword">return</span> x<span class="token punctuation">.</span>values<span class="token punctuation">,</span> x<span class="token punctuation">.</span>indices<span class="token punctuation">,</span> x<span class="token punctuation">.</span>dense_shape
<span class="token operator">>></span><span class="token operator">></span>
<span class="token operator">>></span><span class="token operator">></span> indices <span class="token operator">=</span> Tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token operator">>></span><span class="token operator">></span> values <span class="token operator">=</span> Tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>ms<span class="token punctuation">.</span>float32<span class="token punctuation">)</span>
<span class="token operator">>></span><span class="token operator">></span> out <span class="token operator">=</span> Net<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">(</span>indices<span class="token punctuation">,</span> values<span class="token punctuation">)</span>
<span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">print</span><span class="token punctuation">(</span>out<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">.</span> <span class="token number">2</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">]</span>
<span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">print</span><span class="token punctuation">(</span>out<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
<span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">print</span><span class="token punctuation">(</span>out<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h5 id="classmindspore-SparseTensor-indices-values-dense-shape-source"><a href="#classmindspore-SparseTensor-indices-values-dense-shape-source" class="headerlink" title="classmindspore.SparseTensor(indices, values, dense_shape)[source]"></a><em>class<em><code>mindspore.SparseTensor</code>(</em>indices</em>, <em>values</em>, <em>dense_shape</em>)<a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/_modules/mindspore/common/tensor.html#SparseTensor" target="_blank" rel="noopener">[source]</a></h5><p>A sparse representation of a set of nonzero elememts from a tensor at given indices.</p>
<p>SparseTensor can only be used in the Cellâ€™s construct method.</p>
<p>Pynative mode not supported at the moment.</p>
<p>For a tensor dense, its SparseTensor(indices, values, dense_shape) has dense[indices[i]] = values[i].</p>
<ul>
<li><p>Parameters</p>
<p><strong>indices</strong> (<a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/api_python/mindspore/mindspore.Tensor.html#mindspore.Tensor" target="_blank" rel="noopener"><em>Tensor</em></a>) â€“ A 2-D integer Tensor of shape [N, ndims], where N and ndims are the number of values and number of dimensions in the SparseTensor, respectively.</p>
<p><strong>values</strong> (<a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/api_python/mindspore/mindspore.Tensor.html#mindspore.Tensor" target="_blank" rel="noopener"><em>Tensor</em></a>) â€“ A 1-D tensor of any type and shape [N], which supplies the values for each element in indices.</p>
<p><strong>dense_shape</strong> (<a href="https://docs.python.org/library/stdtypes.html#tuple" target="_blank" rel="noopener"><em>tuple</em></a><em>(</em><a href="https://docs.python.org/library/functions.html#int" target="_blank" rel="noopener"><em>int</em></a><em>)</em>) â€“ A integer tuple of size ndims, which specifies the dense_shape of the sparse tensor.</p>
</li>
<li><p>Returns</p>
<p>SparseTensor, composed of indices, values, and dense_shape.</p>
</li>
</ul>
<p><strong>Examples</strong></p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">import</span> mindspore <span class="token keyword">as</span> ms
<span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">import</span> mindspore<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn
<span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">from</span> mindspore <span class="token keyword">import</span> SparseTensor
<span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">class</span> <span class="token class-name">Net</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Cell<span class="token punctuation">)</span><span class="token punctuation">:</span>
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>     <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> dense_shape<span class="token punctuation">)</span><span class="token punctuation">:</span>
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>         super<span class="token punctuation">(</span>Net<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>         self<span class="token punctuation">.</span>dense_shape <span class="token operator">=</span> dense_shape
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>     <span class="token keyword">def</span> <span class="token function">construct</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> indices<span class="token punctuation">,</span> values<span class="token punctuation">)</span><span class="token punctuation">:</span>
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>         x <span class="token operator">=</span> SparseTensor<span class="token punctuation">(</span>indices<span class="token punctuation">,</span> values<span class="token punctuation">,</span> self<span class="token punctuation">.</span>dense_shape<span class="token punctuation">)</span>
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>         <span class="token keyword">return</span> x<span class="token punctuation">.</span>values<span class="token punctuation">,</span> x<span class="token punctuation">.</span>indices<span class="token punctuation">,</span> x<span class="token punctuation">.</span>dense_shape
<span class="token operator">>></span><span class="token operator">></span>
<span class="token operator">>></span><span class="token operator">></span> indices <span class="token operator">=</span> Tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token operator">>></span><span class="token operator">></span> values <span class="token operator">=</span> Tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>ms<span class="token punctuation">.</span>float32<span class="token punctuation">)</span>
<span class="token operator">>></span><span class="token operator">></span> out <span class="token operator">=</span> Net<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">(</span>indices<span class="token punctuation">,</span> values<span class="token punctuation">)</span>
<span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">print</span><span class="token punctuation">(</span>out<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">.</span> <span class="token number">2</span><span class="token punctuation">.</span><span class="token punctuation">]</span>
<span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">print</span><span class="token punctuation">(</span>out<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0</span> <span class="token number">1</span><span class="token punctuation">]</span>
 <span class="token punctuation">[</span><span class="token number">1</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">]</span>
<span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">print</span><span class="token punctuation">(</span>out<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<blockquote>
<p><code>mindspore.ms_function</code>(<em>fn=None</em>, <em>obj=None</em>, <em>input_signature=None</em>)<a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/_modules/mindspore/common/api.html#ms_function" target="_blank" rel="noopener">[source]</a></p>
</blockquote>
<p>Create a callable MindSpore graph from a python function.</p>
<p>This allows the MindSpore runtime to apply optimizations based on graph.</p>
<ul>
<li><p>Parameters</p>
<p><strong>fn</strong> (<em>Function</em>) â€“ The Python function that will be run as a graph. Default: None.</p>
<p><strong>obj</strong> (<em>Object</em>) â€“ The python object that provides the information for identifying the compiled function. Default: None.</p>
<p><strong>input_signature</strong> (<a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/api_python/mindspore/mindspore.Tensor.html#mindspore.Tensor" target="_blank" rel="noopener"><em>Tensor</em></a>) â€“ The Tensor which describes the input arguments. The shape and dtype of the Tensor will be supplied to this function. If input_signature is specified, each input to fn must be a Tensor. And the input parameters of fn cannot accept **kwargs. The shape and dtype of actual inputs should keep the same as input_signature. Otherwise, TypeError will be raised. Default: None.</p>
</li>
<li><p>Returns</p>
<p>Function, if fn is not None, returns a callable function that will execute the compiled function; If fn is None, returns a decorator and when this decorator invokes with a single fn argument, the callable function is equal to the case when fn is not None.</p>
</li>
<li><p>Supported Platforms:</p>
<p><code>Ascend</code> <code>GPU</code> <code>CPU</code></p>
</li>
</ul>
<p><strong>Examples</strong></p>
<pre><code>&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; from mindspore import Tensor
&gt;&gt;&gt; from mindspore import ms_function
...
&gt;&gt;&gt; x = Tensor(np.ones([1, 1, 3, 3]).astype(np.float32))
&gt;&gt;&gt; y = Tensor(np.ones([1, 1, 3, 3]).astype(np.float32))
...
&gt;&gt;&gt; # create a callable MindSpore graph by calling ms_function
&gt;&gt;&gt; def tensor_add(x, y):
...     z = x + y
...     return z
...
&gt;&gt;&gt; tensor_add_graph = ms_function(fn=tensor_add)
&gt;&gt;&gt; out = tensor_add_graph(x, y)
...
&gt;&gt;&gt; # create a callable MindSpore graph through decorator @ms_function
&gt;&gt;&gt; @ms_function
... def tensor_add_with_dec(x, y):
...     z = x + y
...     return z
...
&gt;&gt;&gt; out = tensor_add_with_dec(x, y)
...
&gt;&gt;&gt; # create a callable MindSpore graph through decorator @ms_function with input_signature parameter
&gt;&gt;&gt; @ms_function(input_signature=(Tensor(np.ones([1, 1, 3, 3]).astype(np.float32)),
...                               Tensor(np.ones([1, 1, 3, 3]).astype(np.float32))))
... def tensor_add_with_sig(x, y):
...     z = x + y
...     return z
...
&gt;&gt;&gt; out = tensor_add_with_sig(x, y)</code></pre><h5 id="classmindspore-Parameter-default-input-name-None-requires-grad-True-layerwise-parallel-False-parallel-optimizer-True-source"><a href="#classmindspore-Parameter-default-input-name-None-requires-grad-True-layerwise-parallel-False-parallel-optimizer-True-source" class="headerlink" title="classmindspore.Parameter(default_input, name=None, requires_grad=True, layerwise_parallel=False, parallel_optimizer=True)[source]"></a><em>class<em><code>mindspore.Parameter</code>(</em>default_input</em>, <em>name=None</em>, <em>requires_grad=True</em>, <em>layerwise_parallel=False</em>, <em>parallel_optimizer=True</em>)<a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/_modules/mindspore/common/parameter.html#Parameter" target="_blank" rel="noopener">[source]</a></h5><p>Parameter types of cell models, after initialized Parameter is a subtype of Tensor.</p>
<blockquote>
<p>Note</p>
<p>In auto_parallel mode of â€œsemi_auto_parallelâ€ and â€œauto_parallelâ€, if init Parameter by an Tensor, the type of Parameter will be Tensor. Tensor will save the shape and type info of a tensor with no memory usage. The shape can be changed while compiling for auto-parallel. Call init_data will return a Tensor Parameter with initialized data. If there is an operator in the network that requires part of the inputs to be Parameter, then the Parameters as this part of the inputs are not allowed to be cast. It is recommended to use the default value of name when initialize a parameter as one attribute of a cell, otherwise, the parameter name may be different than expected.</p>
</blockquote>
<ul>
<li><p>Parameters</p>
<p><strong>default_input</strong> (<em>Union**[</em><a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/api_python/mindspore/mindspore.Tensor.html#mindspore.Tensor" target="_blank" rel="noopener"><em>Tensor</em></a><em>,</em> <a href="https://docs.python.org/library/functions.html#int" target="_blank" rel="noopener"><em>int</em></a><em>,</em> <a href="https://docs.python.org/library/functions.html#float" target="_blank" rel="noopener"><em>float</em></a><em>,</em> <a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" target="_blank" rel="noopener"><em>numpy.ndarray</em></a><em>,</em> <a href="https://docs.python.org/library/stdtypes.html#list" target="_blank" rel="noopener"><em>list</em></a><em>]</em>) â€“ Parameter data,<strong>initialize the parameter data.</strong> (<em>to</em>) â€“</p>
<p><strong>name</strong> (<a href="https://docs.python.org/library/stdtypes.html#str" target="_blank" rel="noopener"><em>str</em></a>) â€“ Name of the child parameter. Default: None.</p>
<p><strong>requires_grad</strong> (<a href="https://docs.python.org/library/functions.html#bool" target="_blank" rel="noopener"><em>bool</em></a>) â€“ True if the parameter requires gradient. Default: True.</p>
<p><strong>layerwise_parallel</strong> (<a href="https://docs.python.org/library/functions.html#bool" target="_blank" rel="noopener"><em>bool</em></a>) â€“ When layerwise_parallel is true in data/hybrid parallel mode, broadcast and gradients communication would not be applied to parameters. Default: False.</p>
<p><strong>parallel_optimizer</strong> (<a href="https://docs.python.org/library/functions.html#bool" target="_blank" rel="noopener"><em>bool</em></a>) â€“ It is used to filter the weight shard operation in semi auto or auto parallel mode. It works only when enable parallel optimizer in mindspore.context.set_auto_parallel_context(). Default: True.</p>
</li>
</ul>
<p><strong>Examples</strong></p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
<span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">from</span> mindspore <span class="token keyword">import</span> Parameter<span class="token punctuation">,</span> Tensor
<span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">import</span> mindspore<span class="token punctuation">.</span>ops <span class="token keyword">as</span> ops
<span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">import</span> mindspore<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn
<span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">import</span> mindspore
<span class="token operator">>></span><span class="token operator">></span>
<span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">class</span> <span class="token class-name">Net</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Cell<span class="token punctuation">)</span><span class="token punctuation">:</span>
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>     <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>         super<span class="token punctuation">(</span>Net<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>         self<span class="token punctuation">.</span>matmul <span class="token operator">=</span> ops<span class="token punctuation">.</span>MatMul<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>         self<span class="token punctuation">.</span>weight <span class="token operator">=</span> Parameter<span class="token punctuation">(</span>Tensor<span class="token punctuation">(</span>np<span class="token punctuation">.</span>ones<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> mindspore<span class="token punctuation">.</span>float32<span class="token punctuation">)</span><span class="token punctuation">,</span> name<span class="token operator">=</span><span class="token string">"w"</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>     <span class="token keyword">def</span> <span class="token function">construct</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>         out <span class="token operator">=</span> self<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>self<span class="token punctuation">.</span>weight<span class="token punctuation">,</span> x<span class="token punctuation">)</span>
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>         <span class="token keyword">return</span> out
<span class="token operator">>></span><span class="token operator">></span> net <span class="token operator">=</span> Net<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token operator">>></span><span class="token operator">></span> x <span class="token operator">=</span> Tensor<span class="token punctuation">(</span>np<span class="token punctuation">.</span>ones<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> mindspore<span class="token punctuation">.</span>float32<span class="token punctuation">)</span>
<span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">print</span><span class="token punctuation">(</span>net<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">]</span>
<span class="token operator">>></span><span class="token operator">></span> net<span class="token punctuation">.</span>weight<span class="token punctuation">.</span>set_data<span class="token punctuation">(</span>Tensor<span class="token punctuation">(</span>np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> mindspore<span class="token punctuation">.</span>float32<span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">print</span><span class="token punctuation">(</span>net<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<blockquote>
<p><em>property</em><code>cache_enable</code></p>
</blockquote>
<p>Return whether the parameter is cache enable.</p>
<blockquote>
<p><em>property</em><code>cache_shape</code></p>
</blockquote>
<p>Return the cache shape corresponding to the parameter if use cache.</p>
<blockquote>
<p><code>clone</code>(<em>init=â€sameâ€</em>)<a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/_modules/mindspore/common/parameter.html#Parameter.clone" target="_blank" rel="noopener">[source]</a></p>
</blockquote>
<p>Clone the parameter.</p>
<ul>
<li><p>Parameters</p>
<p><strong>init</strong> (<em>Union**[</em><a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/api_python/mindspore/mindspore.Tensor.html#mindspore.Tensor" target="_blank" rel="noopener"><em>Tensor</em></a><em>,</em> <a href="https://docs.python.org/library/stdtypes.html#str" target="_blank" rel="noopener"><em>str</em></a><em>,</em> <a href="https://docs.python.org/library/numbers.html#numbers.Number" target="_blank" rel="noopener"><em>numbers.Number</em></a><em>]</em>) â€“ Initialize the shape and dtype of the parameter. If init is a Tensor or numbers.Number, clone a new parameter with the same shape and dtype, and the data of the new parameter will be set according to init. If init is a str, the init should be the alias of the class inheriting from Initializer. For example, if init is â€˜sameâ€™, clone a new parameter with the same data, shape, and dtype. Default: â€˜sameâ€™.</p>
</li>
<li><p>Returns</p>
<p>Parameter, a new parameter.</p>
</li>
</ul>
<blockquote>
<p><em>property</em><code>comm_fusion</code></p>
</blockquote>
<p>Get and set the fusion type (int) for communication operators corresponding to this parameter.</p>
<p>In AUTO_PARALLEL and SEMI_AUTO_PARALLEL mode, some communication operators used for parameters or gradients aggregation are inserted automatically. Set the fusion type for communication operators generated for this parameter. The value of fusion must be greater than or equal to 0. When the value of fusion is 0, operators will not be fused together.</p>
<p>Only support in Ascend environment with Graph mode.</p>
<blockquote>
<p><em>property</em><code>data</code></p>
</blockquote>
<p>Return the parameter object.</p>
<blockquote>
<p><code>init_data</code>(<em>layout=None</em>, <em>set_sliced=False</em>)<a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/_modules/mindspore/common/parameter.html#Parameter.init_data" target="_blank" rel="noopener">[source]</a></p>
</blockquote>
<p>Initialize the parameterâ€™s data.</p>
<ul>
<li><p>Parameters</p>
<p><strong>layout</strong> (<em>Union**[</em><a href="https://docs.python.org/library/constants.html#None" target="_blank" rel="noopener"><em>None</em></a><em>,</em> <a href="https://docs.python.org/library/stdtypes.html#tuple" target="_blank" rel="noopener"><em>tuple</em></a><em>(</em><a href="https://docs.python.org/library/stdtypes.html#list" target="_blank" rel="noopener"><em>list</em></a><em>(</em><a href="https://docs.python.org/library/functions.html#int" target="_blank" rel="noopener"><em>int</em></a><em>)<strong>)</strong>]</em>) â€“Parameter slice layout [dev_mat, tensor_map, slice_shape]. Default: None.dev_mat (list(int)): Device matrix.tensor_map (list(int)): Tensor map.slice_shape (list(int)): Shape of slice.</p>
<p><strong>set_sliced</strong> (<a href="https://docs.python.org/library/functions.html#bool" target="_blank" rel="noopener"><em>bool</em></a>) â€“ True if the parameter is set sliced after initializing the data. Default: False.</p>
</li>
<li><p>Raises</p>
<p><a href="https://docs.python.org/library/exceptions.html#RuntimeError" target="_blank" rel="noopener"><strong>RuntimeError</strong></a> â€“ If it is from Initializer, and parallel mode has changed after the Initializer created.</p>
<p><a href="https://docs.python.org/library/exceptions.html#ValueError" target="_blank" rel="noopener"><strong>ValueError</strong></a> â€“ If the length of the layout is less than 3.</p>
<p><a href="https://docs.python.org/library/exceptions.html#TypeError" target="_blank" rel="noopener"><strong>TypeError</strong></a> â€“ If layout is not tuple.</p>
</li>
<li><p>Returns</p>
<p>Parameter, the Parameter after initializing data. If current Parameter was already initialized before, returns the same initialized Parameter.</p>
</li>
</ul>
<blockquote>
<p><em>property</em><code>inited_param</code></p>
</blockquote>
<p>Get the new parameter after call the init_data.</p>
<p>Default is a None, If self is a Parameter with out data, after call the init_data the initialized Parameter with data will be recorded here.</p>
<blockquote>
<p><em>property</em><code>is_init</code></p>
</blockquote>
<p>Get the initialization status of the parameter.This flag only work in GE, and it will be set to False in other backend.</p>
<blockquote>
<p><em>property</em><code>layerwise_parallel</code></p>
</blockquote>
<p>When layerwise_parallel is true in data/hybrid parallel mode, broadcast and gradients communication would not be applied to parameters.</p>
<blockquote>
<p><em>property</em><code>name</code></p>
</blockquote>
<p>Get the name of the parameter.</p>
<blockquote>
<p><em>property</em><code>parallel_optimizer</code></p>
</blockquote>
<p>It is used to filter the weight shard operation in semi auto or auto parallel mode. It works only when enable parallel optimizer in mindspore.context.set_auto_parallel_context().</p>
<blockquote>
<p><em>property</em><code>requires_grad</code></p>
</blockquote>
<p>Return whether the parameter requires gradient.</p>
<blockquote>
<p><code>set_data</code>(<em>data</em>, <em>slice_shape=False</em>)<a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/_modules/mindspore/common/parameter.html#Parameter.set_data" target="_blank" rel="noopener">[source]</a></p>
</blockquote>
<p>Set Parameterâ€™s data.</p>
<ul>
<li><p>Parameters</p>
<p><strong>data</strong> (<em>Union**[</em><a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/api_python/mindspore/mindspore.Tensor.html#mindspore.Tensor" target="_blank" rel="noopener"><em>Tensor</em></a><em>,</em> <a href="https://docs.python.org/library/functions.html#int" target="_blank" rel="noopener"><em>int</em></a><em>,</em> <a href="https://docs.python.org/library/functions.html#float" target="_blank" rel="noopener"><em>float</em></a><em>]</em>) â€“ new data.</p>
<p><strong>slice_shape</strong> (<a href="https://docs.python.org/library/functions.html#bool" target="_blank" rel="noopener"><em>bool</em></a>) â€“ If slice the parameter is set to true, the shape is not checked for consistency. Default: False.</p>
</li>
<li><p>Returns</p>
<p>Parameter, the parameter after set data.</p>
</li>
</ul>
<blockquote>
<p><code>set_param_fl</code>(<em>push_to_server=False</em>, <em>pull_from_server=False</em>, <em>requires_aggr=True</em>)<a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/_modules/mindspore/common/parameter.html#Parameter.set_param_fl" target="_blank" rel="noopener">[source]</a></p>
</blockquote>
<p>Set the way of parameter and server interaction.</p>
<ul>
<li><p>Parameters</p>
<p><strong>push_to_server</strong> (<a href="https://docs.python.org/library/functions.html#bool" target="_blank" rel="noopener"><em>bool</em></a>) â€“ Whether the parameter should be pushed to server. Default: False.<strong>pull_from_server</strong> (<a href="https://docs.python.org/library/functions.html#bool" target="_blank" rel="noopener"><em>bool</em></a>) â€“ Whether the parameter should be pulled from server. Default: False.<strong>requires_aggr</strong> (<a href="https://docs.python.org/library/functions.html#bool" target="_blank" rel="noopener"><em>bool</em></a>) â€“ Whether the parameter should be aggregated in the server. Default: True.</p>
</li>
</ul>
<blockquote>
<p><code>set_param_ps</code>(<em>init_in_server=False</em>)<a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/_modules/mindspore/common/parameter.html#Parameter.set_param_ps" target="_blank" rel="noopener">[source]</a></p>
</blockquote>
<p>Set whether the trainable parameter is updated by parameter server and whether the trainable parameter is initialized on server.</p>
<blockquote>
<p>Note</p>
<p>It only works when a running task is in the parameter server mode.</p>
</blockquote>
<ul>
<li><p>Parameters</p>
<p><strong>init_in_server</strong> (<a href="https://docs.python.org/library/functions.html#bool" target="_blank" rel="noopener"><em>bool</em></a>) â€“ Whether trainable parameter updated by parameter server is initialized on server. Default: False.</p>
</li>
</ul>
<blockquote>
<p><em>property</em><code>sliced</code></p>
</blockquote>
<p>Get slice status of the parameter.</p>
<blockquote>
<p><em>property</em><code>unique</code></p>
</blockquote>
<p>whether the parameter is already unique or not.</p>
<h5 id="classmindspore-ParameterTuple-source"><a href="#classmindspore-ParameterTuple-source" class="headerlink" title="classmindspore.ParameterTuple[source]"></a><em>class</em><code>mindspore.ParameterTuple</code><a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/_modules/mindspore/common/parameter.html#ParameterTuple" target="_blank" rel="noopener">[source]</a></h5><p>Class for storing tuple of parameters.</p>
<blockquote>
<p>Note</p>
<p>It is used to store the parameters of the network into the parameter tuple collection.</p>
</blockquote>
<blockquote>
<p><code>clone</code>(<em>prefix</em>, <em>init=â€sameâ€</em>)<a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/_modules/mindspore/common/parameter.html#ParameterTuple.clone" target="_blank" rel="noopener">[source]</a></p>
</blockquote>
<p>Clone the parameters in ParameterTuple element-wisely to generate a new ParameterTuple.</p>
<ul>
<li><p>Parameters</p>
<p><strong>prefix</strong> (<a href="https://docs.python.org/library/stdtypes.html#str" target="_blank" rel="noopener"><em>str</em></a>) â€“ Namespace of parameter.</p>
<p><strong>init</strong> (<em>Union**[</em><a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/api_python/mindspore/mindspore.Tensor.html#mindspore.Tensor" target="_blank" rel="noopener"><em>Tensor</em></a><em>,</em> <a href="https://docs.python.org/library/stdtypes.html#str" target="_blank" rel="noopener"><em>str</em></a><em>,</em> <a href="https://docs.python.org/library/numbers.html#numbers.Number" target="_blank" rel="noopener"><em>numbers.Number</em></a><em>]</em>) â€“ Initialize the shape and dtype of the parameters. The definition of init is the same as in Parameter API. If init is â€˜sameâ€™, the parameters in the new parameter tuple are the same as those in the original parameter tuple. Default: â€˜sameâ€™.</p>
</li>
<li><p>Raises</p>
<p><a href="https://docs.python.org/library/exceptions.html#RuntimeError" target="_blank" rel="noopener"><strong>RuntimeError</strong></a> â€“ If parameterâ€™s name is not end with embedding_table.</p>
</li>
<li><p>Returns</p>
<p>Tuple, the new Parameter tuple.</p>
</li>
</ul>
<blockquote>
<p><code>mindspore.set_seed</code>(<em>seed</em>)<a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/_modules/mindspore/common/seed.html#set_seed" target="_blank" rel="noopener">[source]</a></p>
</blockquote>
<p>Set global seed.</p>
<blockquote>
<p>Note</p>
<p>The global seed is used by numpy.random, mindspore.common.Initializer, mindspore.ops.composite.random_ops and mindspore.nn.probability.distribution.</p>
<p>If global seed is not set, these packages will use their own default seed independently, numpy.random and mindspore.common.Initializer will choose a random seed, mindspore.ops.composite.random_ops and mindspore.nn.probability.distribution will use zero.</p>
<p>Seed set by numpy.random.seed() only used by numpy.random, while seed set by this API will also used by numpy.random, so just set all seed by this API is recommended.</p>
</blockquote>
<ul>
<li><p>Parameters</p>
<p><strong>seed</strong> (<a href="https://docs.python.org/library/functions.html#int" target="_blank" rel="noopener"><em>int</em></a>) â€“ The seed to be set.</p>
</li>
<li><p>Raises</p>
<p><a href="https://docs.python.org/library/exceptions.html#ValueError" target="_blank" rel="noopener"><strong>ValueError</strong></a> â€“ If seed is invalid (&lt; 0).</p>
<p><a href="https://docs.python.org/library/exceptions.html#TypeError" target="_blank" rel="noopener"><strong>TypeError</strong></a> â€“ If seed isnâ€™t a int.</p>
</li>
</ul>
<p><strong>Examples</strong></p>
<pre><code>&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; import mindspore.ops as ops
&gt;&gt;&gt; from mindspore import Tensor, set_seed, Parameter
&gt;&gt;&gt; from mindspore.common.initializer import initializer
&gt;&gt;&gt;
&gt;&gt;&gt; # Note: (1) Please make sure the code is running in PYNATIVE MODE;
&gt;&gt;&gt; # (2) Because Composite-level ops need parameters to be Tensors, for below examples,
&gt;&gt;&gt; # when using ops.uniform operator, minval and maxval are initialised as:
&gt;&gt;&gt; minval = Tensor(1.0, ms.float32)
&gt;&gt;&gt; maxval = Tensor(2.0, ms.float32)
&gt;&gt;&gt;
&gt;&gt;&gt; # 1. If global seed is not set, numpy.random and initializer will choose a random seed:
&gt;&gt;&gt; np_1 = np.random.normal(0, 1, [1]).astype(np.float32) # A1
&gt;&gt;&gt; np_1 = np.random.normal(0, 1, [1]).astype(np.float32) # A2
&gt;&gt;&gt; w1 = Parameter(initializer("uniform", [2, 2], ms.float32), name="w1") # W1
&gt;&gt;&gt; w1 = Parameter(initializer("uniform", [2, 2], ms.float32), name="w1") # W2
&gt;&gt;&gt; # Rerun the program will get different results:
&gt;&gt;&gt; np_1 = np.random.normal(0, 1, [1]).astype(np.float32) # A3
&gt;&gt;&gt; np_1 = np.random.normal(0, 1, [1]).astype(np.float32) # A4
&gt;&gt;&gt; w1 = Parameter(initializer("uniform", [2, 2], ms.float32), name="w1") # W3
&gt;&gt;&gt; w1 = Parameter(initializer("uniform", [2, 2], ms.float32), name="w1") # W4
&gt;&gt;&gt;
&gt;&gt;&gt; # 2. If global seed is set, numpy.random and initializer will use it:
&gt;&gt;&gt; set_seed(1234)
&gt;&gt;&gt; np_1 = np.random.normal(0, 1, [1]).astype(np.float32) # A1
&gt;&gt;&gt; np_1 = np.random.normal(0, 1, [1]).astype(np.float32) # A2
&gt;&gt;&gt; w1 = Parameter(initializer("uniform", [2, 2], ms.float32), name="w1") # W1
&gt;&gt;&gt; w1 = Parameter(initializer("uniform", [2, 2], ms.float32), name="w1") # W2
&gt;&gt;&gt; # Rerun the program will get the same results:
&gt;&gt;&gt; set_seed(1234)
&gt;&gt;&gt; np_1 = np.random.normal(0, 1, [1]).astype(np.float32) # A1
&gt;&gt;&gt; np_1 = np.random.normal(0, 1, [1]).astype(np.float32) # A2
&gt;&gt;&gt; w1 = Parameter(initializer("uniform", [2, 2], ms.float32), name="w1") # W1
&gt;&gt;&gt; w1 = Parameter(initializer("uniform", [2, 2], ms.float32), name="w1") # W2
&gt;&gt;&gt;
&gt;&gt;&gt; # 3. If neither global seed nor op seed is set, mindspore.ops.composite.random_ops and
&gt;&gt;&gt; # mindspore.nn.probability.distribution will choose a random seed:
&gt;&gt;&gt; c1 = ops.uniform((1, 4), minval, maxval) # C1
&gt;&gt;&gt; c2 = ops.uniform((1, 4), minval, maxval) # C2
&gt;&gt;&gt; # Rerun the program will get different results:
&gt;&gt;&gt; c1 = ops.uniform((1, 4), minval, maxval) # C3
&gt;&gt;&gt; c2 = ops.uniform((1, 4), minval, maxval) # C4
&gt;&gt;&gt;
&gt;&gt;&gt; # 4. If global seed is set, but op seed is not set, mindspore.ops.composite.random_ops and
&gt;&gt;&gt; # mindspore.nn.probability.distribution will calculate a seed according to global seed and
&gt;&gt;&gt; # default op seed. Each call will change the default op seed, thus each call get different
&gt;&gt;&gt; # results.
&gt;&gt;&gt; set_seed(1234)
&gt;&gt;&gt; c1 = ops.uniform((1, 4), minval, maxval) # C1
&gt;&gt;&gt; c2 = ops.uniform((1, 4), minval, maxval) # C2
&gt;&gt;&gt; # Rerun the program will get the same results:
&gt;&gt;&gt; set_seed(1234)
&gt;&gt;&gt; c1 = ops.uniform((1, 4), minval, maxval) # C1
&gt;&gt;&gt; c2 = ops.uniform((1, 4), minval, maxval) # C2
&gt;&gt;&gt;
&gt;&gt;&gt; # 5. If both global seed and op seed are set, mindspore.ops.composite.random_ops and
&gt;&gt;&gt; # mindspore.nn.probability.distribution will calculate a seed according to global seed and
&gt;&gt;&gt; # op seed counter. Each call will change the op seed counter, thus each call get different
&gt;&gt;&gt; # results.
&gt;&gt;&gt; set_seed(1234)
&gt;&gt;&gt; c1 = ops.uniform((1, 4), minval, maxval, seed=2) # C1
&gt;&gt;&gt; c2 = ops.uniform((1, 4), minval, maxval, seed=2) # C2
&gt;&gt;&gt; # Rerun the program will get the same results:
&gt;&gt;&gt; set_seed(1234)
&gt;&gt;&gt; c1 = ops.uniform((1, 4), minval, maxval, seed=2) # C1
&gt;&gt;&gt; c2 = ops.uniform((1, 4), minval, maxval, seed=2) # C2
&gt;&gt;&gt;
&gt;&gt;&gt; # 6. If op seed is set but global seed is not set, 0 will be used as global seed. Then
&gt;&gt;&gt; # mindspore.ops.composite.random_ops and mindspore.nn.probability.distribution act as in
&gt;&gt;&gt; # condition 5.
&gt;&gt;&gt; c1 = ops.uniform((1, 4), minval, maxval, seed=2) # C1
&gt;&gt;&gt; c2 = ops.uniform((1, 4), minval, maxval, seed=2) # C2
&gt;&gt;&gt; # Rerun the program will get the same results:
&gt;&gt;&gt; c1 = ops.uniform((1, 4), minval, maxval, seed=2) # C1
&gt;&gt;&gt; c2 = ops.uniform((1, 4), minval, maxval, seed=2) # C2
&gt;&gt;&gt;
&gt;&gt;&gt; # 7. Recall set_seed() in the program will reset numpy seed and op seed counter of
&gt;&gt;&gt; # mindspore.ops.composite.random_ops and mindspore.nn.probability.distribution.
&gt;&gt;&gt; set_seed(1234)
&gt;&gt;&gt; np_1 = np.random.normal(0, 1, [1]).astype(np.float32) # A1
&gt;&gt;&gt; c1 = ops.uniform((1, 4), minval, maxval, seed=2) # C1
&gt;&gt;&gt; set_seed(1234)
&gt;&gt;&gt; np_2 = np.random.normal(0, 1, [1]).astype(np.float32) # still get A1
&gt;&gt;&gt; c2 = ops.uniform((1, 4), minval, maxval, seed=2) # still get C1</code></pre><blockquote>
<p><code>mindspore.get_seed</code>()<a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/_modules/mindspore/common/seed.html#get_seed" target="_blank" rel="noopener">[source]</a></p>
</blockquote>
<p>Get global seed.</p>
<ul>
<li><p>Returns</p>
<p>Integer. The global seed.</p>
</li>
</ul>
<h5 id="classmindspore-Model-network-loss-fn-None-optimizer-None-metrics-None-eval-network-None-eval-indexes-None-amp-level-â€O0â€-boost-level-â€O0â€-kwargs-source"><a href="#classmindspore-Model-network-loss-fn-None-optimizer-None-metrics-None-eval-network-None-eval-indexes-None-amp-level-â€O0â€-boost-level-â€O0â€-kwargs-source" class="headerlink" title="classmindspore.Model(network, loss_fn=None, optimizer=None, metrics=None, eval_network=None, eval_indexes=None, amp_level=â€O0â€, boost_level=â€O0â€, **kwargs)[source]"></a><em>class<em><code>mindspore.Model</code>(</em>network</em>, <em>loss_fn=None</em>, <em>optimizer=None</em>, <em>metrics=None</em>, <em>eval_network=None</em>, <em>eval_indexes=None</em>, <em>amp_level=â€O0â€</em>, <em>boost_level=â€O0â€</em>, <em>**kwargs</em>)<a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/_modules/mindspore/train/model.html#Model" target="_blank" rel="noopener">[source]</a></h5><p>High-Level API for Training or Testing.</p>
<p>Model groups layers into an object with training and inference features.</p>
<ul>
<li><p>Parameters</p>
<p><strong>network</strong> (<a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/api_python/nn/mindspore.nn.Cell.html#mindspore.nn.Cell" target="_blank" rel="noopener"><em>Cell</em></a>) â€“ A training or testing network.</p>
<p><strong>loss_fn</strong> (<a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/api_python/nn/mindspore.nn.Cell.html#mindspore.nn.Cell" target="_blank" rel="noopener"><em>Cell</em></a>) â€“ Objective function, if loss_fn is None, the network should contain the logic of loss and grads calculation, and the logic of parallel if needed. Default: None.</p>
<p><strong>optimizer</strong> (<a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/api_python/nn/mindspore.nn.Cell.html#mindspore.nn.Cell" target="_blank" rel="noopener"><em>Cell</em></a>) â€“ Optimizer for updating the weights. Default: None.</p>
<p><strong>metrics</strong> (<em>Union**[</em><a href="https://docs.python.org/library/stdtypes.html#dict" target="_blank" rel="noopener"><em>dict</em></a><em>,</em> <a href="https://docs.python.org/library/stdtypes.html#set" target="_blank" rel="noopener"><em>set</em></a><em>]</em>) â€“ A Dictionary or a set of metrics to be evaluated by the model during training and testing. eg: {â€˜accuracyâ€™, â€˜recallâ€™}. Default: None.</p>
<p><strong>eval_network</strong> (<a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/api_python/nn/mindspore.nn.Cell.html#mindspore.nn.Cell" target="_blank" rel="noopener"><em>Cell</em></a>) â€“ Network for evaluation. If not defined, network and loss_fn would be wrapped as eval_network . Default: None.</p>
<p><strong>eval_indexes</strong> (<a href="https://docs.python.org/library/stdtypes.html#list" target="_blank" rel="noopener"><em>list</em></a>) â€“ When defining the eval_network, if eval_indexes is None, all outputs of the eval_network would be passed to metrics, otherwise eval_indexes must contain three elements, including the positions of loss value, predicted value and label. The loss value would be passed to the Loss metric, the predicted value and label would be passed to other metric. Default: None.</p>
<p><strong>amp_level</strong> (<a href="https://docs.python.org/library/stdtypes.html#str" target="_blank" rel="noopener"><em>str</em></a>) â€“Option for argument level in mindspore.amp.build_train_network , level for mixed precision training. Supports [â€œO0â€, â€œO2â€, â€œO3â€, â€œautoâ€]. Default: â€œO0â€.O0: Do not change.O2: Cast network to float16, keep batchnorm run in float32, using dynamic loss scale.O3: Cast network to float16, with additional property keep_batchnorm_fp32=False .auto: Set to level to recommended level in different devices. Set level to O2 on GPU, Set level to O3 Ascend. The recommended level is choose by the export experience, cannot always general. User should specify the level for special network.O2 is recommended on GPU, O3 is recommended on Ascend.The more detailed explanation of amp_level setting can be found at mindspore.amp.build_train_network .</p>
<p><strong>boost_level</strong> (<a href="https://docs.python.org/library/stdtypes.html#str" target="_blank" rel="noopener"><em>str</em></a>) â€“Option for argument level in mindspore.boost , level for boost mode training. Supports [â€œO0â€, â€œO1â€, â€œO2â€]. Default: â€œO0â€.O0: Do not change.O1: Enable the boost mode, the performance is improved by about 20%, and the accuracy is the same as the original accuracy.O2: Enable the boost mode, the performance is improved by about 30%, and the accuracy is reduced by less than 3%.</p>
</li>
</ul>
<p><strong>Examples</strong></p>
<pre><code>&gt;&gt;&gt; from mindspore import Model, nn
&gt;&gt;&gt;
&gt;&gt;&gt; class Net(nn.Cell):
...     def __init__(self, num_class=10, num_channel=1):
...         super(Net, self).__init__()
...         self.conv1 = nn.Conv2d(num_channel, 6, 5, pad_mode='valid')
...         self.conv2 = nn.Conv2d(6, 16, 5, pad_mode='valid')
...         self.fc1 = nn.Dense(16*5*5, 120, weight_init='ones')
...         self.fc2 = nn.Dense(120, 84, weight_init='ones')
...         self.fc3 = nn.Dense(84, num_class, weight_init='ones')
...         self.relu = nn.ReLU()
...         self.max_pool2d = nn.MaxPool2d(kernel_size=2, stride=2)
...         self.flatten = nn.Flatten()
...
...     def construct(self, x):
...         x = self.max_pool2d(self.relu(self.conv1(x)))
...         x = self.max_pool2d(self.relu(self.conv2(x)))
...         x = self.flatten(x)
...         x = self.relu(self.fc1(x))
...         x = self.relu(self.fc2(x))
...         x = self.fc3(x)
...         return x
&gt;&gt;&gt;
&gt;&gt;&gt; net = Net()
&gt;&gt;&gt; loss = nn.SoftmaxCrossEntropyWithLogits()
&gt;&gt;&gt; optim = nn.Momentum(params=net.trainable_params(), learning_rate=0.1, momentum=0.9)
&gt;&gt;&gt; model = Model(net, loss_fn=loss, optimizer=optim, metrics=None)
&gt;&gt;&gt; # For details about how to build the dataset, please refer to the tutorial
&gt;&gt;&gt; # document on the official website.
&gt;&gt;&gt; dataset = create_custom_dataset()
&gt;&gt;&gt; model.train(2, dataset)</code></pre><blockquote>
<p><code>build</code>(<em>train_dataset=None</em>, <em>valid_dataset=None</em>, <em>sink_size=-1</em>)<a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/_modules/mindspore/train/model.html#Model.build" target="_blank" rel="noopener">[source]</a></p>
</blockquote>
<p>Build computational graphs and data graphs with the sink mode.</p>
<blockquote>
<p>Warning</p>
<p>This is an experimental prototype that is subject to change and/or deletion.</p>
<p>Note</p>
<p>Pre-build process only supports GRAPH_MODE and Ascend target currently. The interface builds the computational graphs, when the interface is executed first, â€˜model.trainâ€™ only performs the graphs execution. It only support dataset sink mode.</p>
</blockquote>
<ul>
<li><p>Parameters</p>
<p><strong>train_dataset</strong> (<em>Dataset</em>) â€“ A training dataset iterator. If train_dataset is defined, training graphs will be initialized. Default: None.<strong>valid_dataset</strong> (<em>Dataset</em>) â€“ An evaluating dataset iterator. If valid_dataset is defined, evaluation graphs will be initialized, and metrics in Model can not be None. Default: None.<strong>sink_size</strong> (<a href="https://docs.python.org/library/functions.html#int" target="_blank" rel="noopener"><em>int</em></a>) â€“ Control the amount of data in each sink. Default: -1.</p>
</li>
</ul>
<p><strong>Examples</strong></p>
<pre><code>&gt;&gt;&gt; from mindspore import Model, nn, FixedLossScaleManager
&gt;&gt;&gt;
&gt;&gt;&gt; # For details about how to build the dataset, please refer to the tutorial
&gt;&gt;&gt; # document on the official website.
&gt;&gt;&gt; dataset = create_custom_dataset()
&gt;&gt;&gt; net = Net()
&gt;&gt;&gt; loss = nn.SoftmaxCrossEntropyWithLogits()
&gt;&gt;&gt; loss_scale_manager = FixedLossScaleManager()
&gt;&gt;&gt; optim = nn.Momentum(params=net.trainable_params(), learning_rate=0.1, momentum=0.9)
&gt;&gt;&gt; model = Model(net, loss_fn=loss, optimizer=optim, metrics=None, loss_scale_manager=loss_scale_manager)
&gt;&gt;&gt; model.build(dataset)
&gt;&gt;&gt; model.train(2, dataset)</code></pre><blockquote>
<p><code>eval</code>(<em>valid_dataset</em>, <em>callbacks=None</em>, <em>dataset_sink_mode=True</em>)<a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/_modules/mindspore/train/model.html#Model.eval" target="_blank" rel="noopener">[source]</a></p>
</blockquote>
<p>Evaluation API where the iteration is controlled by python front-end.</p>
<p>Configure to pynative mode or CPU, the evaluating process will be performed with dataset non-sink mode.</p>
<blockquote>
<p>Note</p>
<p>If dataset_sink_mode is True, data will be sent to device. If device is Ascend, features of data will be transferred one by one. The limitation of data transmission per time is 256M. When dataset_sink_mode is True, step_end method of the Callback class will be executed when the epoch_end method is called.</p>
</blockquote>
<ul>
<li><p>Parameters</p>
<p><strong>valid_dataset</strong> (<em>Dataset</em>) â€“ Dataset to evaluate the model.</p>
<p><strong>callbacks</strong> (<em>Optional**[</em><a href="https://docs.python.org/library/stdtypes.html#list" target="_blank" rel="noopener"><em>list</em></a><em>(</em><a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/api_python/mindspore.train.html#mindspore.train.callback.Callback" target="_blank" rel="noopener"><em>Callback</em></a><em>)**]</em>) â€“ List of callback objects which should be executed while training. Default: None.</p>
<p><strong>dataset_sink_mode</strong> (<a href="https://docs.python.org/library/functions.html#bool" target="_blank" rel="noopener"><em>bool</em></a>) â€“ Determines whether to pass the data through dataset channel. Default: True.</p>
</li>
<li><p>Returns</p>
<p>Dict, which returns the loss value and metrics values for the model in the test mode.</p>
</li>
</ul>
<p><strong>Examples</strong></p>
<pre><code>&gt;&gt;&gt; from mindspore import Model, nn
&gt;&gt;&gt;
&gt;&gt;&gt; # For details about how to build the dataset, please refer to the tutorial
&gt;&gt;&gt; # document on the official website.
&gt;&gt;&gt; dataset = create_custom_dataset()
&gt;&gt;&gt; net = Net()
&gt;&gt;&gt; loss = nn.SoftmaxCrossEntropyWithLogits()
&gt;&gt;&gt; model = Model(net, loss_fn=loss, optimizer=None, metrics={'acc'})
&gt;&gt;&gt; acc = model.eval(dataset, dataset_sink_mode=False)</code></pre><blockquote>
<p><em>property</em><code>eval_network</code></p>
</blockquote>
<p>Get the modelâ€™s eval_network.</p>
<blockquote>
<p><code>infer_predict_layout</code>(<em>*predict_data</em>)<a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/_modules/mindspore/train/model.html#Model.infer_predict_layout" target="_blank" rel="noopener">[source]</a></p>
</blockquote>
<p>Generate parameter layout for the predict network in auto or semi auto parallel mode.</p>
<p>Data could be a single tensor or multiple tensors.</p>
<blockquote>
<p>Note</p>
<p>Batch data should be put together in one tensor.</p>
</blockquote>
<ul>
<li><p>Parameters</p>
<p><strong>predict_data</strong> (<a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/api_python/mindspore/mindspore.Tensor.html#mindspore.Tensor" target="_blank" rel="noopener"><em>Tensor</em></a>) â€“ One tensor or multiple tensors of predict data.</p>
</li>
<li><p>Returns</p>
<p>Dict, Parameter layout dictionary used for load distributed checkpoint.</p>
</li>
<li><p>Raises</p>
<p><a href="https://docs.python.org/library/exceptions.html#RuntimeError" target="_blank" rel="noopener"><strong>RuntimeError</strong></a> â€“ If get_context is not GRAPH_MODE.</p>
</li>
</ul>
<p>Examples</p>
<pre><code>&gt;&gt;&gt; # This example should be run with multiple devices. Refer to the tutorial &gt; Distributed Training on
&gt;&gt;&gt; # mindspore.cn.
&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; import mindspore as ms
&gt;&gt;&gt; from mindspore import Model, context, Tensor
&gt;&gt;&gt; from mindspore.context import ParallelMode
&gt;&gt;&gt; from mindspore.communication import init
&gt;&gt;&gt;
&gt;&gt;&gt; context.set_context(mode=context.GRAPH_MODE)
&gt;&gt;&gt; init()
&gt;&gt;&gt; context.set_auto_parallel_context(full_batch=True, parallel_mode=ParallelMode.SEMI_AUTO_PARALLEL)
&gt;&gt;&gt; input_data = Tensor(np.random.randint(0, 255, [1, 1, 32, 32]), ms.float32)
&gt;&gt;&gt; model = Model(Net())
&gt;&gt;&gt; predict_map = model.infer_predict_layout(input_data)</code></pre><blockquote>
<p><code>infer_train_layout</code>(<em>train_dataset</em>, <em>dataset_sink_mode=True</em>, <em>sink_size=-1</em>)<a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/_modules/mindspore/train/model.html#Model.infer_train_layout" target="_blank" rel="noopener">[source]</a></p>
</blockquote>
<p>Generate parameter layout for the train network in auto or semi auto parallel mode. Only dataset sink mode is supported for now.</p>
<blockquote>
<p>Warning</p>
<p>This is an experimental prototype that is subject to change and/or deletion.</p>
<p>Note</p>
<p>This is a pre-compile function. The arguments should be the same with model.train() function.</p>
</blockquote>
<ul>
<li><p>Parameters</p>
<p><strong>train_dataset</strong> (<em>Dataset</em>) â€“ A training dataset iterator. If there is no loss_fn, a tuple with multiple data (data1, data2, data3, â€¦) should be returned and passed to the network. Otherwise, a tuple (data, label) should be returned. The data and label would be passed to the network and loss function respectively.</p>
<p><strong>dataset_sink_mode</strong> (<a href="https://docs.python.org/library/functions.html#bool" target="_blank" rel="noopener"><em>bool</em></a>) â€“ Determines whether to pass the data through dataset channel. Default: True. Configure pynative mode or CPU, the training process will be performed with dataset not sink. Default: True.</p>
<p><strong>sink_size</strong> (<a href="https://docs.python.org/library/functions.html#int" target="_blank" rel="noopener"><em>int</em></a>) â€“ Control the amount of data in each sink. If sink_size = -1, sink the complete dataset for each epoch. If sink_size &gt; 0, sink sink_size data for each epoch. If dataset_sink_mode is False, set sink_size as invalid. Default: -1.</p>
</li>
<li><p>Returns</p>
<p>Dict, Parameter layout dictionary used for load distributed checkpoint</p>
</li>
</ul>
<p><strong>Examples</strong></p>
<pre><code>&gt;&gt;&gt; # This example should be run with multiple devices. Refer to the tutorial &gt; Distributed Training on
&gt;&gt;&gt; # mindspore.cn.
&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; import mindspore as ms
&gt;&gt;&gt; from mindspore import Model, context, Tensor, nn, FixedLossScaleManager
&gt;&gt;&gt; from mindspore.context import ParallelMode
&gt;&gt;&gt; from mindspore.communication import init
&gt;&gt;&gt;
&gt;&gt;&gt; context.set_context(mode=context.GRAPH_MODE)
&gt;&gt;&gt; init()
&gt;&gt;&gt; context.set_auto_parallel_context(parallel_mode=ParallelMode.SEMI_AUTO_PARALLEL)
&gt;&gt;&gt;
&gt;&gt;&gt; # For details about how to build the dataset, please refer to the tutorial
&gt;&gt;&gt; # document on the official website.
&gt;&gt;&gt; dataset = create_custom_dataset()
&gt;&gt;&gt; net = Net()
&gt;&gt;&gt; loss = nn.SoftmaxCrossEntropyWithLogits()
&gt;&gt;&gt; loss_scale_manager = FixedLossScaleManager()
&gt;&gt;&gt; optim = nn.Momentum(params=net.trainable_params(), learning_rate=0.1, momentum=0.9)
&gt;&gt;&gt; model = Model(net, loss_fn=loss, optimizer=optim, metrics=None, loss_scale_manager=loss_scale_manager)
&gt;&gt;&gt; layout_dict = model.infer_train_layout(dataset)</code></pre><blockquote>
<p><code>predict</code>(<em>*predict_data</em>)<a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/_modules/mindspore/train/model.html#Model.predict" target="_blank" rel="noopener">[source]</a></p>
</blockquote>
<p>Generate output predictions for the input samples.</p>
<p>Data could be a single tensor, a list of tensor, or a tuple of tensor.</p>
<p>Note</p>
<p>This is a pre-compile function. The arguments should be the same with model.predict() function.</p>
<ul>
<li><p>Parameters</p>
<p><strong>predict_data</strong> (<a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/api_python/mindspore/mindspore.Tensor.html#mindspore.Tensor" target="_blank" rel="noopener"><em>Tensor</em></a>) â€“ The predict data, can be bool, int, float, str, None, tensor, or tuple, list and dict that store these types.</p>
</li>
<li><p>Returns</p>
<p>Tensor, array(s) of predictions.</p>
</li>
</ul>
<p><strong>Examples</strong></p>
<pre><code>&gt;&gt;&gt; import mindspore as ms
&gt;&gt;&gt; from mindspore import Model, Tensor
&gt;&gt;&gt;
&gt;&gt;&gt; input_data = Tensor(np.random.randint(0, 255, [1, 1, 32, 32]), ms.float32)
&gt;&gt;&gt; model = Model(Net())
&gt;&gt;&gt; result = model.predict(input_data)</code></pre><blockquote>
<p><em>property</em><code>predict_network</code></p>
</blockquote>
<p>Get the modelâ€™s predict_network.</p>
<blockquote>
<p><code>train</code>(<em>epoch</em>, <em>train_dataset</em>, <em>callbacks=None</em>, <em>dataset_sink_mode=True</em>, <em>sink_size=-1</em>)<a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/_modules/mindspore/train/model.html#Model.train" target="_blank" rel="noopener">[source]</a></p>
</blockquote>
<p>Training API where the iteration is controlled by python front-end.</p>
<p>When setting pynative mode or CPU, the training process will be performed with dataset not sink.</p>
<blockquote>
<p>Note</p>
<p>If dataset_sink_mode is True, data will be sent to device. If device is Ascend, features of data will be transferred one by one. The limitation of data transmission per time is 256M. When dataset_sink_mode is True, step_end method of the Callback class will be executed when the epoch_end method is called. If sink_size &gt; 0, each epoch the dataset can be traversed unlimited times until you get sink_size elements of the dataset. Next epoch continues to traverse from the end position of the previous traversal. The interface builds the computational graphs and then executes the computational graphs. However, when the â€˜model.buildâ€™ is executed first, it only performs the graphs execution.</p>
</blockquote>
<ul>
<li><p>Parameters</p>
<p><strong>epoch</strong> (<a href="https://docs.python.org/library/functions.html#int" target="_blank" rel="noopener"><em>int</em></a>) â€“ Generally, total number of iterations on the data per epoch. When dataset_sink_mode is set to true and sink_size&gt;0, each epoch sink sink_size steps on the data instead of total number of iterations.</p>
<p><strong>train_dataset</strong> (<em>Dataset</em>) â€“ A training dataset iterator. If there is no loss_fn, a tuple with multiple data (data1, data2, data3, â€¦) should be returned and passed to the network. Otherwise, a tuple (data, label) should be returned. The data and label would be passed to the network and loss function respectively.</p>
<p><strong>callbacks</strong> (<em>Optional**[</em><a href="https://docs.python.org/library/stdtypes.html#list" target="_blank" rel="noopener"><em>list</em></a><em>[*<a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/api_python/mindspore.train.html#mindspore.train.callback.Callback" target="_blank" rel="noopener"><em>Callback</em></a><em>]*</em>,</em> <a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/api_python/mindspore.train.html#mindspore.train.callback.Callback" target="_blank" rel="noopener"><em>Callback</em></a><em>]</em>) â€“ List of callback objects or callback object, which should be executed while training. Default: None.</p>
<p><strong>dataset_sink_mode</strong> (<a href="https://docs.python.org/library/functions.html#bool" target="_blank" rel="noopener"><em>bool</em></a>) â€“ Determines whether to pass the data through dataset channel. Default: True. Configure pynative mode or CPU, the training process will be performed with dataset not sink. Default: True.</p>
<p><strong>sink_size</strong> (<a href="https://docs.python.org/library/functions.html#int" target="_blank" rel="noopener"><em>int</em></a>) â€“ Control the amount of data in each sink. If sink_size = -1, sink the complete dataset for each epoch. If sink_size &gt; 0, sink sink_size data for each epoch. If dataset_sink_mode is False, set sink_size as invalid. Default: -1.</p>
</li>
</ul>
<p><strong>Examples</strong></p>
<pre><code>&gt;&gt;&gt; from mindspore import Model, nn, FixedLossScaleManager
&gt;&gt;&gt;
&gt;&gt;&gt; # For details about how to build the dataset, please refer to the tutorial
&gt;&gt;&gt; # document on the official website.
&gt;&gt;&gt; dataset = create_custom_dataset()
&gt;&gt;&gt; net = Net()
&gt;&gt;&gt; loss = nn.SoftmaxCrossEntropyWithLogits()
&gt;&gt;&gt; loss_scale_manager = FixedLossScaleManager()
&gt;&gt;&gt; optim = nn.Momentum(params=net.trainable_params(), learning_rate=0.1, momentum=0.9)
&gt;&gt;&gt; model = Model(net, loss_fn=loss, optimizer=optim, metrics=None, loss_scale_manager=loss_scale_manager)
&gt;&gt;&gt; model.train(2, dataset)</code></pre><blockquote>
<p><em>property</em><code>train_network</code></p>
</blockquote>
<p>Get the modelâ€™s train_network.</p>
<h5 id="classmindspore-DatasetHelper-dataset-dataset-sink-mode-True-sink-size-1-epoch-num-1-source"><a href="#classmindspore-DatasetHelper-dataset-dataset-sink-mode-True-sink-size-1-epoch-num-1-source" class="headerlink" title="classmindspore.DatasetHelper(dataset, dataset_sink_mode=True, sink_size=-1, epoch_num=1)[source]"></a><em>class<em><code>mindspore.DatasetHelper</code>(</em>dataset</em>, <em>dataset_sink_mode=True</em>, <em>sink_size=-1</em>, <em>epoch_num=1</em>)<a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/_modules/mindspore/train/dataset_helper.html#DatasetHelper" target="_blank" rel="noopener">[source]</a></h5><p>DatasetHelper is a class to process the MindData dataset and it provides the information of dataset.</p>
<p>According to different contexts, change the iterations of dataset and use the same iteration for loop in different contexts.</p>
<blockquote>
<p>Note</p>
<p>The iteration of DatasetHelper will provide one epoch data.</p>
</blockquote>
<ul>
<li><p>Parameters</p>
<p><strong>dataset</strong> (<em>Dataset</em>) â€“ The training dataset iterator. The dataset can be generated by dataset generator API in <a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/api_python/mindspore.dataset.html#module-mindspore.dataset" target="_blank" rel="noopener"><code>mindspore.dataset</code></a>, such as <a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/api_python/dataset/mindspore.dataset.ImageFolderDataset.html#mindspore.dataset.ImageFolderDataset" target="_blank" rel="noopener"><code>mindspore.dataset.ImageFolderDataset</code></a>.</p>
<p><strong>dataset_sink_mode</strong> (<a href="https://docs.python.org/library/functions.html#bool" target="_blank" rel="noopener"><em>bool</em></a>) â€“ If true use GetNext to fetch the data, or else feed the data from host. Default: True.</p>
<p><strong>sink_size</strong> (<a href="https://docs.python.org/library/functions.html#int" target="_blank" rel="noopener"><em>int</em></a>) â€“ Control the amount of data in each sink. If sink_size=-1, sink the complete dataset for each epoch. If sink_size&gt;0, sink sink_size data for each epoch. Default: -1.</p>
<p><strong>epoch_num</strong> (<a href="https://docs.python.org/library/functions.html#int" target="_blank" rel="noopener"><em>int</em></a>) â€“ Control the number of epoch data to send. Default: 1.</p>
</li>
</ul>
<p><strong>Examples</strong></p>
<pre><code>&gt;&gt;&gt; from mindspore import DatasetHelper
&gt;&gt;&gt;
&gt;&gt;&gt; train_dataset = create_custom_dataset()
&gt;&gt;&gt; set_helper = DatasetHelper(train_dataset, dataset_sink_mode=False)
&gt;&gt;&gt; # Object of DatasetHelper is iterable
&gt;&gt;&gt; for next_element in set_helper:
...     next_element</code></pre><blockquote>
<p><code>continue_send</code>()<a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/_modules/mindspore/train/dataset_helper.html#DatasetHelper.continue_send" target="_blank" rel="noopener">[source]</a></p>
</blockquote>
<p>Continue send data to device at the beginning of epoch.</p>
<blockquote>
<p><code>dynamic_min_max_shapes</code>()<a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/_modules/mindspore/train/dataset_helper.html#DatasetHelper.dynamic_min_max_shapes" target="_blank" rel="noopener">[source]</a></p>
</blockquote>
<p>Get shape range(min shape, max shape) of dynamic data.</p>
<blockquote>
<p><code>get_data_info</code>()<a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/_modules/mindspore/train/dataset_helper.html#DatasetHelper.get_data_info" target="_blank" rel="noopener">[source]</a></p>
</blockquote>
<p>Get the types and shape of current batch.</p>
<blockquote>
<p><code>release</code>()<a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/_modules/mindspore/train/dataset_helper.html#DatasetHelper.release" target="_blank" rel="noopener">[source]</a></p>
</blockquote>
<p>Free up resources about data sink.</p>
<blockquote>
<p><code>sink_size</code>()<a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/_modules/mindspore/train/dataset_helper.html#DatasetHelper.sink_size" target="_blank" rel="noopener">[source]</a></p>
</blockquote>
<p>Get sink_size for each iteration.</p>
<blockquote>
<p><code>stop_send</code>()<a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/_modules/mindspore/train/dataset_helper.html#DatasetHelper.stop_send" target="_blank" rel="noopener">[source]</a></p>
</blockquote>
<p>stop send data about data sink.</p>
<blockquote>
<p><code>types_shapes</code>()<a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/_modules/mindspore/train/dataset_helper.html#DatasetHelper.types_shapes" target="_blank" rel="noopener">[source]</a></p>
</blockquote>
<p>Get the types and shapes from dataset on the current configuration.</p>
<h5 id="mindspore-connect-network-with-dataset-network-dataset-helper-source"><a href="#mindspore-connect-network-with-dataset-network-dataset-helper-source" class="headerlink" title="mindspore.connect_network_with_dataset(network, dataset_helper)[source]"></a><code>mindspore.connect_network_with_dataset</code>(<em>network</em>, <em>dataset_helper</em>)<a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/_modules/mindspore/train/dataset_helper.html#connect_network_with_dataset" target="_blank" rel="noopener">[source]</a></h5><p>Connect the network with dataset in dataset_helper.</p>
<p>This function wraps the input network with â€˜GetNextâ€™ so that the data can be fetched automatically from the data channel corresponding to the â€˜queue_nameâ€™ and passed to the input network during forward computation.</p>
<blockquote>
<p>Note</p>
<p>In the case of running the network on Ascend/GPU in graph mode, this function will wrap the input network with â€˜GetNextâ€™, in other cases, the input network will be returned with no change. The â€˜GetNextâ€™ is required to get data only in sink mode, so this function is not applicable to no-sink mode.</p>
</blockquote>
<ul>
<li><p>Parameters</p>
<p><strong>network</strong> (<a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/api_python/nn/mindspore.nn.Cell.html#mindspore.nn.Cell" target="_blank" rel="noopener"><em>Cell</em></a>) â€“ The training network for dataset.</p>
<p><strong>dataset_helper</strong> (<a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/api_python/mindspore/mindspore.DatasetHelper.html#mindspore.DatasetHelper" target="_blank" rel="noopener"><em>DatasetHelper</em></a>) â€“ A class to process the MindData dataset, it provides the type, shape and queue name of the dataset to wrap the GetNext.</p>
</li>
<li><p>Returns</p>
<p>Cell, a new network wrapped with â€˜GetNextâ€™ in the case of running the task on Ascend in graph mode, otherwise it is the input network.</p>
</li>
<li><p>Supported Platforms:</p>
<p><code>Ascend</code> <code>GPU</code></p>
</li>
</ul>
<p><strong>Examples</strong></p>
<pre><code>&gt;&gt;&gt; from mindspore import DatasetHelper
&gt;&gt;&gt;
&gt;&gt;&gt; # call create_dataset function to create a regular dataset, refer to mindspore.dataset
&gt;&gt;&gt; train_dataset = create_custom_dataset()
&gt;&gt;&gt; dataset_helper = DatasetHelper(train_dataset, dataset_sink_mode=True)
&gt;&gt;&gt; net = Net()
&gt;&gt;&gt; net_with_get_next = connect_network_with_dataset(net, dataset_helper)</code></pre><h5 id="mindspore-build-train-network-network-optimizer-loss-fn-None-level-â€O0â€-boost-level-â€O0â€-kwargs-source"><a href="#mindspore-build-train-network-network-optimizer-loss-fn-None-level-â€O0â€-boost-level-â€O0â€-kwargs-source" class="headerlink" title="mindspore.build_train_network(network, optimizer, loss_fn=None, level=â€O0â€, boost_level=â€O0â€, **kwargs)[source]"></a><code>mindspore.build_train_network</code>(<em>network</em>, <em>optimizer</em>, <em>loss_fn=None</em>, <em>level=â€O0â€</em>, <em>boost_level=â€O0â€</em>, <em>**kwargs</em>)<a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/_modules/mindspore/train/amp.html#build_train_network" target="_blank" rel="noopener">[source]</a></h5><p>Build the mixed precision training cell automatically.</p>
<ul>
<li><p>Parameters</p>
<p><strong>network</strong> (<a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/api_python/nn/mindspore.nn.Cell.html#mindspore.nn.Cell" target="_blank" rel="noopener"><em>Cell</em></a>) â€“ Definition of the network.</p>
<p><strong>loss_fn</strong> (<em>Union**[</em><a href="https://docs.python.org/library/constants.html#None" target="_blank" rel="noopener"><em>None</em></a><em>,</em> <a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/api_python/nn/mindspore.nn.Cell.html#mindspore.nn.Cell" target="_blank" rel="noopener"><em>Cell</em></a><em>]</em>) â€“ Definition of the loss_fn. If None, the network should have the loss inside. Default: None.<strong>optimizer</strong> (<a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/api_python/nn/mindspore.nn.Optimizer.html#mindspore.nn.Optimizer" target="_blank" rel="noopener"><em>Optimizer</em></a>) â€“ Optimizer to update the Parameter.</p>
<p><strong>level</strong> (<a href="https://docs.python.org/library/stdtypes.html#str" target="_blank" rel="noopener"><em>str</em></a>) â€“Supports [â€œO0â€, â€œO2â€, â€œO3â€, â€œautoâ€]. Default: â€œO0â€.O0: Do not change.O2: Cast network to float16, keep batchnorm and loss_fn (if set) run in float32, using dynamic loss scale.O3: Cast network to float16, with additional property keep_batchnorm_fp32=False .auto: Set to level to recommended level in different devices. Set level to O2 on GPU, Set level to O3 Ascend. The recommended level is choose by the export experience, cannot always general. User should specify the level for special network.O2 is recommended on GPU, O3 is recommended on Ascend.Property of keep_batchnorm_fp32 , cast_model_type and loss_scale_manager determined by level setting may be overwritten by settings in kwargs .</p>
<p><strong>cast_model_type</strong> (<a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/api_python/mindspore.html#mindspore.dtype" target="_blank" rel="noopener"><code>mindspore.dtype</code></a>) â€“ Supports mstype.float16 or mstype.float32 . If set, the network will be casted to cast_model_type ( mstype.float16 or mstype.float32 ), but not to be casted to the type determined by level setting.</p>
<p><strong>keep_batchnorm_fp32</strong> (<a href="https://docs.python.org/library/functions.html#bool" target="_blank" rel="noopener"><em>bool</em></a>) â€“ Keep Batchnorm run in float32 when the network is set to cast to float16 . If set, the level setting will take no effect on this property.</p>
<p><strong>loss_scale_manager</strong> (<em>Union**[</em><a href="https://docs.python.org/library/constants.html#None" target="_blank" rel="noopener"><em>None</em></a><em>,</em> <a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/api_python/mindspore/mindspore.LossScaleManager.html#mindspore.LossScaleManager" target="_blank" rel="noopener"><em>LossScaleManager</em></a><em>]</em>) â€“ If None, not scale the loss, otherwise scale the loss by LossScaleManager . If set, the level setting will take no effect on this property.</p>
</li>
<li><p>Raises</p>
<p><a href="https://docs.python.org/library/exceptions.html#ValueError" target="_blank" rel="noopener"><strong>ValueError</strong></a> â€“ Auto mixed precision only supported on device GPU and Ascend. If device is CPU, a ValueError exception will be raised.</p>
<p><a href="https://docs.python.org/library/exceptions.html#ValueError" target="_blank" rel="noopener"><strong>ValueError</strong></a> â€“ If device is CPU, property loss_scale_manager only can be set as None or FixedLossScaleManager (with property drop_overflow_update=False ), or a ValueError exception will be raised.</p>
</li>
</ul>
<h5 id="classmindspore-LossScaleManager-source"><a href="#classmindspore-LossScaleManager-source" class="headerlink" title="classmindspore.LossScaleManager[source]"></a><em>class</em><code>mindspore.LossScaleManager</code><a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/_modules/mindspore/train/loss_scale_manager.html#LossScaleManager" target="_blank" rel="noopener">[source]</a></h5><p>Loss scale manager abstract class.</p>
<blockquote>
<p><code>get_loss_scale</code>()<a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/_modules/mindspore/train/loss_scale_manager.html#LossScaleManager.get_loss_scale" target="_blank" rel="noopener">[source]</a></p>
</blockquote>
<p>Get loss scale value.</p>
<blockquote>
<p><code>get_update_cell</code>()<a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/_modules/mindspore/train/loss_scale_manager.html#LossScaleManager.get_update_cell" target="_blank" rel="noopener">[source]</a></p>
</blockquote>
<p>Get the loss scaling update logic cell.</p>
<blockquote>
<p><code>update_loss_scale</code>(<em>overflow</em>)<a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/_modules/mindspore/train/loss_scale_manager.html#LossScaleManager.update_loss_scale" target="_blank" rel="noopener">[source]</a></p>
</blockquote>
<p>Update loss scale value.</p>
<ul>
<li><p>Parameters</p>
<p><strong>overflow</strong> (<a href="https://docs.python.org/library/functions.html#bool" target="_blank" rel="noopener"><em>bool</em></a>) â€“ Whether it overflows.</p>
</li>
</ul>
<h5 id="classmindspore-FixedLossScaleManager-loss-scale-128-0-drop-overflow-update-True-source"><a href="#classmindspore-FixedLossScaleManager-loss-scale-128-0-drop-overflow-update-True-source" class="headerlink" title="classmindspore.FixedLossScaleManager(loss_scale=128.0, drop_overflow_update=True)[source]"></a><em>class<em><code>mindspore.FixedLossScaleManager</code>(</em>loss_scale=128.0</em>, <em>drop_overflow_update=True</em>)<a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/_modules/mindspore/train/loss_scale_manager.html#FixedLossScaleManager" target="_blank" rel="noopener">[source]</a></h5><p>Loss scale with a fixed value, inherits from LossScaleManager.</p>
<ul>
<li><p>Parameters</p>
<p><strong>loss_scale</strong> (<a href="https://docs.python.org/library/functions.html#float" target="_blank" rel="noopener"><em>float</em></a>) â€“ Loss scale. Note that if drop_overflow_update is set to False, the value of loss_scale in optimizer that you used need to be set to the same value as here. Default: 128.0.</p>
<p><strong>drop_overflow_update</strong> (<a href="https://docs.python.org/library/functions.html#bool" target="_blank" rel="noopener"><em>bool</em></a>) â€“ Whether to execute optimizer if there is an overflow. If True, the optimizer will not executed when overflow occurs. Default: True.</p>
</li>
</ul>
<p><strong>Examples</strong></p>
<pre><code>&gt;&gt;&gt; from mindspore import Model, nn, FixedLossScaleManager
&gt;&gt;&gt;
&gt;&gt;&gt; net = Net()
&gt;&gt;&gt; #1) Drop the parameter update if there is an overflow
&gt;&gt;&gt; loss_scale_manager = FixedLossScaleManager()
&gt;&gt;&gt; optim = nn.Momentum(params=net.trainable_params(), learning_rate=0.1, momentum=0.9)
&gt;&gt;&gt; model = Model(net, loss_scale_manager=loss_scale_manager, optimizer=optim)
&gt;&gt;&gt;
&gt;&gt;&gt; #2) Execute parameter update even if overflow occurs
&gt;&gt;&gt; loss_scale = 1024.0
&gt;&gt;&gt; loss_scale_manager = FixedLossScaleManager(loss_scale, False)
&gt;&gt;&gt; optim = nn.Momentum(params=net.trainable_params(), learning_rate=0.1, momentum=0.9, loss_scale=loss_scale)
&gt;&gt;&gt; model = Model(net, loss_scale_manager=loss_scale_manager, optimizer=optim)</code></pre><blockquote>
<p><code>get_drop_overflow_update</code>()<a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/_modules/mindspore/train/loss_scale_manager.html#FixedLossScaleManager.get_drop_overflow_update" target="_blank" rel="noopener">[source]</a></p>
</blockquote>
<p>Get the flag whether to drop optimizer update when there is an overflow.</p>
<ul>
<li><p>Returns</p>
<p>bool, drop_overflow_update value.</p>
</li>
</ul>
<blockquote>
<p><code>get_loss_scale</code>()<a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/_modules/mindspore/train/loss_scale_manager.html#FixedLossScaleManager.get_loss_scale" target="_blank" rel="noopener">[source]</a></p>
</blockquote>
<p>Get loss scale value.</p>
<ul>
<li><p>Returns</p>
<p>bool, loss_scale value.</p>
</li>
</ul>
<blockquote>
<p><code>get_update_cell</code>()<a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/_modules/mindspore/train/loss_scale_manager.html#FixedLossScaleManager.get_update_cell" target="_blank" rel="noopener">[source]</a></p>
</blockquote>
<p>Returns the update cell for TrainOneStepWithLossScaleCell.</p>
<ul>
<li><p>Returns</p>
<p>None or Cell. Cell object, used to update loss_scale, when drop_overflow_update is True. None when drop_overflow_update is False.</p>
</li>
</ul>
<blockquote>
<p><code>update_loss_scale</code>(<em>overflow</em>)<a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/_modules/mindspore/train/loss_scale_manager.html#FixedLossScaleManager.update_loss_scale" target="_blank" rel="noopener">[source]</a></p>
</blockquote>
<p>Update loss scale value. The interface at FixedLossScaleManager will do nothing.</p>
<ul>
<li><p>Parameters</p>
<p><strong>overflow</strong> (<a href="https://docs.python.org/library/functions.html#bool" target="_blank" rel="noopener"><em>bool</em></a>) â€“ Whether it overflows.</p>
</li>
</ul>
<h5 id="classmindspore-DynamicLossScaleManager-init-loss-scale-2-24-scale-factor-2-scale-window-2000-source"><a href="#classmindspore-DynamicLossScaleManager-init-loss-scale-2-24-scale-factor-2-scale-window-2000-source" class="headerlink" title="classmindspore.DynamicLossScaleManager(init_loss_scale=2 ** 24, scale_factor=2, scale_window=2000)[source]"></a><em>class<em><code>mindspore.DynamicLossScaleManager</code>(</em>init_loss_scale=2 ** 24</em>, <em>scale_factor=2</em>, <em>scale_window=2000</em>)<a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/_modules/mindspore/train/loss_scale_manager.html#DynamicLossScaleManager" target="_blank" rel="noopener">[source]</a></h5><p>Loss scale that dynamically adjusts itself, inherits from LossScaleManager.</p>
<ul>
<li><p>Parameters</p>
<p><strong>init_loss_scale</strong> (<a href="https://docs.python.org/library/functions.html#float" target="_blank" rel="noopener"><em>float</em></a>) â€“ Initialize loss scale. Default: 2**24.</p>
<p><strong>scale_factor</strong> (<a href="https://docs.python.org/library/functions.html#int" target="_blank" rel="noopener"><em>int</em></a>) â€“ Coefficient of increase and decrease. Default: 2.</p>
<p><strong>scale_window</strong> (<a href="https://docs.python.org/library/functions.html#int" target="_blank" rel="noopener"><em>int</em></a>) â€“ Maximum continuous normal steps when there is no overflow. Default: 2000.</p>
</li>
</ul>
<p><strong>Examples</strong></p>
<pre><code>&gt;&gt;&gt; from mindspore import Model, nn, DynamicLossScaleManager
&gt;&gt;&gt;
&gt;&gt;&gt; net = Net()
&gt;&gt;&gt; loss_scale_manager = DynamicLossScaleManager()
&gt;&gt;&gt; optim = nn.Momentum(params=net.trainable_params(), learning_rate=0.1, momentum=0.9)
&gt;&gt;&gt; model = Model(net, loss_scale_manager=loss_scale_manager, optimizer=optim)</code></pre><blockquote>
<p><code>get_drop_overflow_update</code>()<a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/_modules/mindspore/train/loss_scale_manager.html#DynamicLossScaleManager.get_drop_overflow_update" target="_blank" rel="noopener">[source]</a></p>
</blockquote>
<p>Get the flag whether to drop optimizer update when there is an overflow.</p>
<ul>
<li>Returns</li>
</ul>
<p>â€‹    bool, always return True at DynamicLossScaleManager.</p>
<blockquote>
<p><code>get_loss_scale</code>()<a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/_modules/mindspore/train/loss_scale_manager.html#DynamicLossScaleManager.get_loss_scale" target="_blank" rel="noopener">[source]</a></p>
</blockquote>
<p>Get loss scale value.</p>
<ul>
<li>Returns</li>
</ul>
<p>â€‹    bool, loss_scale value.</p>
<blockquote>
<p><code>get_update_cell</code>()<a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/_modules/mindspore/train/loss_scale_manager.html#DynamicLossScaleManager.get_update_cell" target="_blank" rel="noopener">[source]</a></p>
</blockquote>
<p>Returns the update cell for TrainOneStepWithLossScaleCell.</p>
<ul>
<li>Returns</li>
</ul>
<p>â€‹    Cell, cell object used to update loss_scale.</p>
<blockquote>
<p><code>update_loss_scale</code>(<em>overflow</em>)<a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/_modules/mindspore/train/loss_scale_manager.html#DynamicLossScaleManager.update_loss_scale" target="_blank" rel="noopener">[source]</a></p>
</blockquote>
<p>Update loss scale value.</p>
<ul>
<li><p>Parameters</p>
<p><strong>overflow</strong> (<a href="https://docs.python.org/library/functions.html#bool" target="_blank" rel="noopener"><em>bool</em></a>) â€“ Whether it overflows.</p>
</li>
</ul>
<blockquote>
<p><code>mindspore.save_checkpoint</code>(<em>save_obj</em>, <em>ckpt_file_name</em>, <em>integrated_save=True</em>, <em>async_save=False</em>, <em>append_dict=None</em>, <em>enc_key=None</em>, <em>enc_mode=â€AES-GCMâ€</em>)<a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/_modules/mindspore/train/serialization.html#save_checkpoint" target="_blank" rel="noopener">[source]</a></p>
</blockquote>
<p>Save checkpoint info to a specified file.</p>
<ul>
<li><p>Parameters</p>
<p><strong>save_obj</strong> (<em>Union**[</em><a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/api_python/nn/mindspore.nn.Cell.html#mindspore.nn.Cell" target="_blank" rel="noopener"><em>Cell</em></a><em>,</em> <a href="https://docs.python.org/library/stdtypes.html#list" target="_blank" rel="noopener"><em>list</em></a><em>]</em>) â€“ The cell object or data list(each element is a dictionary, like [{â€œnameâ€: param_name, â€œdataâ€: param_data},â€¦], the type of param_name would be string, and the type of param_data would be parameter or Tensor).</p>
<p><strong>ckpt_file_name</strong> (<a href="https://docs.python.org/library/stdtypes.html#str" target="_blank" rel="noopener"><em>str</em></a>) â€“ Checkpoint file name. If the file name already exists, it will be overwritten.</p>
<p><strong>ntegrated_save</strong> (<a href="https://docs.python.org/library/functions.html#bool" target="_blank" rel="noopener"><em>bool</em></a>) â€“ Whether to integrated save in automatic model parallel scene. Default: True</p>
<p><strong>async_save</strong> (<a href="https://docs.python.org/library/functions.html#bool" target="_blank" rel="noopener"><em>bool</em></a>) â€“ Whether asynchronous execution saves the checkpoint to a file. Default: False</p>
<p><strong>append_dict</strong> (<a href="https://docs.python.org/library/stdtypes.html#dict" target="_blank" rel="noopener"><em>dict</em></a>) â€“ Additional information that needs to be saved. The key of dict must be str, the value of dict must be one of int float and bool. Default: None</p>
<p><strong>enc_key</strong> (<em>Union**[</em><a href="https://docs.python.org/library/constants.html#None" target="_blank" rel="noopener"><em>None</em></a><em>,</em> <a href="https://docs.python.org/library/stdtypes.html#bytes" target="_blank" rel="noopener"><em>bytes</em></a><em>]</em>) â€“ Byte type key used for encryption. If the value is None, the encryption is not required. Default: None.</p>
<p><strong>enc_mode</strong> (<a href="https://docs.python.org/library/stdtypes.html#str" target="_blank" rel="noopener"><em>str</em></a>) â€“ This parameter is valid only when enc_key is not set to None. Specifies the encryption mode, currently supports â€˜AES-GCMâ€™ and â€˜AES-CBCâ€™. Default: â€˜AES-GCMâ€™.</p>
</li>
<li><p>Raises</p>
<p><a href="https://docs.python.org/library/exceptions.html#TypeError" target="_blank" rel="noopener"><strong>TypeError</strong></a> â€“ If the parameter save_obj is not nn.Cell or list type. And if the parameter integrated_save and async_save are not bool type.</p>
</li>
</ul>
<p><strong>Examples</strong></p>
<pre><code>&gt;&gt;&gt; from mindspore import save_checkpoint
&gt;&gt;&gt;
&gt;&gt;&gt; net = Net()
&gt;&gt;&gt; save_checkpoint(net, "lenet.ckpt")</code></pre><blockquote>
<p><code>mindspore.load_checkpoint</code>(<em>ckpt_file_name</em>, <em>net=None</em>, <em>strict_load=False</em>, <em>filter_prefix=None</em>, <em>dec_key=None</em>, <em>dec_mode=â€AES-GCMâ€</em>)<a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/_modules/mindspore/train/serialization.html#load_checkpoint" target="_blank" rel="noopener">[source]</a></p>
</blockquote>
<p>Load checkpoint info from a specified file.</p>
<ul>
<li><p>Parameters</p>
<p><strong>ckpt_file_name</strong> (<a href="https://docs.python.org/library/stdtypes.html#str" target="_blank" rel="noopener"><em>str</em></a>) â€“ Checkpoint file name.</p>
<p><strong>net</strong> (<a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/api_python/nn/mindspore.nn.Cell.html#mindspore.nn.Cell" target="_blank" rel="noopener"><em>Cell</em></a>) â€“ Cell network. Default: None</p>
<p><strong>strict_load</strong> (<a href="https://docs.python.org/library/functions.html#bool" target="_blank" rel="noopener"><em>bool</em></a>) â€“ Whether to strict load the parameter into net. If False, it will load parameter in the param_dict into net with the same suffix and load parameter with different accuracy. Default: False.</p>
<p><strong>filter_prefix</strong> (<em>Union**[</em><a href="https://docs.python.org/library/stdtypes.html#str" target="_blank" rel="noopener"><em>str</em></a><em>,</em> <a href="https://docs.python.org/library/stdtypes.html#list" target="_blank" rel="noopener"><em>list</em></a><em>[*<a href="https://docs.python.org/library/stdtypes.html#str" target="_blank" rel="noopener"><em>str</em></a><em>]*</em>,</em> <a href="https://docs.python.org/library/stdtypes.html#tuple" target="_blank" rel="noopener"><em>tuple</em></a><em>[*<a href="https://docs.python.org/library/stdtypes.html#str" target="_blank" rel="noopener"><em>str</em></a><em>]*</em>]</em>) â€“ Parameters starting with the filter_prefix will not be loaded. Default: None.</p>
<p><strong>dec_key</strong> (<em>Union**[</em><a href="https://docs.python.org/library/constants.html#None" target="_blank" rel="noopener"><em>None</em></a><em>,</em> <a href="https://docs.python.org/library/stdtypes.html#bytes" target="_blank" rel="noopener"><em>bytes</em></a><em>]</em>) â€“ Byte type key used for decryption. If the value is None, the decryption is not required. Default: None.</p>
<p><strong>dec_mode</strong> (<a href="https://docs.python.org/library/stdtypes.html#str" target="_blank" rel="noopener"><em>str</em></a>) â€“ This parameter is valid only when dec_key is not set to None. Specifies the decryption mode, currently supports â€˜AES-GCMâ€™ and â€˜AES-CBCâ€™. Default: â€˜AES-GCMâ€™.</p>
</li>
<li><p>Returns</p>
<p>Dict, key is parameter name, value is a Parameter.</p>
</li>
<li><p>Raises</p>
<p><a href="https://docs.python.org/library/exceptions.html#ValueError" target="_blank" rel="noopener"><strong>ValueError</strong></a> â€“ Checkpoint file is incorrect.</p>
</li>
</ul>
<p><strong>Examples</strong></p>
<pre><code>&gt;&gt;&gt; from mindspore import load_checkpoint
&gt;&gt;&gt;
&gt;&gt;&gt; ckpt_file_name = "./checkpoint/LeNet5-1_32.ckpt"
&gt;&gt;&gt; param_dict = load_checkpoint(ckpt_file_name, filter_prefix="conv1")
&gt;&gt;&gt; print(param_dict["conv2.weight"])
Parameter (name=conv2.weight, shape=(16, 6, 5, 5), dtype=Float32, requires_grad=True)</code></pre><blockquote>
<p><code>mindspore.load_param_into_net</code>(<em>net</em>, <em>parameter_dict</em>, <em>strict_load=False</em>)<a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/_modules/mindspore/train/serialization.html#load_param_into_net" target="_blank" rel="noopener">[source]</a></p>
</blockquote>
<p>Load parameters into network.</p>
<ul>
<li><p>Parameters</p>
<p><strong>net</strong> (<a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/api_python/nn/mindspore.nn.Cell.html#mindspore.nn.Cell" target="_blank" rel="noopener"><em>Cell</em></a>) â€“ Cell network.</p>
<p><strong>parameter_dict</strong> (<a href="https://docs.python.org/library/stdtypes.html#dict" target="_blank" rel="noopener"><em>dict</em></a>) â€“ Parameter dictionary.</p>
<p><strong>strict_load</strong> (<a href="https://docs.python.org/library/functions.html#bool" target="_blank" rel="noopener"><em>bool</em></a>) â€“ Whether to strict load the parameter into net. If False, it will load parameter in the param_dict into net with the same suffix and load parameter with different accuracy. Default: False.</p>
</li>
<li><p>Returns</p>
<p>List, parameters not loaded in the network.</p>
</li>
<li><p>Raises</p>
<p><a href="https://docs.python.org/library/exceptions.html#TypeError" target="_blank" rel="noopener"><strong>TypeError</strong></a> â€“ Argument is not a Cell, or parameter_dict is not a Parameter dictionary.</p>
</li>
</ul>
<p><strong>Examples</strong></p>
<pre><code>&gt;&gt;&gt; from mindspore import load_checkpoint, load_param_into_net
&gt;&gt;&gt;
&gt;&gt;&gt; net = Net()
&gt;&gt;&gt; ckpt_file_name = "./checkpoint/LeNet5-1_32.ckpt"
&gt;&gt;&gt; param_dict = load_checkpoint(ckpt_file_name, filter_prefix="conv1")
&gt;&gt;&gt; param_not_load = load_param_into_net(net, param_dict)
&gt;&gt;&gt; print(param_not_load)</code></pre><blockquote>
<p><code>mindspore.export</code>(<em>net</em>, <strong>inputs<em>, *file_name</em>, <em>file_format=â€AIRâ€</em>, *</strong>kwargs*)<a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/_modules/mindspore/train/serialization.html#export" target="_blank" rel="noopener">[source]</a></p>
</blockquote>
<p>Export the MindSpore prediction model to a file in the specified format.</p>
<blockquote>
<p>Note</p>
<ol>
<li>When exporting to AIRã€ONNX format, the size of a single tensor can not exceed 2GB.</li>
<li>When file_name does not have a suffix, the system will automatically add according to the file_format.</li>
</ol>
</blockquote>
<ul>
<li><p>Parameters</p>
<p><strong>net</strong> (<a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/api_python/nn/mindspore.nn.Cell.html#mindspore.nn.Cell" target="_blank" rel="noopener"><em>Cell</em></a>) â€“ MindSpore network.</p>
<p><strong>inputs</strong> (<a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/api_python/mindspore/mindspore.Tensor.html#mindspore.Tensor" target="_blank" rel="noopener"><em>Tensor</em></a>) â€“ Inputs of the net, if the network has multiple inputs, incoming tuple(Tensor).</p>
<p><strong>file_name</strong> (<a href="https://docs.python.org/library/stdtypes.html#str" target="_blank" rel="noopener"><em>str</em></a>) â€“ File name of the model to be exported.</p>
<p><strong>file_format</strong> (<a href="https://docs.python.org/library/stdtypes.html#str" target="_blank" rel="noopener"><em>str</em></a>) â€“MindSpore currently supports â€˜AIRâ€™, â€˜ONNXâ€™ and â€˜MINDIRâ€™ format for exported model.AIR: Ascend Intermediate Representation. An intermediate representation format of Ascend model.ONNX: Open Neural Network eXchange. An open format built to represent machine learning models.MINDIR: MindSpore Native Intermediate Representation for Anf. An intermediate representation format for MindSpore models.</p>
<p><strong>kwargs</strong> (<a href="https://docs.python.org/library/stdtypes.html#dict" target="_blank" rel="noopener"><em>dict</em></a>) â€“Configuration options dictionary.quant_mode (str): If the network is quantization aware training network, the quant_mode should be set to â€œQUANTâ€, else the quant_mode should be set to â€œNONQUANTâ€.mean (float): The mean of input data after preprocessing, used for quantizing the first layer of network. Default: 127.5.std_dev (float): The variance of input data after preprocessing, used for quantizing the first layer of network. Default: 127.5.enc_key (byte): Byte type key used for encryption. Tha valid length is 16, 24, or 32.enc_mode (str): Specifies the encryption mode, take effect when enc_key is set. Option: â€˜AES-GCMâ€™ | â€˜AES-CBCâ€™. Default: â€˜AES-GCMâ€™.dataset (Dataset): Specifies the preprocess methods of network.</p>
</li>
</ul>
<p><strong>Examples</strong></p>
<pre><code>&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; from mindspore import export, Tensor
&gt;&gt;&gt;
&gt;&gt;&gt; net = LeNet()
&gt;&gt;&gt; input = Tensor(np.ones([1, 1, 32, 32]).astype(np.float32))
&gt;&gt;&gt; export(net, Tensor(input), file_name='lenet', file_format='MINDIR')</code></pre><blockquote>
<p><code>mindspore.load</code>(<em>file_name</em>, <em>**kwargs</em>)<a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/_modules/mindspore/train/serialization.html#load" target="_blank" rel="noopener">[source]</a></p>
</blockquote>
<p>Load MindIR.</p>
<p>The returned object can be executed by a GraphCell, see class <code>mindspore.nn.GraphCell</code> for more details.</p>
<ul>
<li><p>Parameters</p>
<p><strong>file_name</strong> (<a href="https://docs.python.org/library/stdtypes.html#str" target="_blank" rel="noopener"><em>str</em></a>) â€“ MindIR file name.</p>
<p><strong>kwargs</strong> (<a href="https://docs.python.org/library/stdtypes.html#dict" target="_blank" rel="noopener"><em>dict</em></a>) â€“Configuration options dictionary.dec_key (bytes): Byte type key used for decryption. Tha valid length is 16, 24, or 32.dec_mode (str): Specifies the decryption mode, take effect when dec_key is set. Option: â€˜AES-GCMâ€™ | â€˜AES-CBCâ€™. Default: â€˜AES-GCMâ€™.</p>
</li>
<li><p>Returns</p>
<p>Object, a compiled graph that can executed by GraphCell.</p>
</li>
<li><p>Raises</p>
<p><a href="https://docs.python.org/library/exceptions.html#ValueError" target="_blank" rel="noopener"><strong>ValueError</strong></a> â€“ MindIR file name is incorrect.</p>
<p><a href="https://docs.python.org/library/exceptions.html#RuntimeError" target="_blank" rel="noopener"><strong>RuntimeError</strong></a> â€“ Failed to parse MindIR file.</p>
</li>
</ul>
<p><strong>Examples</strong></p>
<pre><code>&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; import mindspore.nn as nn
&gt;&gt;&gt; from mindspore import Tensor, export, load
&gt;&gt;&gt;
&gt;&gt;&gt; net = nn.Conv2d(1, 1, kernel_size=3, weight_init="ones")
&gt;&gt;&gt; input = Tensor(np.ones([1, 1, 3, 3]).astype(np.float32))
&gt;&gt;&gt; export(net, input, file_name="net", file_format="MINDIR")
&gt;&gt;&gt; graph = load("net.mindir")
&gt;&gt;&gt; net = nn.GraphCell(graph)
&gt;&gt;&gt; output = net(input)
&gt;&gt;&gt; print(output)</code></pre><blockquote>
<p><code>mindspore.parse_print</code>(<em>print_file_name</em>)<a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/_modules/mindspore/train/serialization.html#parse_print" target="_blank" rel="noopener">[source]</a></p>
</blockquote>
<p>Load Print data from a specified file.</p>
<ul>
<li><p>Parameters</p>
<p><strong>print_file_name</strong> (<a href="https://docs.python.org/library/stdtypes.html#str" target="_blank" rel="noopener"><em>str</em></a>) â€“ The file name of saved print data.</p>
</li>
<li><p>Returns</p>
<p>List, element of list is Tensor.</p>
</li>
<li><p>Raises</p>
<p><a href="https://docs.python.org/library/exceptions.html#ValueError" target="_blank" rel="noopener"><strong>ValueError</strong></a> â€“ The print file may be empty, please make sure enter the correct file name.</p>
</li>
</ul>
<blockquote>
<p><code>mindspore.build_searched_strategy</code>(<em>strategy_filename</em>)<a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/_modules/mindspore/train/serialization.html#build_searched_strategy" target="_blank" rel="noopener">[source]</a></p>
</blockquote>
<p>Build strategy of every parameter in network.</p>
<ul>
<li><p>Parameters</p>
<p><strong>strategy_filename</strong> (<a href="https://docs.python.org/library/stdtypes.html#str" target="_blank" rel="noopener"><em>str</em></a>) â€“ Name of strategy file.</p>
</li>
<li><p>Returns</p>
<p>Dict, whose key is parameter name and value is slice strategy of this parameter.</p>
</li>
<li><p>Raises</p>
<p><a href="https://docs.python.org/library/exceptions.html#ValueError" target="_blank" rel="noopener"><strong>ValueError</strong></a> â€“ Strategy file is incorrect.</p>
<p><a href="https://docs.python.org/library/exceptions.html#TypeError" target="_blank" rel="noopener"><strong>TypeError</strong></a> â€“ strategy_filename is not str.</p>
</li>
</ul>
<blockquote>
<p><code>mindspore.merge_sliced_parameter</code>(<em>sliced_parameters</em>, <em>strategy=None</em>)<a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/_modules/mindspore/train/serialization.html#merge_sliced_parameter" target="_blank" rel="noopener">[source]</a></p>
</blockquote>
<p>Merge parameter slices to one whole parameter.</p>
<ul>
<li><p>Parameters</p>
<p><strong>sliced_parameters</strong> (<a href="https://docs.python.org/library/stdtypes.html#list" target="_blank" rel="noopener"><em>list</em></a><em>[*<a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/api_python/mindspore/mindspore.Parameter.html#mindspore.Parameter" target="_blank" rel="noopener"><em>Parameter</em></a>*]</em>) â€“ Parameter slices in order of rank_id.</p>
<p><strong>strategy</strong> (<em>Optional**[</em><a href="https://docs.python.org/library/stdtypes.html#dict" target="_blank" rel="noopener"><em>dict</em></a><em>]</em>) â€“ Parameter slice strategy, whose key is parameter name and value is slice strategy of this parameter. If strategy is None, just merge parameter slices in 0 axis order. Default: None.</p>
</li>
<li><p>Returns</p>
<p>Parameter, the merged parameter which has the whole data.</p>
</li>
<li><p>Raises</p>
<p><a href="https://docs.python.org/library/exceptions.html#ValueError" target="_blank" rel="noopener"><strong>ValueError</strong></a> â€“ Failed to merge.</p>
<p><a href="https://docs.python.org/library/exceptions.html#TypeError" target="_blank" rel="noopener"><strong>TypeError</strong></a> â€“ The sliced_parameters is incorrect or strategy is not dict.</p>
<p><a href="https://docs.python.org/library/exceptions.html#KeyError" target="_blank" rel="noopener"><strong>KeyError</strong></a> â€“ The parameter name is not in keys of strategy.</p>
</li>
</ul>
<p><strong>Examples</strong></p>
<pre><code>&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; from mindspore import Tensor, merge_sliced_parameter, Parameter
&gt;&gt;&gt;
&gt;&gt;&gt; sliced_parameters = [
...                      Parameter(Tensor(np.array([0.00023915, 0.00013939, -0.00098059])),
...                                "network.embedding_table"),
...                      Parameter(Tensor(np.array([0.00015815, 0.00015458, -0.00012125])),
...                                "network.embedding_table"),
...                      Parameter(Tensor(np.array([0.00042165, 0.00029692, -0.00007941])),
...                                "network.embedding_table"),
...                      Parameter(Tensor(np.array([0.00084451, 0.00089960, -0.00010431])),
...                                "network.embedding_table")]
&gt;&gt;&gt; merged_parameter = merge_sliced_parameter(sliced_parameters)
&gt;&gt;&gt; print(merged_parameter)</code></pre><blockquote>
<p><code>mindspore.load_distributed_checkpoint</code>(<em>network</em>, <em>checkpoint_filenames</em>, <em>predict_strategy=None</em>, <em>train_strategy_filename=None</em>, <em>dec_key=None</em>, <em>dec_mode=â€AES-GCMâ€</em>)<a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/_modules/mindspore/train/serialization.html#load_distributed_checkpoint" target="_blank" rel="noopener">[source]</a></p>
</blockquote>
<p>Load checkpoint into net for distributed predication.</p>
<ul>
<li><p>Parameters</p>
<p><strong>network</strong> (<a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/api_python/nn/mindspore.nn.Cell.html#mindspore.nn.Cell" target="_blank" rel="noopener"><em>Cell</em></a>) â€“ Network for distributed predication.</p>
<p><strong>checkpoint_filenames</strong> (<a href="https://docs.python.org/library/stdtypes.html#list" target="_blank" rel="noopener"><em>list</em></a><em>[*<a href="https://docs.python.org/library/stdtypes.html#str" target="_blank" rel="noopener"><em>str</em></a>*]</em>) â€“ The name of Checkpoint files in order of rank id.</p>
<p><strong>predict_strategy</strong> (<a href="https://docs.python.org/library/stdtypes.html#dict" target="_blank" rel="noopener"><em>dict</em></a>) â€“ Strategy of predication process, whose key is parameter name, and value is a list or a tuple that the first four elements are [dev_matrix, tensor_map, param_split_shape, field]. If None, it means that the predication process just uses single device. Default: None.</p>
<p><strong>train_strategy_filename</strong> (<a href="https://docs.python.org/library/stdtypes.html#str" target="_blank" rel="noopener"><em>str</em></a>) â€“ Train strategy proto file name. Default: None.</p>
<p><strong>dec_key</strong> (<em>Union**[</em><a href="https://docs.python.org/library/constants.html#None" target="_blank" rel="noopener"><em>None</em></a><em>,</em> <a href="https://docs.python.org/library/stdtypes.html#bytes" target="_blank" rel="noopener"><em>bytes</em></a><em>]</em>) â€“ Byte type key used for decryption. If the value is None, the decryption is not required. Default: None.</p>
<p><strong>dec_mode</strong> (<a href="https://docs.python.org/library/stdtypes.html#str" target="_blank" rel="noopener"><em>str</em></a>) â€“ This parameter is valid only when dec_key is not set to None. Specifies the decryption mode, currently supports â€˜AES-GCMâ€™ and â€˜AES-CBCâ€™. Default: â€˜AES-GCMâ€™.</p>
</li>
<li><p>Raises</p>
<p><a href="https://docs.python.org/library/exceptions.html#TypeError" target="_blank" rel="noopener"><strong>TypeError</strong></a> â€“ The type of inputs do not match the requirements.</p>
<p><a href="https://docs.python.org/library/exceptions.html#ValueError" target="_blank" rel="noopener"><strong>ValueError</strong></a> â€“ Failed to load checkpoint into net.</p>
</li>
</ul>
<blockquote>
<p><code>mindspore.async_ckpt_thread_status</code>()<a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/_modules/mindspore/train/serialization.html#async_ckpt_thread_status" target="_blank" rel="noopener">[source]</a></p>
</blockquote>
<p>Get the status of asynchronous save checkpoint thread.</p>
<ul>
<li><p>Returns</p>
<p>True, Asynchronous save checkpoint thread is running.</p>
<p>False, Asynchronous save checkpoint thread is not executing.</p>
</li>
</ul>
<blockquote>
<p><code>mindspore.get_level</code>()<a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/_modules/mindspore/log.html#get_level" target="_blank" rel="noopener">[source]</a></p>
</blockquote>
<p>Get the logger level.</p>
<ul>
<li><p>Returns</p>
<p>str, the Log level includes 4(EXCEPTION), 3(ERROR), 2(WARNING), 1(INFO), 0(DEBUG).</p>
</li>
</ul>
<p><strong>Examples</strong></p>
<pre><code>&gt;&gt;&gt; import os
&gt;&gt;&gt; os.environ['GLOG_v'] = '0'
&gt;&gt;&gt; from mindspore import log as logger
&gt;&gt;&gt; logger.get_level()</code></pre><blockquote>
<p><code>mindspore.get_log_config</code>()<a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/_modules/mindspore/log.html#get_log_config" target="_blank" rel="noopener">[source]</a></p>
</blockquote>
<p>Get logger configurations.</p>
<ul>
<li><p>Returns</p>
<p>Dict, the dictionary of logger configurations.</p>
</li>
</ul>
<p><strong>Examples</strong></p>
<pre><code>&gt;&gt;&gt; import os
&gt;&gt;&gt; os.environ['GLOG_v'] = '1'
&gt;&gt;&gt; os.environ['GLOG_logtostderr'] = '0'
&gt;&gt;&gt; os.environ['GLOG_log_dir'] = '/var/log'
&gt;&gt;&gt; os.environ['logger_maxBytes'] = '5242880'
&gt;&gt;&gt; os.environ['logger_backupCount'] = '10'
&gt;&gt;&gt; os.environ['GLOG_stderrthreshold'] = '2'
&gt;&gt;&gt; from mindspore import log as logger
&gt;&gt;&gt; logger.get_log_config()</code></pre><h1 id="mindspore-common-initializer"><a href="#mindspore-common-initializer" class="headerlink" title="mindspore.common.initializer"></a>mindspore.common.initializer</h1><p>Initializer for cell parameters.</p>
<h5 id="classmindspore-common-initializer-Initializer-kwargs-source"><a href="#classmindspore-common-initializer-Initializer-kwargs-source" class="headerlink" title="classmindspore.common.initializer.Initializer(**kwargs)[source]"></a><em>class<em><code>mindspore.common.initializer.Initializer</code>(**</em>kwargs</em>)<a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/_modules/mindspore/common/initializer.html#Initializer" target="_blank" rel="noopener">[source]</a></h5><p>The base class of the initializer. Initialization of tensor basic attributes and model weight values.</p>
<ul>
<li><p>Parameters</p>
<p><strong>kwargs</strong> (<a href="https://docs.python.org/library/stdtypes.html#dict" target="_blank" rel="noopener"><em>dict</em></a>) â€“ Keyword arguments for Initializer.</p>
</li>
</ul>
<blockquote>
<p><code>mindspore.common.initializer.initializer</code>(<em>init</em>, <em>shape=None</em>, <em>dtype=mstype.float32</em>)<a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/_modules/mindspore/common/initializer.html#initializer" target="_blank" rel="noopener">[source]</a></p>
</blockquote>
<p>Create and initialize a tensor.</p>
<ul>
<li><p>Parameters</p>
<p><strong>init</strong> (<em>Union**[</em><a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/api_python/mindspore/mindspore.Tensor.html#mindspore.Tensor" target="_blank" rel="noopener"><em>Tensor</em></a><em>,</em> <a href="https://docs.python.org/library/stdtypes.html#str" target="_blank" rel="noopener"><em>str</em></a><em>,</em> <a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/api_python/mindspore.common.initializer.html#mindspore.common.initializer.Initializer" target="_blank" rel="noopener"><em>Initializer</em></a><em>,</em> <a href="https://docs.python.org/library/numbers.html#numbers.Number" target="_blank" rel="noopener"><em>numbers.Number</em></a><em>]</em>) â€“Initialize value.str: The init should be the alias of the class inheriting from Initializer and the corresponding class will be called. The value of â€˜initâ€™ can be â€œnormalâ€, â€œonesâ€ or â€œzerosâ€, etc.Initializer: The init should be the class inheriting from Initializer to initialize tensor.numbers.Number: The Constant will be called to initialize tensor.</p>
<p><strong>shape</strong> (<em>Union**[</em><a href="https://docs.python.org/library/stdtypes.html#tuple" target="_blank" rel="noopener"><em>tuple</em></a><em>,</em> <a href="https://docs.python.org/library/stdtypes.html#list" target="_blank" rel="noopener"><em>list</em></a><em>,</em> <a href="https://docs.python.org/library/functions.html#int" target="_blank" rel="noopener"><em>int</em></a><em>]</em>) â€“ A list of integers, a tuple of integers or an integer as the shape of output. Default: None.</p>
<p><strong>dtype</strong> (<a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/api_python/mindspore.html#mindspore.dtype" target="_blank" rel="noopener"><code>mindspore.dtype</code></a>) â€“ The type of data in initialized tensor. Default: mindspore.float32.</p>
</li>
<li><p>Returns</p>
<p>Union[Tensor], return is Tensor object.</p>
</li>
</ul>
<p><strong>Examples</strong></p>
<pre><code>&gt;&gt;&gt; import mindspore
&gt;&gt;&gt; from mindspore.common.initializer import initializer, One
&gt;&gt;&gt; tensor1 = initializer('ones', [1, 2, 3], mindspore.float32)
&gt;&gt;&gt; tensor2 = initializer(One(), [1, 2, 3], mindspore.float32)
&gt;&gt;&gt; tensor3 = initializer(0, [1, 2, 3], mindspore.float32)</code></pre><h5 id="classmindspore-common-initializer-TruncatedNormal-sigma-0-01-source"><a href="#classmindspore-common-initializer-TruncatedNormal-sigma-0-01-source" class="headerlink" title="classmindspore.common.initializer.TruncatedNormal(sigma=0.01)[source]"></a><em>class<em><code>mindspore.common.initializer.TruncatedNormal</code>(</em>sigma=0.01</em>)<a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/_modules/mindspore/common/initializer.html#TruncatedNormal" target="_blank" rel="noopener">[source]</a></h5><p>Initialize a truncated normal distribution which is a bounded normal distribution within N(extlow,exthigh)N(extlow,exthigh).</p>
<ul>
<li><p>Parameters</p>
<p><strong>sigma</strong> (<a href="https://docs.python.org/library/functions.html#float" target="_blank" rel="noopener"><em>float</em></a>) â€“ The sigma of the array. Default: 0.01.</p>
</li>
</ul>
<p><strong>Examples</strong></p>
<pre><code>&gt;&gt;&gt; import mindspore
&gt;&gt;&gt; from mindspore.common.initializer import initializer, TruncatedNormal
&gt;&gt;&gt; tensor1 = initializer(TruncatedNormal(), [1, 2, 3], mindspore.float32)
&gt;&gt;&gt; tensor2 = initializer('truncatedNormal', [1, 2, 3], mindspore.float32)</code></pre><h5 id="classmindspore-common-initializer-Normal-sigma-0-01-mean-0-0-source"><a href="#classmindspore-common-initializer-Normal-sigma-0-01-mean-0-0-source" class="headerlink" title="classmindspore.common.initializer.Normal(sigma=0.01, mean=0.0)[source]"></a><em>class<em><code>mindspore.common.initializer.Normal</code>(</em>sigma=0.01</em>, <em>mean=0.0</em>)<a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/_modules/mindspore/common/initializer.html#Normal" target="_blank" rel="noopener">[source]</a></h5><p>Initialize a normal array, and obtain values N(sigma,mean)N(sigma,mean) from the normal distribution to fill the input tensor.</p>
<p><img src="/images/loading.gif" data-original="../images/basic/image-20210921161030139.png" alt=""></p>
<ul>
<li><p>Parameters</p>
<p><strong>sigma</strong> (<a href="https://docs.python.org/library/functions.html#float" target="_blank" rel="noopener"><em>float</em></a>) â€“ The sigma of the array. Default: 0.01.</p>
<p><strong>mean</strong> (<a href="https://docs.python.org/library/functions.html#float" target="_blank" rel="noopener"><em>float</em></a>) â€“ The mean of the array. Default: 0.0.</p>
</li>
</ul>
<p><strong>Examples</strong></p>
<pre><code>&gt;&gt;&gt; import mindspore
&gt;&gt;&gt; from mindspore.common.initializer import initializer, Normal
&gt;&gt;&gt; tensor1 = initializer(Normal(), [1, 2, 3], mindspore.float32)
&gt;&gt;&gt; tensor2 = initializer('normal', [1, 2, 3], mindspore.float32)</code></pre><h5 id="classmindspore-common-initializer-Uniform-scale-0-07-source"><a href="#classmindspore-common-initializer-Uniform-scale-0-07-source" class="headerlink" title="classmindspore.common.initializer.Uniform(scale=0.07)[source]"></a><em>class<em><code>mindspore.common.initializer.Uniform</code>(</em>scale=0.07</em>)<a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/_modules/mindspore/common/initializer.html#Uniform" target="_blank" rel="noopener">[source]</a></h5><p>Initialize a uniform array, and obtain values U(âˆ’extscale,extscale)U(âˆ’extscale,extscale) from the uniform distribution to fill the input tensor.</p>
<ul>
<li><p>Parameters</p>
<p><strong>scale</strong> (<a href="https://docs.python.org/library/functions.html#float" target="_blank" rel="noopener"><em>float</em></a>) â€“ The scale of the array. Default: 0.07.</p>
</li>
</ul>
<p><strong>Examples</strong></p>
<pre><code>&gt;&gt;&gt; import mindspore
&gt;&gt;&gt; from mindspore.common.initializer import initializer, Uniform
&gt;&gt;&gt; tensor1 = initializer(Uniform(), [1, 2, 3], mindspore.float32)
&gt;&gt;&gt; tensor2 = initializer('uniform', [1, 2, 3], mindspore.float32)</code></pre><h5 id="classmindspore-common-initializer-HeNormal-negative-slope-0-mode-â€fan-inâ€-nonlinearity-â€leaky-reluâ€-source"><a href="#classmindspore-common-initializer-HeNormal-negative-slope-0-mode-â€fan-inâ€-nonlinearity-â€leaky-reluâ€-source" class="headerlink" title="classmindspore.common.initializer.HeNormal(negative_slope=0, mode=â€fan_inâ€, nonlinearity=â€leaky_reluâ€)[source]"></a><em>class<em><code>mindspore.common.initializer.HeNormal</code>(</em>negative_slope=0</em>, <em>mode=â€fan_inâ€</em>, <em>nonlinearity=â€leaky_reluâ€</em>)<a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/_modules/mindspore/common/initializer.html#HeNormal" target="_blank" rel="noopener">[source]</a></h5><p>Initialize the array with HeKaiming Normal algorithm, and from a normal distribution collect samples within N(0,sigma2)N(0,sigma2) where</p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210921161154659.png" alt=""></p>
<ul>
<li>where gaingain is an optional scaling factor.</li>
<li>where modemode is the number of input units or output units in the weight tensor.</li>
</ul>
<p>For details of HeUniform algorithm, please check <a href="https://arxiv.org/abs/1502.01852" target="_blank" rel="noopener">https://arxiv.org/abs/1502.01852</a>.</p>
<ul>
<li><p>Parameters</p>
<p><strong>negative_slope</strong> (<a href="https://docs.python.org/library/functions.html#int" target="_blank" rel="noopener"><em>int</em></a><em>,</em> <a href="https://docs.python.org/library/functions.html#float" target="_blank" rel="noopener"><em>float</em></a><em>,</em> <a href="https://docs.python.org/library/functions.html#bool" target="_blank" rel="noopener"><em>bool</em></a>) â€“ The negative slope of the rectifier used after this layer (only used when nonlinearity is â€˜leaky_reluâ€™). Default: 0.</p>
<p><strong>mode</strong> (<a href="https://docs.python.org/library/stdtypes.html#str" target="_blank" rel="noopener"><em>str</em></a>) â€“ Either â€˜fan_inâ€™ or â€˜fan_outâ€™. Choosing â€˜fan_inâ€™ preserves the magnitude of the variance of the weights in the forward pass. Choosing â€˜fan_outâ€™ preserves the magnitudes in the backwards pass. Default: fan_in.</p>
<p><strong>nonlinearity</strong> (<a href="https://docs.python.org/library/stdtypes.html#str" target="_blank" rel="noopener"><em>str</em></a>) â€“ The non-linear function, recommended to use only with â€˜reluâ€™ or â€˜leaky_reluâ€™. Default: leaky_relu.</p>
</li>
</ul>
<p><strong>Examples</strong></p>
<pre><code>&gt;&gt;&gt; import mindspore
&gt;&gt;&gt; from mindspore.common.initializer import initializer, HeNormal
&gt;&gt;&gt; tensor1 = initializer(HeNormal(), [1, 2, 3], mindspore.float32)
&gt;&gt;&gt; tensor2 = initializer('he_normal', [1, 2, 3], mindspore.float32)</code></pre><h5 id="classmindspore-common-initializer-XavierUniform-gain-1-source"><a href="#classmindspore-common-initializer-XavierUniform-gain-1-source" class="headerlink" title="classmindspore.common.initializer.XavierUniform(gain=1)[source]"></a><em>class<em><code>mindspore.common.initializer.XavierUniform</code>(</em>gain=1</em>)<a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/_modules/mindspore/common/initializer.html#XavierUniform" target="_blank" rel="noopener">[source]</a></h5><p>Initialize the array with xavier uniform algorithm, and from a uniform distribution collect samples within U(âˆ’boundary,boundary)U(âˆ’boundary,boundary) where:</p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210921161239017.png" alt=""></p>
<ul>
<li>where gaingain is an optional scaling factor.</li>
<li>where ninnin is the number of input units in the weight tensor.</li>
<li>where noutnout is the number of output units in the weight tensor.</li>
</ul>
<p>For details of XavierUniform algorithm, please check <a href="http://proceedings.mlr.press/v9/glorot10a.html" target="_blank" rel="noopener">http://proceedings.mlr.press/v9/glorot10a.html</a>.</p>
<ul>
<li><p>Parameters</p>
<p><strong>gain</strong> (<a href="https://docs.python.org/library/functions.html#float" target="_blank" rel="noopener"><em>float</em></a>) â€“ An optional scaling factor. Default: 1.</p>
</li>
</ul>
<p><strong>Examples</strong></p>
<pre><code>&gt;&gt;&gt; import mindspore
&gt;&gt;&gt; from mindspore.common.initializer import initializer, XavierUniform
&gt;&gt;&gt; tensor1 = initializer(XavierUniform(), [1, 2, 3], mindspore.float32)
&gt;&gt;&gt; tensor2 = initializer('xavier_uniform', [1, 2, 3], mindspore.float32)</code></pre><h5 id="classmindspore-common-initializer-One-kwargs-source"><a href="#classmindspore-common-initializer-One-kwargs-source" class="headerlink" title="classmindspore.common.initializer.One(**kwargs)[source]"></a><em>class<em><code>mindspore.common.initializer.One</code>(**</em>kwargs</em>)<a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/_modules/mindspore/common/initializer.html#One" target="_blank" rel="noopener">[source]</a></h5><p>Fills the input array with the values one.</p>
<ul>
<li><p>Parameters</p>
<p><strong>arr</strong> (<em>Array</em>) â€“ The array to be assigned.</p>
</li>
</ul>
<p><strong>Examples</strong></p>
<pre><code>&gt;&gt;&gt; import mindspore
&gt;&gt;&gt; from mindspore.common.initializer import initializer, One
&gt;&gt;&gt; tensor1 = initializer(One(), [1, 2, 3], mindspore.float32)
&gt;&gt;&gt; tensor2 = initializer('ones', [1, 2, 3], mindspore.float32)</code></pre><h5 id="classmindspore-common-initializer-Zero-kwargs-source"><a href="#classmindspore-common-initializer-Zero-kwargs-source" class="headerlink" title="classmindspore.common.initializer.Zero(**kwargs)[source]"></a><em>class<em><code>mindspore.common.initializer.Zero</code>(**</em>kwargs</em>)<a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/_modules/mindspore/common/initializer.html#Zero" target="_blank" rel="noopener">[source]</a></h5><p>Fills the input array with the values zero.</p>
<ul>
<li><p>Parameters</p>
<p><strong>arr</strong> (<em>Array</em>) â€“ The array to be assigned.</p>
</li>
</ul>
<p><strong>Examples</strong></p>
<pre><code>&gt;&gt;&gt; import mindspore
&gt;&gt;&gt; from mindspore.common.initializer import initializer, Zero
&gt;&gt;&gt; tensor1 = initializer(Zero(), [1, 2, 3], mindspore.float32)
&gt;&gt;&gt; tensor2 = initializer('zeros', [1, 2, 3], mindspore.float32)</code></pre><h5 id="classmindspore-common-initializer-Constant-value-source"><a href="#classmindspore-common-initializer-Constant-value-source" class="headerlink" title="classmindspore.common.initializer.Constant(value)[source]"></a><em>class<em><code>mindspore.common.initializer.Constant</code>(</em>value</em>)<a href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/_modules/mindspore/common/initializer.html#Constant" target="_blank" rel="noopener">[source]</a></h5><p>Initialize a constant.</p>
<ul>
<li><p>Parameters</p>
<p><strong>value</strong> (<em>Union**[</em><a href="https://docs.python.org/library/functions.html#int" target="_blank" rel="noopener"><em>int</em></a><em>,</em> <a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" target="_blank" rel="noopener"><em>numpy.ndarray</em></a><em>]</em>) â€“ The value to initialize.</p>
</li>
</ul>
<p><strong>Examples</strong></p>
<pre><code>&gt;&gt;&gt; import mindspore
&gt;&gt;&gt; from mindspore.common.initializer import initializer
&gt;&gt;&gt; tensor1 = initializer(0, [1, 2, 3], mindspore.float32)
&gt;&gt;&gt; tensor2 = initializer(5, [1, 2, 3], mindspore.float32)</code></pre><h1 id="mindspore-communication"><a href="#mindspore-communication" class="headerlink" title="mindspore.communication"></a>mindspore.communication</h1><p>Collective communication interface.</p>
<script>
        document.querySelectorAll('.github-emoji')
          .forEach(el => {
            if (!el.dataset.src) { return; }
            const img = document.createElement('img');
            img.style = 'display:none !important;';
            img.src = el.dataset.src;
            img.addEventListener('error', () => {
              img.remove();
              el.style.color = 'inherit';
              el.style.backgroundImage = 'none';
              el.style.background = 'none';
            });
            img.addEventListener('load', () => {
              img.remove();
            });
            document.body.appendChild(img);
          });
      </script>
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://jackhcc.github.io" rel="external nofollow noreferrer">æ°å…‹æˆ</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://jackhcc.github.io/posts/mindspore.html">https://jackhcc.github.io/posts/mindspore.html</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="https://jackhcc.github.io" target="_blank">æ°å…‹æˆ</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/tags/MindSpore/">
                                    <span class="chip bg-color">MindSpore</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/libs/share/css/share.min.css">

<div id="article-share">
    
    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
                <style>
    #reward {
        margin: 40px 0;
        text-align: center;
    }

    #reward .reward-link {
        font-size: 1.4rem;
        line-height: 38px;
    }

    #reward .btn-floating:hover {
        box-shadow: 0 6px 12px rgba(0, 0, 0, 0.2), 0 5px 15px rgba(0, 0, 0, 0.2);
    }

    #rewardModal {
        width: 320px;
        height: 350px;
    }

    #rewardModal .reward-title {
        margin: 15px auto;
        padding-bottom: 5px;
    }

    #rewardModal .modal-content {
        padding: 10px;
    }

    #rewardModal .close {
        position: absolute;
        right: 15px;
        top: 15px;
        color: rgba(0, 0, 0, 0.5);
        font-size: 1.3rem;
        line-height: 20px;
        cursor: pointer;
    }

    #rewardModal .close:hover {
        color: #ef5350;
        transform: scale(1.3);
        -moz-transform:scale(1.3);
        -webkit-transform:scale(1.3);
        -o-transform:scale(1.3);
    }

    #rewardModal .reward-tabs {
        margin: 0 auto;
        width: 210px;
    }

    .reward-tabs .tabs {
        height: 38px;
        margin: 10px auto;
        padding-left: 0;
    }

    .reward-content ul {
        padding-left: 0 !important;
    }

    .reward-tabs .tabs .tab {
        height: 38px;
        line-height: 38px;
    }

    .reward-tabs .tab a {
        color: #fff;
        background-color: #ccc;
    }

    .reward-tabs .tab a:hover {
        background-color: #ccc;
        color: #fff;
    }

    .reward-tabs .wechat-tab .active {
        color: #fff !important;
        background-color: #22AB38 !important;
    }

    .reward-tabs .alipay-tab .active {
        color: #fff !important;
        background-color: #019FE8 !important;
    }

    .reward-tabs .reward-img {
        width: 210px;
        height: 210px;
    }
</style>

<div id="reward">
    <a href="#rewardModal" class="reward-link modal-trigger btn-floating btn-medium waves-effect waves-light red">èµ</a>

    <!-- Modal Structure -->
    <div id="rewardModal" class="modal">
        <div class="modal-content">
            <a class="close modal-close"><i class="fas fa-times"></i></a>
            <h4 class="reward-title">ä½ çš„èµè¯†æ˜¯æˆ‘å‰è¿›çš„åŠ¨åŠ›</h4>
            <div class="reward-content">
                <div class="reward-tabs">
                    <ul class="tabs row">
                        <li class="tab col s6 alipay-tab waves-effect waves-light"><a href="#alipay">æ”¯ä»˜å®</a></li>
                        <li class="tab col s6 wechat-tab waves-effect waves-light"><a href="#wechat">å¾® ä¿¡</a></li>
                    </ul>
                    <div id="alipay">
                        <img src="/images/loading.gif" data-original="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/medias/reward/aliqr.png" class="reward-img" alt="æ”¯ä»˜å®æ‰“èµäºŒç»´ç ">
                    </div>
                    <div id="wechat">
                        <img src="/images/loading.gif" data-original="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/medias/reward/wxqr.png" class="reward-img" alt="å¾®ä¿¡æ‰“èµäºŒç»´ç ">
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>

<script>
    $(function () {
        $('.tabs').tabs();
    });
</script>
            
        </div>
    </div>

    
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/libs/gitalk/gitalk.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/css/my-gitalk.css">

<div class="card gitalk-card" data-aos="fade-up">
    <div class="comment_headling" style="font-size: 20px; font-weight: 700; position: relative; left: 20px; top: 15px; padding-bottom: 5px;">
        <i class="fas fa-comments fa-fw" aria-hidden="true"></i>
        <span>è¯„è®º</span>
    </div>
    <div id="gitalk-container" class="card-content"></div>
</div>

<script src="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/libs/gitalk/gitalk.min.js"></script>
<script>
    let gitalk = new Gitalk({
        clientID: '3821a0bbb773038a51fc',
        clientSecret: '4b30b507d67ec5497ec0e77f43f80cb3e0d7dd3a',
        repo: 'JackHCC.github.io',
        owner: 'JackHCC',
        admin: "JackHCC",
        id: '2021-09-18T23-22-01',
        distractionFreeMode: false  // Facebook-like distraction free mode
    });

    gitalk.render('gitalk-container');
</script>
    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/posts/network-protocol.html">
                    <div class="card-image">
                        
                        
                        <img src="/images/loading.gif" data-original="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/medias/featureimages/16.jpg" class="responsive-img" alt="Network ProtocolåŸºç¡€">
                        
                        <span class="card-title">Network ProtocolåŸºç¡€</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            ç½‘ç»œåè®®åŸºç¡€
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2021-09-19
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/Network/" class="post-category">
                                    Network
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/Network-Protocol/">
                        <span class="chip bg-color">Network Protocol</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/posts/kubernetes.html">
                    <div class="card-image">
                        
                        
                        <img src="/images/loading.gif" data-original="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/medias/featureimages/23.jpg" class="responsive-img" alt="Kubernetes è¯¦è§£">
                        
                        <span class="card-title">Kubernetes è¯¦è§£</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Kuberneteså­¦ä¹ æ–‡æ¡£
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2021-09-17
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/Container/" class="post-category">
                                    Container
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/Kubernetes/">
                        <span class="chip bg-color">Kubernetes</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/libs/codeBlock/codeBlockFuction.js"></script>

<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/libs/codeBlock/codeShrink.js"></script>


<!-- ä»£ç å—æŠ˜è¡Œ -->

<style type="text/css">
code[class*="language-"], pre[class*="language-"] { white-space: pre !important; }
</style>


    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('4'),
            headingSelector: 'h1, h2, h3, h4'
        });

        // modify the toc link href to support Chinese.
        let i = 0;
        let tocHeading = 'toc-heading-';
        $('#toc-content a').each(function () {
            $(this).attr('href', '#' + tocHeading + (++i));
        });

        // modify the heading title id to support Chinese.
        i = 0;
        $('#articleContent').children('h1, h2, h3, h4').each(function () {
            $(this).attr('id', tocHeading + (++i));
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>

    

</main>



    <footer class="page-footer bg-color">
    <div class="container row center-align" style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            <span id="year">2020</span>
            <a href="https://jackhcc.github.io" target="_blank">æ°å…‹æˆ</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            <br>
            
            &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                class="white-color">3591.2k</span>&nbsp;å­—
            
            
            
            
            
            
            <span id="busuanzi_container_site_pv">
                |&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;<span id="busuanzi_value_site_pv"
                    class="white-color"></span>&nbsp;æ¬¡
            </span>
            
            
            <span id="busuanzi_container_site_uv">
                |&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;<span id="busuanzi_value_site_uv"
                    class="white-color"></span>&nbsp;äºº
            </span>
            
            <br>
            
            <span id="sitetime">è½½å…¥è¿è¡Œæ—¶é—´...</span>
            <script>
                function siteTime() {
                    var seconds = 1000;
                    var minutes = seconds * 60;
                    var hours = minutes * 60;
                    var days = hours * 24;
                    var years = days * 365;
                    var today = new Date();
                    var startYear = "2020";
                    var startMonth = "2";
                    var startDate = "27";
                    var startHour = "6";
                    var startMinute = "30";
                    var startSecond = "0";
                    var todayYear = today.getFullYear();
                    var todayMonth = today.getMonth() + 1;
                    var todayDate = today.getDate();
                    var todayHour = today.getHours();
                    var todayMinute = today.getMinutes();
                    var todaySecond = today.getSeconds();
                    var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                    var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                    var diff = t2 - t1;
                    var diffYears = Math.floor(diff / years);
                    var diffDays = Math.floor((diff / days) - diffYears * 365);
                    var diffHours = Math.floor((diff - (diffYears * 365 + diffDays) * days) / hours);
                    var diffMinutes = Math.floor((diff - (diffYears * 365 + diffDays) * days - diffHours * hours) /
                        minutes);
                    var diffSeconds = Math.floor((diff - (diffYears * 365 + diffDays) * days - diffHours * hours -
                        diffMinutes * minutes) / seconds);
                    if (startYear == todayYear) {
                        document.getElementById("year").innerHTML = todayYear;
                        document.getElementById("sitetime").innerHTML = "æœ¬ç«™å·²å®‰å…¨è¿è¡Œ " + diffDays + " å¤© " + diffHours +
                            " å°æ—¶ " + diffMinutes + " åˆ†é’Ÿ " + diffSeconds + " ç§’";
                    } else {
                        document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                        document.getElementById("sitetime").innerHTML = "æœ¬ç«™å·²å®‰å…¨è¿è¡Œ " + diffYears + " å¹´ " + diffDays +
                            " å¤© " + diffHours + " å°æ—¶ " + diffMinutes + " åˆ†é’Ÿ " + diffSeconds + " ç§’";
                    }
                }
                setInterval(siteTime, 1000);
            </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/JackHCC" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:jackcc0701@163.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>



    <a href="https://www.facebook.com/profile.php?id=100046343443643" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„Facebook: https://www.facebook.com/profile.php?id=100046343443643" data-position="top" data-delay="50">
        <i class="fab fa-facebook-f"></i>
    </a>



    <a href="https://twitter.com/JackChe66021834" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„Twitter: https://twitter.com/JackChe66021834" data-position="top" data-delay="50">
        <i class="fab fa-twitter"></i>
    </a>



    <a href="tencent://AddContact/?fromId=50&fromSubId=1&subcmd=all&uin=2508074836" class="tooltipped" target="_blank" data-tooltip="QQè”ç³»æˆ‘: 2508074836" data-position="top" data-delay="50">
        <i class="fab fa-qq"></i>
    </a>



    <a href="https://weibo.com/u/6885584679" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„å¾®åš: https://weibo.com/u/6885584679" data-position="top" data-delay="50">
        <i class="fab fa-weibo"></i>
    </a>



    <a href="https://www.zhihu.com/people/8f8482f01f0d6a04e844efe32e0f0710" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/8f8482f01f0d6a04e844efe32e0f0710" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script src="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/js/search.js"></script>
<script type="text/javascript">
$(function () {
    searchFunc("https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/" + "search.xml", 'searchInput', 'searchResult');
});
</script>
    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/libs/materialize/materialize.min.js"></script>
    <script src="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/libs/aos/aos.js"></script>
    <script src="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/js/matery.js"></script>

    <canvas class="fireworks" style="position: fixed;left: 0;top: 0;z-index: 1; pointer-events: none;" ></canvas>
    <script type="text/javascript" src="//cdn.bootcss.com/animejs/2.2.0/anime.min.js"></script>
    <script type="text/javascript" src="/js/fireworks.js"></script>

    <script type="text/javascript">
        //åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
        var windowWidth = $(window).width();
        if (windowWidth > 768) {
            document.write('<script type="text/javascript" src="/js/sakura.js"><\/script>'); }
    </script>

    <!-- weather -->
	<script type="text/javascript">
	WIDGET = {FID: 'TToslpmkVO'}
	</script>
	<script type="text/javascript" src="https://apip.weatherdt.com/float/static/js/r.js?v=1111"></script>


    <!-- Global site tag (gtag.js) - Google Analytics -->


    <!-- Baidu Analytics -->

<script>
    var _hmt = _hmt || [];
    (function () {
        var hm = document.createElement("script");
        hm.src = "https://hm.baidu.com/hm.js?";
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(hm, s);
    })();
</script>

    <!-- Baidu Push -->

    
    
    <script async src="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/libs/others/busuanzi.pure.mini.js"></script>
    

    
        <script src="//code.tidio.co/kqhlkxviiccyoa0czpfpu4ijuey9hfre.js"></script>
        <script> 
            $(document).ready(function () {
                setInterval(change_Tidio, 50);  
                function change_Tidio() { 
                    var tidio=$("#tidio-chat iframe");
                    if(tidio.css("display")=="block"&& $(window).width()>977 ){
                        document.getElementById("tidio-chat-iframe").style.bottom= ($("div#backTop.top-scroll").css("display")=="none" &&$(window).width()>977)>0? "-40px" : ($("div.toc-title").length&&$(window).width()>977)>0?"85px":"20px";   
                        document.getElementById("tidio-chat-iframe").style.right="-15px";   
                        document.getElementById("tidio-chat-iframe").style.height=parseInt(tidio.css("height"))>=520?"520px":tidio.css("height");
                        document.getElementById("tidio-chat-iframe").style.zIndex="997";
                    } 
                    else if(tidio.css("display")=="block"&&$(window).width()>601 &&$(window).width()<992 ){
                        document.getElementById("tidio-chat-iframe").style.bottom= ($("div#backTop.top-scroll").css("display")=="none" && 601< $(window).width()<992)>0? "-40px":"20px" ;   
                        document.getElementById("tidio-chat-iframe").style.right="-15px"; 
                        document.getElementById("tidio-chat-iframe").style.zIndex="997";
                    }
                    else if(tidio.css("display")=="block"&&$(window).width()<601 && parseInt(tidio.css("height"))<230){
                        document.getElementById("tidio-chat-iframe").style.bottom= ($("div#backTop.top-scroll").css("display")=="none" && $(window).width()<601)>0? "-10px":"45px" ;   
                        document.getElementById("tidio-chat-iframe").style.zIndex="997";
                    }
                    if( tidio.css("display")=="block"&&$(window).width()<601 && parseInt(tidio.css("height"))>=230){
                        document.getElementById("tidio-chat-iframe").style.zIndex="998";
                    }
                } 
            }); 
        </script>
    

    

    
    <script type="text/javascript" color="0,0,255"
        pointColor="0,0,255" opacity='0.7'
        zIndex="-1" count="99"
        src="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/libs/background/canvas-nest.js"></script>
    

    

    
    <script type="text/javascript" src="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/libs/background/ribbon-dynamic.js" async="async"></script>
    
    
    
    <script src="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/libs/instantpage/instantpage.js" type="module"></script>
    

        <script src="//cdn.jsdelivr.net/npm/js-base64/base64.min.js"></script>
        <script>
        $('a').each(function() {
          const $this = $(this);
          const href = $this.attr('href');
          if (href && href.match('^((http|https|thunder|qqdl|ed2k|Flashget|qbrowser|ftp|rtsp|mms)://)')) {
            const strs = href.split('/');
            if (strs.length >= 3) {
                const host = strs[2];
                if (host !== 'your_domain' || window.location.host) {
                    $this.attr('href', '/go.html?u='+Base64.encode(href)+'').attr('rel', 'external nofollow noopener noreferrer');
                    if (true) {
                        $this.attr('target', '_blank');
                    }
                }
            }
          }
        });
        </script><script>!function(e){var c=Array.prototype.slice.call(document.querySelectorAll("img[data-original]"));function i(){for(var r=0;r<c.length;r++)t=c[r],0<=(n=t.getBoundingClientRect()).bottom&&0<=n.left&&n.top<=(e.innerHeight||document.documentElement.clientHeight)&&function(){var t,n,e,i,o=c[r];t=o,n=function(){c=c.filter(function(t){return o!==t})},e=new Image,i=t.getAttribute("data-original"),e.onload=function(){t.src=i,n&&n()},e.src=i}();var t,n}i(),e.addEventListener("scroll",function(){var t,n;t=i,n=e,clearTimeout(t.tId),t.tId=setTimeout(function(){t.call(n)},500)})}(this);</script><script>window.addEventListener("load",function(){var t=/\.(gif|jpg|jpeg|tiff|png)$/i,r=/^data:image\/[a-z]+;base64,/;Array.prototype.slice.call(document.querySelectorAll("img[data-original]")).forEach(function(a){var e=a.parentNode;"A"===e.tagName&&(e.href.match(t)||e.href.match(r))&&(e.href=a.dataset.original)})});</script></body>

</html>

