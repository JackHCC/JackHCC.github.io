<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="CNN Model详解, JackHCC">
    <meta name="description" content="CNN Model经典模型解释">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <title>CNN Model详解 | JackHCC</title>
    <link rel="icon" type="image/png" href="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/medias/favicon.png">

    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/libs/awesome/css/all.css">
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/libs/materialize/materialize.min.css">
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/libs/aos/aos.css">
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/libs/animate/animate.min.css">
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/libs/lightGallery/css/lightgallery.min.css">
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/css/matery.css">
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/css/my.css">
    
    <script src="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/libs/jquery/jquery.min.js"></script>
    
<meta name="generator" content="Hexo 4.2.0"><link rel="alternate" href="/atom.xml" title="JackHCC" type="application/atom+xml">
<link rel="stylesheet" href="/css/prism-hopscotch.css" type="text/css">
<link rel="stylesheet" href="/css/prism-line-numbers.css" type="text/css"></head>


<body>
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper head-container">
            <div class="brand-logo">
                <a href="/" class="waves-effect waves-light">
                    
                    <img src="/images/loading.gif" data-original="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/medias/me.jpg" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">JackHCC</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="" class="waves-effect waves-light">

      
      <i class="fas fa-list" style="zoom: 0.6;"></i>
      
      <span>Tools</span>
      <i class="fas fa-chevron-down" aria-hidden="true" style="zoom: 0.6;"></i>
    </a>
    <ul class="sub-nav menus_item_child ">
      
      <li>
        <a href="https://creativecc.cn/" target="_blank" rel="noopener">
          
          <i class="fas fa-book" style="margin-top: -20px; zoom: 0.6;"></i>
          
          <span>Creative工具导航</span>
        </a>
      </li>
      
      <li>
        <a href="https://blog.creativecc.cn/Arxiv-NLP-Reporter/" target="_blank" rel="noopener">
          
          <i class="fas fa-film" style="margin-top: -20px; zoom: 0.6;"></i>
          
          <span>NLP每日论文</span>
        </a>
      </li>
      
      <li>
        <a href="http://chat.creativecc.cn/" target="_blank" rel="noopener">
          
          <i class="fas fa-music" style="margin-top: -20px; zoom: 0.6;"></i>
          
          <span>RocketChat聊天室</span>
        </a>
      </li>
      
      <li>
        <a href="/contact">
          
          <i class="fas fa-comments" style="margin-top: -20px; zoom: 0.6;"></i>
          
          <span>Contact留言板</span>
        </a>
      </li>
      
    </ul>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/friends" class="waves-effect waves-light">
      
      <i class="fas fa-address-book" style="zoom: 0.6;"></i>
      
      <span>友情链接</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/about" class="waves-effect waves-light">
      
      <i class="fas fa-user-circle" style="zoom: 0.6;"></i>
      
      <span>关于</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>

<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/images/loading.gif" data-original="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/medias/me.jpg" class="logo-img circle responsive-img">
        
        <div class="logo-name">JackHCC</div>
        <div class="logo-desc">
            
            Make the world betterrrr!!!
            
        </div>
    </div>

    

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="javascript:;">
			
				<i class="fa-fw fas fa-list"></i>
			
			Tools
			<span class="m-icon"><i class="fas fa-chevron-right"></i></span>
		</a>
            <ul  style="background:  ;" >
              
                <li>   
				
                  <a href="https://creativecc.cn/ " target="_blank" rel="noopener" style="margin-left:75px";>
				  
				   <i class="fa fas fa-book" style="position: absolute;left:50px" ></i>
			      
		          <span>Creative工具导航</span>
                  </a>
                </li>
              
                <li>   
				
                  <a href="https://blog.creativecc.cn/Arxiv-NLP-Reporter/ " target="_blank" rel="noopener" style="margin-left:75px";>
				  
				   <i class="fa fas fa-film" style="position: absolute;left:50px" ></i>
			      
		          <span>NLP每日论文</span>
                  </a>
                </li>
              
                <li>   
				
                  <a href="http://chat.creativecc.cn/ " target="_blank" rel="noopener" style="margin-left:75px";>
				  
				   <i class="fa fas fa-music" style="position: absolute;left:50px" ></i>
			      
		          <span>RocketChat聊天室</span>
                  </a>
                </li>
              
                <li>   
				
                  <a href="/contact " style="margin-left:75px";>
				  
				   <i class="fa fas fa-comments" style="position: absolute;left:50px" ></i>
			      
		          <span>Contact留言板</span>
                  </a>
                </li>
               
            </ul>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/friends" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-address-book"></i>
			
			友情链接
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/about" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-user-circle"></i>
			
			关于
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/JackHCC/JackHCC.github.io" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>

        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/JackHCC/JackHCC.github.io" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    
<script src="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/libs/cryptojs/crypto-js.min.js"></script>
<script>
    (function() {
        let pwd = '';
        if (pwd && pwd.length > 0) {
            if (pwd !== CryptoJS.SHA256(prompt('请输入访问本文章的密码')).toString(CryptoJS.enc.Hex)) {
                alert('密码错误，将返回主页！');
                location.href = '/';
            }
        }
    })();
</script>




<div class="bg-cover pd-header post-cover" style="background-image: url('https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/medias/featureimages/9.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">CNN Model详解</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/libs/tocbot/tocbot.css">
<style>
    #articleContent h1::before,
    #articleContent h2::before,
    #articleContent h3::before,
    #articleContent h4::before,
    #articleContent h5::before,
    #articleContent h6::before {
        display: block;
        content: " ";
        height: 100px;
        margin-top: -100px;
        visibility: hidden;
    }

    #articleContent :focus {
        outline: none;
    }

    .toc-fixed {
        position: fixed;
        top: 64px;
    }

    .toc-widget {
        width: 345px;
        padding-left: 20px;
    }

    .toc-widget .toc-title {
        margin: 35px 0 15px 0;
        padding-left: 17px;
        font-size: 1.5rem;
        font-weight: bold;
        line-height: 1.5rem;
    }

    .toc-widget ol {
        padding: 0;
        list-style: none;
    }

    #toc-content {
        height: calc(100vh - 250px);
        overflow: auto;
    }

    #toc-content ol {
        padding-left: 10px;
    }

    #toc-content ol li {
        padding-left: 10px;
    }

    #toc-content .toc-link:hover {
        color: #42b983;
        font-weight: 700;
        text-decoration: underline;
    }

    #toc-content .toc-link::before {
        background-color: transparent;
        max-height: 25px;
    }

    #toc-content .is-active-link {
        color: #42b983;
    }

    #toc-content .is-active-link::before {
        background-color: #42b983;
    }

    #floating-toc-btn {
        position: fixed;
        right: 30px;
        bottom: 146px;
        padding-top: 15px;
        margin-bottom: 0;
        z-index: 998;
    }

    #floating-toc-btn .btn-floating {
        width: 48px;
        height: 48px;
    }

    #floating-toc-btn .btn-floating i {
        line-height: 48px;
        font-size: 1.4rem;
    }
</style>
<div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/tags/CNN/">
                                <span class="chip bg-color">CNN</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/categories/Deep-Learning/" class="post-category">
                                Deep Learning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2021-09-03
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2021-09-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    18.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    69 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>

        </div>
        <hr class="clearfix">
        <div class="card-content article-card-content">
            <div id="articleContent">
                <h1 id="LeNet"><a href="#LeNet" class="headerlink" title="LeNet"></a>LeNet</h1><ol>
<li><p>1998年<code>LeCun</code> 推出了<code>LeNet</code> 网络，它是第一个广为流传的卷积神经网络。</p>
<p><img src="/images/loading.gif" data-original="../images/ML/8725a44f7fa00511ad52170b6e29660a.jpeg" alt=""></p>
</li>
<li><p><code>LeNet</code> 网络包含了卷积层、池化层、全连接层，这些都是现代<code>CNN</code> 网络的基本组件。</p>
<ul>
<li><p>输入层：二维图像，尺寸为<code>32x32</code>。</p>
</li>
<li><p><code>C1、C3、C5</code> 层：二维卷积层。</p>
<p>其中<code>C5</code> 将输入的 <code>feature map</code>（尺寸 <code>16@5x5</code> ）转化为尺寸为<code>120x1x1</code> 的 <code>feature map</code>，然后转换为长度为<code>120</code> 的一维向量。</p>
<p>这是一种常见的、将卷积层的输出转换为全连接层的输入的一种方法。</p>
</li>
<li><p><code>S2、S4</code> 层：池化层。使用<code>sigmoid</code> 函数作为激活函数。</p>
<blockquote>
<p>后续的 <code>CNN</code> 都使用<code>ReLU</code> 作为激活函数。</p>
</blockquote>
</li>
<li><p><code>F6</code> 层：全连接层。</p>
</li>
<li><p>输出层：由欧式径向基函数单元组成。</p>
<blockquote>
<p>后续的<code>CNN</code> 使用<code>softmax</code> 输出单元。</p>
</blockquote>
<p>下表中，<code>@</code> 分隔了通道数量和<code>feature map</code> 的宽、高。</p>
<table>
<thead>
<tr>
<th align="left">网络层</th>
<th align="left">核/池大小</th>
<th align="left">核数量</th>
<th align="left">步长</th>
<th align="left">输入尺寸</th>
<th align="left">输出尺寸</th>
</tr>
</thead>
<tbody><tr>
<td align="left">INPUT</td>
<td align="left">-</td>
<td align="left">-</td>
<td align="left">-</td>
<td align="left">-</td>
<td align="left">1@32x32</td>
</tr>
<tr>
<td align="left">C1</td>
<td align="left">5x5</td>
<td align="left">6</td>
<td align="left">1</td>
<td align="left">1@32x32</td>
<td align="left">6@28x28</td>
</tr>
<tr>
<td align="left">S2</td>
<td align="left">2x2</td>
<td align="left">-</td>
<td align="left">2</td>
<td align="left">6@28x28</td>
<td align="left">6@14x14</td>
</tr>
<tr>
<td align="left">C3</td>
<td align="left">5x5</td>
<td align="left">16</td>
<td align="left">1</td>
<td align="left">6@14x14</td>
<td align="left">16@10x10</td>
</tr>
<tr>
<td align="left">S4</td>
<td align="left">2x2</td>
<td align="left">-</td>
<td align="left">2</td>
<td align="left">16@10x10</td>
<td align="left">16@5x5</td>
</tr>
<tr>
<td align="left">C5</td>
<td align="left">5x5</td>
<td align="left">120</td>
<td align="left">1</td>
<td align="left">16@5x5</td>
<td align="left">120@1x1</td>
</tr>
<tr>
<td align="left">F6</td>
<td align="left">-</td>
<td align="left">-</td>
<td align="left">-</td>
<td align="left">120</td>
<td align="left">84</td>
</tr>
<tr>
<td align="left">OUTPUT</td>
<td align="left">-</td>
<td align="left">-</td>
<td align="left">-</td>
<td align="left">84</td>
<td align="left">10</td>
</tr>
</tbody></table>
</li>
</ul>
</li>
</ol>
<h1 id="AlexNet"><a href="#AlexNet" class="headerlink" title="AlexNet"></a>AlexNet</h1><p>2012年<code>Hinton</code> 和他的学生推出了<code>AlexNet</code> 。在当年的<code>ImageNet</code> 图像分类竞赛中，<code>AlexeNet</code> 以远超第二名的成绩夺冠，使得深度学习重回历史舞台，具有重大历史意义。</p>
<h2 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h2><ol>
<li><p><code>AlexNet</code> 有5个广义卷积层和3个广义全连接层。</p>
<ul>
<li>广义的卷积层：包含了卷积层、池化层、<code>ReLU</code>、<code>LRN</code> 层等。</li>
<li>广义全连接层：包含了全连接层、<code>ReLU</code>、<code>Dropout</code> 层等。</li>
</ul>
<p><img src="/images/loading.gif" data-original="../images/ML/8a04aec2a961f8890efbd0e5edcf02d5.jpeg" alt=""></p>
</li>
<li><p>网络结构如下表所示：</p>
<ul>
<li><p>输入层会将<code>3@224x224</code> 的三维图片预处理变成<code>3@227x227</code> 的三维图片。</p>
</li>
<li><p>第二层广义卷积层、第四层广义卷积层、第五层广义卷积层都是分组卷积，仅采用本<code>GPU</code> 内的通道数据进行计算。</p>
<p>第一层广义卷积层、第三层广义卷积层、第六层连接层、第七层连接层、第八层连接层执行的是全部通道数据的计算。</p>
</li>
<li><p>第二层广义卷积层的卷积、第三层广义卷积层的卷积、第四层广义卷积层的卷积、第五层广义卷积层的卷积均采用<code>same</code> 填充。</p>
<blockquote>
<p>当卷积的步长为1，核大小为<code>3x3</code> 时，如果不填充0，则<code>feature map</code> 的宽/高都会缩减 2 。因此这里填充0，使得输出<code>feature map</code> 的宽/高保持不变。</p>
</blockquote>
<p>其它层的卷积，以及所有的池化都是<code>valid</code> 填充（即：不填充 0 ）。</p>
</li>
<li><p>第六层广义连接层的卷积之后，会将<code>feature map</code> 展平为长度为 4096 的一维向量。</p>
</li>
</ul>
<table>
<thead>
<tr>
<th align="left">编号</th>
<th align="left">网络层</th>
<th align="left">子层</th>
<th align="left">核/池大小</th>
<th align="left">核数量</th>
<th align="left">步长</th>
<th align="left">激活函数</th>
<th align="left">输入尺寸</th>
<th align="left">输出尺寸</th>
</tr>
</thead>
<tbody><tr>
<td align="left">第0层</td>
<td align="left">输入层</td>
<td align="left">-</td>
<td align="left">-</td>
<td align="left">-</td>
<td align="left">-</td>
<td align="left">-</td>
<td align="left">-</td>
<td align="left">3@224x224</td>
</tr>
<tr>
<td align="left">第1层</td>
<td align="left">广义卷积层</td>
<td align="left">卷积</td>
<td align="left">11x11</td>
<td align="left">96</td>
<td align="left">4</td>
<td align="left">ReLU</td>
<td align="left">3@227x227</td>
<td align="left">96@55x55</td>
</tr>
<tr>
<td align="left">第1层</td>
<td align="left">广义卷积层</td>
<td align="left">池化</td>
<td align="left">3x3</td>
<td align="left">-</td>
<td align="left">2</td>
<td align="left">-</td>
<td align="left">96@55x55</td>
<td align="left">96@27x27</td>
</tr>
<tr>
<td align="left">第1层</td>
<td align="left">广义卷积层</td>
<td align="left">LRN</td>
<td align="left">-</td>
<td align="left">-</td>
<td align="left">-</td>
<td align="left">-</td>
<td align="left">96@27x27</td>
<td align="left">96@27x27</td>
</tr>
<tr>
<td align="left">第2层</td>
<td align="left">广义卷积层</td>
<td align="left">卷积</td>
<td align="left">5x5</td>
<td align="left">256</td>
<td align="left">1</td>
<td align="left">ReLU</td>
<td align="left">96@27x27</td>
<td align="left">256@27x27</td>
</tr>
<tr>
<td align="left">第2层</td>
<td align="left">广义卷积层</td>
<td align="left">池化</td>
<td align="left">3x3</td>
<td align="left">-</td>
<td align="left">2</td>
<td align="left">-</td>
<td align="left">256@27x27</td>
<td align="left">256@13x13</td>
</tr>
<tr>
<td align="left">第2层</td>
<td align="left">广义卷积层</td>
<td align="left">LRN</td>
<td align="left">-</td>
<td align="left">-</td>
<td align="left">-</td>
<td align="left">-</td>
<td align="left">256@13x13</td>
<td align="left">256@13x13</td>
</tr>
<tr>
<td align="left">第3层</td>
<td align="left">广义卷积层</td>
<td align="left">卷积</td>
<td align="left">3x3</td>
<td align="left">384</td>
<td align="left">1</td>
<td align="left">ReLU</td>
<td align="left">256@13x13</td>
<td align="left">384@13x13</td>
</tr>
<tr>
<td align="left">第4层</td>
<td align="left">广义卷积层</td>
<td align="left">卷积</td>
<td align="left">3x3</td>
<td align="left">384</td>
<td align="left">1</td>
<td align="left">ReLU</td>
<td align="left">384@13x13</td>
<td align="left">384@13x13</td>
</tr>
<tr>
<td align="left">第5层</td>
<td align="left">广义卷积层</td>
<td align="left">卷积</td>
<td align="left">3x3</td>
<td align="left">256</td>
<td align="left">1</td>
<td align="left">ReLU</td>
<td align="left">384@13x13</td>
<td align="left">256@13x13</td>
</tr>
<tr>
<td align="left">第5层</td>
<td align="left">广义卷积层</td>
<td align="left">池化</td>
<td align="left">3x3</td>
<td align="left">-</td>
<td align="left">2</td>
<td align="left">-</td>
<td align="left">256@13x13</td>
<td align="left">256@6x6</td>
</tr>
<tr>
<td align="left">第6层</td>
<td align="left">广义连接层</td>
<td align="left">卷积</td>
<td align="left">6x6</td>
<td align="left">4096</td>
<td align="left">1</td>
<td align="left">ReLU</td>
<td align="left">256@6x6</td>
<td align="left">4096@1x1</td>
</tr>
<tr>
<td align="left">第6层</td>
<td align="left">广义连接层</td>
<td align="left">dropout</td>
<td align="left">-</td>
<td align="left">-</td>
<td align="left">-</td>
<td align="left">-</td>
<td align="left">4096@1x1</td>
<td align="left">4096@1x1</td>
</tr>
<tr>
<td align="left">第7层</td>
<td align="left">广义连接层</td>
<td align="left">全连接</td>
<td align="left">-</td>
<td align="left">-</td>
<td align="left">-</td>
<td align="left">ReLU</td>
<td align="left">4096</td>
<td align="left">4096</td>
</tr>
<tr>
<td align="left">第7层</td>
<td align="left">广义连接层</td>
<td align="left">dropout</td>
<td align="left">-</td>
<td align="left">-</td>
<td align="left">-</td>
<td align="left">-</td>
<td align="left">4096</td>
<td align="left">4096</td>
</tr>
<tr>
<td align="left">第8层</td>
<td align="left">广义连接层</td>
<td align="left">全连接</td>
<td align="left">-</td>
<td align="left">-</td>
<td align="left">-</td>
<td align="left">-</td>
<td align="left">4096</td>
<td align="left">1000</td>
</tr>
</tbody></table>
</li>
<li><p>网络参数数量：总计约 6237万。</p>
<ul>
<li><p>输出<code>Tensor size</code> 采用<code>channel last</code> 风格描述。即<code>227x227x3</code> 等价于前文的 <code>3@227x227</code> 。</p>
</li>
<li><p>第6层广义连接层的卷积的参数数量最多，约3770万，占整体六千万参数的 60%。</p>
<p>原因是该子层的卷积核较大、输入通道数量较大、输出通道数量太多。该卷积需要的参数数量为：</p>
<p><img src="/images/loading.gif" data-original="../images/ML/9a99e0c7bc1e0ff182557721e60c3113.svg" alt=""> 。</p>
</li>
</ul>
<table>
<thead>
<tr>
<th align="left">编号</th>
<th align="left">网络层</th>
<th align="left">子层</th>
<th align="left">输出 Tensor size</th>
<th align="left">权重个数</th>
<th align="left">偏置个数</th>
<th align="left">参数数量</th>
</tr>
</thead>
<tbody><tr>
<td align="left">第0层</td>
<td align="left">输入层</td>
<td align="left">-</td>
<td align="left">227x227x3</td>
<td align="left">0</td>
<td align="left">0</td>
<td align="left">0</td>
</tr>
<tr>
<td align="left">第1层</td>
<td align="left">广义卷积层</td>
<td align="left">卷积</td>
<td align="left">55x55x96</td>
<td align="left">34848</td>
<td align="left">96</td>
<td align="left">34944</td>
</tr>
<tr>
<td align="left">第1层</td>
<td align="left">广义卷积层</td>
<td align="left">池化</td>
<td align="left">27x27x96</td>
<td align="left">0</td>
<td align="left">0</td>
<td align="left">0</td>
</tr>
<tr>
<td align="left">第1层</td>
<td align="left">广义卷积层</td>
<td align="left">LRN</td>
<td align="left">27x27x96</td>
<td align="left">0</td>
<td align="left">0</td>
<td align="left">0</td>
</tr>
<tr>
<td align="left">第2层</td>
<td align="left">广义卷积层</td>
<td align="left">卷积</td>
<td align="left">27x27x256</td>
<td align="left">614400</td>
<td align="left">256</td>
<td align="left">614656</td>
</tr>
<tr>
<td align="left">第2层</td>
<td align="left">广义卷积层</td>
<td align="left">池化</td>
<td align="left">13x13x256</td>
<td align="left">0</td>
<td align="left">0</td>
<td align="left">0</td>
</tr>
<tr>
<td align="left">第2层</td>
<td align="left">广义卷积层</td>
<td align="left">LRN</td>
<td align="left">13x13x256</td>
<td align="left">0</td>
<td align="left">0</td>
<td align="left">0</td>
</tr>
<tr>
<td align="left">第3层</td>
<td align="left">广义卷积层</td>
<td align="left">卷积</td>
<td align="left">13x13x384</td>
<td align="left">884736</td>
<td align="left">384</td>
<td align="left">885120</td>
</tr>
<tr>
<td align="left">第4层</td>
<td align="left">广义卷积层</td>
<td align="left">卷积</td>
<td align="left">13x13x384</td>
<td align="left">1327104</td>
<td align="left">384</td>
<td align="left">1327488</td>
</tr>
<tr>
<td align="left">第5层</td>
<td align="left">广义卷积层</td>
<td align="left">卷积</td>
<td align="left">13x13x256</td>
<td align="left">884736</td>
<td align="left">256</td>
<td align="left">884992</td>
</tr>
<tr>
<td align="left">第5层</td>
<td align="left">广义卷积层</td>
<td align="left">池化</td>
<td align="left">6x6x256</td>
<td align="left">0</td>
<td align="left">0</td>
<td align="left">0</td>
</tr>
<tr>
<td align="left">第6层</td>
<td align="left">广义连接层</td>
<td align="left">卷积</td>
<td align="left">4096×1</td>
<td align="left">37748736</td>
<td align="left">4096</td>
<td align="left">37752832</td>
</tr>
<tr>
<td align="left">第6层</td>
<td align="left">广义连接层</td>
<td align="left">dropout</td>
<td align="left">4096×1</td>
<td align="left">0</td>
<td align="left">0</td>
<td align="left">0</td>
</tr>
<tr>
<td align="left">第7层</td>
<td align="left">广义连接层</td>
<td align="left">全连接</td>
<td align="left">4096×1</td>
<td align="left">16777216</td>
<td align="left">4096</td>
<td align="left">16781312</td>
</tr>
<tr>
<td align="left">第7层</td>
<td align="left">广义连接层</td>
<td align="left">dropout</td>
<td align="left">4096×1</td>
<td align="left">0</td>
<td align="left">0</td>
<td align="left">0</td>
</tr>
<tr>
<td align="left">第8层</td>
<td align="left">广义连接层</td>
<td align="left">全连接</td>
<td align="left">1000×1</td>
<td align="left">4096000</td>
<td align="left">1000</td>
<td align="left">4097000</td>
</tr>
<tr>
<td align="left">总计</td>
<td align="left">-</td>
<td align="left">-</td>
<td align="left">-</td>
<td align="left">-</td>
<td align="left">-</td>
<td align="left">62,378,344</td>
</tr>
</tbody></table>
</li>
</ol>
<h2 id="设计技巧"><a href="#设计技巧" class="headerlink" title="设计技巧"></a>设计技巧</h2><p><code>AlexNet</code> 成功的主要原因在于：</p>
<ul>
<li>使用<code>ReLU</code> 激活函数。</li>
<li>使用<code>dropout</code>、数据集增强 、重叠池化等防止过拟合的方法。</li>
<li>使用百万级的大数据集来训练。</li>
<li>使用<code>GPU</code>训练，以及的<code>LRN</code> 使用。</li>
<li>使用带动量的 <code>mini batch</code> 随机梯度下降来训练。</li>
</ul>
<h3 id="数据集增强"><a href="#数据集增强" class="headerlink" title="数据集增强"></a>数据集增强</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210903120402149.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210903120420935.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/dbaab968a0be751bd2ee9c301c1de1d1.png" alt=""></p>
<h3 id="局部响应规范化"><a href="#局部响应规范化" class="headerlink" title="局部响应规范化"></a>局部响应规范化</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210903120456613.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/fa1d4c30fa64c609af620f05838c9f5b.png" alt=""></p>
<h3 id="多GPU-训练"><a href="#多GPU-训练" class="headerlink" title="多GPU 训练"></a>多GPU 训练</h3><p><code>AlexNet</code> 使用两个<code>GPU</code>训练。网络结构图由上、下两部分组成：一个<code>GPU</code>运行图上方的通道数据，一个<code>GPU</code> 运行图下方的通道数据，两个<code>GPU</code> 只在特定的网络层通信。即：执行分组卷积。</p>
<ul>
<li>第二、四、五层卷积层的核只和同一个<code>GPU</code> 上的前一层的<code>feature map</code> 相连。</li>
<li>第三层卷积层的核和前一层所有<code>GPU</code> 的<code>feature map</code> 相连。</li>
<li>全连接层中的神经元和前一层中的所有神经元相连。</li>
</ul>
<h3 id="重叠池化"><a href="#重叠池化" class="headerlink" title="重叠池化"></a>重叠池化</h3><ol>
<li><p>一般的池化是不重叠的，池化区域的大小与步长相同。<code>Alexnet</code> 中，池化是可重叠的，即：步长小于池化区域的大小。</p>
<p>重叠池化可以缓解过拟合，该策略贡献了<code>0.4%</code> 的错误率。</p>
</li>
<li><p>为什么重叠池化会减少过拟合，很难用数学甚至直观上的观点来解答。一个稍微合理的解释是：重叠池化会带来更多的特征，这些特征很可能会有利于提高模型的泛化能力。</p>
</li>
</ol>
<h3 id="优化算法"><a href="#优化算法" class="headerlink" title="优化算法"></a>优化算法</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210903120625737.png" alt=""></p>
<h1 id="VGG-Net"><a href="#VGG-Net" class="headerlink" title="VGG-Net"></a>VGG-Net</h1><ol>
<li><code>VGG-Net</code> 是牛津大学计算机视觉组和<code>DeepMind</code>公司共同研发一种深度卷积网络，并且在2014年在<code>ILSVRC</code>比赛上获得了分类项目的第二名和定位项目的第一名。</li>
<li><code>VGG-Net</code> 的主要贡献是：<ul>
<li>证明了小尺寸卷积核（<code>3x3</code> ）的深层网络要优于大尺寸卷积核的浅层网络。</li>
<li>证明了深度对网络的泛化性能的重要性。</li>
<li>验证了尺寸抖动<code>scale jittering</code> 这一数据增强技术的有效性。</li>
</ul>
</li>
<li><code>VGG-Net</code> 最大的问题在于参数数量，<code>VGG-19</code> 基本上是参数数量最多的卷积网络架构。</li>
</ol>
<h2 id="网络结构-1"><a href="#网络结构-1" class="headerlink" title="网络结构"></a>网络结构</h2><p><img src="/images/loading.gif" data-original="../images/ML/c03124a8c5f65c874007158471e5419a.png" alt=""></p>
<ol>
<li><p>通用结构：</p>
<ul>
<li><p>输入层：固定大小的<code>224x224</code> 的<code>RGB</code> 图像。</p>
</li>
<li><p>卷积层：卷积步长均为1。</p>
<ul>
<li><p>填充方式：填充卷积层的输入，使得卷积前后保持同样的空间分辨率。</p>
<ul>
<li><code>3x3</code> 卷积：<code>same</code> 填充，即：输入的上下左右各填充1个像素。</li>
<li><code>1x1</code> 卷积：不需要填充。</li>
</ul>
</li>
<li><p>卷积核尺寸：有<code>3x3</code> 和<code>1x1</code> 两种。</p>
<ul>
<li><p><code>3x3</code> 卷积核：这是捕获左右、上下、中心等概念的最小尺寸。</p>
</li>
<li><p><code>1x1</code> 卷积核：用于输入通道的线性变换。</p>
<p>在它之后接一个<code>ReLU</code> 激活函数，使得输入通道执行了非线性变换。</p>
</li>
</ul>
</li>
</ul>
</li>
<li><p>池化层：采用最大池化。</p>
<ul>
<li>池化层连接在卷积层之后，但并不是所有的卷积层之后都有池化。</li>
<li>池化窗口为<code>2x2</code>，步长为 2 。</li>
</ul>
</li>
<li><p>网络最后四层为：：三个全连接层 + 一个<code>softmax</code> 层。</p>
<ul>
<li>前两个全连接层都是 4096个神经元，第三个全连接层是 1000 个神经元（因为执行的是 1000 类的分类）。</li>
<li>最后一层是<code>softmax</code> 层用于输出类别的概率。</li>
</ul>
</li>
<li><p>所有隐层都使用<code>ReLU</code> 激活函数。</p>
</li>
</ul>
</li>
<li><p><code>VGG-Net</code> 网络参数数量：</p>
<p>其中第一个全连接层的参数数量为：<code>7x7x512x4096=1.02亿</code> ，因此网络绝大部分参数来自于该层。</p>
<blockquote>
<p>与<code>AlexNet</code> 相比，<code>VGG-Net</code> 在第一个全连接层的输入<code>feature map</code> 较大：<code>7x7 vs 6x6</code>，<code>512 vs 256</code> 。</p>
</blockquote>
<table>
<thead>
<tr>
<th align="left">网络</th>
<th align="left">A , A-LRN</th>
<th align="left">B</th>
<th align="left">C</th>
<th align="left">D</th>
<th align="left">E</th>
</tr>
</thead>
<tbody><tr>
<td align="left">参数数量</td>
<td align="left">1.13亿</td>
<td align="left">1.33亿</td>
<td align="left">1.34亿</td>
<td align="left">1.38亿</td>
<td align="left">1.44</td>
</tr>
</tbody></table>
</li>
</ol>
<h2 id="设计技巧-1"><a href="#设计技巧-1" class="headerlink" title="设计技巧"></a>设计技巧</h2><p><img src="/images/loading.gif" data-original="../images/ML/image-20210903121105102.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210903121127028.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210903121149885.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210903121204805.png" alt=""></p>
<h1 id="Inception"><a href="#Inception" class="headerlink" title="Inception"></a>Inception</h1><ol>
<li><p><code>Inception</code> 网络是卷积神经网络的一个重要里程碑。在<code>Inception</code> 之前，大部分流行的卷积神经网络仅仅是把卷积层堆叠得越来越多，使得网络越来越深。这使得网络越来越复杂，参数越来越多，从而导致网络容易出现过拟合，增加计算量。</p>
<p>而<code>Inception</code> 网络考虑的是多种卷积核的并行计算，扩展了网络的宽度。</p>
</li>
<li><p><code>Inception Net</code> 核心思想是：稀疏连接。因为生物神经连接是稀疏的。</p>
</li>
<li><p><code>Inception</code> 网络的最大特点是大量使用了<code>Inception</code> 模块。</p>
</li>
</ol>
<h2 id="Inception-v1"><a href="#Inception-v1" class="headerlink" title="Inception v1"></a>Inception v1</h2><h3 id="网络结构-2"><a href="#网络结构-2" class="headerlink" title="网络结构"></a>网络结构</h3><ol>
<li><code>InceptionNet V1</code> 是一个22层的深度网络。 如果考虑池化层，则有29层。如下图中的<code>depth</code> 列所示。</li>
</ol>
<p>网络具有三组<code>Inception</code> 模块，分别为：<code>inception(3a)/inception(3b)</code>、<code>inception(4a)/inception(4b)/inception(4c)/inception(4d)/inception(4e)</code>、<code>inception(5a)、inception(5b)</code>。三组<code>Inception</code> 模块被池化层分</p>
<p><img src="/images/loading.gif" data-original="../images/ML/d992bb403b9072c433130509a8f76c20-16306423944652.jpeg" alt=""></p>
<ol start="2">
<li>下图给出了网络的层次结构和参数，其中：</li>
</ol>
<ul>
<li><code>type</code> 列：给出了每个模块/层的类型。</li>
<li><code>patch size/stride</code> 列：给出了卷积层/池化层的尺寸和步长。</li>
<li><code>output size</code> 列：给出了每个模块/层的输出尺寸和输出通道数。</li>
<li><code>depth</code>列：给出了每个模块/层包含的、含有训练参数层的数量。</li>
<li><code>#1x1</code>列：给出了每个模块/层包含的<code>1x1</code> 卷积核的数量，它就是<code>1x1</code> 卷积核的输出通道数。</li>
<li><code>#3x3 reduce</code>列：给出了每个模块/层包含的、放置在<code>3x3</code> 卷积层之前的<code>1x1</code> 卷积核的数量，它就是<code>1x1</code> 卷积核的输出通道数。</li>
<li><code>#3x3</code>列：给出了每个模块/层包含的<code>3x3</code> 卷积核的数量，它就是<code>3x3</code> 卷积核的输出通道数。</li>
<li><code>#5x5 reduce</code>列：给出了每个模块/层包含的、放置在<code>5x5</code> 卷积层之前的<code>1x1</code> 卷积核的数量，它就是<code>1x1</code> 卷积核的输出通道数。</li>
<li><code>#5x5</code>列：给出了每个模块/层包含的<code>5x5</code> 卷积核的数量，它就是<code>5x5</code>卷积核的输出通道数。</li>
<li><code>pool proj</code>列：给出了每个模块/层包含的、放置在池化层之后的<code>1x1</code> 卷积核的数量，它就是<code>1x1</code> 卷积核的输出通道数。</li>
<li><code>params</code>列：给出了每个模块/层的参数数量。</li>
<li><code>ops</code>列：给出了每个模块/层的计算量。</li>
</ul>
<p><img src="/images/loading.gif" data-original="../images/ML/1ec7fde8459ea7ae0742d2763b174484.png" alt=""></p>
<ol start="3">
<li><code>Inception V1</code> 的参数数量为 697.7 万，其参数数量远远小于<code>AlexNet</code>（6千万）、<code>VGG-Net</code>（超过1亿）。</li>
</ol>
<p><code>Inception V1</code> 参数数量能缩减的一个主要技巧是：在<code>inception(5b)</code>输出到<code>linear</code>之间插入一个平均池化层<code>avg pool</code>。</p>
<ul>
<li>如果没有平均池化层，则<code>inception(5b)</code> 到 <code>linear</code> 之间的参数数量为：<code>7x7x1024x1024</code>，约为 5 千万。</li>
<li>插入了平均池化层之后，<code>inception(5b)</code> 到 <code>linear</code> 之间的参数数量为：<code>1x1x1024x1024</code>，约为 1百万。</li>
</ul>
<h3 id="Inception-模块"><a href="#Inception-模块" class="headerlink" title="Inception 模块"></a>Inception 模块</h3><ol>
<li><p>原始的<code>Inception</code> 模块对输入同时执行：3个不同大小的卷积操作（<code>1x1、3x3、5x5</code>）、1个最大池化操作（<code>3x3</code> ）。所有操作的输出都在深度方向拼接起来，向后一级传递。</p>
<ul>
<li><p>三种不同大小卷积：通过不同尺寸的卷积核抓取不同大小的对象的特征。</p>
<p>使用<code>1x1、3x3、5x5</code> 这些具体尺寸仅仅是为了便利性，事实上也可以使用更多的、其它尺寸的滤波器。</p>
</li>
<li><p>1个最大池化：提取图像的原始特征（不经过过滤器）。</p>
</li>
</ul>
<p><img src="/images/loading.gif" data-original="../images/ML/d265fe5e03257490ac19bffbc0da846a.png" alt=""></p>
</li>
<li><p>原始<code>Inception</code> 模块中，模块的输出通道数量为四个子层的输出通道数的叠加。这种叠加不可避免的使得<code>Inception</code> 模块的输出通道数增加，这就增加了<code>Inception</code> 模块中每个卷积的计算量。因此在经过若干个模块之后，计算量会爆炸性增长。</p>
<p>解决方案是：在<code>3x3</code> 和 <code>5x5</code> 卷积层之前额外添加<code>1x1</code> 卷积层，来限制输入给卷积层的输入通道的数量。</p>
<p>注意：</p>
<ul>
<li><code>1x1</code> 卷积是在最大池化层之后，而不是之前。这是因为：池化层是为了提取图像的原始特征，一旦它接在<code>1x1</code> 卷积之后就失去了最初的本意。</li>
<li><code>1x1</code> 卷积在<code>3x3</code>、<code>5x5</code> 卷积之前。这是因为：如果<code>1x1</code> 卷积在它们之后，则<code>3x3</code> 卷积、<code>5x5</code> 卷积的输入通道数太大，导致计算量仍然巨大。</li>
</ul>
<p><img src="/images/loading.gif" data-original="../images/ML/a126d50cd07775945950909e8137e210.png" alt=""></p>
</li>
</ol>
<h3 id="辅助分类器"><a href="#辅助分类器" class="headerlink" title="辅助分类器"></a>辅助分类器</h3><ol>
<li><p>为了缓解梯度消失的问题，<code>InceptionNet V1</code> 给出了两个辅助分类器。这两个辅助分类器被添加到网络的中间层，它们和主分类器共享同一套训练数据及其标记。其中：</p>
<ul>
<li>第一个辅助分类器位于<code>Inception(4a)</code> 之后，<code>Inception(4a)</code> 模块的输出作为它的输入。</li>
<li>第二个辅助分类器位于<code>Inception(4d)</code> 之后，<code>Inception(4d)</code> 模块的输出作为它的输入。</li>
<li>两个辅助分类器的结构相同，包括以下组件：<ul>
<li>一个尺寸为<code>5x5</code>、步长为<code>3</code>的平均池化层。</li>
<li>一个尺寸为<code>1x1</code>、输出通道数为<code>128</code> 的卷积层。</li>
<li>一个具有<code>1024</code> 个单元的全连接层。</li>
<li>一个<code>drop rate = 70%</code>的 <code>dropout</code> 层。</li>
<li>一个使用<code>softmax</code> 损失的线性层作为输出层。</li>
</ul>
</li>
</ul>
</li>
<li><p>在训练期间，两个辅助分类器的损失函数的权重是0.3，它们的损失被叠加到网络的整体损失上。在推断期间，这两个辅助网络被丢弃。</p>
<p>在<code>Inception v3</code> 的实验中表明：辅助网络的影响相对较小，只需要其中一个就能够取得同样的效果。</p>
<p>事实上辅助分类器在训练早期并没有多少贡献。只有在训练接近结束，辅助分支网络开始发挥作用，获得超出无辅助分类器网络的结果。</p>
</li>
<li><p>两个辅助分类器的作用：提供正则化的同时，克服了梯度消失问题。</p>
</li>
</ol>
<h2 id="Inception-v2"><a href="#Inception-v2" class="headerlink" title="Inception v2"></a>Inception v2</h2><ol>
<li><p><code>Inception v2</code> 的主要贡献是提出了<code>Batch Normalization</code> 。论文指出，使用了<code>Batch Normalization</code> 之后：</p>
<ul>
<li><p>可以加速网络的学习。</p>
<p>相比<code>Inception v1</code>，训练速度提升了14倍。因为应用了<code>BN</code> 之后，网络可以使用更高的学习率，同时删除了某些层。</p>
</li>
<li><p>网络具有更好的泛化能力。</p>
<p>在<code>ImageNet</code> 分类问题的<code>top5</code> 上达到<code>4.8%</code>，超过了人类标注 <code>top5</code> 的准确率。</p>
</li>
</ul>
</li>
<li><p><code>Inception V2</code> 网络训练的技巧有：</p>
<ul>
<li>使用更高的学习率。</li>
<li>删除<code>dropout</code>层、<code>LRN</code> 层。</li>
<li>减小<code>L2</code> 正则化的系数。</li>
<li>更快的衰减学习率。学习率以指数形式衰减。</li>
<li>更彻底的混洗训练样本，使得一组样本在不同的<code>epoch</code> 中处于不同的<code>mini batch</code> 中。</li>
<li>减少图片的形变。</li>
</ul>
</li>
<li><p><code>Inception v2</code> 的网络结构比<code>Inception v1</code> 有少量改动：</p>
<ul>
<li><p><code>5x5</code> 卷积被两个<code>3x3</code> 卷积替代。</p>
<p>这使得网络的最大深度增加了 9 层，同时网络参数数量增加 25%，计算量增加 30%。</p>
</li>
<li><p><code>28x28</code> 的<code>inception</code> 模块从2个增加到3个。</p>
</li>
<li><p>在<code>inception</code> 模块中，有的采用最大池化，有的采用平均池化。</p>
</li>
<li><p>在<code>inception</code> 模块之间取消了用作连接的池化层。</p>
</li>
<li><p><code>inception(3c),inception(4e)</code> 的子层采用步长为 2 的卷积/池化。</p>
</li>
</ul>
<blockquote>
<p><code>Pool+proj</code> 列给出了<code>inception</code> 中的池化操作。</p>
<ul>
<li><code>avg+32</code> 意义为：平均池化层后接一个尺寸<code>1x1</code>、输出通道<code>32</code> 的卷积层。</li>
<li><code>max+pass through</code> 意义为：最大池化层后接一个尺寸<code>1x1</code>、输出通道数等于输入通道数的卷积层。</li>
</ul>
</blockquote>
<p><img src="/images/loading.gif" data-original="../images/ML/8c8af97bd546cc7572ad6db75228c4f7.png" alt=""></p>
</li>
<li><p><code>Inception V2</code> 的网络参数约为<code>1126</code> 万。</p>
<table>
<thead>
<tr>
<th align="left">层</th>
<th align="left">参数数量</th>
</tr>
</thead>
<tbody><tr>
<td align="left">conv1</td>
<td align="left">9408</td>
</tr>
<tr>
<td align="left">conv2</td>
<td align="left">114688</td>
</tr>
<tr>
<td align="left">inception-3a</td>
<td align="left">218094</td>
</tr>
<tr>
<td align="left">inception-3b</td>
<td align="left">259072</td>
</tr>
<tr>
<td align="left">inception-3c</td>
<td align="left">384000</td>
</tr>
<tr>
<td align="left">inception-4a</td>
<td align="left">608193</td>
</tr>
<tr>
<td align="left">inception-4b</td>
<td align="left">663552</td>
</tr>
<tr>
<td align="left">inception-4c</td>
<td align="left">912384</td>
</tr>
<tr>
<td align="left">inception-4d</td>
<td align="left">1140736</td>
</tr>
<tr>
<td align="left">inception-4e</td>
<td align="left">1447936</td>
</tr>
<tr>
<td align="left">inception-5a</td>
<td align="left">2205696</td>
</tr>
<tr>
<td align="left">inception-5b</td>
<td align="left">2276352</td>
</tr>
<tr>
<td align="left">fc</td>
<td align="left">1024000</td>
</tr>
<tr>
<td align="left">共</td>
<td align="left">11264111</td>
</tr>
</tbody></table>
</li>
<li><p><code>Inception V2</code> 在<code>ImageNet</code> 测试集上的误差率：</p>
<p><img src="/images/loading.gif" data-original="../images/ML/673e1e13b286b96c4a2095909b954b54.png" alt=""></p>
</li>
</ol>
<h2 id="Inception-v3"><a href="#Inception-v3" class="headerlink" title="Inception v3"></a>Inception v3</h2><p>虽然<code>Inception v1</code> 的参数较少，但是它的结构比较复杂，难以进行修改。原因有以下两点：</p>
<ul>
<li>如果单纯的放大网络（如增加<code>Inception</code> 模块的数量、扩展<code>Inception</code> 模块的大小），则参数的数量会显著增长，计算代价太大。</li>
<li><code>Inception v1</code> 结构中的各种设计，其对最终结果的贡献尚未明确。</li>
</ul>
<p>因此<code>Inception v3</code> 的论文重点探讨了网络结构设计的原则。</p>
<h3 id="网络结构-3"><a href="#网络结构-3" class="headerlink" title="网络结构"></a>网络结构</h3><ol>
<li><p><code>Inception v3</code> 的网络深度为42层，它相对于<code>Inception v1</code> 网络主要做了以下改动：</p>
<ul>
<li><p><code>7x7</code> 卷积替换为3个<code>3x3</code> 卷积。</p>
</li>
<li><p>3个<code>Inception</code>模块：模块中的<code>5x5</code> 卷积替换为2个<code>3x3</code> 卷积，同时使用后面描述的网格尺寸缩减技术。</p>
</li>
<li><p>5个<code>Inception</code> 模块：模块中的<code>5x5</code> 卷积替换为2个<code>3x3</code> 卷积之后，所有的<code>nxn</code> 卷积进行非对称分解，同时使用后面描述的网格尺寸缩减技术。</p>
</li>
<li><p>2个<code>Inception</code> 模块：结构如下。它也使用了卷积分解技术，以及网格尺寸缩减技术。</p>
<p><img src="/images/loading.gif" data-original="../images/ML/385af1ea02cafdce6aceba8e1f4a108c.png" alt=""></p>
</li>
</ul>
</li>
<li><p><code>Inception v3</code> 的网络结构如下所示：</p>
<ul>
<li><p><code>3xInception</code> 表示三个<code>Inception</code> 模块，<code>4xInception</code> 表示四个<code>Inception</code> 模块，<code>5xInception</code> 表示五个<code>Inception</code> 模块。</p>
</li>
<li><p><code>conv padded</code> 表示使用0填充的卷积，它可以保持<code>feature map</code> 的尺寸。</p>
<p>在<code>Inception</code> 模块内的卷积也使用0填充，所有其它的卷积/池化不再使用填充。</p>
</li>
</ul>
<p><img src="/images/loading.gif" data-original="../images/ML/76bfbec0292b27da6b364c1921d5683b.png" alt=""></p>
</li>
<li><p>在<code>3xInception</code> 模块的输出之后设有一个辅助分类器。其结构如下：</p>
<p><img src="/images/loading.gif" data-original="../images/ML/f6eeb90bf3651ddfb43acbfc10f914f9.png" alt=""></p>
</li>
<li><p><code>Inception v3</code> 整体参数数量约 23,626,728万（论文<code>Xception: Deep Learning with Depthwise Separable Convolutions</code>）。</p>
</li>
</ol>
<h3 id="设计技巧-2"><a href="#设计技巧-2" class="headerlink" title="设计技巧"></a>设计技巧</h3><p><code>Inception v3</code> 总结出网络设计的一套通用设计原则：</p>
<ul>
<li><p>避免<code>representation</code> 瓶颈：<code>representation</code> 的大小应该从输入到输出缓缓减小，避免极端压缩。在缩小<code>feature map</code> 尺寸的同时，应该增加<code>feature map</code> 的通道数。</p>
<p><code>representation</code> 大小通常指的是<code>feature map</code> 的容量，即<code>feature map</code> 的<code>width x height x channel</code> 。</p>
</li>
<li><p>空间聚合：可以通过空间聚合来完成低维嵌入，而不会在表达能力上有较大的损失。因此通常在<code>nxn</code> 卷积之前，先利用<code>1x1</code> 卷积来降低输入维度。</p>
<p>猜测的原因是：空间维度之间的强相关性导致了空间聚合过程中的信息丢失较少。</p>
</li>
<li><p>平衡网络的宽度和深度：增加网络的宽度或者深度都可以提高网络的泛化能力，因此计算资源需要在网络的深度和宽度之间取得平衡。</p>
</li>
</ul>
<h4 id="卷积尺寸分解"><a href="#卷积尺寸分解" class="headerlink" title="卷积尺寸分解"></a>卷积尺寸分解</h4><ol>
<li><p>大卷积核的分解：将大卷积核分解为多个小的卷积核。</p>
<p>如：使用2个<code>3x3</code> 卷积替换<code>5x5</code> 卷积，则其参数数量大约是1个<code>5x5</code> 卷积的 72% 。</p>
<p><img src="/images/loading.gif" data-original="https://static.sitestack.cn/projects/huaxiaozhuan-ai/2c92f1001f9b11640f8a355a93c91929.png" alt=""></p>
</li>
<li><p><code>nxn</code> 卷积核的非对称分解：将<code>nxn</code> 卷积替换为<code>1xn</code> 卷积和<code>nx1</code> 卷积。</p>
<ul>
<li>这种非对称分解的参数数量是原始卷积数量的 <img src="/images/loading.gif" data-original="https://static.sitestack.cn/projects/huaxiaozhuan-ai/5a78da54f8a65a89ae3bbb8a287c476b.svg" alt="四、Inception - 图11"> 。随着<code>n</code> 的增加，计算成本的节省非常显著。</li>
<li>论文指出：对于较大的<code>feature map</code> ，这种分解不能很好的工作；但是对于中等大小的 <code>feature map</code> （尺寸在<code>12～20</code> 之间），这种分解效果非常好。</li>
</ul>
<p><img src="/images/loading.gif" data-original="../images/ML/b8065251f3244818a7696a454f0b5dfe.png" alt=""></p>
</li>
</ol>
<h4 id="网格尺寸缩减"><a href="#网格尺寸缩减" class="headerlink" title="网格尺寸缩减"></a>网格尺寸缩减</h4><p><img src="/images/loading.gif" data-original="../images/ML/image-20210903121836383.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210903121847934.png" alt=""></p>
<h4 id="标签平滑正则化"><a href="#标签平滑正则化" class="headerlink" title="标签平滑正则化"></a>标签平滑正则化</h4><p><img src="/images/loading.gif" data-original="../images/ML/image-20210903121905571.png" alt=""></p>
<h2 id="Inception-v4-amp-Inception-ResNet"><a href="#Inception-v4-amp-Inception-ResNet" class="headerlink" title="Inception v4 &amp; Inception - ResNet"></a>Inception v4 &amp; Inception - ResNet</h2><ol>
<li><p><code>Inception v4</code> 和 <code>Inception-ResNet</code> 在同一篇论文中给出。论文通过实验证明了：结合残差连接可以显著加速<code>Inception</code> 的训练。</p>
</li>
<li><p>性能比较：（综合采用了 <code>144 crops/dense</code> 评估的结果，数据集：<code>ILSVRC 2012</code> 的验证集 ）</p>
<table>
<thead>
<tr>
<th align="left">网络</th>
<th align="left">crops</th>
<th align="left">Top-1 Error</th>
<th align="left">Top-5 Error</th>
</tr>
</thead>
<tbody><tr>
<td align="left">ResNet-151</td>
<td align="left">dense</td>
<td align="left">19.4%</td>
<td align="left">4.5%</td>
</tr>
<tr>
<td align="left">Inception-v3</td>
<td align="left">144</td>
<td align="left">18.9%</td>
<td align="left">4.3%</td>
</tr>
<tr>
<td align="left">Inception-ResNet-v1</td>
<td align="left">144</td>
<td align="left">18.8%</td>
<td align="left">4.3%</td>
</tr>
<tr>
<td align="left">Inception-v4</td>
<td align="left">144</td>
<td align="left">17.7%</td>
<td align="left">3.8%</td>
</tr>
<tr>
<td align="left">Inception-ResNet-v2</td>
<td align="left">144</td>
<td align="left">17.8%</td>
<td align="left">3.7%</td>
</tr>
</tbody></table>
</li>
<li><p><code>Inception-ResNet-v2</code> 参数数量约为 5500万，<code>Inception-ResNet-v1/Inception-v4</code> 的参数数量也在该量级。</p>
</li>
</ol>
<h3 id="Inception-v4"><a href="#Inception-v4" class="headerlink" title="Inception v4"></a>Inception v4</h3><ol>
<li><p>在<code>Inception v4</code> 结构的主要改动：</p>
<ul>
<li><p>修改了 <code>stem</code> 部分。</p>
</li>
<li><p>引入了<code>Inception-A</code>、<code>Inception-B</code>、<code>Inception-C</code> 三个模块。这些模块看起来和<code>Inception v3</code> 变体非常相似。</p>
<p><code>Inception-A/B/C</code> 模块中，输入<code>feature map</code> 和输出<code>feature map</code> 形状相同。而<code>Reduction-A/B</code> 模块中，输出<code>feature map</code> 的宽/高减半、通道数增加。</p>
</li>
<li><p>引入了专用的“缩减块”(<code>reduction block</code>)，它被用于缩减<code>feature map</code> 的宽、高。</p>
<p>早期的版本并没有明确使用缩减块，但是也实现了其功能。</p>
</li>
</ul>
</li>
<li><p><code>Inception v4</code> 结构如下：（没有标记<code>V</code> 的卷积使用<code>same</code>填充；标记<code>V</code> 的卷积使用<code>valid</code> 填充）</p>
<p><img src="/images/loading.gif" data-original="../images/ML/9fd32f1f415315489202bf6a7b9adb6e.png" alt=""></p>
<ul>
<li><p><code>stem</code> 部分的结构：</p>
<p><img src="/images/loading.gif" data-original="../images/ML/e8eb154e3947dc505f7e6b87f82f3f37.png" alt=""></p>
</li>
<li><p><code>Inception-A</code>模块（这样的模块有4个）：</p>
<p><img src="/images/loading.gif" data-original="../images/ML/ac31c20c430186ab85e83c84c384ae12.png" alt=""></p>
</li>
<li><p><code>Inception-B</code>模块（这样的模块有7个）：</p>
<p><img src="/images/loading.gif" data-original="../images/ML/07d0592dd90e74f3131a6adaa20a6955.png" alt=""></p>
</li>
<li><p><code>Inception-C</code>模块（这样的模块有3个）：</p>
<p><img src="/images/loading.gif" data-original="../images/ML/e19450d2e2034b400dd9b6bacbb2ed12.png" alt=""></p>
</li>
<li><p><code>Reduction-A</code>模块：(其中 <img src="/images/loading.gif" data-original="https://static.sitestack.cn/projects/huaxiaozhuan-ai/ef19fa9427e4386914670e4686d4e47d.svg" alt="四、Inception - 图35"> 分别表示滤波器的数量)</p>
<p><img src="/images/loading.gif" data-original="../images/ML/675907b11bad5e48887359cef78a9c91.png" alt=""></p>
<table>
<thead>
<tr>
<th align="left">网络</th>
<th align="left">k</th>
<th align="left">l</th>
<th align="left">m</th>
<th align="left">n</th>
</tr>
</thead>
<tbody><tr>
<td align="left">Inception-v4</td>
<td align="left">192</td>
<td align="left">224</td>
<td align="left">256</td>
<td align="left">384</td>
</tr>
<tr>
<td align="left">Inception-ResNet-v1</td>
<td align="left">192</td>
<td align="left">192</td>
<td align="left">256</td>
<td align="left">384</td>
</tr>
<tr>
<td align="left">Inception-ResNet-v2</td>
<td align="left">256</td>
<td align="left">256</td>
<td align="left">256</td>
<td align="left">384</td>
</tr>
</tbody></table>
</li>
<li><p><code>Reduction-B</code>模块：</p>
<p><img src="/images/loading.gif" data-original="../images/ML/5b1cde5088d7b6941e1f422e3d38360b.png" alt=""></p>
</li>
</ul>
</li>
</ol>
<h3 id="Inception-ResNet"><a href="#Inception-ResNet" class="headerlink" title="Inception-ResNet"></a>Inception-ResNet</h3><ol>
<li><p>在<code>Inception-ResNet</code> 中，使用了更廉价的<code>Inception</code> 块：<code>inception</code> 模块的池化运算由残差连接替代。</p>
<blockquote>
<p>在<code>Reduction</code> 模块中能够找到池化运算。</p>
</blockquote>
</li>
<li><p><code>Inception ResNet</code> 有两个版本：<code>v1</code> 和 <code>v2</code> 。</p>
<ul>
<li><code>v1</code> 的计算成本和<code>Inception v3</code> 的接近，<code>v2</code> 的计算成本和<code>Inception v4</code> 的接近。</li>
<li><code>v1</code> 和<code>v2</code> 具有不同的<code>stem</code> 。</li>
<li>两个版本都有相同的模块<code>A、B、C</code> 和缩减块结构，唯一不同在于超参数设置。</li>
</ul>
</li>
<li><p><code>Inception-ResNet-v1</code> 结构如下：</p>
<p><img src="/images/loading.gif" data-original="../images/ML/d9b949d2601104a4638c946bbf2411d6.png" alt=""></p>
<ul>
<li><p><code>stem</code> 部分的结构：</p>
<p><img src="/images/loading.gif" data-original="../images/ML/f236c7b2f08544326226259a4222fad8.png" alt=""></p>
</li>
<li><p><code>Inception-ResNet-A</code>模块（这样的模块有5个）：</p>
<p><img src="/images/loading.gif" data-original="../images/ML/6d60194b1177d8342c90fa983e55f5c5.png" alt=""></p>
</li>
<li><p><code>Inception-B</code>模块（这样的模块有10个）：</p>
<p><img src="/images/loading.gif" data-original="../images/ML/662cfe0edcb99a8dd8a237ba65cdffa6.png" alt=""></p>
</li>
<li><p><code>Inception-C</code>模块（这样的模块有5个）：</p>
<p><img src="/images/loading.gif" data-original="../images/ML/6b6dbe32518a72c4d5edd1f24351fdbe.png" alt=""></p>
</li>
<li><p><code>Reduction-A</code>模块：同<code>inception_v4</code> 的 <code>Reduction-A</code>模块</p>
</li>
<li><p><code>Reduction-B</code>模块：</p>
<p><img src="/images/loading.gif" data-original="../images/ML/1445b311a676711a5f59b78cbd31f384.png" alt=""></p>
</li>
</ul>
</li>
<li><p><code>Inception-ResNet-v2</code> 结构与<code>Inception-ResNet-v1</code> 基本相同 ：</p>
<ul>
<li><p><code>stem</code> 部分的结构：同<code>inception_v4</code> 的 <code>stem</code> 部分。</p>
<p><code>Inception-ResNet-v2</code> 使用了<code>inception v4</code> 的 <code>stem</code> 部分，因此后续的通道数量与<code>Inception-ResNet-v1</code> 不同。</p>
</li>
<li><p><code>Inception-ResNet-A</code>模块（这样的模块有5个）：它的结构与<code>Inception-ResNet-v1</code> 的<code>Inception-ResNet-A</code>相同，只是通道数发生了改变。</p>
<p><img src="/images/loading.gif" data-original="../images/ML/1cbdb9729a68442ae6b7da4355ef2160.png" alt=""></p>
</li>
<li><p><code>Inception-B</code>模块（这样的模块有10个）：它的结构与<code>Inception-ResNet-v1</code> 的<code>Inception-ResNet-B</code>相同，只是通道数发生了改变。</p>
<p><img src="/images/loading.gif" data-original="../images/ML/6150857c488f66afa9a958cf9f3e9ca1.png" alt=""></p>
</li>
<li><p><code>Inception-C</code>模块（这样的模块有5个）：它的结构与<code>Inception-ResNet-v1</code> 的<code>Inception-ResNet-C</code>相同，只是通道数发生了改变。 <img src="/images/loading.gif" data-original="../images/ML/baac29fe0ae7aa28403da03a1e4882b9.png" alt=""></p>
</li>
<li><p><code>Reduction-A</code>模块：同<code>inception_v4</code> 的 <code>Reduction-A</code>模块。</p>
</li>
<li><p><code>Reduction-B</code>模块：它的结构与<code>Inception-ResNet-v1</code> 的<code>Reduction-B</code>相同，只是通道数发生了改变。</p>
<p><img src="/images/loading.gif" data-original="../images/ML/a0622d89b4487dcf868d09ad85d06edd.png" alt=""></p>
</li>
</ul>
</li>
<li><p>如果滤波器数量超过1000，则残差网络开始出现不稳定，同时网络会在训练过程早期出现“死亡”：经过成千上万次迭代之后，在平均池化之前的层开始只生成 0 。</p>
<p>解决方案：在残差模块添加到<code>activation</code> 激活层之前，对其进行缩放能够稳定训练。降低学习率或者增加额外的<code>BN</code>都无法避免这种状况。</p>
<p>这就是<code>Inception ResNet</code> 中的 <code>Inception-A,Inception-B,Inception-C</code> 为何如此设计的原因。</p>
<ul>
<li>将<code>Inception-A,Inception-B,Inception-C</code> 放置在两个<code>Relu activation</code> 之间。</li>
<li>通过线性的<code>1x1 Conv</code>（不带激活函数）来执行对残差的线性缩放。</li>
</ul>
<p><img src="/images/loading.gif" data-original="../images/ML/1e66cac08f15c39d5342ea45b5ac3c67.png" alt=""></p>
</li>
</ol>
<h2 id="Xception"><a href="#Xception" class="headerlink" title="Xception"></a>Xception</h2><ol>
<li><p>一个常规的卷积核尝试在三维空间中使用滤波器抽取特征，包括：两个空间维度（宽度和高度）、一个通道维度。因此单个卷积核的任务是：同时映射跨通道的相关性和空间相关性。</p>
<p><code>Inception</code> 将这个过程明确的分解为一系列独立的相关性的映射：要么考虑跨通道相关性，要么考虑空间相关性。<code>Inception</code> 的做法是：</p>
<ul>
<li>首先通过一组<code>1x1</code> 卷积来查看跨通道的相关性，将输入数据映射到比原始输入空间小的三个或者四个独立空间。</li>
<li>然后通过常规的<code>3x3</code> 或者 <code>5x5</code> 卷积，将所有的相关性（包含了跨通道相关性和空间相关性）映射到这些较小的三维空间中。</li>
</ul>
<p>一个典型的<code>Inception</code> 模块（<code>Inception V3</code> )如下：</p>
<p><img src="/images/loading.gif" data-original="../images/ML/2c92f1001f9b11640f8a355a93c91929.png" alt=""></p>
<p>可以简化为：</p>
<p><img src="/images/loading.gif" data-original="../images/ML/4f0483ddceacf5c632739f42f82c74dc.png" alt=""></p>
</li>
<li><p><code>Xception</code> 将这一思想发挥到极致：首先使用<code>1x1</code> 卷积来映射跨通道相关性，然后分别映射每个输出通道的空间相关性，从而将跨通道相关性和空间相关性解耦。因此该网络被称作<code>Xception:Extreme Inception</code> ，其中的<code>Inception</code> 块被称作 <code>Xception</code> 块。</p>
<p><img src="/images/loading.gif" data-original="../images/ML/aee3ae3e8f61bf08d7b17e98ce846f5c.png" alt=""></p>
</li>
<li><p><code>Xception</code> 块类似于深度可分离卷积，但是它与深度可分离卷积之间有两个细微的差异：</p>
<ul>
<li>操作顺序不同：<ul>
<li>深度可分离卷积通常首先执行<code>channel-wise</code> 空间卷积，然后再执行<code>1x1</code> 卷积。</li>
<li><code>Xception</code> 块首先执行<code>1x1</code> 卷积，然后再进行<code>channel-wise</code> 空间卷积。</li>
</ul>
</li>
<li>第一次卷积操作之后是否存在非线性：<ul>
<li>深度可分离卷积只有第二个卷积(<code>1x1</code> )使用了<code>ReLU</code> 非线性激活函数，<code>channel-wise</code> 空间卷积不使用非线性激活函数。</li>
<li><code>Xception</code> 块的两个卷积（<code>1x1</code> 和 <code>3x3</code> ）都使用了<code>ReLU</code> 非线性激活函数。</li>
</ul>
</li>
</ul>
<p>其中第二个差异更为重要。</p>
</li>
<li><p>对<code>Xception</code> 进行以下的修改，都可以加快网络收敛速度，并获取更高的准确率：</p>
<ul>
<li>引入类似<code>ResNet</code> 的残差连接机制。</li>
<li>在<code>1x1</code> 卷积和<code>3x3</code> 卷积之间不加入任何非线性。</li>
</ul>
</li>
<li><p><code>Xception</code> 的参数数量与<code>Inception V3</code> 相同，但是性能表现显著优于<code>Inception V3</code> 。这表明<code>Xception</code> 更加高效的利用了模型参数。</p>
<ul>
<li><p>根据论文<code>Xception: Deep Learning with Depthwise Separable Convolutions</code>，<code>Inception V3</code> 参数数量为 23626728，<code>Xception</code> 参数数量为 22855952 。</p>
</li>
<li><p>在<code>ImageNet</code> 上的<code>benchmark</code> 为（单个模型，单次<code>crop</code> ）：</p>
<table>
<thead>
<tr>
<th align="left">模型</th>
<th align="left">top-1 accuracy</th>
<th align="left">top-5 accuracy</th>
</tr>
</thead>
<tbody><tr>
<td align="left">VGG-16</td>
<td align="left">71.5%</td>
<td align="left">90.1%</td>
</tr>
<tr>
<td align="left">ResNet-152</td>
<td align="left">77.0%</td>
<td align="left">93.3%</td>
</tr>
<tr>
<td align="left">Inception V3</td>
<td align="left">78.2%</td>
<td align="left">94.1%</td>
</tr>
<tr>
<td align="left">Xception</td>
<td align="left">79.0%</td>
<td align="left">94.5%</td>
</tr>
</tbody></table>
</li>
</ul>
</li>
</ol>
<h1 id="ResNet"><a href="#ResNet" class="headerlink" title="ResNet"></a>ResNet</h1><ol>
<li><p><code>ResNet</code> 提出了一种残差学习框架来解决网络退化问题，从而训练更深的网络。这种框架可以结合已有的各种网络结构，充分发挥二者的优势。</p>
</li>
<li><p><code>ResNet</code>以三种方式挑战了传统的神经网络架构：</p>
<ul>
<li><p><code>ResNet</code> 通过引入跳跃连接来绕过残差层，这允许数据直接流向任何后续层。</p>
<p>这与传统的、顺序的<code>pipeline</code> 形成鲜明对比：传统的架构中，网络依次处理低级<code>feature</code> 到高级<code>feature</code> 。</p>
</li>
<li><p><code>ResNet</code> 的层数非常深，高达1202层。而<code>ALexNet</code> 这样的架构，网络层数要小两个量级。</p>
</li>
<li><p>通过实验发现，训练好的 <code>ResNet</code> 中去掉单个层并不会影响其预测性能。而训练好的<code>AlexNet</code> 等网络中，移除层会导致预测性能损失。</p>
</li>
</ul>
</li>
<li><p>在<code>ImageNet</code>分类数据集中，拥有152层的残差网络，以<code>3.75% top-5</code> 的错误率获得了<code>ILSVRC 2015</code> 分类比赛的冠军。</p>
</li>
<li><p>很多证据表明：残差学习是通用的，不仅可以应用于视觉问题，也可应用于非视觉问题。</p>
</li>
</ol>
<h2 id="网络退化问题"><a href="#网络退化问题" class="headerlink" title="网络退化问题"></a>网络退化问题</h2><ol>
<li><p>学习更深的网络的一个障碍是梯度消失/爆炸，该问题可以通过<code>Batch Normalization</code> 在很大程度上解决。</p>
</li>
<li><p><code>ResNet</code> 论文作者发现：随着网络的深度的增加，准确率达到饱和之后迅速下降，而这种下降不是由过拟合引起的。这称作网络退化问题。</p>
<p>如果更深的网络训练误差更大，则说明是由于优化算法引起的：越深的网络，求解优化问题越难。如下所示：更深的网络导致更高的训练误差和测试误差。</p>
<p><img src="/images/loading.gif" data-original="../images/ML/8f3821e89fa594fd9d7d096a2525872c.png" alt=""></p>
</li>
<li><p>理论上讲，较深的模型不应该比和它对应的、较浅的模型更差。因为较深的模型是较浅的模型的超空间。较深的模型可以这样得到：先构建较浅的模型，然后添加很多恒等映射的网络层。</p>
<p>实际上我们的较深的模型后面添加的不是恒等映射，而是一些非线性层。因此，退化问题表明：通过多个非线性层来近似横等映射可能是困难的。</p>
</li>
<li><p>解决网络退化问题的方案：学习残差。</p>
</li>
</ol>
<h2 id="残差块"><a href="#残差块" class="headerlink" title="残差块"></a>残差块</h2><p><img src="/images/loading.gif" data-original="../images/ML/image-20210903122325884.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210903122340321.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210903122400583.png" alt=""></p>
<h2 id="ResNet-分析"><a href="#ResNet-分析" class="headerlink" title="ResNet 分析"></a>ResNet 分析</h2><ol>
<li><p><code>Veit et al.</code> 认为<code>ResNet</code> 工作较好的原因是：一个<code>ResNet</code> 网络可以看做是一组较浅的网络的集成模型。</p>
<p>但是<code>ResNet</code> 的作者认为这个解释是不正确的。因为集成模型要求每个子模型是独立训练的，而这组较浅的网络是共同训练的。</p>
</li>
<li><p>论文<code>《Residual Networks Bahave Like Ensemble of Relatively Shallow Networks》</code> 对<code>ResNet</code> 进行了深入的分析。</p>
<ul>
<li>通过分解视图表明：<code>ResNet</code> 可以被视作许多路径的集合。</li>
<li>通过研究<code>ResNet</code> 的梯度流表明：网络训练期间只有短路径才会产生梯度流，深的路径不是必须的。</li>
<li>通过破坏性实验，表明：<ul>
<li>即使这些路径是共同训练的，它们也不是相互依赖的。</li>
<li>这些路径的行为类似集成模型，其预测准确率平滑地与有效路径的数量有关。</li>
</ul>
</li>
</ul>
</li>
</ol>
<h3 id="分解视图"><a href="#分解视图" class="headerlink" title="分解视图"></a>分解视图</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210903122439175.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210903122449975.png" alt=""></p>
<h3 id="路径长度分析"><a href="#路径长度分析" class="headerlink" title="路径长度分析"></a>路径长度分析</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210903122508750.png" alt=""></p>
<h3 id="路径梯度分析"><a href="#路径梯度分析" class="headerlink" title="路径梯度分析"></a>路径梯度分析</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210903122530690.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210903122548978.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210903122603744.png" alt=""></p>
<h3 id="路径破坏性分析"><a href="#路径破坏性分析" class="headerlink" title="路径破坏性分析"></a>路径破坏性分析</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210903122626539.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210903122648750.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210903122705894.png" alt=""></p>
<h2 id="网络性能"><a href="#网络性能" class="headerlink" title="网络性能"></a>网络性能</h2><ol>
<li><p><code>plain</code> 网络：一些简单网络结构的叠加，如下图所示。图中给出了四种<code>plain</code> 网络，它们的区别主要是网络深度不同。其中，输入图片尺寸 224x224 。</p>
<p><code>ResNet</code> 简单的在<code>plain</code> 网络上添加快捷连接来实现。</p>
<blockquote>
<p><code>FLOPs</code>：<code>floating point operations</code> 的缩写，意思是浮点运算量，用于衡量算法/模型的复杂度。</p>
<p><code>FLOPS</code>：<code>floating point per second</code>的缩写，意思是每秒浮点运算次数，用于衡量计算速度。</p>
</blockquote>
<p><img src="/images/loading.gif" data-original="../images/ML/fd6d4b6aeb633a43497b18ee0c227a04.png" alt=""></p>
</li>
<li><p>相对于输入的<code>feature map</code>，残差块的输出<code>feature map</code> 尺寸可能会发生变化：</p>
<ul>
<li><p>输出 <code>feature map</code> 的通道数增加，此时需要扩充快捷连接的输出<code>feature map</code> 。否则快捷连接的输出 <code>feature map</code> 无法和残差块的<code>feature map</code> 累加。</p>
<p>有两种扩充方式：</p>
<ul>
<li>直接通过 0 来填充需要扩充的维度，在图中以实线标识。</li>
<li>通过<code>1x1</code> 卷积来扩充维度，在图中以虚线标识。</li>
</ul>
</li>
<li><p>输出 <code>feature map</code> 的尺寸减半。此时需要对快捷连接执行步长为 2 的池化/卷积：如果快捷连接已经采用 <code>1x1</code> 卷积，则该卷积步长为2 ；否则采用步长为 2 的最大池化 。</p>
</li>
</ul>
<p><img src="/images/loading.gif" data-original="../images/ML/9555206d32faac5bd2592be60953f406.png" alt=""></p>
</li>
<li><p>计算复杂度：</p>
<table>
<thead>
<tr>
<th align="left"></th>
<th align="left">VGG-19</th>
<th align="left">34层 plain 网络</th>
<th align="left">Resnet-34</th>
</tr>
</thead>
<tbody><tr>
<td align="left">计算复杂度(FLOPs)</td>
<td align="left">19.6 billion</td>
<td align="left">3.5 billion</td>
<td align="left">3.6 billion</td>
</tr>
</tbody></table>
</li>
<li><p>模型预测能力：在<code>ImageNet</code> 验证集上执行<code>10-crop</code> 测试的结果。</p>
<ul>
<li><code>A</code> 类模型：快捷连接中，所有需要扩充的维度的填充 0 。</li>
<li><code>B</code> 类模型：快捷连接中，所有需要扩充的维度通过<code>1x1</code> 卷积来扩充。</li>
<li><code>C</code> 类模型：所有快捷连接都通过<code>1x1</code> 卷积来执行线性变换。</li>
</ul>
<p>可以看到<code>C</code> 优于<code>B</code>，<code>B</code> 优于<code>A</code>。但是 <code>C</code> 引入更多的参数，相对于这种微弱的提升，性价比较低。所以后续的<code>ResNet</code> 均采用 <code>B</code> 类模型。</p>
<table>
<thead>
<tr>
<th align="left">模型</th>
<th align="left">top-1 误差率</th>
<th align="left">top-5 误差率</th>
</tr>
</thead>
<tbody><tr>
<td align="left">VGG-16</td>
<td align="left">28.07%</td>
<td align="left">9.33%</td>
</tr>
<tr>
<td align="left">GoogleNet</td>
<td align="left">-</td>
<td align="left">9.15%</td>
</tr>
<tr>
<td align="left">PReLU-net</td>
<td align="left">24.27%</td>
<td align="left">7.38%</td>
</tr>
<tr>
<td align="left">plain-34</td>
<td align="left">28.54%</td>
<td align="left">10.02%</td>
</tr>
<tr>
<td align="left">ResNet-34 A</td>
<td align="left">25.03%</td>
<td align="left">7.76%</td>
</tr>
<tr>
<td align="left">ResNet-34 B</td>
<td align="left">24.52%</td>
<td align="left">7.46%</td>
</tr>
<tr>
<td align="left">ResNet-34 C</td>
<td align="left">24.19%</td>
<td align="left">7.40%</td>
</tr>
<tr>
<td align="left">ResNet-50</td>
<td align="left">22.85%</td>
<td align="left">6.71%</td>
</tr>
<tr>
<td align="left">ResNet-101</td>
<td align="left">21.75%</td>
<td align="left">6.05%</td>
</tr>
<tr>
<td align="left">ResNet-152</td>
<td align="left">21.43%</td>
<td align="left">5.71%</td>
</tr>
</tbody></table>
</li>
</ol>
<h1 id="ResNet-变种"><a href="#ResNet-变种" class="headerlink" title="ResNet 变种"></a>ResNet 变种</h1><h2 id="恒等映射修正"><a href="#恒等映射修正" class="headerlink" title="恒等映射修正"></a>恒等映射修正</h2><p>在论文<code>《Identity Mappings in Deep Residual Networks》</code>中，<code>ResNet</code> 的作者通过实验证明了恒等映射的重要性，并且提出了一个新的残差单元来简化恒等映射。</p>
<h3 id="新残差块"><a href="#新残差块" class="headerlink" title="新残差块"></a>新残差块</h3><ol>
<li>新的残差单元中，恒等映射添加到<code>ReLU</code> 激活函数之后。它使得训练变得更简单，并且提高了网络的泛化能力。</li>
</ol>
<p><img src="/images/loading.gif" data-original="../images/ML/185b5a705a3fab83b1ff0837b1924f9f.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210903122918745.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210903122933971.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210903122953650.png" alt=""></p>
<h3 id="快捷连接验证"><a href="#快捷连接验证" class="headerlink" title="快捷连接验证"></a>快捷连接验证</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210903123021812.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210903123036588.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/6950a0a701ad35619dc7e50d2be1d355.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210903123137393.png" alt=""></p>
<h3 id="激活函数验证"><a href="#激活函数验证" class="headerlink" title="激活函数验证"></a>激活函数验证</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210903123220111.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210903123245944.png" alt=""></p>
<h3 id="网络性能-1"><a href="#网络性能-1" class="headerlink" title="网络性能"></a>网络性能</h3><p>在 <code>ILSVRC 2012</code> 验证集上的评估结果：</p>
<table>
<thead>
<tr>
<th align="left">方法</th>
<th align="left">数据集增强</th>
<th align="left">train crop</th>
<th align="left">test crop</th>
<th align="left">top-1 误差</th>
<th align="left">top-5 误差</th>
</tr>
</thead>
<tbody><tr>
<td align="left">ResNet-152，原始残差块</td>
<td align="left">scale</td>
<td align="left">224x224</td>
<td align="left">224x224</td>
<td align="left">23.0%</td>
<td align="left">6.7%</td>
</tr>
<tr>
<td align="left">ResNet-152，原始残差块</td>
<td align="left">scale</td>
<td align="left">224x224</td>
<td align="left">320x320</td>
<td align="left">21.3%</td>
<td align="left">5.5%</td>
</tr>
<tr>
<td align="left">ResNet-152，full pre-activation</td>
<td align="left">scale</td>
<td align="left">224x224</td>
<td align="left">320x320</td>
<td align="left">21.1%</td>
<td align="left">5.5%</td>
</tr>
<tr>
<td align="left">ResNet-200，原始残差块</td>
<td align="left">scale</td>
<td align="left">224x224</td>
<td align="left">320x320</td>
<td align="left">21.8%</td>
<td align="left">6.0%</td>
</tr>
<tr>
<td align="left">ResNet-200，full pre-activation</td>
<td align="left">scale</td>
<td align="left">224x224</td>
<td align="left">320x320</td>
<td align="left">20.7%</td>
<td align="left">5.3%</td>
</tr>
<tr>
<td align="left">ResNet-200，full pre-activation</td>
<td align="left">scale + asp ratio</td>
<td align="left">224x224</td>
<td align="left">320x320</td>
<td align="left">20.1%</td>
<td align="left">4.8%</td>
</tr>
<tr>
<td align="left">Inception v3</td>
<td align="left">scale + asp ratio</td>
<td align="left">299x299</td>
<td align="left">299x299</td>
<td align="left">21.2%</td>
<td align="left">5.6%</td>
</tr>
</tbody></table>
<h2 id="ResNeXt"><a href="#ResNeXt" class="headerlink" title="ResNeXt"></a>ResNeXt</h2><ol>
<li><p>通常提高模型准确率的方法是加深网络深度或者加宽网络宽度，但这些方法会增加超参数的数量、参数数量和计算量。</p>
<p><code>ResNeXt</code> 网络可以在不增加网络参数复杂度的前提下提高准确率，同时还减少了超参数的数量。</p>
</li>
<li><p><code>ResNeXt</code> 的设计参考了<code>VGG</code> 和<code>Inception</code> 的设计哲学。</p>
<ul>
<li><p><code>VGG</code>：网络通过简单地层叠相同结构的层来实现，因此网络结构简单。其缺点是网络参数太多，计算量太大。</p>
</li>
<li><p><code>Inception</code>：通过执行<code>分裂-变换-合并</code>策略来精心设计拓扑结构，使得网络参数较少，计算复杂度较低。这种<code>分裂-变换-合并</code>行为预期能够达到一个大的<code>dense</code> 层的表达能力，但是计算复杂度要低的多。</p>
<p>其缺点是：</p>
<ul>
<li>每个“变换”中，滤波器的数量和尺寸等超参数都需要精细的设计。</li>
<li>一旦需要训练新的任务（如新任务是一个<code>NLP</code> 任务），可能需要重新设计网络结构。因此可扩展性不高。</li>
</ul>
</li>
<li><p><code>ResNeXt</code> 结合了二者的优点：</p>
<ul>
<li>网络结构也是通过简单地层叠相同结构的层来实现。</li>
<li>网络的每一层都执行了<code>分裂-变换-合并</code>策略。</li>
</ul>
</li>
</ul>
</li>
<li><p>在相同的参数数量和计算复杂度的情况下，<code>ResNeXt</code> 的预测性能要优于<code>ResNet</code> 。</p>
<ul>
<li>它在<code>ILSVRC 2016</code> 分类任务中取得了第二名的成绩。</li>
<li><code>101</code> 层的<code>ResNeXt</code> 就能够获得超过<code>200</code> 层<code>ResNet</code> 的准确率，并且计算量只有后者的一半。</li>
</ul>
</li>
<li><p><code>ResNeXt</code> 改进了<code>ResNet</code> 网络结构，并提出了一个新的维度，称作“基数”<code>cardinality</code>。基数是网络的深度和网络的宽度之外的另一个重要因素。</p>
<p>作者通过实验表明：增加基数比增加网络的深度或者网络的宽度更有效。</p>
</li>
</ol>
<h3 id="分裂-变换-合并"><a href="#分裂-变换-合并" class="headerlink" title="分裂-变换-合并"></a>分裂-变换-合并</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210903123444675.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210903123500670.png" alt=""></p>
<h3 id="ResNeXt-块"><a href="#ResNeXt-块" class="headerlink" title="ResNeXt 块"></a>ResNeXt 块</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210903123524608.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210903123543942.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/3d4cac95a1f9508228c92d1c753b4952.png" alt=""></p>
<p>通常<code>ResNeXt</code> 模块至少有三层。事实上它也可以有两层，此时它等效于一个宽的、密集模块。</p>
<ul>
<li>此时并没有通过 <code>1x1</code> 卷积进行降维与升维，而是在降维的过程中同时进行变换，在升维的过程中也进行变换。</li>
<li>如下图所示，它等价于图<code>(c)</code> 中，去掉中间的变换层(<code>128,3x3,128</code> 层)，同时将第一层、第三层的 <code>1x1</code> 替换为<code>3x3</code> 卷积层。</li>
</ul>
<p><img src="/images/loading.gif" data-original="../images/ML/a6558f9149c3ae9fc825abbff1433e8b.png" alt=""></p>
<h3 id="网络性能-2"><a href="#网络性能-2" class="headerlink" title="网络性能"></a>网络性能</h3><ol>
<li><p><code>ResNeXt</code> 的两种重要超参数是：基数<code>C</code> 和颈宽<code>d</code> 。</p>
<ul>
<li>基数 <code>C</code>：决定了每个<code>ResNeXt</code> 模块有多少条路径。</li>
<li>颈宽（<code>bottleneck width</code>）<code>d</code>：决定了<code>ResNeXt</code> 模块中第一层<code>1x1</code> 卷积降维的维度。</li>
</ul>
<blockquote>
<p>这二者也决定了<code>ResNeXt</code> 模块等价形式中，通道分组卷积的通道数量为 <code>Cxd</code> 。</p>
</blockquote>
</li>
<li><p><code>ResNeXt</code> 的网络参数和计算量与同等结构的<code>ResNet</code> 几乎相同。以<code>ResNet-50</code> 为例（输入图片尺寸<code>224x224</code> ）：</p>
<blockquote>
<p><code>ResNeXt-50(32x4d)</code> 意思是：基数<code>C=32</code>，颈宽<code>d=4</code> 。</p>
</blockquote>
<p><img src="/images/loading.gif" data-original="../images/ML/45bb4057d5f8dd1fadde1e6a174a778e.png" alt=""></p>
</li>
<li><p>在<code>ImageNet</code> 上进行的对比实验（验证集误差，<code>single crop</code> ）：</p>
<ul>
<li><p>基数 vs 颈宽：基数越大越好。</p>
<table>
<thead>
<tr>
<th align="left">模型</th>
<th align="left">配置</th>
<th align="left">top-1 error(%)</th>
</tr>
</thead>
<tbody><tr>
<td align="left">ResNet-50</td>
<td align="left">C=1,d=64</td>
<td align="left">23.9</td>
</tr>
<tr>
<td align="left">ResNeXt-50</td>
<td align="left">C=2,d=40</td>
<td align="left">23.0</td>
</tr>
<tr>
<td align="left">ResNeXt-50</td>
<td align="left">C=4,d=24</td>
<td align="left">22.6</td>
</tr>
<tr>
<td align="left">ResNeXt-50</td>
<td align="left">C=8,d=14</td>
<td align="left">22.3</td>
</tr>
<tr>
<td align="left">ResNeXt-50</td>
<td align="left">C=32,d=4</td>
<td align="left">22.2</td>
</tr>
<tr>
<td align="left">ResNet-101</td>
<td align="left">C=1,d=64</td>
<td align="left">22.0</td>
</tr>
<tr>
<td align="left">ResNeXt-101</td>
<td align="left">C=2,d=40</td>
<td align="left">21.7</td>
</tr>
<tr>
<td align="left">ResNeXt-101</td>
<td align="left">C=4,d=24</td>
<td align="left">21.4</td>
</tr>
<tr>
<td align="left">ResNeXt-101</td>
<td align="left">C=8,d=14</td>
<td align="left">21.3</td>
</tr>
<tr>
<td align="left">ResNeXt-101</td>
<td align="left">C=32,d=4</td>
<td align="left">21.2</td>
</tr>
</tbody></table>
</li>
<li><p>基数 vs 深度/宽度：基数越大越好。</p>
<p><img src="/images/loading.gif" data-original="../images/ML/5fdd0d80660e292e530c37427da58166.png" alt=""></p>
</li>
</ul>
</li>
<li><p>与其它模型的预测能力比较（验证集误差，<code>single crop</code>）：</p>
<blockquote>
<p><code>ResNet/ResNeXt</code> 的图片尺寸为<code>224x224</code> 和 <code>320x320</code>；<code>Inception</code> 的图片尺寸为<code>299x299</code> 。</p>
</blockquote>
<p><img src="/images/loading.gif" data-original="../images/ML/8bca39acb0a63d1c09992707ed42f072.png" alt=""></p>
</li>
</ol>
<h2 id="随机深度网络"><a href="#随机深度网络" class="headerlink" title="随机深度网络"></a>随机深度网络</h2><ol>
<li><p>随机深度网络提出了训练时随机丢弃网络层的思想，从而能够让网络深度增加到超过1000层，并仍然可以减少测试误差。</p>
<p>如图所示：在<code>CIFAR-10</code> 上，<code>1202</code> 层的<code>ResNet</code> 测试误差要高于 <code>110</code> 层的<code>ResNet</code> ，表现出明显的过拟合。而 <code>1202</code> 层的随机深度网络（结合了<code>ResNet</code> ）的测试误差要低于 <code>110</code> 层的<code>ResNet</code> 。</p>
<p><img src="/images/loading.gif" data-original="../images/ML/51162f3aebeedfe3a21e60b16edcf1ff.png" alt=""></p>
</li>
<li><p>神经网络的表达能力主要由网络深度来决定，但是过深的网络会带来三个问题：反向传播过程中的梯度消失、前向传播过程中的<code>feature</code> 消失、训练时间过长。</p>
<ul>
<li><p>虽然较浅的网络能够缓解这几个问题，但是较浅的网络表达能力不足，容易陷入欠拟合。</p>
</li>
<li><p>随机深度网络解决这一矛盾的策略是：构建具有足够表达能力的深度神经网络（具有数百层甚至数千层），然后：</p>
<ul>
<li><p>在网络训练期间，对每个<code>mini batch</code> 随机地移除部分层来显著的减小网络的深度。</p>
<p>移除操作：删除对应的层，并用跳跃连接来代替。</p>
</li>
<li><p>在网络测试期间，使用全部的网络层。</p>
</li>
</ul>
</li>
</ul>
</li>
<li><p>随机深度的思想可以和<code>ResNet</code> 结合。因为<code>ResNet</code> 已经包含了跳跃连接，因此可以直接修改。</p>
</li>
</ol>
<h3 id="随机深度"><a href="#随机深度" class="headerlink" title="随机深度"></a>随机深度</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210903123815145.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210903123830083.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210903123845605.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210903123858830.png" alt=""></p>
<h3 id="网络性能-3"><a href="#网络性能-3" class="headerlink" title="网络性能"></a>网络性能</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210903123920221.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/457bb547f546099b8c3ef2a967ff4a90.png" alt=""></p>
<h1 id="SENet"><a href="#SENet" class="headerlink" title="SENet"></a>SENet</h1><ol>
<li><code>SENet</code> 提出了一种新的架构单元来解决通道之间相互依赖的问题。它通过显式地对通道之间的相互依赖关系建模，自适应的重新校准通道维的特征响应，从而提高了网络的表达能力。</li>
<li><code>SENet</code> 以<code>2.251% top-5</code> 的错误率获得了<code>ILSVRC 2017</code> 分类比赛的冠军。</li>
<li><code>SENet</code> 是和<code>ResNet</code> 一样，都是一种网络框架。它可以直接与其他网络架构一起融合使用，只需要付出微小的计算成本就可以产生显著的性能提升。</li>
</ol>
<h2 id="SE-块"><a href="#SE-块" class="headerlink" title="SE 块"></a>SE 块</h2><p><img src="/images/loading.gif" data-original="../images/ML/image-20210903124006982.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210903124019519.png" alt=""></p>
<h3 id="squeeze-操作"><a href="#squeeze-操作" class="headerlink" title="squeeze 操作"></a>squeeze 操作</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210903124046043.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210903124100122.png" alt=""></p>
<h3 id="excitation-操作"><a href="#excitation-操作" class="headerlink" title="excitation 操作"></a>excitation 操作</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210903124127995.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210903124137532.png" alt=""></p>
<h3 id="SE-块使用"><a href="#SE-块使用" class="headerlink" title="SE 块使用"></a>SE 块使用</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210903124154572.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210903124211423.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210903124248048.png" alt=""></p>
<h2 id="网络性能-4"><a href="#网络性能-4" class="headerlink" title="网络性能"></a>网络性能</h2><ol>
<li><p>网络结构：其中 <code>fc,[16,256]</code> 表示 <code>SE</code> 块中的两个全连接层的输出维度。</p>
<p><img src="/images/loading.gif" data-original="../images/ML/40267293b3dd895c5b54e4bd9ba19c5f.png" alt=""></p>
</li>
<li><p>在<code>ImageNet</code> 验证集上的计算复杂度和预测误差比较(<code>single-crop</code>)。</p>
<ul>
<li><code>original</code> 列：从各自原始论文中给出的结果报告。</li>
<li><code>re-implementation</code> 列：重新训练得到的结果报告。</li>
<li><code>SENet</code> 列：通过引入<code>SE</code>块之后的结果报告。</li>
<li><code>GFLOPs/MFLOPs</code>：计算复杂度，单位为 <code>G/M FLOPs</code> 。</li>
<li><code>MobileNet</code> 采用的是 <code>1.0 MobileNet-224</code>，<code>ShuffleNet</code> 采用的是 <code>1.0 ShuffleNet 1x(g=3)</code> 。</li>
<li>数据集增强和归一化：<ul>
<li>随机裁剪成 <code>224x224</code> 大小（<code>Inception</code> 系列裁剪成 <code>299x299</code> ）。</li>
<li>随机水平翻转。</li>
<li>输入图片沿着通道归一化：每个像素减去本通道的像素均值。</li>
</ul>
</li>
</ul>
<p><img src="/images/loading.gif" data-original="../images/ML/0ccdd6a125348fb239b99bd7f7b8af39.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="https://static.sitestack.cn/projects/huaxiaozhuan-ai/0bf732d3b333f19152775f6fbe0a32c7.png" alt="七、SENet - 图70"></p>
</li>
<li><p>在<code>ImageNet</code> 验证集上的预测误差比较（<code>single-crop</code>）：</p>
<p>其中 <code>SENet-154(post-challenge)</code> 是采用 <code>320x320</code> 大小的图片来训练的。</p>
<p><img src="/images/loading.gif" data-original="../images/ML/422c78b735a640a9ba1bd07845f08a10.png" alt=""></p>
</li>
</ol>
<h1 id="DenseNet"><a href="#DenseNet" class="headerlink" title="DenseNet"></a>DenseNet</h1><ol>
<li><p><code>DenseNet</code>不是通过更深或者更宽的结构，而是通过特征重用来提升网络的学习能力。</p>
</li>
<li><p><code>ResNet</code> 的思想是：创建从“靠近输入的层” 到 “靠近输出的层” 的直连。而<code>DenseNet</code> 做得更为彻底：将所有层以前馈的形式相连，这种网络因此称作<code>DenseNet</code> 。</p>
</li>
<li><p><code>DenseNet</code> 具有以下的优点：</p>
<ul>
<li>缓解梯度消失的问题。因为每层都可以直接从损失函数中获取梯度、从原始输入中获取信息，从而易于训练。</li>
<li>密集连接还具有正则化的效应，缓解了小训练集任务的过拟合。</li>
<li>鼓励特征重用。网络将不同层学到的 <code>feature map</code> 进行组合。</li>
<li>大幅度减少参数数量。因为每层的卷积核尺寸都比较小，输出通道数较少 (由增长率 <strong><em>k</em></strong> 决定)。</li>
</ul>
</li>
<li><p><code>DenseNet</code> 具有比传统卷积网络更少的参数，因为它不需要重新学习多余的<code>feature map</code> 。</p>
<ul>
<li><p>传统的前馈神经网络可以视作在层与层之间传递<code>状态</code>的算法，每一层接收前一层的<code>状态</code>，然后将新的<code>状态</code>传递给下一层。</p>
<p>这会改变<code>状态</code>，但是也传递了需要保留的信息。</p>
</li>
<li><p><code>ResNet</code> 通过恒等映射来直接传递需要保留的信息，因此层之间只需要传递<code>状态的变化</code> 。</p>
</li>
<li><p><code>DenseNet</code> 会将所有层的<code>状态</code> 全部保存到<code>集体知识</code>中，同时每一层增加很少数量的<code>feture map</code> 到网络的<code>集体知识中</code>。</p>
</li>
</ul>
</li>
<li><p><code>DenseNet</code> 的层很窄（即：<code>feature map</code> 的通道数很小），如：每一层的输出只有 12 个通道。</p>
</li>
</ol>
<h2 id="DenseNet-块"><a href="#DenseNet-块" class="headerlink" title="DenseNet 块"></a>DenseNet 块</h2><p><img src="/images/loading.gif" data-original="../images/ML/image-20210903124437782.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210903124454507.png" alt=""></p>
<h3 id="增长率"><a href="#增长率" class="headerlink" title="增长率"></a>增长率</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210903124514762.png" alt=""></p>
<h3 id="非线性变换-H-l"><a href="#非线性变换-H-l" class="headerlink" title="非线性变换 H_l"></a>非线性变换 H_l</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210903124535777.png" alt=""></p>
<h3 id="bottleneck"><a href="#bottleneck" class="headerlink" title="bottleneck"></a>bottleneck</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210903124603224.png" alt=""></p>
<h2 id="过渡层"><a href="#过渡层" class="headerlink" title="过渡层"></a>过渡层</h2><p><img src="/images/loading.gif" data-original="../images/ML/image-20210903124621662.png" alt=""></p>
<h2 id="网络性能-5"><a href="#网络性能-5" class="headerlink" title="网络性能"></a>网络性能</h2><p><img src="/images/loading.gif" data-original="../images/ML/image-20210903124648479.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210903124701832.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210903124716288.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210903124726735.png" alt=""></p>
<h2 id="内存优化"><a href="#内存优化" class="headerlink" title="内存优化"></a>内存优化</h2><h3 id="内存消耗"><a href="#内存消耗" class="headerlink" title="内存消耗"></a>内存消耗</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210903124810213.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210903124823705.png" alt=""></p>
<h3 id="内存优化-1"><a href="#内存优化-1" class="headerlink" title="内存优化"></a>内存优化</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210903124843930.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/9fd44e3c33eabe1d2285f0d759be6fc9.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210903124905915.png" alt=""></p>
<h3 id="优化结果"><a href="#优化结果" class="headerlink" title="优化结果"></a>优化结果</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210903124934804.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210903124949437.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/8dc742dcdda504e3c075810ee86639ac.png" alt=""></p>
<h1 id="小型网络"><a href="#小型网络" class="headerlink" title="小型网络"></a>小型网络</h1><ol>
<li><p>目前神经网络领域的研究基本可以概括为两个方向：探索模型更好的预测能力，关注模型在实际应用中的难点。</p>
<p>事实上卷积神经网络在图像识别领域超越了人类的表现，但是这些先进的网络需要较高的计算资源。这些资源需求超出了很多移动设备和嵌入式设备的能力（如：无人驾驶），导致实际应用中难以在这些设备上应用。</p>
<p>小型网络就是为解决这个难点来设计的。</p>
</li>
<li><p>小型网络的设计和优化目标并不是模型的准确率，而是在满足一定准确率的条件下，尽可能的使得模型小，从而降低对计算资源的需求，降低计算延迟。</p>
</li>
<li><p>小型高效的神经网络的构建方法大致可以归为两类：对已经训练好的网络进行压缩，直接训练小网络模型。</p>
</li>
<li><p>模型参数的数量决定了模型的大小，所谓的<code>小型网络</code> 指的是网络的参数数量较少。</p>
<p>小型网络具有至少以下三个优势：</p>
<ul>
<li><p>较小的模型具有更高效的分布式训练效率。</p>
<p><code>Worker</code> 与<code>PS</code> 以及 <code>Worker</code> 与<code>Worker</code> 之间的通信是神经网络分布式训练的重要限制因素。在分布式数据并行训练中，通信开销与模型的参数数量成正比。较小的模型需要更少的通信，从而可以更快的训练。</p>
</li>
<li><p>较小的模型在模型导出时开销更低。</p>
<p>当在<code>tensorflow</code> 等框架中训练好模型并准备部署时，需要将模型导出。如：将训练好的模型导出到自动驾驶汽车上。模型越小，则数据导出需要传输的数据量越少，这样可以支持更频繁的模型更新。</p>
</li>
<li><p>较小的模型可以在<code>FPGA</code> 和嵌入式硬件上部署。</p>
<p><code>FPGA</code> 通常只有小于<code>10MB</code> 的片上内存，并且没有外部存储。因此如果希望在<code>FPGA</code> 上部署模型，则要求模型足够小从而满足内存限制。</p>
</li>
</ul>
</li>
</ol>
<h2 id="SqueezeNet-系列"><a href="#SqueezeNet-系列" class="headerlink" title="SqueezeNet 系列"></a>SqueezeNet 系列</h2><h3 id="SqueezeNet"><a href="#SqueezeNet" class="headerlink" title="SqueezeNet"></a>SqueezeNet</h3><ol>
<li><p><code>squeezenet</code> 提出了<code>Fire</code> 模块，并通过该模型构成一种小型<code>CNN</code> 网络，在满足<code>AlexNet</code> 级别准确率的条件下大幅度降低参数数量。</p>
</li>
<li><p><code>CNN</code> 结构设计三个主要策略：</p>
<ul>
<li><p>策略 1：部分的使用<code>1x1</code> 卷积替换<code>3x3</code> 卷积。因为<code>1x1</code> 卷积的参数数量比<code>3x3</code> 卷积的参数数量少了 <code>9</code> 倍。</p>
</li>
<li><p>策略 2：减少<code>3x3</code> 卷积输入通道的数量。这会进一步降低网络的参数数量。</p>
</li>
<li><p>策略 3：将网络中下采样的时机推迟到网络的后面。这会使得网络整体具有尺寸较大的<code>feature map</code> 。</p>
<p>其直觉是：在其它不变的情况下，尺寸大的<code>feature map</code> 具有更高的分类准确率。</p>
</li>
</ul>
<p>策略<code>1、2</code> 是关于在尽可能保持模型准确率的条件下减少模型的参数数量，策略<code>3</code> 是关于在有限的参数数量下最大化准确率。</p>
</li>
</ol>
<h4 id="Fire-模块"><a href="#Fire-模块" class="headerlink" title="Fire 模块"></a>Fire 模块</h4><p><img src="/images/loading.gif" data-original="../images/ML/image-20210903125528293.png" alt=""></p>
<h4 id="网络性能-6"><a href="#网络性能-6" class="headerlink" title="网络性能"></a>网络性能</h4><ol>
<li>网络设计：</li>
</ol>
<ul>
<li><code>SqueezeNet</code> 从一个独立的卷积层（<code>conv1</code>）开始，后跟 8 个<code>Fire</code> 模块（<code>fire2~9</code> ），最后连接卷积层<code>conv10</code>、全局平均池化层、<code>softmax</code> 输出层。</li>
<li>从网络开始到末尾，每个<code>Fire</code> 模块的输出通道数逐渐增加。</li>
<li>在<code>conv1、fire4、fire8</code> 之后执行最大池化，步长为2。这种相对较晚的执行池化操作是采用了策略3。</li>
<li>在<code>expand</code> 层中的<code>3x3</code> 执行的是<code>same</code> 卷积，即：在原始输入上下左右各添加一个像素，使得输出的<code>feature map</code> 尺寸不变。</li>
<li>在<code>fire9</code> 之后使用<code>Dropout</code>，遗忘比例为 0.5 。</li>
</ul>
<p><img src="/images/loading.gif" data-original="../images/ML/6f1f004da0ecab8dc7cdc6bc7e6f3b86.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210903125614311.png" alt=""></p>
<ol start="3">
<li>模型性能：</li>
</ol>
<ul>
<li>网络压缩方法列：给出了网络裁剪方法，包括<code>SVD(Denton et al. 2014)</code>、<code>Network pruning (Han et al. 2015b)</code>、<code>Deep compression (Han et al. 2015a)</code> 。</li>
<li>数据类型列：给出了计算精度。</li>
<li>模型压缩比例列：给出了模型相对于原始<code>AlexNet</code> 的模型大小压缩的比例。</li>
<li><code>top-1 Accuracy/top-5 Accuracy</code> 列：给出了模型在<code>ImageNet</code> 测试集上的评估结果。</li>
</ul>
<p>可以看到，<code>SqueezeNet</code> 在满足同样准确率的情况下，模型大小比<code>AlexNet</code> 压缩了 50 倍。如果使用 <code>Deep Compression</code> 以及 <code>6 bit</code> 精度的情况下，模型大小比<code>AlexNet</code> 压缩了 510 倍。</p>
<table>
<thead>
<tr>
<th align="left">CNN 网络结构</th>
<th align="left">网络压缩方法</th>
<th align="left">数据类型</th>
<th align="left">模型大小</th>
<th align="left">模型压缩比例</th>
<th align="left">top-1 Accuracy</th>
<th align="left">top-5 Accuracy</th>
</tr>
</thead>
<tbody><tr>
<td align="left">AlexNet</td>
<td align="left">None</td>
<td align="left">32 bit</td>
<td align="left">240 MB</td>
<td align="left">1x</td>
<td align="left">57.2%</td>
<td align="left">80.3%</td>
</tr>
<tr>
<td align="left">AlexNet</td>
<td align="left">SVD</td>
<td align="left">32 bit</td>
<td align="left">48 MB</td>
<td align="left">5x</td>
<td align="left">56.0%</td>
<td align="left">79.4%</td>
</tr>
<tr>
<td align="left">AlexNet</td>
<td align="left">Network Pruning</td>
<td align="left">32 bit</td>
<td align="left">27 MB</td>
<td align="left">9x</td>
<td align="left">57.2%</td>
<td align="left">80.3%</td>
</tr>
<tr>
<td align="left">AlexNet</td>
<td align="left">Deep Compression</td>
<td align="left">5-8 bit</td>
<td align="left">6.9 MB</td>
<td align="left">35x</td>
<td align="left">57.2%</td>
<td align="left">80.3%</td>
</tr>
<tr>
<td align="left">SqueezeNet</td>
<td align="left">None</td>
<td align="left">32 bit</td>
<td align="left">4.8 MB</td>
<td align="left">50x</td>
<td align="left">57.5%</td>
<td align="left">80.3%</td>
</tr>
<tr>
<td align="left">SqueezeNet</td>
<td align="left">Deep Compression</td>
<td align="left">8 bit</td>
<td align="left">0.66 MB</td>
<td align="left">363x</td>
<td align="left">57.5%</td>
<td align="left">80.3%</td>
</tr>
<tr>
<td align="left">SqueezeNet</td>
<td align="left">Deep Compression</td>
<td align="left">6 bit</td>
<td align="left">0.47 MB</td>
<td align="left">510x</td>
<td align="left">57.5%</td>
<td align="left">80.3%</td>
</tr>
</tbody></table>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210903125653748.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210903125709260.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210903125731414.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/15245b37fdd457743116d8b9d66bd760.png" alt=""></p>
<p>它们在<code>ImageNet</code> 测试集上的表现：</p>
<table>
<thead>
<tr>
<th align="left">模型结构</th>
<th align="left">top-1 准确率</th>
<th align="left">top-5 准确率</th>
<th align="left">model size</th>
</tr>
</thead>
<tbody><tr>
<td align="left">SqueezeNet</td>
<td align="left">57.5%</td>
<td align="left">80.3%</td>
<td align="left">4.8MB</td>
</tr>
<tr>
<td align="left">SqueezeNet + 简单旁路连接</td>
<td align="left">60.4%</td>
<td align="left">82.5%</td>
<td align="left">4.8MB</td>
</tr>
<tr>
<td align="left">SqueezeNet + 复杂旁路连接</td>
<td align="left">58.8%</td>
<td align="left">82.0%</td>
<td align="left">7.7MB</td>
</tr>
</tbody></table>
<p>因此添加简单旁路连接能够提升模型的准确率，还能保持模型的大小不变。</p>
<h3 id="SqueezeNext"><a href="#SqueezeNext" class="headerlink" title="SqueezeNext"></a>SqueezeNext</h3><p>现有的神经网络在嵌入式系统上部署的主要挑战之一是内存和功耗，<code>SqueezeNext</code> 针对内存和功耗进行优化，是为功耗和内存有限的嵌入式设备设计的神经网络。</p>
<h4 id="SqueezeNext-Block"><a href="#SqueezeNext-Block" class="headerlink" title="SqueezeNext Block"></a>SqueezeNext Block</h4><ol>
<li><p><code>SqueezeNext</code> 块是在<code>Fire</code> 块的基础进行修改：</p>
<ul>
<li>将 <code>expand</code> 层的<code>3x3</code> 卷积替换为<code>1x3 + 3x1</code> 卷积，同时移除了 <code>expand</code> 层的拼接 <code>1x1</code> 卷积、添加了<code>1x1</code> 卷积来恢复通道数。</li>
<li>通过两阶段的 <code>squeeze</code> 得到更激进的通道缩减，每个阶段的<code>squeeze</code> 都将通道数减半。</li>
</ul>
<p><img src="/images/loading.gif" data-original="https://static.sitestack.cn/projects/huaxiaozhuan-ai/0ad3bccb20301de873cbb34dfecd9802.png" alt=""></p>
</li>
<li><p><code>SqueezeNext</code> 块也采用类似<code>ResNet</code> 的旁路连接，从而可以训练更深的网络。</p>
<p>下图中，左图为 <code>ResNet</code> 块，中图为 <code>SqueezeNet</code> 的 <code>Fire</code> 块，右图为<code>SqueezeNext</code> 块。</p>
<p><img src="/images/loading.gif" data-original="../images/ML/5980635d6282978f6f99e81870a71e05.png" alt=""></p>
</li>
</ol>
<h4 id="网络性能-7"><a href="#网络性能-7" class="headerlink" title="网络性能"></a>网络性能</h4><ol>
<li><p>网络结构：如下为一个23层的<code>SqueezeNext</code> 网络（记做<code>SqueezeNext-23</code>）。</p>
<ul>
<li>相同颜色的<code>SqueezeNext</code> 块具有相同的输入<code>feature map</code> 尺寸和通道数。</li>
<li>该网络结构描述为<code>[6,6,8,1]</code>，意思是：在第一个<code>conv/pooling</code> 层之后有四组<code>SqueezeNext</code> 块，每组<code>SqueezeNext</code> 分别有6个、6个、8个、1个 <code>SqueezeNext</code> 块。</li>
<li>在全连接层之前插入一个<code>1x1</code> 卷积层来降低全连接层的输入通道数，从而大幅降低全连接层的参数数量。</li>
</ul>
<p><img src="/images/loading.gif" data-original="../images/ML/779b4d744263b73a786958bc67529eb0.png" alt=""></p>
<p>23层<code>SqueezeNext</code> 网络的另一种结构（记做<code>SqueezeNext-23v5</code>）：结构描述为<code>[2,4,14,1]</code> 。</p>
<p><img src="/images/loading.gif" data-original="../images/ML/f37311f066c295a9229b88381b48f747.png" alt=""></p>
</li>
<li><p><code>SqueezeNext</code>网络在<code>ImageNet</code> 上的预测准确率：</p>
<ul>
<li>参数降低倍数是相对于<code>AlexNet</code> 网络的参数而言。</li>
<li><code>G-SqueezeNext-23</code> 是<code>SqueezeNext-23</code> 采用分组卷积的版本。</li>
</ul>
<table>
<thead>
<tr>
<th align="left">模型</th>
<th align="left">top-1 准确率</th>
<th align="left">top-5 准确率</th>
<th align="left">参数数量(百万)</th>
<th align="left">参数降低倍数</th>
</tr>
</thead>
<tbody><tr>
<td align="left">AlexNet</td>
<td align="left">57.10%</td>
<td align="left">80.30%</td>
<td align="left">60.9</td>
<td align="left">1x</td>
</tr>
<tr>
<td align="left">SqueezeNet</td>
<td align="left">57.50%</td>
<td align="left">80.30%</td>
<td align="left">1.2</td>
<td align="left">51x</td>
</tr>
<tr>
<td align="left">SqueezeNext-23</td>
<td align="left">59.05%</td>
<td align="left">82.60%</td>
<td align="left">0.72</td>
<td align="left">84x</td>
</tr>
<tr>
<td align="left">G-SqueezeNext-23</td>
<td align="left">57.16%</td>
<td align="left">80.23%</td>
<td align="left">0.54</td>
<td align="left">112x</td>
</tr>
<tr>
<td align="left">SqueezeNext-34</td>
<td align="left">61.39%</td>
<td align="left">84.31%</td>
<td align="left">1.0</td>
<td align="left">61x</td>
</tr>
<tr>
<td align="left">SqueezeNext-44</td>
<td align="left">62.64%</td>
<td align="left">85.15%</td>
<td align="left">1.2</td>
<td align="left">51x</td>
</tr>
</tbody></table>
</li>
<li><p>更宽和更深版本的<code>SqueezeNext</code>网络在<code>ImageNet</code> 上的预测准确率：</p>
<ul>
<li><code>1.5/2.0</code> 分别表示将网络拓宽<code>1.5/2</code> 倍。拓宽指的增加网络的<code>feature map</code> 的通道数，做法是增加第一个<code>conv</code> 的输出通道数。</li>
<li>括号中的准确率是采用了数据集增强和超参数优化之后的最佳结果。</li>
</ul>
<table>
<thead>
<tr>
<th align="left">模型</th>
<th align="left">top-1 准确率</th>
<th align="left">top-5 准确率</th>
<th align="left">参数数量(百万)</th>
</tr>
</thead>
<tbody><tr>
<td align="left">1.5-SqueezeNext-23</td>
<td align="left">63.52%</td>
<td align="left">85.66%</td>
<td align="left">1.4</td>
</tr>
<tr>
<td align="left">1.5-SqueezeNext-34</td>
<td align="left">66.00%</td>
<td align="left">87.40%</td>
<td align="left">2.1</td>
</tr>
<tr>
<td align="left">1.5-SqueezeNext-44</td>
<td align="left">67.28%</td>
<td align="left">88.15%</td>
<td align="left">2.6</td>
</tr>
<tr>
<td align="left">VGG-19</td>
<td align="left">68.50%</td>
<td align="left">88.50%</td>
<td align="left">138</td>
</tr>
<tr>
<td align="left">2.0-SqueezeNext-23</td>
<td align="left">67.18%</td>
<td align="left">88.17%</td>
<td align="left">2.4</td>
</tr>
<tr>
<td align="left">2.0-SqueezeNext-34</td>
<td align="left">68.46%</td>
<td align="left">88.78%</td>
<td align="left">3.8</td>
</tr>
<tr>
<td align="left">2.0-SqueezeNext-44</td>
<td align="left">69.59%</td>
<td align="left">89.53%</td>
<td align="left">4.4</td>
</tr>
<tr>
<td align="left">MobileNet</td>
<td align="left">67.50%(70.9%)</td>
<td align="left">86.59%(89.9%)</td>
<td align="left">4.2</td>
</tr>
<tr>
<td align="left">2.0-SqueezeNext-23v5</td>
<td align="left">67.44%(69.8%)</td>
<td align="left">88.20%(89.5%)</td>
<td align="left">3.2</td>
</tr>
</tbody></table>
</li>
<li><p>硬件仿真结果：</p>
<ul>
<li>括号中的准确率是采用了数据集增强和超参数优化之后的最佳结果。</li>
<li><code>Time</code> 表示模型的推断时间（相对耗时），<code>Energy</code> 表示模型的推断功耗。</li>
<li><code>8x8,32KB</code> 和<code>16x16,128KB</code> 表示仿真硬件的配置：<ul>
<li><code>NxN</code> 表示硬件具有<code>NxN</code> 个<code>PE</code> 阵列。<code>processing element:PE</code> 是单个计算单元。</li>
<li><code>32KB/128KB</code> 表示全局缓存。</li>
</ul>
</li>
</ul>
<p><img src="/images/loading.gif" data-original="../images/ML/7bb7e734ec3d3a873204056dfb36179f.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/756b8cfce8baf0c658534d17997df7a3.png" alt=""></p>
</li>
<li><p>深度可分离卷积的计算密集性较差，因为其<code>计算/带宽</code> 比例较低，所以在某些移动设备上表现较差。</p>
<blockquote>
<p>一个可分离卷积的计算需要多次 <code>IO</code> 和计算才能完成，相比而言普通卷积只需要一次<code>IO</code> 和计算。</p>
</blockquote>
</li>
</ol>
<h2 id="MobileNet-系列"><a href="#MobileNet-系列" class="headerlink" title="MobileNet 系列"></a>MobileNet 系列</h2><h3 id="MobileNet"><a href="#MobileNet" class="headerlink" title="MobileNet"></a>MobileNet</h3><ol>
<li><p><code>MobileNet</code> 应用了<code>Depthwise</code> 深度可分离卷积来代替常规卷积，从而降低计算量，减少模型参数。</p>
</li>
<li><p><code>MobileNet</code> 不仅产生了小型网络，还重点优化了预测延迟。</p>
<p>与之相比，有一些小型网络虽然网络参数较少，但是预测延迟较大</p>
</li>
</ol>
<h4 id="深度可分离卷积"><a href="#深度可分离卷积" class="headerlink" title="深度可分离卷积"></a>深度可分离卷积</h4><p><img src="/images/loading.gif" data-original="../images/ML/image-20210903130251273.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210903130312569.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/06ff01e54aa079e1783a6a35b03728d7.png" alt=""></p>
<h4 id="网络结构-4"><a href="#网络结构-4" class="headerlink" title="网络结构"></a>网络结构</h4><ol>
<li><p><code>MobileNeet</code> 网络结构如下表所示。其中：</p>
<ul>
<li><code>Conv</code> 表示标准卷积，<code>Conv dw</code> 表示深度可分离卷积。</li>
<li>所有层之后都跟随<code>BN</code> 和 <code>ReLU</code> （除了最后的全连接层，该层的输出直接送入到<code>softmax</code> 层进行分类）。</li>
</ul>
<p><img src="/images/loading.gif" data-original="../images/ML/15f95fb7344d1d578eb41ea74f798468-163064545184760.png" alt=""></p>
</li>
<li><p><code>MobileNet</code> 大量的参数和计算量都被消耗在 <code>1x1</code> 卷积上：</p>
<ul>
<li><code>Conv 1x1</code> 包含了所有的<code>1x1</code> 卷积层，包括可分离卷积中的<code>1x1</code> 卷积。</li>
<li><code>Conv DW 3x3</code> 仅包括可分离卷积中的 <code>3x3</code> 卷积。</li>
</ul>
<table>
<thead>
<tr>
<th align="left">层类型</th>
<th align="left">乘-加运算</th>
<th align="left">参数数量</th>
</tr>
</thead>
<tbody><tr>
<td align="left">Conv 1x1</td>
<td align="left">94.86%</td>
<td align="left">74.59%</td>
</tr>
<tr>
<td align="left">Conv DW 3x3</td>
<td align="left">3.06%</td>
<td align="left">1.06%</td>
</tr>
<tr>
<td align="left">Conv 3x3</td>
<td align="left">1.19%</td>
<td align="left">0.02%</td>
</tr>
<tr>
<td align="left">全连接层</td>
<td align="left">0.18%</td>
<td align="left">24.33%</td>
</tr>
</tbody></table>
</li>
<li><p>与训练大模型相反，训练<code>MobileNet</code> 时较少的采用正则化和数据集增强技术，因为<code>MobileNet</code> 是小模型，而小模型不容易过拟合。</p>
<p>论文特别提到：在<code>depthwise</code> 滤波器上使用很少或者不使用 <code>L2</code> 正则化，因为它们的参数很少。</p>
</li>
</ol>
<h4 id="宽度乘子-amp-分辨率乘子"><a href="#宽度乘子-amp-分辨率乘子" class="headerlink" title="宽度乘子 &amp; 分辨率乘子"></a>宽度乘子 &amp; 分辨率乘子</h4><p><img src="/images/loading.gif" data-original="../images/ML/image-20210903130448108.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210903130500375.png" alt=""></p>
<h4 id="网络性能-8"><a href="#网络性能-8" class="headerlink" title="网络性能"></a>网络性能</h4><p><img src="/images/loading.gif" data-original="../images/ML/image-20210903130530179.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210903130548952.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210903130610369.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210903130624686.png" alt=""></p>
<h3 id="MobileNet-V2"><a href="#MobileNet-V2" class="headerlink" title="MobileNet V2"></a>MobileNet V2</h3><p><code>MobileNet V2</code> 创新性的提出了具有线性<code>bottleneck</code> 的<code>Inverted</code> 残差块。</p>
<p>这种块特别适用于移动设备和嵌入式设备，因为它用到的张量都较小，因此减少了推断期间的内存需求。</p>
<h4 id="线性bottleneck"><a href="#线性bottleneck" class="headerlink" title="线性bottleneck"></a>线性bottleneck</h4><p><img src="/images/loading.gif" data-original="../images/ML/image-20210903130727614.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210903130746915.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210903130806347.png" alt=""></p>
<h4 id="bottleneck-block"><a href="#bottleneck-block" class="headerlink" title="bottleneck block"></a>bottleneck block</h4><ol>
<li><p><code>bottleneck block</code>：输入<code>feature map</code> 首先经过线性 <code>bottleneck</code> 来扩张通道数，然后经过深度可分离卷积，最后通过线性<code>bottleneck</code> 来缩小通道数。</p>
<p>输入<code>bootleneck</code> 输出通道数与输入通道数的比例称作膨胀比。</p>
<ul>
<li>通常较小的网络使用略小的膨胀比效果更好，较大的网络使用略大的膨胀比效果更好。</li>
<li>如果膨胀比小于 1 ，这就是一个典型的 <code>resnet</code> 残差块。</li>
</ul>
</li>
<li><p>可以在 <code>bottleneck block</code> 中引入旁路连接，这种<code>bottleneck block</code> 称作<code>Inverted</code> 残差块，其结构类似<code>ResNet</code> 残差块。</p>
<ul>
<li><p>在<code>ResNeXt</code> 残差块中，首先对输入<code>feature map</code> 执行<code>1x1</code> 卷积来压缩通道数，最后通过<code>1x1</code> 卷积来恢复通道数。</p>
<p>这对应了一个输入 <code>feature map</code> 通道数先压缩、后扩张的过程。</p>
</li>
<li><p>在<code>Inverted</code> 残差块中，首先对输入<code>feature map</code> 执行<code>1x1</code> 卷积来扩张通道数，最后通过<code>1x1</code> 卷积来恢复通道数。</p>
<p>这对应了一个输入 <code>feature map</code> 通道数先扩张、后压缩的过程。这也是<code>Inverted</code> 残差块取名为<code>Inverted</code> 的原因。</p>
</li>
</ul>
<p><img src="/images/loading.gif" data-original="../images/ML/4134c2ca776d1893e96bffb23a7a3b2d.png" alt=""></p>
</li>
<li><p>当深度可分离卷积的步长为 <code>1</code> 时，<code>bottleneck block</code>包含了旁路连接。</p>
<p>当深度可分离卷积的步长不为 <code>1</code> 时，<code>bottleneck block</code>不包含旁路连接。这是因为：输入<code>feature map</code> 的尺寸与块输出<code>feature map</code> 的尺寸不相同，二者无法简单拼接。</p>
<p>虽然可以将旁路连接通过一个同样步长的池化层来解决，但是根据<code>ResNet</code>的研究，破坏旁路连接会引起网络性能的下降。</p>
<p><img src="/images/loading.gif" data-original="../images/ML/f218f294d558b69e2397922217830480.png" alt=""></p>
</li>
<li><p>事实上旁路连接有两个插入的位置：在两个<code>1x1</code> 卷积的前后，或者在两个<code>Dwise</code> 卷积的前后。</p>
<p>通过实验证明：在两个<code>1x1</code> 卷积的前后使用旁路连接的效果最好。在<code>Imagenet</code> 测试集上的表现如下图：</p>
<p><img src="/images/loading.gif" data-original="../images/ML/bd53fff7854c6ad9e7174543f1606c8a.png" alt=""></p>
</li>
<li><p><code>bottleneck block</code> 可以看作是对信息的两阶段处理过程：</p>
<ul>
<li>阶段一：对输入<code>feature map</code> 进行降维，这一部分代表了信息的容量。</li>
<li>阶段二：对信息进行非线性处理，这一部分代表了信息的表达。</li>
</ul>
<p>在<code>MobileNet v2</code> 中这二者是独立的，而传统网络中这二者是相互纠缠的。</p>
</li>
</ol>
<h4 id="网络性能-9"><a href="#网络性能-9" class="headerlink" title="网络性能"></a>网络性能</h4><ol>
<li><p><code>MobileNet V2</code> 的设计基于 <code>MobileNet v1</code> ，其结构如下：</p>
<ul>
<li>每一行代表一个或者一组相同结构的层，层的数量由 <img src="/images/loading.gif" data-original="https://static.sitestack.cn/projects/huaxiaozhuan-ai/f1879df5a6e9563dfbf8c0b55bba0403.svg" alt="九、小型网络 - 图146"> 给定。</li>
<li>相同结构指的是：<ul>
<li>同一行内的层的类型相同，由<code>Operator</code> 指定。其中<code>bottleneck</code> 指的是<code>bottleneck block</code> 。</li>
<li>同一行内的层的膨胀比相同，由 <code>t</code> 指定。</li>
<li>同一行内的层的输出通道数相同，由<code>c</code> 指定。</li>
<li>同一行内的层：第一层采用步幅<code>s</code>，其它层采用步幅<code>1</code> 。</li>
</ul>
</li>
<li>采用<code>ReLU6</code> 激活函数，因为它在低精度浮点运算的环境下表现较好。</li>
<li>训练过程中采用<code>dropout</code> 和<code>BN</code>。</li>
</ul>
</li>
<li><p>与<code>MobileNet V1</code> 类似，<code>MobileNet V2</code> 也可以引入宽度乘子、分辨率乘子这两个超参数。</p>
</li>
<li><p>网络推断期间最大内存需求（<code>通道数/内存消耗(Kb)</code>）：采用 16 bit 的浮点运算。</p>
<p><img src="/images/loading.gif" data-original="../images/ML/5c9bd7506d8166f98e12d7b705c4de52.png" alt=""></p>
</li>
<li><p>网络在<code>ImageNet</code> 测试集上的表现：</p>
<ul>
<li>最后一列给出了预测单张图片的推断时间。</li>
</ul>
<table>
<thead>
<tr>
<th align="left">网络</th>
<th align="left">Top 1</th>
<th align="left">Params（百万）</th>
<th align="left">乘-加 数量（百万）</th>
<th align="left">CPU</th>
</tr>
</thead>
<tbody><tr>
<td align="left">MobileNet V1</td>
<td align="left">70.6</td>
<td align="left">4.2</td>
<td align="left">575</td>
<td align="left">113ms</td>
</tr>
<tr>
<td align="left">ShuffleNet (1.5)</td>
<td align="left">71.5</td>
<td align="left">3.4</td>
<td align="left">292</td>
<td align="left">-</td>
</tr>
<tr>
<td align="left">ShuffleNet (x2)</td>
<td align="left">73.7</td>
<td align="left">5.4</td>
<td align="left">524</td>
<td align="left">-</td>
</tr>
<tr>
<td align="left">NasNet-A</td>
<td align="left">74.0</td>
<td align="left">5.3</td>
<td align="left">564</td>
<td align="left">183ms</td>
</tr>
<tr>
<td align="left">MobileNet V2</td>
<td align="left">72.0</td>
<td align="left">3.4</td>
<td align="left">300</td>
<td align="left">75ms</td>
</tr>
<tr>
<td align="left">MobileNet V2（1.4）</td>
<td align="left">74.7</td>
<td align="left">6.9</td>
<td align="left">585</td>
<td align="left">143ms</td>
</tr>
</tbody></table>
</li>
</ol>
<h2 id="ShuffleNet-系列"><a href="#ShuffleNet-系列" class="headerlink" title="ShuffleNet 系列"></a>ShuffleNet 系列</h2><h3 id="ShuffleNet"><a href="#ShuffleNet" class="headerlink" title="ShuffleNet"></a>ShuffleNet</h3><p><code>ShuffleNet</code> 提出了 <code>1x1分组卷积+通道混洗</code> 的策略，在保证准确率的同时大幅降低计算成本。</p>
<p><code>ShuffleNet</code> 专为计算能力有限的设备（如：<code>10~150MFLOPs</code>）设计。在基于<code>ARM</code> 的移动设备上，<code>ShuffleNet</code> 与<code>AlexNet</code> 相比，在保持相当的准确率的同时，大约 13 倍的加速。</p>
<h4 id="ShuffleNet-block"><a href="#ShuffleNet-block" class="headerlink" title="ShuffleNet block"></a>ShuffleNet block</h4><ol>
<li><p>在<code>Xception</code> 和<code>ResNeXt</code> 中，有大量的<code>1x1</code> 卷积，所以整体而言<code>1x1</code> 卷积的计算开销较大。如<code>ResNeXt</code> 的每个残差块中，<code>1x1</code> 卷积占据了<code>乘-加</code>运算的 93.4% （基数为32时）。</p>
<p>在小型网络中，为了满足计算性能的约束（因为计算资源不够）需要控制计算量。虽然限制通道数量可以降低计算量，但这也可能会严重降低准确率。</p>
<p>解决办法是：对<code>1x1</code> 卷积应用分组卷积，将每个 <code>1x1</code> 卷积仅仅在相应的通道分组上操作，这样就可以降低每个<code>1x1</code> 卷积的计算代价。</p>
</li>
<li><p><code>1x1</code> 卷积仅在相应的通道分组上操作会带来一个副作用：每个通道的输出仅仅与该通道所在分组的输入（一般占总输入的比例较小）有关，与其它分组的输入（一般占总输入的比例较大）无关。这会阻止通道之间的信息流动，降低网络的表达能力。</p>
<p>解决办法是：采用通道混洗，允许分组卷积从不同分组中获取输入。</p>
<ul>
<li>如下图所示：<code>(a)</code> 表示没有通道混洗的分组卷积；<code>(b)</code> 表示进行通道混洗的分组卷积；<code>(c)</code> 为<code>(b)</code> 的等效表示。</li>
<li>由于通道混洗是可微的，因此它可以嵌入到网络中以进行端到端的训练。</li>
</ul>
<p><img src="/images/loading.gif" data-original="../images/ML/a88020f58192a8534be9929ab498274a-163064597610768.png" alt=""></p>
</li>
<li><p><code>ShuffleNet</code> 块的结构从<code>ResNeXt</code> 块改进而来：下图中<code>(a)</code> 是一个<code>ResNeXt</code> 块，<code>(b)</code> 是一个 <code>ShuffleNet</code> 块，<code>(c)</code> 是一个步长为<code>2</code> 的 <code>ShuffleNet</code> 块。</p>
<p>在 <code>ShuffleNet</code> 块中：</p>
<ul>
<li><p>第一个<code>1x1</code> 卷积替换为<code>1x1</code> 分组卷积+通道随机混洗。</p>
</li>
<li><p>第二个<code>1x1</code> 卷积替换为<code>1x1</code> 分组卷积，但是并没有附加通道随机混洗。这是为了简单起见，因为不附加通道随机混洗已经有了很好的结果。</p>
</li>
<li><p>在<code>3x3 depthwise</code> 卷积之后只有<code>BN</code> 而没有<code>ReLU</code> 。</p>
</li>
<li><p>当步长为2时：</p>
<ul>
<li><p>恒等映射直连替换为一个尺寸为 <code>3x3</code> 、步长为<code>2</code> 的平均池化。</p>
</li>
<li><p><code>3x3 depthwise</code> 卷积的步长为<code>2</code> 。</p>
</li>
<li><p>将残差部分与直连部分的<code>feature map</code> 拼接，而不是相加。</p>
<p>因为当<code>feature map</code> 减半时，为了缓解信息丢失需要将输出通道数加倍从而保持模型的有效容量。</p>
</li>
</ul>
</li>
</ul>
<p><img src="/images/loading.gif" data-original="../images/ML/93b315ebe8af6a298422a530e5c39304.png" alt=""></p>
</li>
</ol>
<h4 id="网络性能-10"><a href="#网络性能-10" class="headerlink" title="网络性能"></a>网络性能</h4><ol>
<li><p>在<code>Shufflenet</code> 中，<code>depthwise</code> 卷积仅仅在<code>1x1</code> 卷积的输出 <code>feature map</code> 上执行。这是因为 <code>depthwise</code> 很难在移动设备上高效的实现，因为移动设备的 <code>计算/内存访问</code> 比率较低，所以仅仅在<code>1x1</code> 卷积的输出 <code>feature map</code> 上执行从而降低开销。</p>
</li>
<li><p><code>ShuffleNet</code> 网络由<code>ShuffleNet</code> 块组成。</p>
<ul>
<li><p>网络主要由三个<code>Stage</code> 组成。</p>
<ul>
<li>每个<code>Stage</code> 的第一个块的步长为 2 ，<code>stage</code> 内的块在其它参数上相同。</li>
<li>每个<code>Stage</code> 的输出通道数翻倍。</li>
</ul>
</li>
<li><p>在 <code>Stage2</code> 的第一个<code>1x1</code> 卷积并不执行分组卷积，因此此时的输入通道数相对较小。</p>
</li>
<li><p>每个<code>ShuffleNet</code> 块中的第一个<code>1x1</code> 分组卷积的输出通道数为：该块的输出通道数的 <code>1/4</code> 。</p>
</li>
<li><p>使用较少的数据集增强，因为这一类小模型更多的是遇到欠拟合而不是过拟合。</p>
</li>
<li><p>复杂度给出了计算量（<code>乘-加运算</code>），<code>KSize</code> 给出了卷积核的尺寸，<code>Stride</code> 给出了<code>ShuffleNet block</code> 的步长，<code>Repeat</code> 给出了 <code>ShuffleNet block</code> 重复的次数，<code>g</code> 控制了<code>ShuffleNet block</code> 分组的数量。</p>
<blockquote>
<p><code>g=1</code> 时，<code>1x1</code> 的通道分组卷积退化回原始的<code>1x1</code> 卷积。</p>
</blockquote>
</li>
</ul>
<p><img src="/images/loading.gif" data-original="../images/ML/938ae60af782b645f344190d15ba08a1.png" alt=""></p>
</li>
<li><p>超参数 <code>g</code> 会影响模型的准确率和计算量。在 <code>ImageNet</code> 测试集上的表现如下：</p>
<ul>
<li><p><code>ShuffleNet sx</code> 表示对<code>ShuffleNet</code> 的通道数增加到 <code>s</code> 倍。这通过控制 <code>Conv1</code> 卷积的输出通道数来实现。</p>
</li>
<li><p><code>g</code> 越大，则计算量越小，模型越准确。</p>
<p>其背后的原理是：小模型的表达能力不足，通道数量越大，则小模型的表达能力越强。</p>
<ul>
<li><code>g</code> 越大，则准确率越高。这是因为对于<code>ShuffleNet</code>，分组越大则生成的<code>feature map</code> 通道数量越多，模型表达能力越强。</li>
<li>网络的通道数越小（如<code>ShuffleNet 0.25x</code> ），则这种增益越大。</li>
</ul>
</li>
<li><p>随着分组越来越大，准确率饱和甚至下降。</p>
<p>这是因为随着分组数量增大，每个组内的通道数量变少。虽然总体通道数增加，但是每个分组提取有效信息的能力变弱，降低了模型整体的表征能力。</p>
</li>
<li><p>虽然较大<code>g</code> 的<code>ShuffleNet</code> 通常具有更好的准确率。但是由于它的实现效率较低，会带来较大的推断时间。</p>
</li>
</ul>
<p><img src="/images/loading.gif" data-original="../images/ML/fd3026259ecef37f17d814d1784f18d7.png" alt=""></p>
</li>
<li><p>通道随机混洗的效果要比不混洗好。在 <code>ImageNet</code> 测试集上的表现如下：</p>
<ul>
<li>通道混洗使得分组卷积中，信息能够跨分组流动。</li>
<li>分组数<code>g</code> 越大，这种混洗带来的好处越大。</li>
</ul>
<p><img src="/images/loading.gif" data-original="../images/ML/8a37abb3b85f9d5c9dba11ae7ec12647.png" alt=""></p>
</li>
<li><p>多种模型在<code>ImageNet</code> 测试集上的表现：</p>
<p>比较分为三组，每一组都有相差无几的计算量。<img src="/images/loading.gif" data-original="https://static.sitestack.cn/projects/huaxiaozhuan-ai/04fd4b12d2a2fffcabb30e705c43c24f.svg" alt="九、小型网络 - 图153"> 给出了在该组中，模型相对于 <code>MobileNet</code> 的预测能力的提升。</p>
<ul>
<li><code>MFLOPs</code> 表示<code>乘-加</code>运算量（百万），错误率表示<code>top-1 error</code> 。</li>
<li><code>ShuffleNet 0.5x(shallow,g=3)</code> 是一个更浅的<code>ShuffleNet</code> 。考虑到<code>MobileNet</code> 只有 28 层，而<code>ShuffleNet</code> 有 50 层，因此去掉了<code>Stage 2-4</code> 中一半的块，得到一个教浅的、只有 26 层的 <code>ShuffleNet</code> 。</li>
</ul>
<p><img src="/images/loading.gif" data-original="../images/ML/c8e3f78a7781cd672ec644a7bf82dc81.png" alt=""></p>
</li>
<li><p>在移动设备上的推断时间（<code>Qualcomm Snapdragon 820 processor</code>，单线程）：</p>
<p><img src="/images/loading.gif" data-original="../images/ML/56b9fb405b49f7ab7cf27a10d341b7ae.png" alt=""></p>
</li>
</ol>
<h3 id="ShuffleNet-V2"><a href="#ShuffleNet-V2" class="headerlink" title="ShuffleNet V2"></a>ShuffleNet V2</h3><p><code>ShuffleNet V2</code> 基于一系列对照实验提出了高效网络设计的几个通用准则，并提出了<code>ShuffleNet V2</code> 的网络结构。</p>
<h4 id="小型网络通用设计准则"><a href="#小型网络通用设计准则" class="headerlink" title="小型网络通用设计准则"></a>小型网络通用设计准则</h4><ol>
<li><p>目前衡量模型推断速度的一个通用指标是<code>FLOPs</code>（即<code>乘-加</code> 计算量）。事实上这是一个间接指标，因为它不完全等同于推断速度。如：<code>MobileNet V2</code> 和 <code>NASNET-A</code> 的<code>FLOPs</code> 相差无几，但是<code>MobileNet V2</code> 的推断速度要快得多。</p>
<p>如下所示为几种模型在<code>GPU</code> 和<code>ARM</code> 上的准确率（在<code>ImageNet</code> 验证集上的测试结果）、模型推断速度（通过<code>Batch/秒</code>来衡量）、计算复杂度（通过<code>FLOPs</code> 衡量）的关系。</p>
<ul>
<li><p>在<code>ARM</code> 平台上<code>batchsize=1</code> ， 在<code>GPU</code> 平台上<code>batchsize=8</code> 。</p>
</li>
<li><p>准确率与模型容量成正比，而模型容量与模型计算复杂度成成比、计算复杂度与推断速度成反比。</p>
<p>因此：模型准确率越高，则推断速度越低；模型计算复杂度越高，则推断速度越低。</p>
</li>
</ul>
<p><img src="/images/loading.gif" data-original="../images/ML/a21b13cc4b9bda78d36a1bd42ef041b4.png" alt=""></p>
</li>
<li><p><code>FLOPs</code> 和推断速度之间的差异有两个原因：</p>
<ul>
<li>除了<code>FLOPs</code> 之外，还有几个因素对推断速度有重要的影响。<ul>
<li>内存访问量（<code>memory access cost:MAC</code>）：在某些操作（如分组卷积）中，其时间开销占相当大比例。因此它可能是<code>GPU</code> 这种具有强大计算能力设备的瓶颈。</li>
<li>模型并行度：相同<code>FLOPs</code> 的情况下，高并行度的模型比低并行度的模型快得多。</li>
</ul>
</li>
<li>即使相同的操作、相同的 <code>FLOPs</code>，在不同的硬件设备上、不同的库上，其推断速度也可能不同。</li>
</ul>
</li>
<li><p><code>MobileNet V2</code> 和 <code>ShuffleNet V1</code> 这两个网络非常具有代表性，它们分别采用了<code>group</code> 卷积和 <code>depth-wise</code> 卷积。这两个操作广泛应用于其它的先进网络。</p>
<p>利用实验分析它们的推断速度（以推断时间开销来衡量）。其中：宽度乘子均为1，<code>ShuffleNet V1</code> 的分组数<code>g=3</code>。</p>
<p>从实验结果可知：<code>FLOPs</code> 指标仅仅考虑了卷积部分的计算量。虽然这部分消耗时间最多，但是其它操作包括数据<code>IO</code>、数据混洗、逐元素操作(<code>ReLU</code>、逐元素相加)等等时间开销也较大。</p>
<p><img src="/images/loading.gif" data-original="../images/ML/f0091a133414eb0b93f77c9840132d08.png" alt=""></p>
</li>
<li><p>小型网络通用设计准则：</p>
</li>
</ol>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210903131509723.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210903131532377.png" alt=""></p>
<h4 id="ShuffleNet-V2-block"><a href="#ShuffleNet-V2-block" class="headerlink" title="ShuffleNet V2 block"></a>ShuffleNet V2 block</h4><p><img src="/images/loading.gif" data-original="../images/ML/image-20210903131614879.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/54c247329fb50037350fafb7b3094a24.png" alt=""></p>
<h4 id="网络性能-11"><a href="#网络性能-11" class="headerlink" title="网络性能"></a>网络性能</h4><ol>
<li><p><code>ShuffleNet V2</code> 的网络结构类似<code>ShuffleNet V1</code>，主要有两个不同：</p>
<ul>
<li>用<code>ShuffleNet v2 block</code> 代替 <code>ShuffleNet v1 block</code> 。</li>
<li>在<code>Global Pool</code> 之前加入一个 <code>1x1</code> 卷积。</li>
</ul>
<p><img src="/images/loading.gif" data-original="../images/ML/d41d87ea69bb02c1c7068de73d569cec.png" alt=""></p>
</li>
<li><p><code>ShuffleNet V2</code> 可以结合<code>SENet</code> 的思想，也可以增加层数从而由小网络变身为大网络。</p>
<p>下表为几个模型在<code>ImageNet</code> 验证集上的表现（<code>single-crop</code>）。</p>
<p><img src="/images/loading.gif" data-original="../images/ML/862fd8b3af9031270eece28e6fc9fd7a.png" alt=""></p>
</li>
<li><p><code>ShuffleNet V2</code> 和其它模型的比较：</p>
<ul>
<li>根据计算复杂度分成<code>40,140,300,500+</code> 等四组，单位：<code>MFLOPs</code> 。</li>
<li>准确率指标为模型在<code>ImageNet</code> 验证集上的表现（<code>single-crop</code>）。</li>
<li><code>GPU</code> 上的 <code>batchsize=8</code>，<code>ARM</code> 上的<code>batchsize=1</code> 。</li>
<li>默认图片尺寸为<code>224x224</code>，标记为<code>*</code> 的图片尺寸为<code>160x160</code>，标记为<code>**</code> 的图片尺寸为<code>192x192</code> 。</li>
</ul>
<p><img src="/images/loading.gif" data-original="../images/ML/4d5b7a3d26ad610e230ed9c4033329f9.png" alt=""></p>
</li>
<li><p><code>ShuffleNet V2</code> 准确率更高。有两个原因：</p>
<ul>
<li><p><code>ShuffleNet V2 block</code> 的构建效率更高，因此可以使用更多的通道从而提升模型的容量。</p>
</li>
<li><p>在每个<code>ShuffleNet V2 block</code> ，有一半通道的数据（<img src="/images/loading.gif" data-original="https://static.sitestack.cn/projects/huaxiaozhuan-ai/53475ded3e42299ed8f169ed37e28383.svg" alt="">）直接进入下一层。这可以视作某种特征重用，类似于<code>DenseNet</code> 。</p>
<p>但是<code>ShuffleNet v2 block</code> 在特征<code>Concat</code> 之后又执行通道混洗，这又不同于<code>DenseNet</code> 。</p>
</li>
</ul>
</li>
</ol>
<h2 id="IGCV-系列"><a href="#IGCV-系列" class="headerlink" title="IGCV 系列"></a>IGCV 系列</h2><p>设计小型化网络目前有两个代表性的方向，并且都取得了成功：</p>
<ul>
<li>通过一系列低秩卷积核（其中间输出采用非线性激活函数）去组合成一个线性卷积核或者非线性卷积核。如用<code>1x3 +3x1</code> 卷积去替代<code>3x3</code> 卷积 。</li>
<li>使用一系列稀疏卷积核去组成一个密集卷积核。如：交错卷积中采用一系列分组卷积去替代一个密集卷积。</li>
</ul>
<h3 id="IGCV"><a href="#IGCV" class="headerlink" title="IGCV"></a>IGCV</h3><ol>
<li><p>简化神经网络结构的一个主要方法是消除结构里的冗余。目前大家都认为现在深度神经网络结构里有很强的冗余性。</p>
<p>冗余可能来自与两个地方：</p>
<ul>
<li>卷积核空间方向的冗余。通过小卷积核替代（如，采用<code>3x3</code>、<code>1x3</code>、<code>3x1</code> 卷积核）可以降低这种冗余。</li>
<li>卷积核通道方向的冗余。通过分组卷积、<code>depth-wise</code> 卷积可以降低这种冗余。</li>
</ul>
<p><code>IGCV</code> 通过研究卷积核通道方向的冗余，从而减少网络的冗余性。</p>
</li>
<li><p>事实上解决冗余性的方法有多种：</p>
<ul>
<li><p>二元化：将卷积核变成二值的<code>-1</code> 和<code>+1</code>，此时卷积运算的<code>乘加</code>操作变成了<code>加减</code> 操作。这样计算量就下降很多，存储需求也降低很多（模型变小）。</p>
</li>
<li><p>浮点数转整数：将卷积核的 <code>32</code> 位浮点数转换成<code>16</code> 位整数。此时存储需求会下降（模型变小）。</p>
<blockquote>
<p>除了将卷积核进行二元化或者整数化之外，也可以将 <code>feature map</code> 进行二元化/整数化。</p>
</blockquote>
</li>
<li><p>卷积核低秩分解：将大卷积核分解为两个小卷积核 ，如：将<code>100x100</code> 分解为 <code>100x10</code> 和<code>10x100</code>、将<code>5x5</code> 分解为两个<code>3x3</code> 。</p>
</li>
<li><p>稀疏卷积分解：将一个密集的卷积核分解为多个稀疏卷积核。如分组卷积、<code>depth-wise</code> 卷积。</p>
</li>
</ul>
</li>
</ol>
<h4 id="IGCV-block"><a href="#IGCV-block" class="headerlink" title="IGCV block"></a>IGCV block</h4><ol>
<li><p><code>IGCV</code> 提出了一种交错卷积模块，每个模块由相连的两层分组卷积组成，分别称作第一层分组卷积<code>primary group conv</code> 、第二层分组卷积<code>secondary group conv</code> 。</p>
<ul>
<li><p>第一层分组卷积采用<code>3x3</code> 分组卷积，主要用于处理空间相关性；第二层分组卷积采用<code>1x1</code> 分组卷积，主要用于处理通道相关性。</p>
</li>
<li><p>每层分组卷积的每个组内执行的都是标准的卷积，而不是 <code>depth-wise</code> 卷积。</p>
</li>
<li><p>这两层分组卷积的分组是交错进行的。假设第一层分组卷积有 <code>L</code> 个分组、每个分组 <code>M</code> 个通道，则第二层分组卷积有 <code>M</code> 个分组、每个分组<code>L</code> 个通道。</p>
<ul>
<li><p>第一层分组卷积中，同一个分组的不同通道会进入到第二层分组卷积的不同分组中。</p>
<p>第二层分组卷积中，同一个分组的不同通道来自于第一层分组卷积的不同分组。</p>
</li>
<li><p>这两层分组卷积是互补的、交错的，因此称作交错卷积<code>Interleaved Group Convolution:IGCV</code> 。</p>
</li>
</ul>
</li>
<li><p>这种严格意义上的互补需要对每层分组卷积的输出 <code>feature map</code> 根据通道数重新排列<code>Permutation</code>。这不等于混洗，因为混洗不能保证严格意义上的通道互补。</p>
</li>
</ul>
<p><img src="/images/loading.gif" data-original="../images/ML/073a8b4ba6314c0ffa8a34ad725b262c.png" alt=""></p>
</li>
<li><p>由于分组卷积等价于稀疏卷积核的常规卷积，该稀疏卷积核是<code>block-diagonal</code> （即：只有对角线的块上有非零，其它位置均为 0 ）。</p>
<p>因此<code>IGCV</code> 块等价于一个密集卷积，该密集卷积的卷积核由两个稀疏的、互补的卷积核相乘得到。</p>
</li>
</ol>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210903133644148.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210903133710607.png" alt=""></p>
<h4 id="网络性能-12"><a href="#网络性能-12" class="headerlink" title="网络性能"></a>网络性能</h4><ol>
<li><p><code>IGCV</code>网络结构如下表所示，其中：<code>RegConv-Wc</code> 表示常规卷积， <code>W</code> 表示卷积核的尺寸为<code>WxW</code>，<code>c</code> 表示通道数量；<code>Summation</code> 表示加法融合层（即常规的<code>1x1</code> 卷积，而不是<code>1x1</code> 分组卷积）。</p>
<ul>
<li>在<code>IGCV</code> 块之后紧跟 <code>BN</code> 和 <code>ReLU</code>，即结构为：<code>IGC block+BN+ReLU</code> 。</li>
<li><code>4x(3x3,8)</code> 表示分成4组，每组的卷积核尺寸为<code>3x3</code> 输出通道数为 8 。</li>
<li>网络主要包含3个<code>stage</code>，<code>B</code> 表示每个<code>stage</code> 的块的数量。</li>
<li>某些<code>stage</code> 的输出通道数翻倍（对应于<code>Output size</code> 减半），此时该<code>stage</code> 的最后一个 <code>block</code> 的步长为2（即该<code>block</code> 中的<code>3x3</code> 卷积的步长为2） 。</li>
</ul>
<p><img src="/images/loading.gif" data-original="../images/ML/bec1af4545d75e66aa024fbcf6c4ca14.png" alt=""></p>
</li>
<li><p><code>IGCV</code>网络的模型复杂度如下所示。</p>
<p><code>SumFusion</code> 的参数和计算复杂度类似 <code>RegConv-W16</code>，因此没有给出。</p>
<p><img src="/images/loading.gif" data-original="../images/ML/e82bbd3d5c8513e543da5a7b945ad114.png" alt=""></p>
</li>
<li><p><code>IGCV</code>模型在<code>CIFAR-10</code> 和 <code>CIFAR-100</code> 上的表现：</p>
<p><img src="/images/loading.gif" data-original="../images/ML/5b0691043d1580de5f92f6e9c5a1d327.png" alt=""></p>
<ul>
<li><p>如果增加恒等映射，则结果为：</p>
<p><img src="/images/loading.gif" data-original="../images/ML/0a175ba946c8ae9de283f0731eeda902.png" alt=""></p>
</li>
<li><p>所有的模型比较：</p>
<p><img src="/images/loading.gif" data-original="../images/ML/215fb43d955984254581e2414534061c.png" alt=""></p>
</li>
</ul>
</li>
<li><p><code>IGCV</code>模型在<code>ImageNet</code> 上的表现：（输入<code>224x224</code>，验证集，<code>single-crop</code> ）</p>
<p><img src="/images/loading.gif" data-original="../images/ML/92f500c27477ddb2bac9a8c18c5ae3db.png" alt=""></p>
</li>
<li><p>超参数<code>L</code> 和 <code>M</code> 的选取：</p>
<p>通过<code>IGCV</code>在<code>CIFAR100</code> 上的表现可以发现：<code>L</code> 占据统治地位，随着 <img src="/images/loading.gif" data-original="https://static.sitestack.cn/projects/huaxiaozhuan-ai/cd24fb434e797d93c00b015140a57779.svg" alt="九、小型网络 - 图203"> 的增加模型准确率先上升到最大值然后缓缓下降。</p>
<p><img src="/images/loading.gif" data-original="../images/ML/21bc6a366b8f5a904ee37b92c1f409f6.png" alt=""></p>
</li>
</ol>
<h3 id="IGCV2"><a href="#IGCV2" class="headerlink" title="IGCV2"></a>IGCV2</h3><p><code>IGCV2 block</code> 的主要设计思想是：将 <code>IGCV block</code> 两个结构化的稀疏卷积核组成的交错卷积推广到更多个结构化的稀疏卷积核组成的交错卷积，从而进一步消除冗余性。</p>
<h4 id="IGCV2-block"><a href="#IGCV2-block" class="headerlink" title="IGCV2 block"></a>IGCV2 block</h4><p><img src="/images/loading.gif" data-original="../images/ML/image-20210903134031369.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210903134044368.png" alt=""></p>
<h4 id="互补条件-amp-平衡条件"><a href="#互补条件-amp-平衡条件" class="headerlink" title="互补条件 &amp; 平衡条件"></a>互补条件 &amp; 平衡条件</h4><p><img src="/images/loading.gif" data-original="../images/ML/image-20210903134114696.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210903134133553.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210903134155194.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210903134213626.png" alt=""></p>
<h4 id="网络性能-13"><a href="#网络性能-13" class="headerlink" title="网络性能"></a>网络性能</h4><p><img src="/images/loading.gif" data-original="../images/ML/image-20210903134239195.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210903134257941.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210903134321757.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210903134344212.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210903134400999.png" alt=""></p>
<h3 id="IGCV3"><a href="#IGCV3" class="headerlink" title="IGCV3"></a>IGCV3</h3><p><code>IGCV3</code> 结合了块稀疏的卷积核（如：交错卷积） 和低秩的卷积核（如：<code>bottleneck</code> 模块），并放松了互补条件。</p>
<h4 id="IGCV3-block"><a href="#IGCV3-block" class="headerlink" title="IGCV3 block"></a>IGCV3 block</h4><p><img src="/images/loading.gif" data-original="../images/ML/image-20210903134447967.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210903134458650.png" alt=""></p>
<h4 id="loose-互补条件"><a href="#loose-互补条件" class="headerlink" title="loose 互补条件"></a>loose 互补条件</h4><p><img src="/images/loading.gif" data-original="../images/ML/image-20210903134520876.png" alt=""></p>
<h4 id="网络性能-14"><a href="#网络性能-14" class="headerlink" title="网络性能"></a>网络性能</h4><p><img src="/images/loading.gif" data-original="../images/ML/image-20210903134541541.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210903134615525.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210903134633395.png" alt=""></p>
<h2 id="CondenseNet"><a href="#CondenseNet" class="headerlink" title="CondenseNet"></a>CondenseNet</h2><ol>
<li><p><code>CondenseNet</code> 基于<code>DenseNet</code>网络，它在训练的过程中自动学习一个稀疏的网络从而减少了<code>Densenet</code> 的冗余性。</p>
<p>这种稀疏性类似于分组卷积，但是<code>CondenseNet</code> 从数据中自动学到分组，而不是由人工硬性规定分组。</p>
</li>
<li><p><code>CondenseNet</code> 网络在<code>GPU</code> 上具有高效的训练速度，在移动设备上具有高效的推断速度。</p>
</li>
<li><p><code>MobileNet、ShuffleNet、NASNet</code> 都采用了深度可分离卷积，这种卷积在大多数深度学习库中没有实现。而<code>CondenseNet</code> 采用分组卷积，这种卷积被大多数深度学习库很好的支持。</p>
<blockquote>
<p>准确的说，<code>CondenseNet</code> 在测试阶段采用分组卷积，在训练阶段采用的是常规卷积。</p>
</blockquote>
<p><img src="/images/loading.gif" data-original="../images/ML/47dc3de3dd1825da8edc693f5e501537.png" alt=""></p>
</li>
</ol>
<h3 id="网络剪枝"><a href="#网络剪枝" class="headerlink" title="网络剪枝"></a>网络剪枝</h3><ol>
<li><p>有一些工作（<code>Compressing neural networks with the hashing trick</code>、<code>Deep networks with stochastic depth</code>）表明：<code>CNN</code>网络中存在大量的冗余。</p>
<p><code>DenseNet</code> 通过直接连接每个层的输出特征与之后的所有层来特征重用。但是如果该层的输出特征是冗余的或者不需要的，则这些连接会带来冗余。</p>
</li>
<li><p><code>CNN</code> 网络可以通过权重剪枝从而在不牺牲模型准确率的条件下实现小型化。有不同粒度的剪枝技术：</p>
<ul>
<li>独立的权重剪枝：一种细粒度的剪枝，它可以带来高度稀疏的网络。但是它需要存储大量的索引来记录哪些连接被剪掉，并且依赖特定的硬件/软件设施。</li>
<li><code>filter</code>级别剪枝：一种粗粒度的剪枝，它得到更规则的网络，且易于实现。但是它带来的网络稀疏性较低。</li>
</ul>
</li>
<li><p><code>CondenseNet</code> 也可以认为是一种网络剪枝技术，它与以上的剪枝方法都不同：</p>
<ul>
<li><code>CondenseNet</code> 的网络剪枝发生、且仅仅发生在训练的早期。这比在网络整个训练过程中都采用<code>L1</code> 正则化来获取稀疏权重更高效。</li>
<li><code>CondenseNet</code> 的网络剪枝能产生比<code>filter</code>级别剪枝更高的网络稀疏性，而且网络结构也是规则的、易于实现的。</li>
</ul>
</li>
</ol>
<h3 id="LGC"><a href="#LGC" class="headerlink" title="LGC"></a>LGC</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210903134735325.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210903134747245.png" alt=""></p>
<h3 id="训练和测试"><a href="#训练和测试" class="headerlink" title="训练和测试"></a>训练和测试</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210903134809032.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/72347fa0a049223e86e5bac179b0204c.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210903134836395.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210903134848945.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/2372fda0794611c9702625392795cdb3.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210903134913241.png" alt=""></p>
<h3 id="IGR-和-FDC"><a href="#IGR-和-FDC" class="headerlink" title="IGR 和 FDC"></a>IGR 和 FDC</h3><p><img src="/images/loading.gif" data-original="../images/ML/image-20210903134934614.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/image-20210903134947129.png" alt=""></p>
<p><img src="/images/loading.gif" data-original="../images/ML/8347ea4ef4e8d94b02b40f393c49e612.png" alt=""></p>
<h3 id="网络性能-15"><a href="#网络性能-15" class="headerlink" title="网络性能"></a>网络性能</h3><ol>
<li><p><code>CondenseNet</code> 与其它网络的比较：（<code>*</code> 表示使用<code>cosine</code> 学习率训练 600个 epoch）</p>
<p><img src="/images/loading.gif" data-original="../images/ML/8f934f31d4b52e31498d3ad1b7c91e43.png" alt=""></p>
</li>
<li><p><code>CondenseNet</code> 与其它裁剪技术的比较：</p>
<p><img src="/images/loading.gif" data-original="../images/ML/3828ce72148dec650a7127526584a4d0.png" alt=""></p>
</li>
<li><p><code>CondenseNet</code> 的超参数的实验：(<code>CIFAR-10</code>，<code>DenseNet-50</code> 为基准)</p>
<ul>
<li><p>裁剪策略：（<code>G=4</code> ）</p>
<ul>
<li><code>Full Model</code> ：不进行任何裁剪。</li>
<li><code>Traditional Pruning</code> ：在训练阶段（300个epoch）完成时执行裁剪（因此只裁剪一次），裁剪方法与<code>LGC</code> 一样。然后使用额外的300个 epoch 进行微调。</li>
</ul>
<p><img src="/images/loading.gif" data-original="../images/ML/10b47753f07271a264b847fa127f25a6.png" alt=""></p>
</li>
<li><p>分组数量：（<code>C=8</code>）</p>
<ul>
<li>这里的分组值得是<code>3x3</code> 分组卷积的分组数量。</li>
<li>随着分组数量的增加，测试误差逐渐降低。这表明<code>LGC</code> 可以降低网络的冗余性。</li>
</ul>
<p><img src="/images/loading.gif" data-original="../images/ML/218988e659e9d7d6b486022ef9323708.png" alt=""></p>
</li>
<li><p>浓缩因子：(<code>G=4</code>)</p>
<p>可以看到：<img src="/images/loading.gif" data-original="https://static.sitestack.cn/projects/huaxiaozhuan-ai/027318ac5413abf92cc551698934047f.svg" alt=""> 可以带来更好的效益；但是 <img src="/images/loading.gif" data-original="https://static.sitestack.cn/projects/huaxiaozhuan-ai/c1b94db0a4c54da36c358b0d96aa1a0c.svg" alt=""> 时，网络的准确率和网络的<code>FLOPs</code> 呈现同一个函数关系。这表明裁剪网络权重会带来更小的模型，但是也会导致一定的准确率损失。</p>
<p><img src="/images/loading.gif" data-original="../images/ML/3e5abc1b0a375e96772aa0102d800c3f.png" alt=""></p>
</li>
</ul>
</li>
<li><p>在<code>ImageNet</code> 上的比较：</p>
<ul>
<li><p>网络结构：</p>
<p>为了降低参数，在<code>epoch 60</code> 时（一共<code>120</code>个<code>epoch</code> ）裁剪了全连接层<code>FC layer</code> 50% 的权重。思想与<code>1x1</code> 卷积的<code>LGC</code> 相同，只是 <img src="/images/loading.gif" data-original="https://static.sitestack.cn/projects/huaxiaozhuan-ai/88658ccc30ba4ce0f5d05ae53827ad5c.svg" alt=""> 。</p>
<p><img src="/images/loading.gif" data-original="../images/ML/457897aa53b8785928c5cd67ae31a439.png" alt=""></p>
</li>
<li><p><code>CondenseNet</code> 与其它网络的比较：</p>
<p><img src="/images/loading.gif" data-original="../images/ML/dc18756af213eb18a215ef765631d7a8.png" alt=""></p>
</li>
<li><p>网络在<code>ARM</code> 处理器上的推断时间（输入尺寸为<code>224x224</code> ）：</p>
<p><img src="/images/loading.gif" data-original="../images/ML/c3ad75c9edd859283fcc4db5f3f2ad24.png" alt=""></p>
</li>
</ul>
</li>
</ol>
<script>
        document.querySelectorAll('.github-emoji')
          .forEach(el => {
            if (!el.dataset.src) { return; }
            const img = document.createElement('img');
            img.style = 'display:none !important;';
            img.src = el.dataset.src;
            img.addEventListener('error', () => {
              img.remove();
              el.style.color = 'inherit';
              el.style.backgroundImage = 'none';
              el.style.background = 'none';
            });
            img.addEventListener('load', () => {
              img.remove();
            });
            document.body.appendChild(img);
          });
      </script>
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://jackhcc.github.io" rel="external nofollow noreferrer">杰克成</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://jackhcc.github.io/posts/cnn-model.html">https://jackhcc.github.io/posts/cnn-model.html</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="https://jackhcc.github.io" target="_blank">杰克成</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/tags/CNN/">
                                    <span class="chip bg-color">CNN</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/libs/share/css/share.min.css">

<div id="article-share">
    
    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
                <style>
    #reward {
        margin: 40px 0;
        text-align: center;
    }

    #reward .reward-link {
        font-size: 1.4rem;
        line-height: 38px;
    }

    #reward .btn-floating:hover {
        box-shadow: 0 6px 12px rgba(0, 0, 0, 0.2), 0 5px 15px rgba(0, 0, 0, 0.2);
    }

    #rewardModal {
        width: 320px;
        height: 350px;
    }

    #rewardModal .reward-title {
        margin: 15px auto;
        padding-bottom: 5px;
    }

    #rewardModal .modal-content {
        padding: 10px;
    }

    #rewardModal .close {
        position: absolute;
        right: 15px;
        top: 15px;
        color: rgba(0, 0, 0, 0.5);
        font-size: 1.3rem;
        line-height: 20px;
        cursor: pointer;
    }

    #rewardModal .close:hover {
        color: #ef5350;
        transform: scale(1.3);
        -moz-transform:scale(1.3);
        -webkit-transform:scale(1.3);
        -o-transform:scale(1.3);
    }

    #rewardModal .reward-tabs {
        margin: 0 auto;
        width: 210px;
    }

    .reward-tabs .tabs {
        height: 38px;
        margin: 10px auto;
        padding-left: 0;
    }

    .reward-content ul {
        padding-left: 0 !important;
    }

    .reward-tabs .tabs .tab {
        height: 38px;
        line-height: 38px;
    }

    .reward-tabs .tab a {
        color: #fff;
        background-color: #ccc;
    }

    .reward-tabs .tab a:hover {
        background-color: #ccc;
        color: #fff;
    }

    .reward-tabs .wechat-tab .active {
        color: #fff !important;
        background-color: #22AB38 !important;
    }

    .reward-tabs .alipay-tab .active {
        color: #fff !important;
        background-color: #019FE8 !important;
    }

    .reward-tabs .reward-img {
        width: 210px;
        height: 210px;
    }
</style>

<div id="reward">
    <a href="#rewardModal" class="reward-link modal-trigger btn-floating btn-medium waves-effect waves-light red">赏</a>

    <!-- Modal Structure -->
    <div id="rewardModal" class="modal">
        <div class="modal-content">
            <a class="close modal-close"><i class="fas fa-times"></i></a>
            <h4 class="reward-title">你的赏识是我前进的动力</h4>
            <div class="reward-content">
                <div class="reward-tabs">
                    <ul class="tabs row">
                        <li class="tab col s6 alipay-tab waves-effect waves-light"><a href="#alipay">支付宝</a></li>
                        <li class="tab col s6 wechat-tab waves-effect waves-light"><a href="#wechat">微 信</a></li>
                    </ul>
                    <div id="alipay">
                        <img src="/images/loading.gif" data-original="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/medias/reward/aliqr.png" class="reward-img" alt="支付宝打赏二维码">
                    </div>
                    <div id="wechat">
                        <img src="/images/loading.gif" data-original="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/medias/reward/wxqr.png" class="reward-img" alt="微信打赏二维码">
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>

<script>
    $(function () {
        $('.tabs').tabs();
    });
</script>
            
        </div>
    </div>

    
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/libs/gitalk/gitalk.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/css/my-gitalk.css">

<div class="card gitalk-card" data-aos="fade-up">
    <div class="comment_headling" style="font-size: 20px; font-weight: 700; position: relative; left: 20px; top: 15px; padding-bottom: 5px;">
        <i class="fas fa-comments fa-fw" aria-hidden="true"></i>
        <span>评论</span>
    </div>
    <div id="gitalk-container" class="card-content"></div>
</div>

<script src="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/libs/gitalk/gitalk.min.js"></script>
<script>
    let gitalk = new Gitalk({
        clientID: '3821a0bbb773038a51fc',
        clientSecret: '4b30b507d67ec5497ec0e77f43f80cb3e0d7dd3a',
        repo: 'JackHCC.github.io',
        owner: 'JackHCC',
        admin: "JackHCC",
        id: '2021-09-03T11-06-10',
        distractionFreeMode: false  // Facebook-like distraction free mode
    });

    gitalk.render('gitalk-container');
</script>
    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/posts/windows-software.html">
                    <div class="card-image">
                        
                        
                        <img src="/images/loading.gif" data-original="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/medias/featureimages/7.jpg" class="responsive-img" alt="Windows装机软件">
                        
                        <span class="card-title">Windows装机软件</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Windows高效办公程序员装机软件推荐
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2021-09-04
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/Basic/" class="post-category">
                                    Basic
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/Software/">
                        <span class="chip bg-color">Software</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/posts/word-embedding.html">
                    <div class="card-image">
                        
                        
                        <img src="/images/loading.gif" data-original="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/medias/featureimages/10.jpg" class="responsive-img" alt="Word Embedding详解">
                        
                        <span class="card-title">Word Embedding详解</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Word Embedding方法汇总
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2021-09-02
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/Deep-Learning/" class="post-category">
                                    Deep Learning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/NLP/">
                        <span class="chip bg-color">NLP</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/libs/codeBlock/codeBlockFuction.js"></script>

<!-- 代码语言 -->

<script type="text/javascript" src="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/libs/codeBlock/codeShrink.js"></script>


<!-- 代码块折行 -->

<style type="text/css">
code[class*="language-"], pre[class*="language-"] { white-space: pre !important; }
</style>


    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('4'),
            headingSelector: 'h1, h2, h3, h4'
        });

        // modify the toc link href to support Chinese.
        let i = 0;
        let tocHeading = 'toc-heading-';
        $('#toc-content a').each(function () {
            $(this).attr('href', '#' + tocHeading + (++i));
        });

        // modify the heading title id to support Chinese.
        i = 0;
        $('#articleContent').children('h1, h2, h3, h4').each(function () {
            $(this).attr('id', tocHeading + (++i));
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>

    

</main>



    <footer class="page-footer bg-color">
    <div class="container row center-align" style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            <span id="year">2020</span>
            <a href="https://jackhcc.github.io" target="_blank">杰克成</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            <br>
            
            &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                class="white-color">3591.2k</span>&nbsp;字
            
            
            
            
            
            
            <span id="busuanzi_container_site_pv">
                |&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;<span id="busuanzi_value_site_pv"
                    class="white-color"></span>&nbsp;次
            </span>
            
            
            <span id="busuanzi_container_site_uv">
                |&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;<span id="busuanzi_value_site_uv"
                    class="white-color"></span>&nbsp;人
            </span>
            
            <br>
            
            <span id="sitetime">载入运行时间...</span>
            <script>
                function siteTime() {
                    var seconds = 1000;
                    var minutes = seconds * 60;
                    var hours = minutes * 60;
                    var days = hours * 24;
                    var years = days * 365;
                    var today = new Date();
                    var startYear = "2020";
                    var startMonth = "2";
                    var startDate = "27";
                    var startHour = "6";
                    var startMinute = "30";
                    var startSecond = "0";
                    var todayYear = today.getFullYear();
                    var todayMonth = today.getMonth() + 1;
                    var todayDate = today.getDate();
                    var todayHour = today.getHours();
                    var todayMinute = today.getMinutes();
                    var todaySecond = today.getSeconds();
                    var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                    var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                    var diff = t2 - t1;
                    var diffYears = Math.floor(diff / years);
                    var diffDays = Math.floor((diff / days) - diffYears * 365);
                    var diffHours = Math.floor((diff - (diffYears * 365 + diffDays) * days) / hours);
                    var diffMinutes = Math.floor((diff - (diffYears * 365 + diffDays) * days - diffHours * hours) /
                        minutes);
                    var diffSeconds = Math.floor((diff - (diffYears * 365 + diffDays) * days - diffHours * hours -
                        diffMinutes * minutes) / seconds);
                    if (startYear == todayYear) {
                        document.getElementById("year").innerHTML = todayYear;
                        document.getElementById("sitetime").innerHTML = "本站已安全运行 " + diffDays + " 天 " + diffHours +
                            " 小时 " + diffMinutes + " 分钟 " + diffSeconds + " 秒";
                    } else {
                        document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                        document.getElementById("sitetime").innerHTML = "本站已安全运行 " + diffYears + " 年 " + diffDays +
                            " 天 " + diffHours + " 小时 " + diffMinutes + " 分钟 " + diffSeconds + " 秒";
                    }
                }
                setInterval(siteTime, 1000);
            </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/JackHCC" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:jackcc0701@163.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>



    <a href="https://www.facebook.com/profile.php?id=100046343443643" class="tooltipped" target="_blank" data-tooltip="关注我的Facebook: https://www.facebook.com/profile.php?id=100046343443643" data-position="top" data-delay="50">
        <i class="fab fa-facebook-f"></i>
    </a>



    <a href="https://twitter.com/JackChe66021834" class="tooltipped" target="_blank" data-tooltip="关注我的Twitter: https://twitter.com/JackChe66021834" data-position="top" data-delay="50">
        <i class="fab fa-twitter"></i>
    </a>



    <a href="tencent://AddContact/?fromId=50&fromSubId=1&subcmd=all&uin=2508074836" class="tooltipped" target="_blank" data-tooltip="QQ联系我: 2508074836" data-position="top" data-delay="50">
        <i class="fab fa-qq"></i>
    </a>



    <a href="https://weibo.com/u/6885584679" class="tooltipped" target="_blank" data-tooltip="关注我的微博: https://weibo.com/u/6885584679" data-position="top" data-delay="50">
        <i class="fab fa-weibo"></i>
    </a>



    <a href="https://www.zhihu.com/people/8f8482f01f0d6a04e844efe32e0f0710" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/8f8482f01f0d6a04e844efe32e0f0710" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script src="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/js/search.js"></script>
<script type="text/javascript">
$(function () {
    searchFunc("https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/" + "search.xml", 'searchInput', 'searchResult');
});
</script>
    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/libs/materialize/materialize.min.js"></script>
    <script src="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/libs/aos/aos.js"></script>
    <script src="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/js/matery.js"></script>

    <canvas class="fireworks" style="position: fixed;left: 0;top: 0;z-index: 1; pointer-events: none;" ></canvas>
    <script type="text/javascript" src="//cdn.bootcss.com/animejs/2.2.0/anime.min.js"></script>
    <script type="text/javascript" src="/js/fireworks.js"></script>

    <script type="text/javascript">
        //只在桌面版网页启用特效
        var windowWidth = $(window).width();
        if (windowWidth > 768) {
            document.write('<script type="text/javascript" src="/js/sakura.js"><\/script>'); }
    </script>

    <!-- weather -->
	<script type="text/javascript">
	WIDGET = {FID: 'TToslpmkVO'}
	</script>
	<script type="text/javascript" src="https://apip.weatherdt.com/float/static/js/r.js?v=1111"></script>


    <!-- Global site tag (gtag.js) - Google Analytics -->


    <!-- Baidu Analytics -->

<script>
    var _hmt = _hmt || [];
    (function () {
        var hm = document.createElement("script");
        hm.src = "https://hm.baidu.com/hm.js?";
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(hm, s);
    })();
</script>

    <!-- Baidu Push -->

    
    
    <script async src="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/libs/others/busuanzi.pure.mini.js"></script>
    

    
        <script src="//code.tidio.co/kqhlkxviiccyoa0czpfpu4ijuey9hfre.js"></script>
        <script> 
            $(document).ready(function () {
                setInterval(change_Tidio, 50);  
                function change_Tidio() { 
                    var tidio=$("#tidio-chat iframe");
                    if(tidio.css("display")=="block"&& $(window).width()>977 ){
                        document.getElementById("tidio-chat-iframe").style.bottom= ($("div#backTop.top-scroll").css("display")=="none" &&$(window).width()>977)>0? "-40px" : ($("div.toc-title").length&&$(window).width()>977)>0?"85px":"20px";   
                        document.getElementById("tidio-chat-iframe").style.right="-15px";   
                        document.getElementById("tidio-chat-iframe").style.height=parseInt(tidio.css("height"))>=520?"520px":tidio.css("height");
                        document.getElementById("tidio-chat-iframe").style.zIndex="997";
                    } 
                    else if(tidio.css("display")=="block"&&$(window).width()>601 &&$(window).width()<992 ){
                        document.getElementById("tidio-chat-iframe").style.bottom= ($("div#backTop.top-scroll").css("display")=="none" && 601< $(window).width()<992)>0? "-40px":"20px" ;   
                        document.getElementById("tidio-chat-iframe").style.right="-15px"; 
                        document.getElementById("tidio-chat-iframe").style.zIndex="997";
                    }
                    else if(tidio.css("display")=="block"&&$(window).width()<601 && parseInt(tidio.css("height"))<230){
                        document.getElementById("tidio-chat-iframe").style.bottom= ($("div#backTop.top-scroll").css("display")=="none" && $(window).width()<601)>0? "-10px":"45px" ;   
                        document.getElementById("tidio-chat-iframe").style.zIndex="997";
                    }
                    if( tidio.css("display")=="block"&&$(window).width()<601 && parseInt(tidio.css("height"))>=230){
                        document.getElementById("tidio-chat-iframe").style.zIndex="998";
                    }
                } 
            }); 
        </script>
    

    

    
    <script type="text/javascript" color="0,0,255"
        pointColor="0,0,255" opacity='0.7'
        zIndex="-1" count="99"
        src="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/libs/background/canvas-nest.js"></script>
    

    

    
    <script type="text/javascript" src="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/libs/background/ribbon-dynamic.js" async="async"></script>
    
    
    
    <script src="https://cdn.jsdelivr.net/gh/JackHCC/JackHCC.github.io/libs/instantpage/instantpage.js" type="module"></script>
    

        <script src="//cdn.jsdelivr.net/npm/js-base64/base64.min.js"></script>
        <script>
        $('a').each(function() {
          const $this = $(this);
          const href = $this.attr('href');
          if (href && href.match('^((http|https|thunder|qqdl|ed2k|Flashget|qbrowser|ftp|rtsp|mms)://)')) {
            const strs = href.split('/');
            if (strs.length >= 3) {
                const host = strs[2];
                if (host !== 'your_domain' || window.location.host) {
                    $this.attr('href', '/go.html?u='+Base64.encode(href)+'').attr('rel', 'external nofollow noopener noreferrer');
                    if (true) {
                        $this.attr('target', '_blank');
                    }
                }
            }
          }
        });
        </script><script>!function(e){var c=Array.prototype.slice.call(document.querySelectorAll("img[data-original]"));function i(){for(var r=0;r<c.length;r++)t=c[r],0<=(n=t.getBoundingClientRect()).bottom&&0<=n.left&&n.top<=(e.innerHeight||document.documentElement.clientHeight)&&function(){var t,n,e,i,o=c[r];t=o,n=function(){c=c.filter(function(t){return o!==t})},e=new Image,i=t.getAttribute("data-original"),e.onload=function(){t.src=i,n&&n()},e.src=i}();var t,n}i(),e.addEventListener("scroll",function(){var t,n;t=i,n=e,clearTimeout(t.tId),t.tId=setTimeout(function(){t.call(n)},500)})}(this);</script><script>window.addEventListener("load",function(){var t=/\.(gif|jpg|jpeg|tiff|png)$/i,r=/^data:image\/[a-z]+;base64,/;Array.prototype.slice.call(document.querySelectorAll("img[data-original]")).forEach(function(a){var e=a.parentNode;"A"===e.tagName&&(e.href.match(t)||e.href.match(r))&&(e.href=a.dataset.original)})});</script></body>

</html>

